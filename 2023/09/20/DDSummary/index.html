<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=[object Object]"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', '[object Object]');
</script>
<!-- End Google Analytics -->

  
  <title>Dataset Distillation: A Summary | JourneyToCoding</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="本文是对一些常见的数据蒸馏方法的总结，主要基于论文《Dataset Distillation: A Comprehensive Review》中所总结的方法。">
<meta property="og:type" content="article">
<meta property="og:title" content="Dataset Distillation: A Summary">
<meta property="og:url" content="https://zclzcl0223.github.io/2023/09/20/DDSummary/index.html">
<meta property="og:site_name" content="JourneyToCoding">
<meta property="og:description" content="本文是对一些常见的数据蒸馏方法的总结，主要基于论文《Dataset Distillation: A Comprehensive Review》中所总结的方法。">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://zclzcl0223.github.io/2023/09/20/DDSummary/1.png">
<meta property="og:image" content="https://zclzcl0223.github.io/2023/09/20/DDSummary/2.png">
<meta property="og:image" content="https://zclzcl0223.github.io/2023/09/20/DDSummary/3.png">
<meta property="og:image" content="https://zclzcl0223.github.io/2023/09/20/DDSummary/4.png">
<meta property="article:published_time" content="2023-09-20T10:11:46.000Z">
<meta property="article:modified_time" content="2023-09-20T13:02:26.000Z">
<meta property="article:author" content="ChaosTsang">
<meta property="article:tag" content="Paper">
<meta property="article:tag" content="Dataset Distillation">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://zclzcl0223.github.io/2023/09/20/DDSummary/1.png">
  
    <link rel="alternate" href="/atom.xml" title="JourneyToCoding" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/%5Bobject%20Object%5D">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">JourneyToCoding</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">Code for fun</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/">home</a>
        
          <a class="main-nav-link" href="/about/">about</a>
        
          <a class="main-nav-link" href="/tags/">tags</a>
        
          <a class="main-nav-link" href="/categories/">categories</a>
        
          <a class="main-nav-link" href="/archives/">archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://zclzcl0223.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-DDSummary" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/09/20/DDSummary/" class="article-date">
  <time class="dt-published" datetime="2023-09-20T10:11:46.000Z" itemprop="datePublished">2023-09-20</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Paper/">Paper</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      Dataset Distillation: A Summary
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <span id="more"></span>

<p>文中大多数内容都基于Yu et al.的论文<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2301.07014.pdf">Dataset Distillation: A Comprehensive Review</a>。</p>
<h1 id="Performance-Matching"><a href="#Performance-Matching" class="headerlink" title="Performance Matching"></a>Performance Matching</h1><p>Performance Matching，表现&#x2F;效果匹配，是最先被提出的一种基于机器学习的数据蒸馏方法。这种方法核心思想在于将<em>Synthetic Data</em>视作神经网络中的一个“超参数”，如学习率一般，但不同的是，它是可学习的超参数。其优化过程分两步：</p>
<ol>
<li>用<em>Synthetic Data</em>训练神经网络；</li>
<li>将原训练集$\mathcal{T}$输入用<em>Synthetic Data</em>训练的神经网络，计算得到预测的<em>Loss</em>，以该<em>Loss</em>作为损失函数更新<em>Synthetic Data</em>。</li>
</ol>
<p>最初的基于Performance Matching的方法的精度和训练速度都不怎么样，但后来有学者将第一步中的神经网络替换为了核函数，反而使得Performance Matching超过了多数后来的方法。</p>
<h2 id="Meta-Learning-Based-Methods"><a href="#Meta-Learning-Based-Methods" class="headerlink" title="Meta Learning Based Methods"></a>Meta Learning Based Methods</h2><p>该方法最先在Wang et al.的<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1811.10959.pdf">DATASET DISTILLATION</a>中被提出，也是数据蒸馏开山立宗的方法，一般简称<em>DD</em>。<em>Meta Learning</em>也就是把<em>Synthetic Data</em>视作一个可学习的超参数。它获得<em>Synthetic Data</em>的思路就是上面所提到的，基本公式与流程如下：</p>
<p>$$<br>\begin{align*}<br>    \theta ^{(t)}&amp;&#x3D;\theta ^{(t-1)}-\eta\nabla l(\mathcal{S};\theta ^{(t-1)})\tag{1}\\<br>    \mathcal{S}^{(\tau)}&amp;&#x3D;\mathcal{S}^{(\tau-1)}-\eta&#39;\nabla L(\theta^{(T)},\mathcal{T})\tag{2}<br>\end{align*}<br>$$</p>
<p>其中，式$(1)$是内层循环，用于更新神经网络，式$(2)$是外层循环，用于更新<em>Synthetic Data</em>。</p>
<p><img src="/2023/09/20/DDSummary/1.png" alt="1"></p>
<center style="font-size:12px; font-weight:bold">Fig. 1. Meta Learning Based Methods</center><br>

<p>显而易见，这是一种<em>Bi-level Optimization Algorithm</em>：内层循环更新神经网络，外层循环更新<em>Synthetic Data</em>。而内层循环又需要用到<em>Synthetic Data</em>来产生梯度以训练网络，因而<em>Synthetic Data</em>的更新内在地会包含一个二阶导，这是极耗内存和时间的。而实际上蒸馏出来的数据效果也不是特别好。</p>
<h2 id="Kernel-Ridge-Regression-Based-Methods"><a href="#Kernel-Ridge-Regression-Based-Methods" class="headerlink" title="Kernel Ridge Regression Based Methods"></a>Kernel Ridge Regression Based Methods</h2><p>该方法最初在Nguyen et al.的<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2011.00050.pdf">DATASET META-LEARNING FROM KERNEL RIDGEREGRESSION</a>中被提出，一般简称<em>KIP</em>。这种方法是建立在<strong>无限宽的全连接层可以用特殊的核函数来拟合</strong>这一研究成果的。<em>KIP</em>用到的核函数被称为<em>Neural Tangent Kernel</em>（NTK）。它本质上也是<em>Meta Learning</em>，只不过使用核函数来替代了内层循环，并用<em>Kernel Ridge Regression</em>计算损失函数，使得原本的双层优化变为了一层：</p>
<p>$$<br>\begin{align*}<br>    L(\mathcal{S},\mathcal{T})&amp;&#x3D;||Y _\mathcal{T}-K _{X _\mathcal{T}X _\mathcal{S}}(K _{X _\mathcal{S}X _\mathcal{S}}+\lambda I)^{-1}Y _\mathcal{S}||^2\\<br>    \mathcal{S}^{(\tau)}&amp;&#x3D;\mathcal{S}^{(\tau-1)}-\eta&#39;\nabla L(\mathcal{S},\mathcal{T})<br>\end{align*}<br>$$</p>
<p>当然，还有使用其他核函数的版本，但一般用的都是<em>NTK</em>，此处就不再赘述了。</p>
<blockquote>
<p>文中还提到了另一个脱胎于<em>Kernel Ridge Regression</em>的方法<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2206.00719.pdf"><em>FRePo</em></a>，该方法在目前来说取得了最好的蒸馏效果。</p>
</blockquote>
<h1 id="Parameter-Matching"><a href="#Parameter-Matching" class="headerlink" title="Parameter Matching"></a>Parameter Matching</h1><p>Parameter Matching，参数匹配，这种方法的想法很直观：要想<em>Synthetic Data</em>取得和训练集相近的泛化精度，只要<em>Synthetic Data</em>训练出来的网路的参数（Parameter）和训练集训练出来的网络的参数相近即可。不过实际上并没有人会直接匹配参数，而是转而去匹配训练过程中的梯度，即让<em>Synthetic Data</em>产生和训练集相近的梯度，因而这类方法也可以叫<em>Gradient Matching</em>。</p>
<h2 id="Single-Step-Parameter-Matching"><a href="#Single-Step-Parameter-Matching" class="headerlink" title="Single-Step Parameter Matching"></a>Single-Step Parameter Matching</h2><p>Single-Step Parameter Matching在<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2006.05929.pdf">DATASET CONDENSATION WITH GRADIENT MATCHING</a>中被Zhao et al.提出，是最先被提出的基于参数匹配的数据蒸馏方法，一般称<em>DC</em>。其核心思想很简单：</p>
<ol>
<li>训练集$\mathcal{T}$和<em>Synthetic Data</em> $\mathcal{S}$分别过一次神经网络；</li>
<li>分别计算梯度；</li>
<li>计算梯度差异，作为$Loss$；</li>
<li>更新<em>Synthetic Data</em>，用更新后的<em>Synthetic Data</em>更新神经网络。</li>
</ol>
<p>文中用到的作为$Loss$的梯度差异函数为$\cos$函数，即更关心梯度的方向：</p>
<p>$$<br>Loss(\mathcal{S},\mathcal{T})&#x3D;1-\frac{grad(\mathcal{S})\cdot grad(\mathcal{T})}{||grad(\mathcal{S})||\space||grad(\mathcal{T})||}<br>$$</p>
<p>上面的$grad$是指输入数据在神经网络上所产生的梯度。基本流程如下：</p>
<p><img src="/2023/09/20/DDSummary/2.png" alt="2"></p>
<center style="font-size:12px; font-weight:bold">Fig. 2. Meta Learning Based Methods</center><br>

<blockquote>
<p>后续有不少学者在损失函数$Loss$上做文章，如不仅关心方向，还关心大小（欧式距离）等，此处就不再赘述了。</p>
</blockquote>
<h2 id="Multi-Step-Parameter-Matching"><a href="#Multi-Step-Parameter-Matching" class="headerlink" title="Multi-Step Parameter Matching"></a>Multi-Step Parameter Matching</h2><p>有一步梯度匹配，就会有多步梯度匹配。Multi-Step Parameter Matching最先在<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2203.11932.pdf">Dataset Distillation by Matching Training Trajectories</a>中被Cazenavette et al.提出，一般简称<em>MTT</em>。在Kernel Ridge Regression Based Methods出现之前，这是表现最好的方法。其本质上是对Single-Step的一种优化，因为单步的匹配本质上是一种<em>贪心</em>的算法，它不一定能够得到全局的最优，而用训练集的多步梯度下降来匹配<em>Synthetic Data</em>的一步或两步梯度下降能够为更新<em>Synthetic Data</em>带来更加全局的、稳定的优化路径。方法示意图如下：</p>
<p><img src="/2023/09/20/DDSummary/3.png" alt="3"></p>
<center style="font-size:12px; font-weight:bold">Fig. 3. Multi-Step Parameter Matching</center><br>

<p>此外，文中用到的损失函数也不同，作者使用的损失函数是规范化了的欧式距离（的平方）：</p>
<p>$$<br>Loss(\mathcal{S},\mathcal{T})&#x3D;\frac{||\theta ^{\mathcal{S}} _{t+N}-\theta ^{\mathcal{T}} _{t+M}||^2 _2}{||\theta ^{\mathcal{T}} _{t}-\theta ^{\mathcal{T}} _{t+M}||^2 _2}<br>$$</p>
<p><em>MTT</em>的另一大优点是蒸馏速度很快，因为其训练集训练的网络都是预先训练好的，这些预训练好的网络被称为<em>buffer</em>或者<em>teacher net</em>。后续也有不少的学者对<em>MTT</em>进行了优化，如：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2211.11004.pdf"><em>FTD</em></a>：优化了<em>buffer</em>，使得生成的<em>buffer</em>能够提供更接近于<em>Synthetic Data</em>训练网络的优化路径；</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2209.14609.pdf"><em>DDPP</em></a>：优化了更新<em>Synthetic Data</em>的时机，即预先评估本次更新<em>Synthetic Data</em>能够带来的收益，根据收益大小来决定是否要执行本次更新。评估的方式是为$\theta ^{\mathcal{T}} _{t+M}&#x2F;\theta ^{\mathcal{S}} _{t+N}$设置一个阈值$\epsilon$，当两者的比例小于该阈值时就不执行本次<em>Synthetic Data</em>的更新。</li>
<li>...</li>
</ul>
<h1 id="Distribution-Matching"><a href="#Distribution-Matching" class="headerlink" title="Distribution Matching"></a>Distribution Matching</h1><p>Distribution Matching，分布匹配，核心思想是让<em>Synthetic Data</em>的统计分布尽可能地接近训练集。接近程度的衡量指标一般用的是<em>Maximum Mean Discrepancy</em>（MMD），即用<em>高阶矩</em>来衡量两个分布的接近程度，而高级矩的计算则常以通过神经网络将输入特征映射到高维空间后，计算高维空间向量距离的方式进行。这种方法最大的优点就是蒸馏速度很快。</p>
<h2 id="Single-Layer-Distribution-Matching"><a href="#Single-Layer-Distribution-Matching" class="headerlink" title="Single Layer Distribution Matching"></a>Single Layer Distribution Matching</h2><p>该方法是分布匹配最初的方法，由Zhao et al.在<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2110.04181.pdf">Dataset Condensation with Distribution Matching</a>提出，一般简称<em>DM</em>。思路与前文提到的一致。其计算高阶矩所采用的神经网络是原来用于分类任务的神经网络，不过去掉了softmax操作。基本流程为：</p>
<p><img src="/2023/09/20/DDSummary/4.png" alt="4"></p>
<center style="font-size:12px; font-weight:bold">Fig. 4. Single Layer Distribution Matching</center>

<h2 id="Multi-Layer-Distribution-Matching"><a href="#Multi-Layer-Distribution-Matching" class="headerlink" title="Multi Layer Distribution Matching"></a>Multi Layer Distribution Matching</h2><p>该方法由Wang et al.在<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2203.01531.pdf">Cafe: Learning to condense dataset by aligning features</a>上提出，一般简称<em>CAFE</em>。实际上是对<em>DM</em>的优化，主要的不同点在于：<em>DM</em>只对整个神经网络的输出向量进行匹配，而<em>CAFE</em>则是对每一个隐藏层的输出向量进行匹配并相加。本质上差不多，没有跳出Distribution Matching的框架。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zclzcl0223.github.io/2023/09/20/DDSummary/" data-id="clzik1qtc001pm07kewdc40z5" data-title="Dataset Distillation: A Summary" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Dataset-Distillation/" rel="tag">Dataset Distillation</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Paper/" rel="tag">Paper</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/09/21/ComputationalComplexityTimecom2/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Computational Complexity: Time Complexity 2
        
      </div>
    </a>
  
  
    <a href="/2023/09/12/ComputationalComplexityTimeComp/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Computational Complexity: Time Complexity 1</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Advanced-Model/">Advanced Model</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/CS7313-Computational-Complexity/">CS7313: Computational Complexity</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Configuration/">Configuration</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Dive-Into-Deep-Learning/">Dive Into Deep Learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/GNN/">GNN</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Kernel-Method/">Kernel Method</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/MATH6005-Matrix-Theory/">MATH6005: Matrix Theory</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning-by-AndrewNg/">Machine Learning by AndrewNg</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Math/">Math</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Paper/">Paper</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Programming-Language/">Programming Language</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Tool/">Tool</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Attention-Mechanism/" rel="tag">Attention Mechanism</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CNN/" rel="tag">CNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Configuration/" rel="tag">Configuration</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Data-Analysis/" rel="tag">Data Analysis</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Dataset-Distillation/" rel="tag">Dataset Distillation</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Decision-Trees/" rel="tag">Decision Trees</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Deep-Learning/" rel="tag">Deep Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GCN/" rel="tag">GCN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GNN/" rel="tag">GNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Generative-AI/" rel="tag">Generative AI</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hexo/" rel="tag">Hexo</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Lab/" rel="tag">Lab</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux/" rel="tag">Linux</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-Learning/" rel="tag">Machine Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Markup-Language/" rel="tag">Markup Language</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Math/" rel="tag">Math</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Paper/" rel="tag">Paper</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/" rel="tag">Python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RNN/" rel="tag">RNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Recommender-System/" rel="tag">Recommender System</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Reinforcement-Learning/" rel="tag">Reinforcement Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SVM/" rel="tag">SVM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Supervised-Learning/" rel="tag">Supervised Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Theoretical-Computer-Science/" rel="tag">Theoretical Computer Science</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Tool/" rel="tag">Tool</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Unsupervised-Learning/" rel="tag">Unsupervised Learning</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Attention-Mechanism/" style="font-size: 11.82px;">Attention Mechanism</a> <a href="/tags/CNN/" style="font-size: 12.73px;">CNN</a> <a href="/tags/Configuration/" style="font-size: 10px;">Configuration</a> <a href="/tags/Data-Analysis/" style="font-size: 10px;">Data Analysis</a> <a href="/tags/Dataset-Distillation/" style="font-size: 13.64px;">Dataset Distillation</a> <a href="/tags/Decision-Trees/" style="font-size: 10px;">Decision Trees</a> <a href="/tags/Deep-Learning/" style="font-size: 20px;">Deep Learning</a> <a href="/tags/GCN/" style="font-size: 11.82px;">GCN</a> <a href="/tags/GNN/" style="font-size: 15.45px;">GNN</a> <a href="/tags/Generative-AI/" style="font-size: 10.91px;">Generative AI</a> <a href="/tags/Hexo/" style="font-size: 10.91px;">Hexo</a> <a href="/tags/Lab/" style="font-size: 10.91px;">Lab</a> <a href="/tags/Linux/" style="font-size: 16.36px;">Linux</a> <a href="/tags/Machine-Learning/" style="font-size: 17.27px;">Machine Learning</a> <a href="/tags/Markup-Language/" style="font-size: 10px;">Markup Language</a> <a href="/tags/Math/" style="font-size: 19.09px;">Math</a> <a href="/tags/Paper/" style="font-size: 14.55px;">Paper</a> <a href="/tags/Python/" style="font-size: 18.18px;">Python</a> <a href="/tags/RNN/" style="font-size: 10.91px;">RNN</a> <a href="/tags/Recommender-System/" style="font-size: 10.91px;">Recommender System</a> <a href="/tags/Reinforcement-Learning/" style="font-size: 10.91px;">Reinforcement Learning</a> <a href="/tags/SVM/" style="font-size: 10px;">SVM</a> <a href="/tags/Supervised-Learning/" style="font-size: 11.82px;">Supervised Learning</a> <a href="/tags/Theoretical-Computer-Science/" style="font-size: 15.45px;">Theoretical Computer Science</a> <a href="/tags/Tool/" style="font-size: 18.18px;">Tool</a> <a href="/tags/Unsupervised-Learning/" style="font-size: 11.82px;">Unsupervised Learning</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/06/">June 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/01/">January 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/12/">December 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/06/">June 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/05/">May 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/04/">April 2023</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2024/06/17/Diffusion/">Diffusion</a>
          </li>
        
          <li>
            <a href="/2024/01/20/VAE/">VAE</a>
          </li>
        
          <li>
            <a href="/2023/12/02/CountingComplexity/">Computational Complexity: Complexity of Counting</a>
          </li>
        
          <li>
            <a href="/2023/11/24/MatrixTheory5/">MatrixTheory: 特殊矩阵与矩阵分解</a>
          </li>
        
          <li>
            <a href="/2023/11/13/RandomizedComputation/">Computational Complexity: Randomized Computation</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 ChaosTsang<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/" class="mobile-nav-link">home</a>
  
    <a href="/about/" class="mobile-nav-link">about</a>
  
    <a href="/tags/" class="mobile-nav-link">tags</a>
  
    <a href="/categories/" class="mobile-nav-link">categories</a>
  
    <a href="/archives/" class="mobile-nav-link">archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>





<script src="/js/script.js"></script>





  </div>
</body>
</html>