<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" integrity="sha256-XOqroi11tY4EFQMR9ZYwZWKj5ZXiftSx36RRuC3anlA=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.31/fancybox/fancybox.css" integrity="sha256-gkQVf8UKZgQ0HyuxL/VnacadJ+D2Kox2TCEBuNQg5+w=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"zclzcl0223.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.20.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12,"onmobile":false},"hljswrap":true,"copycode":{"enable":true,"style":"mac","show_result":false},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false,"trigger":"auto"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Reinforcement learning is an area of machine learning concerned with how intelligent agents ought to take actions in an environment in order to maximize cumulative reward. This post includes the cours">
<meta property="og:type" content="article">
<meta property="og:title" content="Reinforcement Learning">
<meta property="og:url" content="https://zclzcl0223.github.io/2023/04/19/ReinforcementLearning/index.html">
<meta property="og:site_name" content="JourneyToCoding">
<meta property="og:description" content="Reinforcement learning is an area of machine learning concerned with how intelligent agents ought to take actions in an environment in order to maximize cumulative reward. This post includes the cours">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://zclzcl0223.github.io/2023/04/19/ReinforcementLearning/1.png">
<meta property="og:image" content="https://zclzcl0223.github.io/2023/04/19/ReinforcementLearning/2.png">
<meta property="og:image" content="https://zclzcl0223.github.io/2023/04/19/ReinforcementLearning/3.png">
<meta property="og:image" content="https://zclzcl0223.github.io/2023/04/19/ReinforcementLearning/4.png">
<meta property="og:image" content="https://zclzcl0223.github.io/2023/04/19/ReinforcementLearning/5.png">
<meta property="article:published_time" content="2023-04-19T15:52:43.000Z">
<meta property="article:modified_time" content="2023-08-09T05:58:12.000Z">
<meta property="article:author" content="Chaolv Zeng">
<meta property="article:tag" content="Machine Learning">
<meta property="article:tag" content="Reinforcement Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://zclzcl0223.github.io/2023/04/19/ReinforcementLearning/1.png">


<link rel="canonical" href="https://zclzcl0223.github.io/2023/04/19/ReinforcementLearning/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://zclzcl0223.github.io/2023/04/19/ReinforcementLearning/","path":"2023/04/19/ReinforcementLearning/","title":"Reinforcement Learning"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Reinforcement Learning | JourneyToCoding</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <a target="_blank" rel="noopener" href="https://github.com/zclzcl0223" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">JourneyToCoding</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Code for Fun</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="Searching..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Introduction"><span class="nav-number">1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Basic-concepts"><span class="nav-number">2.</span> <span class="nav-text">Basic concepts</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#State-action-value-function"><span class="nav-number">3.</span> <span class="nav-text">State-action value function</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Random-stochastic-environment"><span class="nav-number">3.1.</span> <span class="nav-text">Random (stochastic) environment</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Continuous-state"><span class="nav-number">4.</span> <span class="nav-text">Continuous state</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#DQN-algorithm"><span class="nav-number">4.1.</span> <span class="nav-text">DQN algorithm</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#epsilon-greedy-policy"><span class="nav-number">4.2.</span> <span class="nav-text">$\epsilon$-greedy policy</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Algorithm-refinement"><span class="nav-number">4.3.</span> <span class="nav-text">Algorithm refinement</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Mini-batch"><span class="nav-number">4.3.1.</span> <span class="nav-text">Mini-batch</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Soft-update"><span class="nav-number">4.3.2.</span> <span class="nav-text">Soft update</span></a></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Chaolv Zeng"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Chaolv Zeng</p>
  <div class="site-description" itemprop="description">Start of Something New</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">106</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">29</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/zclzcl0223" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zclzcl0223" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:chaostsang0223@gmail.com" title="E-Mail → mailto:chaostsang0223@gmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
        <div class="sidebar-inner sidebar-post-related">
          <div class="animated">
              <div class="links-of-blogroll-title"><i class="fa fa-signs-post fa-fw"></i>
    Related Posts
  </div>
  <ul class="popular-posts">
    <li class="popular-posts-item">
      <a class="popular-posts-link" href="/2023/04/05/MachineLearning/" rel="bookmark">
        <time class="popular-posts-time">2023-04-05</time>
        <br>
      Machine Learning
      </a>
    </li>
    <li class="popular-posts-item">
      <a class="popular-posts-link" href="/2023/04/11/Conda/" rel="bookmark">
        <time class="popular-posts-time">2023-04-11</time>
        <br>
      Conda
      </a>
    </li>
    <li class="popular-posts-item">
      <a class="popular-posts-link" href="/2023/04/15/MachineLearningDevelopmentProcess/" rel="bookmark">
        <time class="popular-posts-time">2023-04-15</time>
        <br>
      Machine Learning Development Process
      </a>
    </li>
    <li class="popular-posts-item">
      <a class="popular-posts-link" href="/2023/04/05/SupervisedLearning/" rel="bookmark">
        <time class="popular-posts-time">2023-04-05</time>
        <br>
      Supervised Learning
      </a>
    </li>
    <li class="popular-posts-item">
      <a class="popular-posts-link" href="/2023/05/05/GammaAndFactorial/" rel="bookmark">
        <time class="popular-posts-time">2023-05-05</time>
        <br>
      Gamma and Factorial
      </a>
    </li>
  </ul>

          </div>
        </div>
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zclzcl0223.github.io/2023/04/19/ReinforcementLearning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Chaolv Zeng">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="JourneyToCoding">
      <meta itemprop="description" content="Start of Something New">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Reinforcement Learning | JourneyToCoding">
      <meta itemprop="description" content="Reinforcement learning is an area of machine learning concerned with how intelligent agents ought to take actions in an environment in order to maximize cumulative reward. This post includes the course notes of reinforcement learning course by Andrew Ng.">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Reinforcement Learning
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-04-19 23:52:43" itemprop="dateCreated datePublished" datetime="2023-04-19T23:52:43+08:00">2023-04-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-08-09 13:58:12" itemprop="dateModified" datetime="2023-08-09T13:58:12+08:00">2023-08-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Machine-Learning-by-AndrewNg/" itemprop="url" rel="index"><span itemprop="name">Machine Learning by AndrewNg</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="Word count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Word count in article: </span>
      <span>1.3k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>5 mins.</span>
    </span>
</div>

            <div class="post-description">Reinforcement learning is an area of machine learning concerned with how intelligent agents ought to take actions in an environment in order to maximize cumulative reward. This post includes the course notes of reinforcement learning course by Andrew Ng.</div>
        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Reinforcement learning differs from supervised learning in not needing labelled input&#x2F;output pairs to be presented, and in not needing sub-optimal actions to be explicitly corrected. Instead the focus is on finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge).</p>
<p>The environment is typically stated in the form of a Markov decision process (MDP), because many reinforcement learning algorithms for this context use dynamic programming techniques. The main difference between the classical dynamic programming methods and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the MDP and they target large MDPs where exact methods become infeasible.</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Reinforcement_learning">wiki: Reinforcement learning</a></p>
</blockquote>
<h1 id="Basic-concepts"><a href="#Basic-concepts" class="headerlink" title="Basic concepts"></a>Basic concepts</h1><ul>
<li>environment $E$: The environment where a agent can perceive.</li>
<li>states $s$: A description of the environment perceived by a agent. It is a subset of state space $S$, that is $s\in S$.</li>
<li>actions $a$: The action a agent can take. All actions form action space $A$, $a\in{A}$.</li>
<li>rewards $r$: The reward a agent gets after it takes a action.</li>
<li>discount factor $\gamma$: A number close to 1. It discounts the gains that require more actions and reduces the losses that require more actions.</li>
<li>return: The sum of reward weighted by $\gamma$.<br>$$a_1+\gamma{a_2}+...+\gamma{^n}a_{n-1}$$</li>
<li>policy $\pi(s)$: A policy $a&#x3D;\pi(s)$ decides the action that a agent in $s_i$ state ought to take to get to next state $s_{i+1}$ so as to maximize the return.</li>
</ul>
<p><img src="/2023/04/19/ReinforcementLearning/1.png" alt="1"></p>
<center style="font-size:12px;font-weight:bold">Fig. 1. A loop of reinforcement learning</center><br>

<p>The figure above shows the procedure of reinforcement learning. A agent in state $s_i$ takes action to affect the environment or itself (The agent itself is actually a part of the environment), gets a certain reward and reach new state $s_{i+1}$. The goal of reinforcement learning is to fina a policy $\pi$ that tells the agent what action to take in every state so as to maximize the return. Nearly all reforcement learning assignment can be described as <strong>Markov decision process (MDP)</strong>. MDP is a process that the future only depends on the <strong>current state $s_i$</strong> and have nothing to do with the prior states or how things get to the current state.</p>
<h1 id="State-action-value-function"><a href="#State-action-value-function" class="headerlink" title="State-action value function"></a>State-action value function</h1><p>State-action value function $Q(s,a)$, which is also called Q-function, $Q^*$ or optimal Q function, is a function that computes the maximum return a agent can get if it is in state $s$ and take action $a$ (once). After the agent takes action $a$, its state will become $s&#39;$ and the actions it could take in state $s&#39;$ are $a&#39;$. It is easy to realize that the computing of $Q$ is actually a dynamic programming process. In reinforcement learning, the method we use to count $Q$ is called <strong>bellman equation</strong>:</p>
<p>$$Q(s,a)&#x3D;R(s)+{\gamma}\max_{a&#39;}Q(s&#39;,a&#39;)\tag{1}$$<br>where $R(s)$ is called immediate reward, the second part is the return weighted by $\gamma$ from behaving optimally starting from state $s&#39;$.</p>
<h2 id="Random-stochastic-environment"><a href="#Random-stochastic-environment" class="headerlink" title="Random (stochastic) environment"></a>Random (stochastic) environment</h2><p>In practice, we can&#39;t guarantee that the agent will execute the actions precisely due to the randomness of the environment. In order to get a more precise value of return, we have to take this randomness into account:</p>
<p>$$<br>Q(s,a)&#x3D;R(s)+{\gamma}E[\max_{a&#39;}Q(s&#39;,a&#39;)]\tag{2}<br>$$<br>$E$ means expectation. When taking randomness into account, the state after taking action $a$ is uncertain. Therefore, what the second term of $Q(s,a)$ computes is the weighted average of the maximum returns weighted by $\gamma$ of the different states the agent may reach after taking action $a$.</p>
<h1 id="Continuous-state"><a href="#Continuous-state" class="headerlink" title="Continuous state"></a>Continuous state</h1><p>In practice, the state of a agent is a vector whose values are continuous. For examples, in lab <em>Lunar Lander</em>, the state of a lunar lander can be described as:<br>$$<br>s&#x3D;[x,y,\dot{x},\dot{y},\theta,\dot{\theta},l,r]<br>$$<br>where $x$, $y$ and $\theta$ are its position since this is a plane problem. $\dot{x}$, $\dot{y}$, $\dot{\theta}$ are its velocity of different directions. $l$ and $r$ (value 0 or 1) indicates whether the left or right leg of lunar lander has landed.</p>
<p><img src="/2023/04/19/ReinforcementLearning/2.png" alt="2"></p>
<center style="font-size:12px;font-weight:bold">Fig. 2. Lunar Lander</center>

<h2 id="DQN-algorithm"><a href="#DQN-algorithm" class="headerlink" title="DQN algorithm"></a>DQN algorithm</h2><p>An algorithm to solve this type of reinforcement learning is DQN algorithm or Deep Q-Network. It is also called deep reinforcement learning as it uses neural networks to solve reinforcement learning problems. Its core idea is to compute $Q(s,a)$ using neural networks:</p>
<p><img src="/2023/04/19/ReinforcementLearning/3.png" alt="3"></p>
<center style="font-size:12px;font-weight:bold">Fig. 3. Deep reinforcement learning</center><br>

<p>where $s$ is the state vector and $a$ is the action vector. In lunar lander, $a$ has four dimensions: do nothing, left thruster, right thruster and main thruster. Since it is actually a supervised learning model, we have to feed it training set $(s,a,Q)$ to get the mapping relationship between $s \And a$ and $Q$. According to bellman equation:</p>
<p>$$Q(s,a)&#x3D;R(s)+{\gamma}\max_{a&#39;}Q(s&#39;,a&#39;)$$</p>
<p>$Q$ is decided by $R(s)$ and $s&#39;$ since we must take all the action $a&#39;$ into account, that is, once we get $(s,a,R(s),s&#39;)$, we get $(s,a,Q)$. Therefore, the steps of DQN are:</p>
<ol>
<li>Initialize the neural netwrok randomly as a guess of $Q(s,a)$;</li>
<li>Simulate the behaviors of the agent to get a number of $(s,a,R(s),s&#39;)$;</li>
<li>Feed $(s&#39;,a&#39;)$ to the neural network to <strong>predict</strong> $Q(s&#39;,a&#39;)$;</li>
<li>Create training set using<br>$$x&#x3D;(s,a) \And Q(s,a)&#x3D;R(s)+{\gamma}\max_{a&#39;}Q(s&#39;,a&#39;)$$</li>
<li>Train the neural network using training set created above, get $Q_{new}$;</li>
<li>Set $Q&#x3D;Q_{new}$.</li>
</ol>
<blockquote>
<p>That is, produce some $(s,a,R(s),s&#39;)$ and guess a structure of $Q$ ($Q$ is the neural network). Use this $Q$ to predict $(s,a,Q)$. Use $(s,a,Q)$ to train the neural network and fine tune $Q$.</p>
</blockquote>
<p>In the architecture above, for each value of $a$ in a certain state $s$, we have to predict it respectively. A more effective architecture is to just feed $s$ and produce $Q$ of all values of $a$:</p>
<p><img src="/2023/04/19/ReinforcementLearning/4.png" alt="4"></p>
<center style="font-size:12px;font-weight:bold">Fig. 4. Improved neural network architecture</center>

<h2 id="epsilon-greedy-policy"><a href="#epsilon-greedy-policy" class="headerlink" title="$\epsilon$-greedy policy"></a>$\epsilon$-greedy policy</h2><p>When simulating the behaviors of the agent, instead of simulating its action randomly, it is more advisable to choose a action that will maximizes $Q(s,a)$. However, choosing the action that maximizes $Q(s,a)$ all the time may reduce the robustness of the agent or miss some good choices. A policy to solve this is $\epsilon$-greedy policy:</p>
<ul>
<li>With probability $1-\epsilon$, pick the action $a$ that maximizes $Q(s,a)$;</li>
<li>With probability $\epsilon$, pick an action $a$ randomly.</li>
</ul>
<blockquote>
<p>The first option is called &#39;Exploitation&#39; and the second option is called &#39;Exploration&#39;. As what the wiki in the <a href="#Introduction">Introduction</a> says, reinforcement learning focuses on the balance between exploration and exploitation.</p>
</blockquote>
<p>It is recommended to start $\epsilon$ high to explore more so as to enrich the robustness and then slightly reduce $\epsilon$.</p>
<h2 id="Algorithm-refinement"><a href="#Algorithm-refinement" class="headerlink" title="Algorithm refinement"></a>Algorithm refinement</h2><h3 id="Mini-batch"><a href="#Mini-batch" class="headerlink" title="Mini-batch"></a>Mini-batch</h3><p>When the training set is too large, it is advisable to use different subsets instead of the whole set in each iteration. This will accelerate the computation of both $J$ and derivatives. Such methods can also be used in regression model:</p>
<p><img src="/2023/04/19/ReinforcementLearning/5.png" alt="5"></p>
<center style="font-size:12px;font-weight:bold">Fig. 5. Mini-batch</center><br>

<blockquote>
<p>Tensorflow trains model using mini-batch.</p>
</blockquote>
<h3 id="Soft-update"><a href="#Soft-update" class="headerlink" title="Soft update"></a>Soft update</h3><p>When updating $Q$, we can&#39;t guarantee that $Q_{new}$ is better than $Q$. Therefore, it is recommended to keep a part of $Q$ and update smoothly, like:<br>$$Q_{new}&#x3D;0.01Q_{new}+0.99Q\tag{3}$$</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a>
              <a href="/tags/Reinforcement-Learning/" rel="tag"># Reinforcement Learning</a>
          </div>
          <script type="text/javascript">
          var tagsall=document.getElementsByClassName("post-tags")
          for (var i = tagsall.length - 1; i >= 0; i--){
            var tags=tagsall[i].getElementsByTagName("a");
            for (var j = tags.length - 1; j >= 0; j--) {
                var r=Math.floor(Math.random()*75+130);
                var g=Math.floor(Math.random()*75+100);
                var b=Math.floor(Math.random()*75+80);
                tags[j].style.background = "rgb("+r+","+g+","+b+")";
            }
          }                        
          </script>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2023/04/17/AnalysisOfVariableInPython/" rel="prev" title="Analysis of Variables in Python">
                  <i class="fa fa-angle-left"></i> Analysis of Variables in Python
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2023/04/21/LabsOfMachineLearningByAndrewNg1/" rel="next" title="Lab: Supervised Machine Learning - Regression and Classification">
                  Lab: Supervised Machine Learning - Regression and Classification <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2026</span>
    <span class="with-love">
      <i class="fas fa-star-of-david"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Chaolv Zeng</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>Word count total: </span>
    <span title="Word count total">168k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>Reading time total &asymp;</span>
    <span title="Reading time total">10:11</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div><script color="240,255,255" opacity="1" zIndex="-1" count="100" src="https://cdn.jsdelivr.net/npm/canvas-nest.js@1/dist/canvas-nest.js"></script>

<!-- <br /> -->
<!-- 网站运行时间的设置 -->
<span id="timeDate">载入天数...</span>
<!-- <span id="times">载入时分秒...</span> -->
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("11/17/2022 8:00:00");//此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); 
        if(String(snum).length ==1 ){snum = "0" + snum;}
        // var times = document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 "+hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
    }
setInterval("createtime()",250);
</script>
    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.31/fancybox/fancybox.umd.js" integrity="sha256-a+H7FYzJv6oU2hfsfDGM2Ohw/cR9v+hPfxHCLdmCrE8=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>




  <script src="/js/third-party/fancybox.js"></script>



  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","mhchem":false,"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
