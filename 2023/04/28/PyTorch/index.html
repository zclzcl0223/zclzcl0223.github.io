<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=[object Object]"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', '[object Object]');
</script>
<!-- End Google Analytics -->

  
  <title>PyTorch | JourneyToCoding</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="PyTorch is a machine learning framework based on the Torch library. PyTorch and TensorFlow have much in common. However, PyTorch is more commonly used than TensorFlow nowadays.">
<meta property="og:type" content="article">
<meta property="og:title" content="PyTorch">
<meta property="og:url" content="https://zclzcl0223.github.io/2023/04/28/PyTorch/index.html">
<meta property="og:site_name" content="JourneyToCoding">
<meta property="og:description" content="PyTorch is a machine learning framework based on the Torch library. PyTorch and TensorFlow have much in common. However, PyTorch is more commonly used than TensorFlow nowadays.">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-04-28T14:29:21.000Z">
<meta property="article:modified_time" content="2023-08-09T05:58:40.000Z">
<meta property="article:author" content="ChaosTsang">
<meta property="article:tag" content="Python">
<meta property="article:tag" content="Deep Learning">
<meta property="article:tag" content="Tool">
<meta property="article:tag" content="Machine Learning">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="JourneyToCoding" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/%5Bobject%20Object%5D">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">JourneyToCoding</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">Code for fun</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/">home</a>
        
          <a class="main-nav-link" href="/about/">about</a>
        
          <a class="main-nav-link" href="/tags/">tags</a>
        
          <a class="main-nav-link" href="/categories/">categories</a>
        
          <a class="main-nav-link" href="/archives/">archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://zclzcl0223.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-PyTorch" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/04/28/PyTorch/" class="article-date">
  <time class="dt-published" datetime="2023-04-28T14:29:21.000Z" itemprop="datePublished">2023-04-28</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Tool/">Tool</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      PyTorch
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <span id="more"></span>

<h1 id="Installation-and-import"><a href="#Installation-and-import" class="headerlink" title="Installation and import"></a>Installation and import</h1><p>Since PyTorch is based on the Torch library, the name of package is actually called <em>torch</em>:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Installation</span></span><br><span class="line">pip install torch</span><br><span class="line">or</span><br><span class="line">conda install torch</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># import</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure>

<h1 id="Basic-concept"><a href="#Basic-concept" class="headerlink" title="Basic concept"></a>Basic concept</h1><p>The basic data type in PyTorch is also tensor which is almost the same as the tensor in TensorFlow. PyTorch provides two high-level features:</p>
<ul>
<li>Tensor computing with strong acceleration via GPUs (NumPy only runs on CPUs);</li>
<li>Deep neural networks built on tape-based automatic differentiation system.</li>
</ul>
<p>Both features have much in common with TensorFlow. However, compared to TensorFlow, the api provided by PyTorch are more closer to NumPy.</p>
<h1 id="Data-manipulation"><a href="#Data-manipulation" class="headerlink" title="Data manipulation"></a>Data manipulation</h1><p>To create a tensor, the operations we used are almost the same as those in NumPy:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x = torch.arange(<span class="number">12.</span>) <span class="comment"># float32</span></span><br><span class="line">x = torch.arange(<span class="number">12</span>) <span class="comment"># int64</span></span><br><span class="line">x = torch.zeros((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">x = torch.ones((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">x = torch.randn(<span class="number">3</span>, <span class="number">4</span>) <span class="comment"># random elements drawn from a standard normal distribution</span></span><br><span class="line">x = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">3</span>]])</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Unless otherwise specified, new tensors are stored in main memory and designated for CPU-based computation. See more indexing, slicing, operations and broadcasting in <a href="/2023/04/22/NumPy/">NumPy</a>.</p>
</blockquote>
<p>Though most operations are similar, there are still some differences. We use <code>torch.cat</code> to concatenate multiple tensors together:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">X = torch.arange(<span class="number">12</span>, dtype=torch.float32).reshape(-<span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">Y = torch.tensor([[<span class="number">2.0</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">3</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]])</span><br><span class="line">torch.cat((X, Y), dim=<span class="number">0</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">We get:</span></span><br><span class="line"><span class="string">tensor([[ 0.,  1.,  2.,  3.],</span></span><br><span class="line"><span class="string">        [ 4.,  5.,  6.,  7.],</span></span><br><span class="line"><span class="string">        [ 8.,  9., 10., 11.],</span></span><br><span class="line"><span class="string">        [ 2.,  1.,  4.,  3.],</span></span><br><span class="line"><span class="string">        [ 1.,  2.,  3.,  4.],</span></span><br><span class="line"><span class="string">        [ 4.,  3.,  2.,  1.]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>In addition to <code>.reshape</code>, we can also use <code>.view</code> to get object of different shapes. There are some differences between the two operations. <code>.view</code> returns a view of the original tensor while <code>.shape</code> may return a copy of the original tensor. <code>.shape</code> only returns the view when the inputs are contiguous in memory.</p>
<h2 id="Saving-memory"><a href="#Saving-memory" class="headerlink" title="Saving memory"></a>Saving memory</h2><p><code>X = X +Y</code> will create a new object and allocate memory while <code>X[:] = X + Y</code> or <code>X += Y</code> will perform in-place operation.</p>
<h2 id="Conversion-to-other-python-objects"><a href="#Conversion-to-other-python-objects" class="headerlink" title="Conversion to other python objects"></a>Conversion to other python objects</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A = X.numpy() <span class="comment"># torch.Tensor-&gt;numpy.ndarray</span></span><br><span class="line">B = torch.from_numpy(A) <span class="comment"># numpy.ndarray-&gt;torch.Tensor</span></span><br><span class="line">b = torch.tensor([<span class="number">2.2</span>]).item() <span class="comment"># size-1 tensor-&gt;python scalar</span></span><br></pre></td></tr></table></figure>

<h1 id="Linear-algebra-function"><a href="#Linear-algebra-function" class="headerlink" title="Linear algebra function"></a>Linear algebra function</h1><ul>
<li><code>A.T</code> shares memory with <code>A</code>;</li>
<li><code>A.clone()</code> returns a new object with the same elements of <code>A</code>;</li>
<li><code>A.mean([axes])</code> and <code>A.sum([axes])</code> return the mean or sum of [axes] in <code>A</code>. In general, [axes] will missing from the shape of the output, but we can add <code>keepdims=True</code> which will make the shape of [axes] 1, to remain [axes];</li>
<li><code>A.cumsum([axes])</code> calculate the cumulative sum of elements of <code>A</code> along some axes;</li>
<li><code>torch.dot(), torch.mv(), torch.mm()</code> (<code>A @ B</code> is also legal) calculate v-v products, m-v products and m-m products respectively;</li>
</ul>
<blockquote>
<p><code>A*B</code> or $A\odot B$ is called <strong>Hadamard product</strong>.</p>
</blockquote>
<h2 id="Norms"><a href="#Norms" class="headerlink" title="Norms"></a>Norms</h2><p>Norms ($||x||$) are often used to measure the length or size of each vector in a vector space (or matrix). They are scalars that satisfy:</p>
<ul>
<li>Non-negativity;</li>
<li>Homogeneity;</li>
<li>Triangle inequality</li>
</ul>
<p>For vectors, $\ell{_2}$ norms measure the (Eucilidean) length of vectors:</p>
<p>$$\vert |x|\vert_2&#x3D;\sqrt{\sum\limits_{i&#x3D;1}^{n}x_i^2}$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.norm(x)</span><br></pre></td></tr></table></figure>
<p>$\ell{_1}$ norms are called Manhattan distance:</p>
<p>$$\vert |x|\vert_1&#x3D;\sqrt{\sum\limits_{i&#x3D;1}^{n}|x|}$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.<span class="built_in">abs</span>(x).<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure>
<p>For matrices, we often use <strong>the Frobenius norm</strong>, which is the same as the $\ell_2$ norm:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.norm(X)</span><br></pre></td></tr></table></figure>

<h1 id="Autograd"><a href="#Autograd" class="headerlink" title="Autograd"></a>Autograd</h1><p>See <a href="/2023/04/28/DiveIntoDeepLearningIntroduction/#Derivative-of-matrix
">Matrix derivative</a> to know more about matrix derivative. </p>
<p>Unlike TensorFlow, PyTorch use implicit construction to produce a computation graph, which allows us to simply use its api and don&#39;t have to declare the computation graph explicitly like <a href="/2023/04/12/Tensorflow/#Adam-algorithm">Autograd in Tensorflow</a>. Whenever we want to compute the derivative of a certain argument, we only need 4 steps in PyTorch:</p>
<ol>
<li>Attach gradients to those variables with respect to which we desire derivatives:<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create independent variable and the space to store derivatives</span></span><br><span class="line">x = torch.arange(<span class="number">4.0</span>, requires_grad=<span class="literal">True</span>) <span class="comment"># or x.requires_grad_(True)</span></span><br></pre></td></tr></table></figure></li>
<li>Record the computation of the target value (dependent variable):<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y = f(a) <span class="comment"># f is the function we define</span></span><br></pre></td></tr></table></figure></li>
<li>Execute the back propagation function:<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y.backward()</span><br></pre></td></tr></table></figure></li>
<li>Access the resulting gradient:<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(x.grad)</span><br></pre></td></tr></table></figure></li>
</ol>
<p>The steps above only work when the dependent variable is a scalar. For non-scalar variables, we sometimes turn them to scalar variables by summing all elements together, like <code>y.sum().backward()</code>. This works because we add the gradients of a specific model parameter together finally. More commonly, we will use a certain row vector $v^T$ to turn $\vec{y}$ to a scalar:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># this is the same as: y.sum().backward(), because torch.dot(torch.ones(len(y)), y) = y.sum()</span></span><br><span class="line">y.backward(gradient=torch.ones(<span class="built_in">len</span>(y)))</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Since neural networks are always computed batch by batch, the result of <code>x.grad</code> will be accumulated. To reset the gradient buffer, we can call <code>x.grad.zero()</code>.</p>
</blockquote>
<h2 id="Detaching-computation"><a href="#Detaching-computation" class="headerlink" title="Detaching computation"></a>Detaching computation</h2><p>If $z&#x3D;f(x,y)$ and $y&#x3D;g(x)$, but we only want to focus on the <strong>direct influence</strong> of $x$ on $z$, we can create a new variable that detaches the connection between $x$ and $y$:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">y = g(x)</span><br><span class="line">u = y.detach() <span class="comment"># remove y from the computation graph of z</span></span><br><span class="line">z = x * u <span class="comment"># u is no longer a function of x but y is still a function of x</span></span><br><span class="line"></span><br><span class="line">z.<span class="built_in">sum</span>().backward()</span><br><span class="line">x.grad == u <span class="comment"># the value is true</span></span><br></pre></td></tr></table></figure>

<h2 id="Updating-parameters"><a href="#Updating-parameters" class="headerlink" title="Updating parameters"></a>Updating parameters</h2><p>The Computation graph (tree) will be built implicitly (if <code>requires_grad=True</code>) whenever we operate the parameters that we want to optimize. Since the parameters are always in leaf nodes, we have to detach it from the graph, otherwise, the whole graph will go wrong.</p>
<blockquote>
<p>PyTorch implements a dynamic graph mechanism. Specifically, the computation graph is constructed during forward propagation, and is destroyed during back propagation. More specifically, the computation graph is destroyed when calling <code>backward()</code>, leaving only parameters in leaf nodes.</p>
</blockquote>
<p>The method <code>PyTorch</code> used is <code>with torch.no_grad():</code>, which makes <code>requires_grad=False</code> when entering and <code>requires_grad=True</code> when leaving:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># General usage</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">  <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">      param -= lr * param.grad / batch_size</span><br><span class="line">      param.grad.zero_()</span><br></pre></td></tr></table></figure>

<h1 id="Net-constructing"><a href="#Net-constructing" class="headerlink" title="Net constructing"></a>Net constructing</h1><p>All layers, blocks or nets in PyTorch are subclasses of <code>nn.Module</code>. We can define our own blocks by inheriting <code>nn.Module</code> and overloading the <code>__init__</code> and <code>forward</code> functions.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()  <span class="comment"># we must call the __init__ function of Module so that we can inherit its parameters</span></span><br><span class="line">        self.hidden = nn.Linear(<span class="number">20</span>, <span class="number">256</span>)  <span class="comment"># a hidden layer</span></span><br><span class="line">        self.out = nn.Linear(<span class="number">256</span>, <span class="number">10</span>)  <span class="comment"># output layer</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># forward propagation: get X and produce output</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="comment"># F defines some basic functions</span></span><br><span class="line">        <span class="keyword">return</span> self.out(F.relu(self.hidden(X)))</span><br><span class="line">net = MLP()</span><br><span class="line">net(X)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>In general, we just need to define the structure of our blocks in <code>__init__</code> and compute the output in <code>forward</code>. <code>forward</code> points to <code>__call__</code>, so <code>net(X)</code> is euqal to <code>net.forward(X)</code>.</p>
</blockquote>
<h2 id="Sequential"><a href="#Sequential" class="headerlink" title="Sequential"></a>Sequential</h2><p><code>nn.Sequential</code> is the built-in subclass of <code>nn.Module</code>. Its working principle is very simple, which just simply connects different blocks. We can define it by ourselves:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MySequential</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, *args</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> args:</span><br><span class="line">            self._modules[block] = block</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> self._modules.values():</span><br><span class="line">            X = block(X)</span><br><span class="line">        <span class="keyword">return</span> X</span><br><span class="line"></span><br><span class="line">net = MySequential(nn.Linear(<span class="number">20</span>, <span class="number">256</span>), nn.ReLU(), nn.Linear(<span class="number">256</span>, <span class="number">10</span>))</span><br><span class="line">net(X)</span><br></pre></td></tr></table></figure>
<p><code>MySequential</code> will compute in the order of <code>nn.Linear</code>, <code>nn.ReLU</code> and <code>nn.Linear</code>.</p>
<blockquote>
<p><code>._modules</code> is an <code>OrderedDict</code> defined in <code>nn.Module</code>. It will store our blocks in order. All the arguments of <code>Sequential</code> should be the subclass of <code>nn.Module</code>.</p>
<p>In PyTorch, the activation is also a layer though it doesn&#39;t have any model parameters.</p>
</blockquote>
<h1 id="Parameter-management"><a href="#Parameter-management" class="headerlink" title="Parameter management"></a>Parameter management</h1><p>The type of model parameters in PyTorch is <code>nn.Parameter</code> which is a compound object containing values (<code>Tensor</code>), gradients (<code>grad</code>) and extra information. <code>grad</code> works when we call <code>.backward</code> and <code>requires_grad=True</code>.</p>
<h2 id="Parameter-visiting"><a href="#Parameter-visiting" class="headerlink" title="Parameter visiting"></a>Parameter visiting</h2><p>For blocks that define model parameters, we can use <code>.state_dict()</code> which returns a dictionary to visit the model parameters:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(nn.Linear(<span class="number">4</span>, <span class="number">8</span>), nn.ReLU(), nn.Linear(<span class="number">8</span>, <span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].state_dict())</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">OrderedDict([(&#x27;weight&#x27;, tensor([[ 0.3016, -0.1901, -0.1991, -0.1220,  0.1121, -0.1424, -0.3060,  0.3400]])), (&#x27;bias&#x27;, tensor([-0.0291]))])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(net[<span class="number">2</span>].bias))</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].bias)</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].bias.data)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&lt;class &#x27;torch.nn.parameter.Parameter&#x27;&gt;</span></span><br><span class="line"><span class="string">Parameter containing:</span></span><br><span class="line"><span class="string">tensor([-0.0291], requires_grad=True)</span></span><br><span class="line"><span class="string">tensor([-0.0291])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p><code>Sequential</code> allows use to visit each block like a <code>list</code>. If we just <code>print(net)</code>, python will output the structure of <code>net</code>.</p>
</blockquote>
<h2 id="Parameter-initialization"><a href="#Parameter-initialization" class="headerlink" title="Parameter initialization"></a>Parameter initialization</h2><p>We can define our own function to initialize the model parameters:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_normal</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br><span class="line">        nn.init.zeros_(m.bias)</span><br><span class="line">net.apply(init_normal)</span><br></pre></td></tr></table></figure>
<blockquote>
<p><code>m</code> is the subclass of <code>nn.Module</code>. <code>net.apply</code> asks PyTorch to call <code>init_normal</code> for each block. Only the block with model parameters will initialize its parameters. (So far, only the built-in <code>nn.Linear</code> has defined model parameters).</p>
</blockquote>
<p>Since each block is the subclass of <code>nn.Module</code>, we could also initialize each block respectively:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net[<span class="number">0</span>].apply(init_normal)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Direct initialization is also possible, like:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">my_init</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.uniform_(m.weight, -<span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line">        m.weight.data *= m.weight.data.<span class="built_in">abs</span>() &gt;= <span class="number">5</span></span><br></pre></td></tr></table></figure>
<p>We must not initialize all <code>weight</code> using the same value. If we do so, all the neurons in a layer are computing the same thing, that is, they will output the same value and their gradients will also be the same.</p>
</blockquote>
<h2 id="Shared-layer"><a href="#Shared-layer" class="headerlink" title="Shared layer"></a>Shared layer</h2><p>Since instances of custom classes are mutable objects in python, we can make two layers share their parameters by passing the same object to PyTorch:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">shared = nn.Linear(<span class="number">8</span>, <span class="number">8</span>)</span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">4</span>, <span class="number">8</span>), nn.ReLU(),</span><br><span class="line">                    shared, nn.ReLU(),</span><br><span class="line">                    shared, nn.ReLU(),</span><br><span class="line">                    nn.Linear(<span class="number">8</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure>

<h2 id="Block-with-parameters"><a href="#Block-with-parameters" class="headerlink" title="Block with parameters"></a>Block with parameters</h2><p>We could define parameters for our own blocks. Since <code>backward</code> will work for all <code>Tensor</code>, we don&#39;t have to deal with <code>backward</code>. We just need to define our parameters and set <code>requires_grad=True</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyLinear</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_units, units</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.weight = nn.Parameter(torch.randn(in_units, units))</span><br><span class="line">        self.bias = nn.Parameter(torch.randn(units,))</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        linear = torch.matmul(X, self.weight.data) + self.bias.data</span><br><span class="line">        <span class="keyword">return</span> F.relu(linear)</span><br></pre></td></tr></table></figure>

<h1 id="Saving-amp-loading-parameters"><a href="#Saving-amp-loading-parameters" class="headerlink" title="Saving &amp; loading parameters"></a>Saving &amp; loading parameters</h1><p>We can save a tensor, a tensor list or a dictionary using <code>torch.save</code> and load them using <code>torch.load</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(<span class="number">4</span>)</span><br><span class="line">y = torch.arange(<span class="number">5</span>)</span><br><span class="line">z = &#123;<span class="string">&#x27;x&#x27;</span>: x, <span class="string">&#x27;y&#x27;</span>: y&#125;</span><br><span class="line">torch.save(x, <span class="string">&#x27;x-file&#x27;</span>)</span><br><span class="line">torch.save([x, y], <span class="string">&#x27;xy-file&#x27;</span>)</span><br><span class="line">torch.save(z, <span class="string">&#x27;z-dict&#x27;</span>)</span><br><span class="line"></span><br><span class="line">x1 = torch.load(<span class="string">&#x27;x-file&#x27;</span>)</span><br><span class="line">xx, yy = torch.load(<span class="string">&#x27;xy-file&#x27;</span>)</span><br><span class="line">zz = torch.load(<span class="string">&#x27;z-dict&#x27;</span>)</span><br><span class="line"></span><br><span class="line">x1 == x, xx == x, yy == y, zz[<span class="string">&#x27;x&#x27;</span>] == x, zz[<span class="string">&#x27;y&#x27;</span>] == y</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">(tensor([True, True, True, True]),</span></span><br><span class="line"><span class="string"> tensor([True, True, True, True]),</span></span><br><span class="line"><span class="string"> tensor([True, True, True, True, True]),</span></span><br><span class="line"><span class="string"> tensor([True, True, True, True]),</span></span><br><span class="line"><span class="string"> tensor([True, True, True, True, True]))</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<p>For a model, PyTorch will save its parameters rather than the whole model:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.hidden = nn.Linear(<span class="number">20</span>, <span class="number">256</span>)</span><br><span class="line">        self.output = nn.Linear(<span class="number">256</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.output(F.relu(self.hidden(x)))</span><br><span class="line"></span><br><span class="line">net = MLP()</span><br><span class="line">torch.save(net.state_dict(), <span class="string">&#x27;mlp.params&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>When we need to load the model, we should rebuild the same structure and load the parameters:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">clone = MLP()</span><br><span class="line">clone.load_state_dict(torch.load(<span class="string">&#x27;mlp.params&#x27;</span>))</span><br></pre></td></tr></table></figure>

<h1 id="Training-in-GPUs"><a href="#Training-in-GPUs" class="headerlink" title="Training in GPUs"></a>Training in GPUs</h1><p>Tensors are created on CPU by default.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">x.device</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">device(type=&#x27;cpu&#x27;)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>To create a tensor in GPU, we must specify the GPU we use:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X = torch.ones(<span class="number">2</span>, <span class="number">3</span>, device=torch.device(<span class="string">&#x27;cuda&#x27;</span>))</span><br><span class="line">X.device</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">device(type=&#x27;cuda&#x27;, index=0)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p><code>cuda</code> means GPU in PyTorch. All Nvidia GPUs owned by the computer are organized into an array. <code>cuda</code> or <code>cuda:0</code> represent the first GPU and <code>cuda:1</code> represent the second GPU. We can use <code>!nvidia-smi</code> to get the information of Nvidia GPUs in our computer and <code>torch.cuda.device_count()</code> to get the number of Nvidia GPUs of our computer.</p>
</blockquote>
<p>If we want to operate different tensors together, we must make sure that they are stored in the same GPU, otherwise, PyTorch will throw an exception <strong>because moving data from CPU to GPU or from GPU to another GPU is time-consuming</strong>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Y = torch.rand(<span class="number">2</span>, <span class="number">3</span>, device=torch.device(<span class="string">&#x27;cuda&#x27;</span>))</span><br><span class="line">Z = torch.rand(<span class="number">2</span>, <span class="number">3</span>)  <span class="comment"># create in CPU</span></span><br><span class="line">X = Z.cuda(<span class="number">0</span>)  <span class="comment"># create a new tensor in GPU 0</span></span><br><span class="line"><span class="built_in">print</span>(X)</span><br><span class="line"><span class="built_in">print</span>(Z)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[0.3206, 0.4276, 0.8653],</span></span><br><span class="line"><span class="string">        [0.3276, 0.4867, 0.1320]], device=&#x27;cuda:0&#x27;)</span></span><br><span class="line"><span class="string">tensor([[0.3206, 0.4276, 0.8653],</span></span><br><span class="line"><span class="string">        [0.3276, 0.4867, 0.1320]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># now we can add Y and X</span></span><br><span class="line">X + Y</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[0.8401, 0.6486, 1.8582],</span></span><br><span class="line"><span class="string">        [1.1932, 0.8063, 1.1152]], device=&#x27;cuda:0&#x27;)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># if we add Y and Z</span></span><br><span class="line">Y + Z</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<h2 id="Neural-networks-in-GPU"><a href="#Neural-networks-in-GPU" class="headerlink" title="Neural networks in GPU"></a>Neural networks in GPU</h2><p>Similarly, we can put the parameters of neural networks in GPU:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(nn.Linear(<span class="number">3</span>, <span class="number">1</span>))</span><br><span class="line">net = net.to(device=torch.device(<span class="string">&#x27;cuda&#x27;</span>))</span><br><span class="line">net(X)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[-0.1712],</span></span><br><span class="line"><span class="string">        [ 0.1713]], device=&#x27;cuda:0&#x27;, grad_fn=&lt;AddmmBackward0&gt;)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<p>Both parameters and data should be stored in the same device. Inadvertently moving data from one device to another can significantly degrade performance, which is what we need to pay attention to. For example, report data to the user on the command line or log it in a NumPy ndarray, both of which will cause data movement from GPU to CPU.</p>
<blockquote>
<p>To enable training the model in GPUs, we should install CUDA and NVIDIA Driver in <a target="_blank" rel="noopener" href="https://developer.nvidia.com/cuda-downloads">CUDA Toolkit</a> and install the corresponding GPU version of pytorch in <a target="_blank" rel="noopener" href="https://pytorch.org/get-started/locally/">PyTorch</a>.</p>
</blockquote>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zclzcl0223.github.io/2023/04/28/PyTorch/" data-id="clzik0lct0078v47k6x64a1tr" data-title="PyTorch" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Deep-Learning/" rel="tag">Deep Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-Learning/" rel="tag">Machine Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Python/" rel="tag">Python</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Tool/" rel="tag">Tool</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/05/02/IteratorAndGenerator/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Iterator &amp; Generator in Python
        
      </div>
    </a>
  
  
    <a href="/2023/04/28/DiveIntoDeepLearningIntroduction/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">D2L: Environment Configuration and Introduction</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Advanced-Model/">Advanced Model</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/CS7313-Computational-Complexity/">CS7313: Computational Complexity</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Configuration/">Configuration</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Dive-Into-Deep-Learning/">Dive Into Deep Learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/GNN/">GNN</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Kernel-Method/">Kernel Method</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/MATH6005-Matrix-Theory/">MATH6005: Matrix Theory</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning-by-AndrewNg/">Machine Learning by AndrewNg</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Math/">Math</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Paper/">Paper</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Programming-Language/">Programming Language</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Tool/">Tool</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Attention-Mechanism/" rel="tag">Attention Mechanism</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CNN/" rel="tag">CNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Configuration/" rel="tag">Configuration</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Data-Analysis/" rel="tag">Data Analysis</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Dataset-Distillation/" rel="tag">Dataset Distillation</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Decision-Trees/" rel="tag">Decision Trees</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Deep-Learning/" rel="tag">Deep Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GCN/" rel="tag">GCN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GNN/" rel="tag">GNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Generative-AI/" rel="tag">Generative AI</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hexo/" rel="tag">Hexo</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Lab/" rel="tag">Lab</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux/" rel="tag">Linux</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-Learning/" rel="tag">Machine Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Markup-Language/" rel="tag">Markup Language</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Math/" rel="tag">Math</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Paper/" rel="tag">Paper</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/" rel="tag">Python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RNN/" rel="tag">RNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Recommender-System/" rel="tag">Recommender System</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Reinforcement-Learning/" rel="tag">Reinforcement Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SVM/" rel="tag">SVM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Supervised-Learning/" rel="tag">Supervised Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Theoretical-Computer-Science/" rel="tag">Theoretical Computer Science</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Tool/" rel="tag">Tool</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Unsupervised-Learning/" rel="tag">Unsupervised Learning</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Attention-Mechanism/" style="font-size: 11.82px;">Attention Mechanism</a> <a href="/tags/CNN/" style="font-size: 12.73px;">CNN</a> <a href="/tags/Configuration/" style="font-size: 10px;">Configuration</a> <a href="/tags/Data-Analysis/" style="font-size: 10px;">Data Analysis</a> <a href="/tags/Dataset-Distillation/" style="font-size: 13.64px;">Dataset Distillation</a> <a href="/tags/Decision-Trees/" style="font-size: 10px;">Decision Trees</a> <a href="/tags/Deep-Learning/" style="font-size: 20px;">Deep Learning</a> <a href="/tags/GCN/" style="font-size: 11.82px;">GCN</a> <a href="/tags/GNN/" style="font-size: 15.45px;">GNN</a> <a href="/tags/Generative-AI/" style="font-size: 10.91px;">Generative AI</a> <a href="/tags/Hexo/" style="font-size: 10.91px;">Hexo</a> <a href="/tags/Lab/" style="font-size: 10.91px;">Lab</a> <a href="/tags/Linux/" style="font-size: 16.36px;">Linux</a> <a href="/tags/Machine-Learning/" style="font-size: 17.27px;">Machine Learning</a> <a href="/tags/Markup-Language/" style="font-size: 10px;">Markup Language</a> <a href="/tags/Math/" style="font-size: 19.09px;">Math</a> <a href="/tags/Paper/" style="font-size: 14.55px;">Paper</a> <a href="/tags/Python/" style="font-size: 18.18px;">Python</a> <a href="/tags/RNN/" style="font-size: 10.91px;">RNN</a> <a href="/tags/Recommender-System/" style="font-size: 10.91px;">Recommender System</a> <a href="/tags/Reinforcement-Learning/" style="font-size: 10.91px;">Reinforcement Learning</a> <a href="/tags/SVM/" style="font-size: 10px;">SVM</a> <a href="/tags/Supervised-Learning/" style="font-size: 11.82px;">Supervised Learning</a> <a href="/tags/Theoretical-Computer-Science/" style="font-size: 15.45px;">Theoretical Computer Science</a> <a href="/tags/Tool/" style="font-size: 18.18px;">Tool</a> <a href="/tags/Unsupervised-Learning/" style="font-size: 11.82px;">Unsupervised Learning</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/06/">June 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/01/">January 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/12/">December 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/06/">June 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/05/">May 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/04/">April 2023</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2024/06/17/Diffusion/">Diffusion</a>
          </li>
        
          <li>
            <a href="/2024/01/20/VAE/">VAE</a>
          </li>
        
          <li>
            <a href="/2023/12/02/CountingComplexity/">Computational Complexity: Complexity of Counting</a>
          </li>
        
          <li>
            <a href="/2023/11/24/MatrixTheory5/">MatrixTheory: </a>
          </li>
        
          <li>
            <a href="/2023/11/13/RandomizedComputation/">Computational Complexity: Randomized Computation</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 ChaosTsang<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/" class="mobile-nav-link">home</a>
  
    <a href="/about/" class="mobile-nav-link">about</a>
  
    <a href="/tags/" class="mobile-nav-link">tags</a>
  
    <a href="/categories/" class="mobile-nav-link">categories</a>
  
    <a href="/archives/" class="mobile-nav-link">archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>





<script src="/js/script.js"></script>





  </div>
</body>
</html>