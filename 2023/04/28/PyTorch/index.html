<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" integrity="sha256-XOqroi11tY4EFQMR9ZYwZWKj5ZXiftSx36RRuC3anlA=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.31/fancybox/fancybox.css" integrity="sha256-gkQVf8UKZgQ0HyuxL/VnacadJ+D2Kox2TCEBuNQg5+w=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"zclzcl0223.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.20.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12,"onmobile":false},"hljswrap":true,"copycode":{"enable":true,"style":"mac","show_result":false},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false,"trigger":"auto"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="PyTorch is a machine learning framework based on the Torch library. PyTorch and TensorFlow have much in common. However, PyTorch is more commonly used than TensorFlow nowadays.">
<meta property="og:type" content="article">
<meta property="og:title" content="PyTorch">
<meta property="og:url" content="https://zclzcl0223.github.io/2023/04/28/PyTorch/index.html">
<meta property="og:site_name" content="JourneyToCoding">
<meta property="og:description" content="PyTorch is a machine learning framework based on the Torch library. PyTorch and TensorFlow have much in common. However, PyTorch is more commonly used than TensorFlow nowadays.">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-04-28T14:29:21.000Z">
<meta property="article:modified_time" content="2023-08-09T05:58:40.000Z">
<meta property="article:author" content="Chaolv Zeng">
<meta property="article:tag" content="Python">
<meta property="article:tag" content="Deep Learning">
<meta property="article:tag" content="Tool">
<meta property="article:tag" content="Machine Learning">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://zclzcl0223.github.io/2023/04/28/PyTorch/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://zclzcl0223.github.io/2023/04/28/PyTorch/","path":"2023/04/28/PyTorch/","title":"PyTorch"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>PyTorch | JourneyToCoding</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <a target="_blank" rel="noopener" href="https://github.com/zclzcl0223" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">JourneyToCoding</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Code for Fun</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="Searching..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Installation-and-import"><span class="nav-number">1.</span> <span class="nav-text">Installation and import</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Basic-concept"><span class="nav-number">2.</span> <span class="nav-text">Basic concept</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Data-manipulation"><span class="nav-number">3.</span> <span class="nav-text">Data manipulation</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Saving-memory"><span class="nav-number">3.1.</span> <span class="nav-text">Saving memory</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Conversion-to-other-python-objects"><span class="nav-number">3.2.</span> <span class="nav-text">Conversion to other python objects</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Linear-algebra-function"><span class="nav-number">4.</span> <span class="nav-text">Linear algebra function</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Norms"><span class="nav-number">4.1.</span> <span class="nav-text">Norms</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Autograd"><span class="nav-number">5.</span> <span class="nav-text">Autograd</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Detaching-computation"><span class="nav-number">5.1.</span> <span class="nav-text">Detaching computation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Updating-parameters"><span class="nav-number">5.2.</span> <span class="nav-text">Updating parameters</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Net-constructing"><span class="nav-number">6.</span> <span class="nav-text">Net constructing</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Sequential"><span class="nav-number">6.1.</span> <span class="nav-text">Sequential</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Parameter-management"><span class="nav-number">7.</span> <span class="nav-text">Parameter management</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Parameter-visiting"><span class="nav-number">7.1.</span> <span class="nav-text">Parameter visiting</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Parameter-initialization"><span class="nav-number">7.2.</span> <span class="nav-text">Parameter initialization</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Shared-layer"><span class="nav-number">7.3.</span> <span class="nav-text">Shared layer</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Block-with-parameters"><span class="nav-number">7.4.</span> <span class="nav-text">Block with parameters</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Saving-amp-loading-parameters"><span class="nav-number">8.</span> <span class="nav-text">Saving &amp; loading parameters</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Training-in-GPUs"><span class="nav-number">9.</span> <span class="nav-text">Training in GPUs</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Neural-networks-in-GPU"><span class="nav-number">9.1.</span> <span class="nav-text">Neural networks in GPU</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Chaolv Zeng"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Chaolv Zeng</p>
  <div class="site-description" itemprop="description">Start of Something New</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">106</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">29</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/zclzcl0223" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zclzcl0223" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:chaostsang0223@gmail.com" title="E-Mail → mailto:chaostsang0223@gmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
        <div class="sidebar-inner sidebar-post-related">
          <div class="animated">
              <div class="links-of-blogroll-title"><i class="fa fa-signs-post fa-fw"></i>
    Related Posts
  </div>
  <ul class="popular-posts">
    <li class="popular-posts-item">
      <a class="popular-posts-link" href="/2023/07/08/BugAndUsageOfTorch/" rel="bookmark">
        <time class="popular-posts-time">2023-07-08</time>
        <br>
      Bug and Usage of Torch
      </a>
    </li>
    <li class="popular-posts-item">
      <a class="popular-posts-link" href="/2023/04/22/NumPy/" rel="bookmark">
        <time class="popular-posts-time">2023-04-22</time>
        <br>
      NumPy
      </a>
    </li>
    <li class="popular-posts-item">
      <a class="popular-posts-link" href="/2023/05/02/WithContextManagers/" rel="bookmark">
        <time class="popular-posts-time">2023-05-02</time>
        <br>
      With: Context Managers
      </a>
    </li>
    <li class="popular-posts-item">
      <a class="popular-posts-link" href="/2023/04/17/UsageOfImportInPython/" rel="bookmark">
        <time class="popular-posts-time">2023-04-17</time>
        <br>
      Usage of Import Keyword in Python
      </a>
    </li>
    <li class="popular-posts-item">
      <a class="popular-posts-link" href="/2023/04/17/AnalysisOfVariableInPython/" rel="bookmark">
        <time class="popular-posts-time">2023-04-17</time>
        <br>
      Analysis of Variables in Python
      </a>
    </li>
  </ul>

          </div>
        </div>
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zclzcl0223.github.io/2023/04/28/PyTorch/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Chaolv Zeng">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="JourneyToCoding">
      <meta itemprop="description" content="Start of Something New">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="PyTorch | JourneyToCoding">
      <meta itemprop="description" content="PyTorch is a machine learning framework based on the Torch library. PyTorch and TensorFlow have much in common. However, PyTorch is more commonly used than TensorFlow nowadays.">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          PyTorch
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-04-28 22:29:21" itemprop="dateCreated datePublished" datetime="2023-04-28T22:29:21+08:00">2023-04-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-08-09 13:58:40" itemprop="dateModified" datetime="2023-08-09T13:58:40+08:00">2023-08-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Tool/" itemprop="url" rel="index"><span itemprop="name">Tool</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="Word count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Word count in article: </span>
      <span>2.2k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>8 mins.</span>
    </span>
</div>

            <div class="post-description">PyTorch is a machine learning framework based on the Torch library. PyTorch and TensorFlow have much in common. However, PyTorch is more commonly used than TensorFlow nowadays.</div>
        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><span id="more"></span>

<h1 id="Installation-and-import"><a href="#Installation-and-import" class="headerlink" title="Installation and import"></a>Installation and import</h1><p>Since PyTorch is based on the Torch library, the name of package is actually called <em>torch</em>:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Installation</span></span><br><span class="line">pip install torch</span><br><span class="line">or</span><br><span class="line">conda install torch</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># import</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure>

<h1 id="Basic-concept"><a href="#Basic-concept" class="headerlink" title="Basic concept"></a>Basic concept</h1><p>The basic data type in PyTorch is also tensor which is almost the same as the tensor in TensorFlow. PyTorch provides two high-level features:</p>
<ul>
<li>Tensor computing with strong acceleration via GPUs (NumPy only runs on CPUs);</li>
<li>Deep neural networks built on tape-based automatic differentiation system.</li>
</ul>
<p>Both features have much in common with TensorFlow. However, compared to TensorFlow, the api provided by PyTorch are more closer to NumPy.</p>
<h1 id="Data-manipulation"><a href="#Data-manipulation" class="headerlink" title="Data manipulation"></a>Data manipulation</h1><p>To create a tensor, the operations we used are almost the same as those in NumPy:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x = torch.arange(<span class="number">12.</span>) <span class="comment"># float32</span></span><br><span class="line">x = torch.arange(<span class="number">12</span>) <span class="comment"># int64</span></span><br><span class="line">x = torch.zeros((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">x = torch.ones((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">x = torch.randn(<span class="number">3</span>, <span class="number">4</span>) <span class="comment"># random elements drawn from a standard normal distribution</span></span><br><span class="line">x = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">3</span>]])</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Unless otherwise specified, new tensors are stored in main memory and designated for CPU-based computation. See more indexing, slicing, operations and broadcasting in <a href="/2023/04/22/NumPy/">NumPy</a>.</p>
</blockquote>
<p>Though most operations are similar, there are still some differences. We use <code>torch.cat</code> to concatenate multiple tensors together:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">X = torch.arange(<span class="number">12</span>, dtype=torch.float32).reshape(-<span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">Y = torch.tensor([[<span class="number">2.0</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">3</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]])</span><br><span class="line">torch.cat((X, Y), dim=<span class="number">0</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">We get:</span></span><br><span class="line"><span class="string">tensor([[ 0.,  1.,  2.,  3.],</span></span><br><span class="line"><span class="string">        [ 4.,  5.,  6.,  7.],</span></span><br><span class="line"><span class="string">        [ 8.,  9., 10., 11.],</span></span><br><span class="line"><span class="string">        [ 2.,  1.,  4.,  3.],</span></span><br><span class="line"><span class="string">        [ 1.,  2.,  3.,  4.],</span></span><br><span class="line"><span class="string">        [ 4.,  3.,  2.,  1.]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>In addition to <code>.reshape</code>, we can also use <code>.view</code> to get object of different shapes. There are some differences between the two operations. <code>.view</code> returns a view of the original tensor while <code>.shape</code> may return a copy of the original tensor. <code>.shape</code> only returns the view when the inputs are contiguous in memory.</p>
<h2 id="Saving-memory"><a href="#Saving-memory" class="headerlink" title="Saving memory"></a>Saving memory</h2><p><code>X = X +Y</code> will create a new object and allocate memory while <code>X[:] = X + Y</code> or <code>X += Y</code> will perform in-place operation.</p>
<h2 id="Conversion-to-other-python-objects"><a href="#Conversion-to-other-python-objects" class="headerlink" title="Conversion to other python objects"></a>Conversion to other python objects</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A = X.numpy() <span class="comment"># torch.Tensor-&gt;numpy.ndarray</span></span><br><span class="line">B = torch.from_numpy(A) <span class="comment"># numpy.ndarray-&gt;torch.Tensor</span></span><br><span class="line">b = torch.tensor([<span class="number">2.2</span>]).item() <span class="comment"># size-1 tensor-&gt;python scalar</span></span><br></pre></td></tr></table></figure>

<h1 id="Linear-algebra-function"><a href="#Linear-algebra-function" class="headerlink" title="Linear algebra function"></a>Linear algebra function</h1><ul>
<li><code>A.T</code> shares memory with <code>A</code>;</li>
<li><code>A.clone()</code> returns a new object with the same elements of <code>A</code>;</li>
<li><code>A.mean([axes])</code> and <code>A.sum([axes])</code> return the mean or sum of [axes] in <code>A</code>. In general, [axes] will missing from the shape of the output, but we can add <code>keepdims=True</code> which will make the shape of [axes] 1, to remain [axes];</li>
<li><code>A.cumsum([axes])</code> calculate the cumulative sum of elements of <code>A</code> along some axes;</li>
<li><code>torch.dot(), torch.mv(), torch.mm()</code> (<code>A @ B</code> is also legal) calculate v-v products, m-v products and m-m products respectively;</li>
</ul>
<blockquote>
<p><code>A*B</code> or $A\odot B$ is called <strong>Hadamard product</strong>.</p>
</blockquote>
<h2 id="Norms"><a href="#Norms" class="headerlink" title="Norms"></a>Norms</h2><p>Norms ($||x||$) are often used to measure the length or size of each vector in a vector space (or matrix). They are scalars that satisfy:</p>
<ul>
<li>Non-negativity;</li>
<li>Homogeneity;</li>
<li>Triangle inequality</li>
</ul>
<p>For vectors, $\ell{_2}$ norms measure the (Eucilidean) length of vectors:</p>
<p>$$\vert |x|\vert_2&#x3D;\sqrt{\sum\limits_{i&#x3D;1}^{n}x_i^2}$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.norm(x)</span><br></pre></td></tr></table></figure>
<p>$\ell{_1}$ norms are called Manhattan distance:</p>
<p>$$\vert |x|\vert_1&#x3D;\sqrt{\sum\limits_{i&#x3D;1}^{n}|x|}$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.<span class="built_in">abs</span>(x).<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure>
<p>For matrices, we often use <strong>the Frobenius norm</strong>, which is the same as the $\ell_2$ norm:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.norm(X)</span><br></pre></td></tr></table></figure>

<h1 id="Autograd"><a href="#Autograd" class="headerlink" title="Autograd"></a>Autograd</h1><p>See <a href="/2023/04/28/DiveIntoDeepLearningIntroduction/#Derivative-of-matrix
">Matrix derivative</a> to know more about matrix derivative. </p>
<p>Unlike TensorFlow, PyTorch use implicit construction to produce a computation graph, which allows us to simply use its api and don&#39;t have to declare the computation graph explicitly like <a href="/2023/04/12/Tensorflow/#Adam-algorithm">Autograd in Tensorflow</a>. Whenever we want to compute the derivative of a certain argument, we only need 4 steps in PyTorch:</p>
<ol>
<li>Attach gradients to those variables with respect to which we desire derivatives:<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create independent variable and the space to store derivatives</span></span><br><span class="line">x = torch.arange(<span class="number">4.0</span>, requires_grad=<span class="literal">True</span>) <span class="comment"># or x.requires_grad_(True)</span></span><br></pre></td></tr></table></figure></li>
<li>Record the computation of the target value (dependent variable):<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y = f(a) <span class="comment"># f is the function we define</span></span><br></pre></td></tr></table></figure></li>
<li>Execute the back propagation function:<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y.backward()</span><br></pre></td></tr></table></figure></li>
<li>Access the resulting gradient:<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(x.grad)</span><br></pre></td></tr></table></figure></li>
</ol>
<p>The steps above only work when the dependent variable is a scalar. For non-scalar variables, we sometimes turn them to scalar variables by summing all elements together, like <code>y.sum().backward()</code>. This works because we add the gradients of a specific model parameter together finally. More commonly, we will use a certain row vector $v^T$ to turn $\vec{y}$ to a scalar:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># this is the same as: y.sum().backward(), because torch.dot(torch.ones(len(y)), y) = y.sum()</span></span><br><span class="line">y.backward(gradient=torch.ones(<span class="built_in">len</span>(y)))</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Since neural networks are always computed batch by batch, the result of <code>x.grad</code> will be accumulated. To reset the gradient buffer, we can call <code>x.grad.zero()</code>.</p>
</blockquote>
<h2 id="Detaching-computation"><a href="#Detaching-computation" class="headerlink" title="Detaching computation"></a>Detaching computation</h2><p>If $z&#x3D;f(x,y)$ and $y&#x3D;g(x)$, but we only want to focus on the <strong>direct influence</strong> of $x$ on $z$, we can create a new variable that detaches the connection between $x$ and $y$:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">y = g(x)</span><br><span class="line">u = y.detach() <span class="comment"># remove y from the computation graph of z</span></span><br><span class="line">z = x * u <span class="comment"># u is no longer a function of x but y is still a function of x</span></span><br><span class="line"></span><br><span class="line">z.<span class="built_in">sum</span>().backward()</span><br><span class="line">x.grad == u <span class="comment"># the value is true</span></span><br></pre></td></tr></table></figure>

<h2 id="Updating-parameters"><a href="#Updating-parameters" class="headerlink" title="Updating parameters"></a>Updating parameters</h2><p>The Computation graph (tree) will be built implicitly (if <code>requires_grad=True</code>) whenever we operate the parameters that we want to optimize. Since the parameters are always in leaf nodes, we have to detach it from the graph, otherwise, the whole graph will go wrong.</p>
<blockquote>
<p>PyTorch implements a dynamic graph mechanism. Specifically, the computation graph is constructed during forward propagation, and is destroyed during back propagation. More specifically, the computation graph is destroyed when calling <code>backward()</code>, leaving only parameters in leaf nodes.</p>
</blockquote>
<p>The method <code>PyTorch</code> used is <code>with torch.no_grad():</code>, which makes <code>requires_grad=False</code> when entering and <code>requires_grad=True</code> when leaving:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># General usage</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">  <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">      param -= lr * param.grad / batch_size</span><br><span class="line">      param.grad.zero_()</span><br></pre></td></tr></table></figure>

<h1 id="Net-constructing"><a href="#Net-constructing" class="headerlink" title="Net constructing"></a>Net constructing</h1><p>All layers, blocks or nets in PyTorch are subclasses of <code>nn.Module</code>. We can define our own blocks by inheriting <code>nn.Module</code> and overloading the <code>__init__</code> and <code>forward</code> functions.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()  <span class="comment"># we must call the __init__ function of Module so that we can inherit its parameters</span></span><br><span class="line">        self.hidden = nn.Linear(<span class="number">20</span>, <span class="number">256</span>)  <span class="comment"># a hidden layer</span></span><br><span class="line">        self.out = nn.Linear(<span class="number">256</span>, <span class="number">10</span>)  <span class="comment"># output layer</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># forward propagation: get X and produce output</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="comment"># F defines some basic functions</span></span><br><span class="line">        <span class="keyword">return</span> self.out(F.relu(self.hidden(X)))</span><br><span class="line">net = MLP()</span><br><span class="line">net(X)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>In general, we just need to define the structure of our blocks in <code>__init__</code> and compute the output in <code>forward</code>. <code>forward</code> points to <code>__call__</code>, so <code>net(X)</code> is euqal to <code>net.forward(X)</code>.</p>
</blockquote>
<h2 id="Sequential"><a href="#Sequential" class="headerlink" title="Sequential"></a>Sequential</h2><p><code>nn.Sequential</code> is the built-in subclass of <code>nn.Module</code>. Its working principle is very simple, which just simply connects different blocks. We can define it by ourselves:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MySequential</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, *args</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> args:</span><br><span class="line">            self._modules[block] = block</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> self._modules.values():</span><br><span class="line">            X = block(X)</span><br><span class="line">        <span class="keyword">return</span> X</span><br><span class="line"></span><br><span class="line">net = MySequential(nn.Linear(<span class="number">20</span>, <span class="number">256</span>), nn.ReLU(), nn.Linear(<span class="number">256</span>, <span class="number">10</span>))</span><br><span class="line">net(X)</span><br></pre></td></tr></table></figure>
<p><code>MySequential</code> will compute in the order of <code>nn.Linear</code>, <code>nn.ReLU</code> and <code>nn.Linear</code>.</p>
<blockquote>
<p><code>._modules</code> is an <code>OrderedDict</code> defined in <code>nn.Module</code>. It will store our blocks in order. All the arguments of <code>Sequential</code> should be the subclass of <code>nn.Module</code>.</p>
<p>In PyTorch, the activation is also a layer though it doesn&#39;t have any model parameters.</p>
</blockquote>
<h1 id="Parameter-management"><a href="#Parameter-management" class="headerlink" title="Parameter management"></a>Parameter management</h1><p>The type of model parameters in PyTorch is <code>nn.Parameter</code> which is a compound object containing values (<code>Tensor</code>), gradients (<code>grad</code>) and extra information. <code>grad</code> works when we call <code>.backward</code> and <code>requires_grad=True</code>.</p>
<h2 id="Parameter-visiting"><a href="#Parameter-visiting" class="headerlink" title="Parameter visiting"></a>Parameter visiting</h2><p>For blocks that define model parameters, we can use <code>.state_dict()</code> which returns a dictionary to visit the model parameters:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(nn.Linear(<span class="number">4</span>, <span class="number">8</span>), nn.ReLU(), nn.Linear(<span class="number">8</span>, <span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].state_dict())</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">OrderedDict([(&#x27;weight&#x27;, tensor([[ 0.3016, -0.1901, -0.1991, -0.1220,  0.1121, -0.1424, -0.3060,  0.3400]])), (&#x27;bias&#x27;, tensor([-0.0291]))])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(net[<span class="number">2</span>].bias))</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].bias)</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].bias.data)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&lt;class &#x27;torch.nn.parameter.Parameter&#x27;&gt;</span></span><br><span class="line"><span class="string">Parameter containing:</span></span><br><span class="line"><span class="string">tensor([-0.0291], requires_grad=True)</span></span><br><span class="line"><span class="string">tensor([-0.0291])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p><code>Sequential</code> allows use to visit each block like a <code>list</code>. If we just <code>print(net)</code>, python will output the structure of <code>net</code>.</p>
</blockquote>
<h2 id="Parameter-initialization"><a href="#Parameter-initialization" class="headerlink" title="Parameter initialization"></a>Parameter initialization</h2><p>We can define our own function to initialize the model parameters:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_normal</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br><span class="line">        nn.init.zeros_(m.bias)</span><br><span class="line">net.apply(init_normal)</span><br></pre></td></tr></table></figure>
<blockquote>
<p><code>m</code> is the subclass of <code>nn.Module</code>. <code>net.apply</code> asks PyTorch to call <code>init_normal</code> for each block. Only the block with model parameters will initialize its parameters. (So far, only the built-in <code>nn.Linear</code> has defined model parameters).</p>
</blockquote>
<p>Since each block is the subclass of <code>nn.Module</code>, we could also initialize each block respectively:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net[<span class="number">0</span>].apply(init_normal)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Direct initialization is also possible, like:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">my_init</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.uniform_(m.weight, -<span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line">        m.weight.data *= m.weight.data.<span class="built_in">abs</span>() &gt;= <span class="number">5</span></span><br></pre></td></tr></table></figure>
<p>We must not initialize all <code>weight</code> using the same value. If we do so, all the neurons in a layer are computing the same thing, that is, they will output the same value and their gradients will also be the same.</p>
</blockquote>
<h2 id="Shared-layer"><a href="#Shared-layer" class="headerlink" title="Shared layer"></a>Shared layer</h2><p>Since instances of custom classes are mutable objects in python, we can make two layers share their parameters by passing the same object to PyTorch:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">shared = nn.Linear(<span class="number">8</span>, <span class="number">8</span>)</span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">4</span>, <span class="number">8</span>), nn.ReLU(),</span><br><span class="line">                    shared, nn.ReLU(),</span><br><span class="line">                    shared, nn.ReLU(),</span><br><span class="line">                    nn.Linear(<span class="number">8</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure>

<h2 id="Block-with-parameters"><a href="#Block-with-parameters" class="headerlink" title="Block with parameters"></a>Block with parameters</h2><p>We could define parameters for our own blocks. Since <code>backward</code> will work for all <code>Tensor</code>, we don&#39;t have to deal with <code>backward</code>. We just need to define our parameters and set <code>requires_grad=True</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyLinear</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_units, units</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.weight = nn.Parameter(torch.randn(in_units, units))</span><br><span class="line">        self.bias = nn.Parameter(torch.randn(units,))</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        linear = torch.matmul(X, self.weight.data) + self.bias.data</span><br><span class="line">        <span class="keyword">return</span> F.relu(linear)</span><br></pre></td></tr></table></figure>

<h1 id="Saving-amp-loading-parameters"><a href="#Saving-amp-loading-parameters" class="headerlink" title="Saving &amp; loading parameters"></a>Saving &amp; loading parameters</h1><p>We can save a tensor, a tensor list or a dictionary using <code>torch.save</code> and load them using <code>torch.load</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(<span class="number">4</span>)</span><br><span class="line">y = torch.arange(<span class="number">5</span>)</span><br><span class="line">z = &#123;<span class="string">&#x27;x&#x27;</span>: x, <span class="string">&#x27;y&#x27;</span>: y&#125;</span><br><span class="line">torch.save(x, <span class="string">&#x27;x-file&#x27;</span>)</span><br><span class="line">torch.save([x, y], <span class="string">&#x27;xy-file&#x27;</span>)</span><br><span class="line">torch.save(z, <span class="string">&#x27;z-dict&#x27;</span>)</span><br><span class="line"></span><br><span class="line">x1 = torch.load(<span class="string">&#x27;x-file&#x27;</span>)</span><br><span class="line">xx, yy = torch.load(<span class="string">&#x27;xy-file&#x27;</span>)</span><br><span class="line">zz = torch.load(<span class="string">&#x27;z-dict&#x27;</span>)</span><br><span class="line"></span><br><span class="line">x1 == x, xx == x, yy == y, zz[<span class="string">&#x27;x&#x27;</span>] == x, zz[<span class="string">&#x27;y&#x27;</span>] == y</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">(tensor([True, True, True, True]),</span></span><br><span class="line"><span class="string"> tensor([True, True, True, True]),</span></span><br><span class="line"><span class="string"> tensor([True, True, True, True, True]),</span></span><br><span class="line"><span class="string"> tensor([True, True, True, True]),</span></span><br><span class="line"><span class="string"> tensor([True, True, True, True, True]))</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<p>For a model, PyTorch will save its parameters rather than the whole model:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.hidden = nn.Linear(<span class="number">20</span>, <span class="number">256</span>)</span><br><span class="line">        self.output = nn.Linear(<span class="number">256</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.output(F.relu(self.hidden(x)))</span><br><span class="line"></span><br><span class="line">net = MLP()</span><br><span class="line">torch.save(net.state_dict(), <span class="string">&#x27;mlp.params&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>When we need to load the model, we should rebuild the same structure and load the parameters:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">clone = MLP()</span><br><span class="line">clone.load_state_dict(torch.load(<span class="string">&#x27;mlp.params&#x27;</span>))</span><br></pre></td></tr></table></figure>

<h1 id="Training-in-GPUs"><a href="#Training-in-GPUs" class="headerlink" title="Training in GPUs"></a>Training in GPUs</h1><p>Tensors are created on CPU by default.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">x.device</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">device(type=&#x27;cpu&#x27;)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>To create a tensor in GPU, we must specify the GPU we use:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X = torch.ones(<span class="number">2</span>, <span class="number">3</span>, device=torch.device(<span class="string">&#x27;cuda&#x27;</span>))</span><br><span class="line">X.device</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">device(type=&#x27;cuda&#x27;, index=0)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p><code>cuda</code> means GPU in PyTorch. All Nvidia GPUs owned by the computer are organized into an array. <code>cuda</code> or <code>cuda:0</code> represent the first GPU and <code>cuda:1</code> represent the second GPU. We can use <code>!nvidia-smi</code> to get the information of Nvidia GPUs in our computer and <code>torch.cuda.device_count()</code> to get the number of Nvidia GPUs of our computer.</p>
</blockquote>
<p>If we want to operate different tensors together, we must make sure that they are stored in the same GPU, otherwise, PyTorch will throw an exception <strong>because moving data from CPU to GPU or from GPU to another GPU is time-consuming</strong>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Y = torch.rand(<span class="number">2</span>, <span class="number">3</span>, device=torch.device(<span class="string">&#x27;cuda&#x27;</span>))</span><br><span class="line">Z = torch.rand(<span class="number">2</span>, <span class="number">3</span>)  <span class="comment"># create in CPU</span></span><br><span class="line">X = Z.cuda(<span class="number">0</span>)  <span class="comment"># create a new tensor in GPU 0</span></span><br><span class="line"><span class="built_in">print</span>(X)</span><br><span class="line"><span class="built_in">print</span>(Z)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[0.3206, 0.4276, 0.8653],</span></span><br><span class="line"><span class="string">        [0.3276, 0.4867, 0.1320]], device=&#x27;cuda:0&#x27;)</span></span><br><span class="line"><span class="string">tensor([[0.3206, 0.4276, 0.8653],</span></span><br><span class="line"><span class="string">        [0.3276, 0.4867, 0.1320]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># now we can add Y and X</span></span><br><span class="line">X + Y</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[0.8401, 0.6486, 1.8582],</span></span><br><span class="line"><span class="string">        [1.1932, 0.8063, 1.1152]], device=&#x27;cuda:0&#x27;)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># if we add Y and Z</span></span><br><span class="line">Y + Z</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<h2 id="Neural-networks-in-GPU"><a href="#Neural-networks-in-GPU" class="headerlink" title="Neural networks in GPU"></a>Neural networks in GPU</h2><p>Similarly, we can put the parameters of neural networks in GPU:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(nn.Linear(<span class="number">3</span>, <span class="number">1</span>))</span><br><span class="line">net = net.to(device=torch.device(<span class="string">&#x27;cuda&#x27;</span>))</span><br><span class="line">net(X)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[-0.1712],</span></span><br><span class="line"><span class="string">        [ 0.1713]], device=&#x27;cuda:0&#x27;, grad_fn=&lt;AddmmBackward0&gt;)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<p>Both parameters and data should be stored in the same device. Inadvertently moving data from one device to another can significantly degrade performance, which is what we need to pay attention to. For example, report data to the user on the command line or log it in a NumPy ndarray, both of which will cause data movement from GPU to CPU.</p>
<blockquote>
<p>To enable training the model in GPUs, we should install CUDA and NVIDIA Driver in <a target="_blank" rel="noopener" href="https://developer.nvidia.com/cuda-downloads">CUDA Toolkit</a> and install the corresponding GPU version of pytorch in <a target="_blank" rel="noopener" href="https://pytorch.org/get-started/locally/">PyTorch</a>.</p>
</blockquote>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Python/" rel="tag"># Python</a>
              <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a>
              <a href="/tags/Tool/" rel="tag"># Tool</a>
              <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a>
          </div>
          <script type="text/javascript">
          var tagsall=document.getElementsByClassName("post-tags")
          for (var i = tagsall.length - 1; i >= 0; i--){
            var tags=tagsall[i].getElementsByTagName("a");
            for (var j = tags.length - 1; j >= 0; j--) {
                var r=Math.floor(Math.random()*75+130);
                var g=Math.floor(Math.random()*75+100);
                var b=Math.floor(Math.random()*75+80);
                tags[j].style.background = "rgb("+r+","+g+","+b+")";
            }
          }                        
          </script>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2023/04/28/DiveIntoDeepLearningIntroduction/" rel="prev" title="D2L: Environment Configuration and Introduction">
                  <i class="fa fa-angle-left"></i> D2L: Environment Configuration and Introduction
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2023/05/02/IteratorAndGenerator/" rel="next" title="Iterator & Generator in Python">
                  Iterator & Generator in Python <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2026</span>
    <span class="with-love">
      <i class="fas fa-star-of-david"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Chaolv Zeng</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>Word count total: </span>
    <span title="Word count total">163k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>Reading time total &asymp;</span>
    <span title="Reading time total">9:51</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div><script color="240,255,255" opacity="1" zIndex="-1" count="100" src="https://cdn.jsdelivr.net/npm/canvas-nest.js@1/dist/canvas-nest.js"></script>

<!-- <br /> -->
<!-- 网站运行时间的设置 -->
<span id="timeDate">载入天数...</span>
<!-- <span id="times">载入时分秒...</span> -->
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("11/17/2022 8:00:00");//此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); 
        if(String(snum).length ==1 ){snum = "0" + snum;}
        // var times = document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 "+hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
    }
setInterval("createtime()",250);
</script>
    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.31/fancybox/fancybox.umd.js" integrity="sha256-a+H7FYzJv6oU2hfsfDGM2Ohw/cR9v+hPfxHCLdmCrE8=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>




  <script src="/js/third-party/fancybox.js"></script>



  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","mhchem":false,"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
