<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" integrity="sha256-XOqroi11tY4EFQMR9ZYwZWKj5ZXiftSx36RRuC3anlA=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.31/fancybox/fancybox.css" integrity="sha256-gkQVf8UKZgQ0HyuxL/VnacadJ+D2Kox2TCEBuNQg5+w=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"zclzcl0223.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.20.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12,"onmobile":false},"hljswrap":true,"copycode":{"enable":true,"style":"mac","show_result":false},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false,"trigger":"auto"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="CNN is a special kind of MLP. Why do we still need CNN even though MLP can work well? This involves a classic problem in the computer field: the trade-off between memory and computing speed. CNN is wi">
<meta property="og:type" content="article">
<meta property="og:title" content="CNN: Feature Extraction">
<meta property="og:url" content="https://zclzcl0223.github.io/2023/05/10/CNN/index.html">
<meta property="og:site_name" content="JourneyToCoding">
<meta property="og:description" content="CNN is a special kind of MLP. Why do we still need CNN even though MLP can work well? This involves a classic problem in the computer field: the trade-off between memory and computing speed. CNN is wi">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://zclzcl0223.github.io/2023/05/10/CNN/1.png">
<meta property="og:image" content="https://zclzcl0223.github.io/2023/05/10/CNN/2.png">
<meta property="og:image" content="https://zclzcl0223.github.io/2023/05/10/CNN/3.png">
<meta property="og:image" content="https://zclzcl0223.github.io/2023/05/10/CNN/4.png">
<meta property="og:image" content="https://zclzcl0223.github.io/2023/05/10/CNN/5.png">
<meta property="og:image" content="https://zclzcl0223.github.io/2023/05/10/CNN/5_1.png">
<meta property="og:image" content="https://zclzcl0223.github.io/2023/05/10/CNN/6.png">
<meta property="og:image" content="https://zclzcl0223.github.io/2023/05/10/CNN/7.png">
<meta property="og:image" content="https://zclzcl0223.github.io/2023/05/10/CNN/8.png">
<meta property="og:image" content="https://zclzcl0223.github.io/2023/05/10/CNN/9.png">
<meta property="og:image" content="https://zclzcl0223.github.io/2023/05/10/CNN/10.png">
<meta property="article:published_time" content="2023-05-10T15:45:10.000Z">
<meta property="article:modified_time" content="2024-09-29T08:33:40.407Z">
<meta property="article:author" content="Chaolv Zeng">
<meta property="article:tag" content="Deep Learning">
<meta property="article:tag" content="CNN">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://zclzcl0223.github.io/2023/05/10/CNN/1.png">


<link rel="canonical" href="https://zclzcl0223.github.io/2023/05/10/CNN/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://zclzcl0223.github.io/2023/05/10/CNN/","path":"2023/05/10/CNN/","title":"CNN: Feature Extraction"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>CNN: Feature Extraction | JourneyToCoding</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <a target="_blank" rel="noopener" href="https://github.com/zclzcl0223" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">JourneyToCoding</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Code for Fun</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="Searching..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#From-Dense-to-Convolution"><span class="nav-number">1.</span> <span class="nav-text">From Dense to Convolution</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Constructing-a-dense-layer"><span class="nav-number">1.1.</span> <span class="nav-text">Constructing a dense layer</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Translation-invariance"><span class="nav-number">1.2.</span> <span class="nav-text">Translation invariance</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Locality"><span class="nav-number">1.3.</span> <span class="nav-text">Locality</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Convolution-Fourier-and-Neural-Networks"><span class="nav-number">2.</span> <span class="nav-text">Convolution, Fourier and Neural Networks</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Stock"><span class="nav-number">2.1.</span> <span class="nav-text">Stock</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Influence"><span class="nav-number">2.2.</span> <span class="nav-text">Influence</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Convolution-in-CNN"><span class="nav-number">2.3.</span> <span class="nav-text">Convolution in CNN</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Hyperparameters-of-convolutional-layer"><span class="nav-number">3.</span> <span class="nav-text">Hyperparameters of convolutional layer</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Kernel-size"><span class="nav-number">3.1.</span> <span class="nav-text">Kernel size</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Padding"><span class="nav-number">3.2.</span> <span class="nav-text">Padding</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Stride"><span class="nav-number">3.3.</span> <span class="nav-text">Stride</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Multiple-input-channels"><span class="nav-number">3.4.</span> <span class="nav-text">Multiple input channels</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Multiple-output-channels"><span class="nav-number">3.5.</span> <span class="nav-text">Multiple output channels</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-times-1-kernel"><span class="nav-number">3.5.1.</span> <span class="nav-text">$1\times 1$ kernel</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Pooling-layer"><span class="nav-number">4.</span> <span class="nav-text">Pooling layer</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Sensitivity-to-position"><span class="nav-number">4.1.</span> <span class="nav-text">Sensitivity to position</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Computational-complexity"><span class="nav-number">4.2.</span> <span class="nav-text">Computational complexity</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Reference"><span class="nav-number">5.</span> <span class="nav-text">Reference</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Chaolv Zeng"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Chaolv Zeng</p>
  <div class="site-description" itemprop="description">Start of Something New</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">106</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">29</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/zclzcl0223" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zclzcl0223" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:chaostsang0223@gmail.com" title="E-Mail → mailto:chaostsang0223@gmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
        <div class="sidebar-inner sidebar-post-related">
          <div class="animated">
              <div class="links-of-blogroll-title"><i class="fa fa-signs-post fa-fw"></i>
    Related Posts
  </div>
  <ul class="popular-posts">
    <li class="popular-posts-item">
      <a class="popular-posts-link" href="/2023/05/14/CommonCNNModels/" rel="bookmark">
        <time class="popular-posts-time">2023-05-14</time>
        <br>
      Common CNN Models
      </a>
    </li>
    <li class="popular-posts-item">
      <a class="popular-posts-link" href="/2023/06/07/Transformer/" rel="bookmark">
        <time class="popular-posts-time">2023-06-07</time>
        <br>
      Transformer: Self-Attention and Parallelization
      </a>
    </li>
    <li class="popular-posts-item">
      <a class="popular-posts-link" href="/2023/04/10/NeuralNetwork/" rel="bookmark">
        <time class="popular-posts-time">2023-04-10</time>
        <br>
      Neural Network
      </a>
    </li>
    <li class="popular-posts-item">
      <a class="popular-posts-link" href="/2023/05/07/NumericalStability/" rel="bookmark">
        <time class="popular-posts-time">2023-05-07</time>
        <br>
      Numerical Stability
      </a>
    </li>
    <li class="popular-posts-item">
      <a class="popular-posts-link" href="/2023/06/06/Attention/" rel="bookmark">
        <time class="popular-posts-time">2023-06-06</time>
        <br>
      Attention Mechanisms: More Targeted Information Extraction
      </a>
    </li>
  </ul>

          </div>
        </div>
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zclzcl0223.github.io/2023/05/10/CNN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Chaolv Zeng">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="JourneyToCoding">
      <meta itemprop="description" content="Start of Something New">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="CNN: Feature Extraction | JourneyToCoding">
      <meta itemprop="description" content="CNN is a special kind of MLP. Why do we still need CNN even though MLP can work well? This involves a classic problem in the computer field: the trade-off between memory and computing speed. CNN is widely used in image processing. An image is characterized by its representation in the computer by millions of pixels. Each pixel is a feature of the image. It is unbearable for GPU to store so many model parameters for these features. Hence, we need CNN to compress the number of parameters and extract features from an image.">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          CNN: Feature Extraction
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-05-10 23:45:10" itemprop="dateCreated datePublished" datetime="2023-05-10T23:45:10+08:00">2023-05-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-09-29 16:33:40" itemprop="dateModified" datetime="2024-09-29T16:33:40+08:00">2024-09-29</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Dive-Into-Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Dive Into Deep Learning</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="Word count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Word count in article: </span>
      <span>2.4k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>9 mins.</span>
    </span>
</div>

            <div class="post-description">CNN is a special kind of MLP. Why do we still need CNN even though MLP can work well? This involves a classic problem in the computer field: the trade-off between memory and computing speed. CNN is widely used in image processing. An image is characterized by its representation in the computer by millions of pixels. Each pixel is a feature of the image. It is unbearable for GPU to store so many model parameters for these features. Hence, we need CNN to compress the number of parameters and extract features from an image.</div>
        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><span id="more"></span>

<h1 id="From-Dense-to-Convolution"><a href="#From-Dense-to-Convolution" class="headerlink" title="From Dense to Convolution"></a>From Dense to Convolution</h1><p>The convolutional layer is the key layer of CNN. It is a special kind of dense layer. CNN is used to deal with images. In this section, we take the grayscale image as example, that is, the feature of an input image is a 2-D matrix. When processing images, firstly, it is reasonable that agents&#39;s awareness of a certain object should not be overly concerned with the precise location of the object in the image because what the object looks like has nothing to do with its position. Besides, the features of one object should only be related to the pixels around it as each pixel only determines the depth of the color and we distinguish objects by their color boundaries. These two priciples are called <em>translation invariance</em> and <em>locality</em> respectively. If fulfilling both priciples, dense layers become convolutional layers.</p>
<h2 id="Constructing-a-dense-layer"><a href="#Constructing-a-dense-layer" class="headerlink" title="Constructing a dense layer"></a>Constructing a dense layer</h2><p>For convinience, we keep the dimension of a input image $X$ 2-D and make the model parameters of one neuron $w$ have the same dimension as $X$. Under such setting, the output of one neuron become:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(X*w).<span class="built_in">sum</span>() <span class="comment"># w is a matrix</span></span><br></pre></td></tr></table></figure>

<p>Now that the output of a hidden layer $H$ should be a image that has the same shape as $X$, we have to place $m*n$ neurons in the hidden layer, where $m$ and $n$ are the dimension of $X$. Hence, for a hidden layer, its model parameter $W$ is:</p>
<p>$$<br>{\begin{bmatrix}<br>    {\begin{bmatrix}<br>        w_{0,0,0,0} &amp; w_{0,0,0,1}\\<br>        w_{0,0,1,0} &amp; w_{0,0,1,1}<br>    \end{bmatrix}<br>    \begin{bmatrix}<br>        w_{0,1,0,0} &amp; w_{0,1,0,1}\\<br>        w_{0,1,1,0} &amp; w_{0,1,1,1}<br>    \end{bmatrix}}\\<br>    {\begin{bmatrix}<br>        w_{1,0,0,0} &amp; w_{1,0,0,1}\\<br>        w_{1,0,1,0} &amp; w_{1,0,1,1}<br>    \end{bmatrix}<br>    \begin{bmatrix}<br>        w_{1,1,0,0} &amp; w_{1,1,0,1}\\<br>        w_{1,1,1,0} &amp; w_{1,1,1,1}<br>    \end{bmatrix}}<br>\end{bmatrix}} _{m\times n\times m\times n&#x3D;2\times 2\times 2\times 2}<br>$$</p>
<p>And an element of output $H$ is:</p>
<p>$$<br>[H]_{i,j}&#x3D;\sum_k \sum_l {[W]} _{i,j,k,l} {[X]} _{k,l} + {[U]} _{i,j}<br>$$</p>
<p>where $U$ contains bias of each output pixel.</p>
<h2 id="Translation-invariance"><a href="#Translation-invariance" class="headerlink" title="Translation invariance"></a>Translation invariance</h2><p>Now we can clearly see that in order to process an image with $m * n$ data, we almost use $(m * n)^2$ model parameters. This is extremely space-consuming. However, if we apply translation invariance priciple to the dense layer, we can cut it to $m * n$.</p>
<p>As mentioned above, the activation of a pixel should have nothing to do with its position. This is only possible when ${[W]} _{i,j}$ and ${[U]} _{i,j}$ are both the same for any $(i,j)$. Hence, for a certain hidden layer, we actually just need a 2-D $W$ and a scalar $u$:</p>
<p>$$<br>{[H]} _{i,j}&#x3D;\sum_k\sum_l{[W]} _{k,l} {[X]} _{k,l}+u<br>$$</p>
<h2 id="Locality"><a href="#Locality" class="headerlink" title="Locality"></a>Locality</h2><p>So far, we have significantly cut the space of model parameters but it is still too large. Now it is time for <em>locality</em>. As motivated above, there is no need to look so far away from the location of input pixel ${[X]}_ {i,j}$. We just need to consider a small window around ${[X]} _{i,j}$:</p>
<p>$${[H]}_ {i, j} &#x3D; u + \sum_{a &#x3D; -\Delta}^{\Delta} \sum_{b &#x3D; -\Delta}^{\Delta} {[V]}_ {a, b}  {[X]}_ {i+a, j+b}.$$</p>
<p>where $\Delta\times\Delta$ is the size of the window and $V$ (size $\Delta\times\Delta$) is the model parameter inside the window, a filter that generates the feature of a zone in an image or a <strong>convolution kernel</strong>.</p>
<p>Now we construct a convolutional layer (<code>nn.Conv2d</code>) with hyperparameters <em>kernel size</em> and <em>stride</em> (see below).</p>
<p>$$Y &#x3D; X \star W+b$$</p>
<p>$W$ and $b$ are model parameters that the agent can learn. $\star$ is cross-correlation operator which is slightly different from convolution but their behaviors are the same in the convolutional layer (see below). The dimensions of $Y$ are smaller than the original dimensions, which is:</p>
<p>$$(n_h-k_h+1)\times(n_w-k_w+1)$$</p>
<p>where $n_h$ and $n_w$ are the dimensions of $X$, $k_h$ and $k_w$ are the dimensions of $W$. It doesn&#39;t matter and we can solve it easily (see below).</p>
<p><img src="/2023/05/10/CNN/1.png" alt="1"></p>
<center style="font-size:12px;font-weight:bold">Fig. 1. Convolutional layer</center>

<h1 id="Convolution-Fourier-and-Neural-Networks"><a href="#Convolution-Fourier-and-Neural-Networks" class="headerlink" title="Convolution, Fourier and Neural Networks"></a>Convolution, Fourier and Neural Networks</h1><p>Convolution is an operation on two functions ($f$ and $g$) that produces a third function ($f*g$) that express how the shape of one is modified by the other:</p>
<p>$$(f*g)(t)&#x3D;\int_{-\infty} ^{+\infty}f(\tau)g(t-\tau)d\tau$$</p>
<p>Physically, we can explain it from two perspectives.</p>
<h2 id="Stock"><a href="#Stock" class="headerlink" title="Stock"></a>Stock</h2><p>Suppose there is a system with input $f(t)$ at each moment. The input starts decaying with $g(t)$ by the time it enters the system, where $g(t)$ is its remaining percentage.</p>
<p><img src="/2023/05/10/CNN/2.png" alt="2"></p>
<center style="font-size:12px;font-weight:bold">Fig. 2. f(t) and g(t)</center><br>

<p>For items that enter the system between $(t,t+\Delta t)$, their margin at $x$ is:</p>
<p>$$f(t)g(x-t)\Delta t$$</p>
<p>Therefore, for items that enter the system between $(t_1,t_2)$, their margin at $x$ is:</p>
<p>$$\int_{t_1}^{t_2}f(t)g(x-t)dt$$</p>
<p>In this case, we take $f(t)$ as the unstable input and $g(t)$ as the stable output to compute the stock of a system after some time.</p>
<h2 id="Influence"><a href="#Influence" class="headerlink" title="Influence"></a>Influence</h2><p>In the explaination above, we actually make $dt$ relate to $f(t)$. Namely, $f(t)dt$ is the input of the system during $t$ and $t+\Delta t$. $g(x-t)$ has nothing to do with $dt$. It is just a decaying factor.</p>
<p>However, we can also take $g(x-t)dt$ as a whole. This will make the $t$ and  $f * g$ more meaningful. For example, we can regard $f(t)$ as a thing happens at $t$ and $g(t)$ as its influence over time. Then $f*g$ can be regarded as the totally influence at $x$ caused by things that happen during $t_1$ and $t_2$:</p>
<p>$$\int_{t_1}^{t_2}f(t)g(x-t)dt$$</p>
<p>To go a step further, we can take $t$ as position, $f(t)$ as a thing happens at $t$ and keep moving from $t$ to $x$, $g(t)$ as its award or loss per distance. For example, some people whose weight $f(t)$ starts running as $t$ and they will loss $g(t)$ kg per meter. Then $f*g$ is the weight loss of all. (the fatter a person is, the more contribution he&#x2F;she makes).</p>
<p><img src="/2023/05/10/CNN/3.png" alt="3"></p>
<center style="font-size:12px;font-weight:bold">Fig. 3. weight and weight loss</center>

<h2 id="Convolution-in-CNN"><a href="#Convolution-in-CNN" class="headerlink" title="Convolution in CNN"></a>Convolution in CNN</h2><p>For convolution in CNN, we&#39;d better expand its dimension to 2-D and take $t$ as a discrete number which represents position $(x,y)$. Then, we can take $X$ as function $f$ and $W$ as $g$. Now the meaning of convolution in CNN can be interpreted as the influence that pixels around $f(x,y)$ exert on pixel $f(x,y)$ or the certain average feature of the pixels around $f(x,y)$. </p>
<p>$$f * g&#x3D;\sum _{i,j}f(i,j)g(x-i,y-j)$$</p>
<p><img src="/2023/05/10/CNN/4.png" alt="4"></p>
<center style="font-size:12px;font-weight:bold">Fig. 4. Convolution in CNN</center><br>

<p>The phrase certain average feature may be a little confusing. However, if we regard the convolution kernel $W$ as a feature filter that extract specific features of a region of an image, it makes sense. Different filters extract different features and their parameters are totally learnt by the agent itself.</p>
<p><img src="/2023/05/10/CNN/5.png" alt="5"></p>
<center style="font-size:12px;font-weight:bold">Fig. 5. The feature filter</center><br>

<p>There are still some differences between convolution and the convolutional layer, that is, $g$ is not the kernel we actually use. For example, convolution computes $f(x-1,y-1)g(1,1)$ rather than $f(x-1,y-1)g(-1,-1)$. However, in CNN, we actually compute $f(x-1,y-1)g(-1,-1)$. In fact, it doesn&#39;t matter. We just need to turn $g$ around and we get the $W$ we use in CNN. Hence, what the agent computes is <strong>cross-correlation</strong> rather than convolution but the results are the same so we just call it convolution.</p>
<h1 id="Hyperparameters-of-convolutional-layer"><a href="#Hyperparameters-of-convolutional-layer" class="headerlink" title="Hyperparameters of convolutional layer"></a>Hyperparameters of convolutional layer</h1><p>Kernel size, padding, stride, output (input) channels are the new hyperparameters that we can adjust in the convolutional layer. </p>
<ol>
<li>Kernel size determines the size of window. It is the simplest parameter among them. We usually set it to $3\times3$ or $5\times5$ for 2-D input.</li>
<li>Padding is used to solve the dimensionality reduction problem of the output image.</li>
<li>Stride determines the movement of window. The stride we use above is $1\times1$.</li>
<li>The number of input channels is not a hyperparameter of the convolutional layer but it determines the size of input.</li>
<li>The number of output channels determines the number of images the convolutional layer outputs.</li>
</ol>
<h2 id="Kernel-size"><a href="#Kernel-size" class="headerlink" title="Kernel size"></a>Kernel size</h2><p>It is recommended to adopt a small kernel size and deep neural networks instead of a large kernel size and shallow neural networks. Their results are almost equivalent but the speed of former is faster as the speed is inversely related to the number of data (For kernel, this is generally on the order of the square). That&#39;s why we always make its size 3, 5 instead of 11, 13.</p>
<blockquote>
<p>The size here refers to the length of every dimension.</p>
</blockquote>
<p>Such a structure is very reasonable as what the kernel does is to gather the features of the edge points together. Therefore, in the final layer, data that the kernel see is the linear combination of all pixels in the image. Namely, its kernel size is actually the whole image:</p>
<p><img src="/2023/05/10/CNN/5_1.png" alt="5_1"></p>
<center style="font-size:12px;font-weight:bold">Fig. 6. Gathering of data</center>

<h2 id="Padding"><a href="#Padding" class="headerlink" title="Padding"></a>Padding</h2><p>Padding adds extra rows and columns around the input images:</p>
<p><img src="/2023/05/10/CNN/6.png" alt="6"></p>
<center style="font-size:12px;font-weight:bold">Fig. 7. Padding</center><br>

<p>After padding, the shape of output becomes:</p>
<p>$$(n_h-k_h+1+p_h)\times(n_w-k_w+1+p_w)$$</p>
<p>Generally, we let:</p>
<p>$$<br>p_h&#x3D;k_h-1,\space p_w&#x3D;k_w-1<br>$$</p>
<p>so that the shape of output is the same as the shape of input. For convinience, we often take $k_h\And k_w$ to be odd number. Therefore, we can pad $p_h&#x2F;2$ ($p_w&#x2F;2$)<br> on both sides. If $k_h\And k_w$ are even, $\lceil p_h&#x2F;2\rceil$ on the upper side and $\lfloor p_h&#x2F;2\rfloor$ on the lower side or vice versa.</p>
<h2 id="Stride"><a href="#Stride" class="headerlink" title="Stride"></a>Stride</h2><p>Stirde refers to the step size of the kernel window on the row and column. When stride is too small, the agent will need a amount of computation to get a small ouput.</p>
<p><img src="/2023/05/10/CNN/7.png" alt="7"></p>
<center style="font-size:12px;font-weight:bold">Fig. 8. Stride (3*2)</center><br>

<p>For a certain stride $s_h\times s_w$, the shape of output becomes:</p>
<p>$$\lfloor(n_h-k_h+p_h)&#x2F;s_h+1\rfloor\times\lfloor(n_w-k_w+p_w)&#x2F;s_w+1\rfloor<br>$$</p>
<p>If:</p>
<p>$$<br>p_h&#x3D;k_h-1,\space p_w&#x3D;k_w-1<br>$$</p>
<p>The shape becomes:</p>
<p>$$\lfloor(n_h-1)&#x2F;s_h+1\rfloor\times\lfloor(n_w-1)&#x2F;s_w+1\rfloor<br>$$</p>
<blockquote>
<p>We can roughly regard that the stride will cut the shape of output by a factor of $s_h\And s_w$.</p>
</blockquote>
<h2 id="Multiple-input-channels"><a href="#Multiple-input-channels" class="headerlink" title="Multiple input channels"></a>Multiple input channels</h2><p>The role of multiple input channels is to combine different features together.</p>
<p><img src="/2023/05/10/CNN/8.png" alt="8"></p>
<center style="font-size:12px;font-weight:bold">Fig. 9. Multiple input channels</center>

<ul>
<li>$X$: $c_i\times n_h\times n_W$</li>
<li>$W$: $c_i\times k_h\times k_w$</li>
<li>$Y$: $m_h\times m_w$</li>
</ul>
<p>For each input channel, there is a kernel. The output of the multiple input channels is the <strong>weighted</strong> sum of the output of each channel. Namely, even though there are several input channels, they just generate one output. To a certain extent, a kernel can be regarded as one of the generalized $w$ of a neuron and the multiple input channels are the model parameters of a neuron.</p>
<h2 id="Multiple-output-channels"><a href="#Multiple-output-channels" class="headerlink" title="Multiple output channels"></a>Multiple output channels</h2><p>An output channel generate an output. It contains an independent batch of kernels which form a 3-D kernel. In other words, the multiple input channels are parts of an output channel. That&#39;s why the input channel is not a hyperparameter. Its size totally depends on the data. For examples, the number of input channels should 3 for RGB images and 1 for grayscale images.</p>
<p><img src="/2023/05/10/CNN/9.png" alt="9"></p>
<center style="font-size:12px;font-weight:bold">Fig. 10. Multiple input and output channels</center>

<ul>
<li>$X$: $c_i\times n_h\times n_W$</li>
<li>$W$: $c_o\times c_i\times k_h\times k_w$</li>
<li>$Y$: $c_o\times m_h\times m_w$</li>
<li>$B$ (bias): $c_o\times c_i$, one kernel one bias.</li>
</ul>
<blockquote>
<p>The computational complexity is roughly $O(c_ik_hk_wc_om_hm_w)$, where $c_ik_hk_w$ is the complexity of computing an output pixel and $c_om_hm_w$ is the number of output pixels.</p>
</blockquote>
<h3 id="1-times-1-kernel"><a href="#1-times-1-kernel" class="headerlink" title="$1\times 1$ kernel"></a>$1\times 1$ kernel</h3><p>The $1\times 1$ kernel is a special kernel which doesn&#39;t recognize spatial patterns but just fuses channels. It can be regarded as a dense layer with input $n_hn_w\times c_i$ ($c_i$ is the number of features) and weight $c_ic_o$.</p>
<p><img src="/2023/05/10/CNN/10.png" alt="10"></p>
<center style="font-size:12px;font-weight:bold">Fig. 11. 1x1 kernel</center>

<h1 id="Pooling-layer"><a href="#Pooling-layer" class="headerlink" title="Pooling layer"></a>Pooling layer</h1><p>The pooling layer is another type of layer in CNN. It has two functions:</p>
<ol>
<li>Reduce the sensitivity of convolutional layers to position;</li>
<li>Reduce the computational complexity.</li>
</ol>
<h2 id="Sensitivity-to-position"><a href="#Sensitivity-to-position" class="headerlink" title="Sensitivity to position"></a>Sensitivity to position</h2><p>The convolutional layer is highly sensitive to the position of pixels. This makes a pixel offset will cause a large change in the output image. However, we may not want such a significant change. Therefore, we need pooling layer to reduce these effects.</p>
<p>The hyperparameters of the pooling layer are almost the same as the convolutional layer. But, the pooling layer is just an operator, that is, it doesn&#39;t have any model parameters to learn. Besides, it doesn&#39;t fuse channels. Therefore, the number of input channels is equal to output channels in the pooling layer.</p>
<p>There are generally two types of pooling layer:</p>
<ol>
<li>Maximum pooling <code>nn.MaxPool2d</code>, computes the maximum value of all elements in the pooling window;</li>
<li>Average pooling <code>nn.AvgPool2d</code>, computes the average value of all elements in the pooling window.</li>
</ol>
<h2 id="Computational-complexity"><a href="#Computational-complexity" class="headerlink" title="Computational complexity"></a>Computational complexity</h2><p>Since the pooling layer windows are the same as kernels in convolutional layer, the pooling layer can reduce the dimension of data. </p>
<p>However, because of the increase in data diversities and its consistency with the convolutional layer in dimensionality reduction, the pooling layer are no longer so important.</p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul>
<li><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1VV411478E">从“卷积”、到“图像卷积操作”、再到“卷积神经网络”，“卷积”意义的3次改变</a></li>
</ul>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a>
              <a href="/tags/CNN/" rel="tag"># CNN</a>
          </div>
          <script type="text/javascript">
          var tagsall=document.getElementsByClassName("post-tags")
          for (var i = tagsall.length - 1; i >= 0; i--){
            var tags=tagsall[i].getElementsByTagName("a");
            for (var j = tags.length - 1; j >= 0; j--) {
                var r=Math.floor(Math.random()*75+130);
                var g=Math.floor(Math.random()*75+100);
                var b=Math.floor(Math.random()*75+80);
                tags[j].style.background = "rgb("+r+","+g+","+b+")";
            }
          }                        
          </script>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2023/05/07/NumericalStability/" rel="prev" title="Numerical Stability">
                  <i class="fa fa-angle-left"></i> Numerical Stability
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2023/05/10/FourierTransform/" rel="next" title="Fourier Transform">
                  Fourier Transform <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2026</span>
    <span class="with-love">
      <i class="fas fa-star-of-david"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Chaolv Zeng</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>Word count total: </span>
    <span title="Word count total">164k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>Reading time total &asymp;</span>
    <span title="Reading time total">9:57</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div><script color="240,255,255" opacity="1" zIndex="-1" count="100" src="https://cdn.jsdelivr.net/npm/canvas-nest.js@1/dist/canvas-nest.js"></script>

<!-- <br /> -->
<!-- 网站运行时间的设置 -->
<span id="timeDate">载入天数...</span>
<!-- <span id="times">载入时分秒...</span> -->
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("11/17/2022 8:00:00");//此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); 
        if(String(snum).length ==1 ){snum = "0" + snum;}
        // var times = document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 "+hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
    }
setInterval("createtime()",250);
</script>
    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.31/fancybox/fancybox.umd.js" integrity="sha256-a+H7FYzJv6oU2hfsfDGM2Ohw/cR9v+hPfxHCLdmCrE8=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>




  <script src="/js/third-party/fancybox.js"></script>



  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","mhchem":false,"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
