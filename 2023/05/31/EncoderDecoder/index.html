<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=[object Object]"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', '[object Object]');
</script>
<!-- End Google Analytics -->

  
  <title>The Encoder-Decoder Architecture | JourneyToCoding</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="The Encoder-Decoder Architecture views neural networks in a new perspective. It takes the neural network a kind of signal processor which encode the input and decode it to generate output.">
<meta property="og:type" content="article">
<meta property="og:title" content="The Encoder-Decoder Architecture">
<meta property="og:url" content="https://zclzcl0223.github.io/2023/05/31/EncoderDecoder/index.html">
<meta property="og:site_name" content="JourneyToCoding">
<meta property="og:description" content="The Encoder-Decoder Architecture views neural networks in a new perspective. It takes the neural network a kind of signal processor which encode the input and decode it to generate output.">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://zclzcl0223.github.io/2023/05/31/EncoderDecoder/1.png">
<meta property="og:image" content="https://zclzcl0223.github.io/2023/05/31/EncoderDecoder/2.png">
<meta property="og:image" content="https://zclzcl0223.github.io/2023/05/31/EncoderDecoder/3.png">
<meta property="og:image" content="https://zclzcl0223.github.io/2023/05/31/EncoderDecoder/4.png">
<meta property="og:image" content="https://zclzcl0223.github.io/2023/05/31/EncoderDecoder/5.png">
<meta property="og:image" content="https://zclzcl0223.github.io/2023/05/31/EncoderDecoder/6.png">
<meta property="og:image" content="https://zclzcl0223.github.io/2023/05/31/EncoderDecoder/7.png">
<meta property="og:image" content="https://zclzcl0223.github.io/2023/05/31/EncoderDecoder/8.png">
<meta property="og:image" content="https://zclzcl0223.github.io/2023/05/31/EncoderDecoder/9.png">
<meta property="article:published_time" content="2023-05-31T05:14:00.000Z">
<meta property="article:modified_time" content="2023-08-09T05:50:40.000Z">
<meta property="article:author" content="ChaosTsang">
<meta property="article:tag" content="Deep Learning">
<meta property="article:tag" content="CNN">
<meta property="article:tag" content="RNN">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://zclzcl0223.github.io/2023/05/31/EncoderDecoder/1.png">
  
    <link rel="alternate" href="/atom.xml" title="JourneyToCoding" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/%5Bobject%20Object%5D">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">JourneyToCoding</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">Code for fun</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/">home</a>
        
          <a class="main-nav-link" href="/about/">about</a>
        
          <a class="main-nav-link" href="/tags/">tags</a>
        
          <a class="main-nav-link" href="/categories/">categories</a>
        
          <a class="main-nav-link" href="/archives/">archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://zclzcl0223.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-EncoderDecoder" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/05/31/EncoderDecoder/" class="article-date">
  <time class="dt-published" datetime="2023-05-31T05:14:00.000Z" itemprop="datePublished">2023-05-31</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Dive-Into-Deep-Learning/">Dive Into Deep Learning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      The Encoder-Decoder Architecture
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <span id="more"></span>

<h1 id="Another-view-to-ANNs"><a href="#Another-view-to-ANNs" class="headerlink" title="Another view to ANNs"></a>Another view to ANNs</h1><p>Firstly, let&#39;s explain CNNs from the perspective of encoder-decoder:</p>
<p><img src="/2023/05/31/EncoderDecoder/1.png" alt="1"></p>
<center style="font-size:12px; font-weight:bold">Fig. 1. View CNNs in a new perspective</center><br>

<p>When the agent extracts features from the input using convolutional layers and pooling layer, it encodes the image as a vector that better reflects some certain characteristics of the image. After that, the agent decodes these features and generates output.</p>
<p>Similarly, for CNNs, the embedding layer and LSTM (GRU) layer encode texts as vectors. Then, The agent decodes these message and generates output.</p>
<p><img src="/2023/05/31/EncoderDecoder/2.png" alt="2"></p>
<center style="font-size:12px; font-weight:bold">Fig. 2. View RNNs in a new perspective</center><br>

<p>This is the encoder-decoder architecture. In this architecture, the encoder deals with input , passes information to the decoder and the decoder generates output. More generally, the information the encoder passes to the decoder is called <em>state</em> which contains features of the input. The decoder can also receive new input so that it will get more information about the thing it copes with.</p>
<p><img src="/2023/05/31/EncoderDecoder/3.png" alt="3"></p>
<center style="font-size:12px; font-weight:bold">Fig. 3. Encoder-Decoder</center><br>

<h1 id="Seq2seq"><a href="#Seq2seq" class="headerlink" title="Seq2seq"></a>Seq2seq</h1><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1409.3215.pdf">Sequence to sequence</a> (<a href="(https://arxiv.org/pdf/1406.1078.pdf)">Seq2seq</a>) is a kind of neural network using the encoder-decoder architecture to complete the task of <em>sequence to sequence learning</em>. It is applied mainly in machine translation.</p>
<p><img src="/2023/05/31/EncoderDecoder/4.png" alt="4"></p>
<center style="font-size:12px; font-weight:bold">Fig. 4. Machine translation and Seq2seq (English to French)</center><br>

<p><img src="/2023/05/31/EncoderDecoder/5.png" alt="5"></p>
<center style="font-size:12px; font-weight:bold">Fig. 5. Structure of Seq2seq</center><br>

<h2 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h2><p>The encoder is a DRNN without regression layers. It works like the other DRNNs: take the source sequence (English) as input and output vectors. What is worth noticing is the <em>embedding layer</em>. In fact, it is an improvement on one-hot encoding. When using one-hot encoding, we have to use a $t\times m$ tensor for a sentence with $t$ tokens and a vocabulary with $m$ tokens. This is space-consuming. However, the embedding layer can map $m$-dimensional vectors to $k$-dimensional vectors ($k&lt;m$). This cuts the size of space we have to spend to store a sentence.</p>
<h2 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h2><p>The decoder takes the target sequence (French) $\mathbf{X}$ as a part of input and also uses the embedding layer. In addition, it also takes the last hidden state of the last RNN layer of the encoder $\mathbf{C}$ as a part of input. $\mathbf{C}$ contains the context about the source sequence. $[\mathbf{X}, \mathbf{C}]$ form the complete input and are fed to the decoder. The last hidden states of all the RNN layers of the encoder are also used to initialize the hidden states of all the RNN layers of the decoder. Hence, the depth of RNN layer in encoder and decoder should be the same (that is $n$).</p>
<blockquote>
<p>For convenience, the length of all the sequences ($t$) is fixed. Hence, for the sentences that are longer than $t$, they are cut to $t$; for the sentences that are shorter than $t$, they are filled up to $t$. When computing the loss using softmax, we should only consider the valid length and set the filled part to $0$.</p>
</blockquote>
<h2 id="Training-and-predicting"><a href="#Training-and-predicting" class="headerlink" title="Training and predicting"></a>Training and predicting</h2><p>When training, what we feed to the decoder are the correct translation sequence, like Fig. 4. However, when predicting, we only feed the token that indicates the start of a sentence (<code>&lt;bos&gt;</code>) to the decoder and the output is fed as input to the decoder until the decoder generates the token that indicates the end of a sentence (<code>&lt;eos&gt;</code>).</p>
<p><img src="/2023/05/31/EncoderDecoder/6.png" alt="6"></p>
<center style="font-size:12px; font-weight:bold">Fig. 6. Making prediction (translation)</center>

<h2 id="Assessment"><a href="#Assessment" class="headerlink" title="Assessment"></a>Assessment</h2><p><a target="_blank" rel="noopener" href="https://aclanthology.org/P02-1040.pdf">Bilingual evaluation understudy</a> (BLEU) is a method to evaluate the accuracy of machine translation or other sequence to sequence application. BLEU is always range between $0$ and $1$. The larger BLEU is, the more accurate the prediction is. It is defined as:</p>
<p>$$<br>\exp(\min(0,1-\frac{\text{len} _{label}}{\text{len} _{pred}}))\prod\limits _{n&#x3D;1}^k p _n ^{1&#x2F;2^n}<br>$$</p>
<p>where $\text{len} _{label}$ is the number of tokens in target sequence $y$ while $\text{len} _{pred}$ is the number of tokens in prediction sequence $\hat{y}$. Since shorter predictions contain less tokens, which makes $\prod\limits _{n&#x3D;1}^k p _n ^{1&#x2F;2^n}$ bigger, there will be penalty for short predictions.</p>
<blockquote>
<p>The smaller $\text{len} _{pred}$ is, the larger $\text{len} _{label}&#x2F;{\text{len} _{pred}}$ is, and thus the smaller $\exp(\min(...))$ is.</p>
</blockquote>
<p>$p_n$ is the accuracy of $n$-grams, namely, the accuracy of prediction of $n$ tokens in sequence. Due to the difficulty to predict long sequence, long sequence will get higher weight than short sequence. $1&#x2F;2^n$ help us do this. Because $p_n$ is often smaller than $1$ and the bigger $n$ is, the smaller $1&#x2F;2^n$ is, long sequence will always get larger value even though $p_n$ keeps the same.</p>
<p><img src="/2023/05/31/EncoderDecoder/7.png" alt="7"></p>
<center style="font-size:12px; font-weight:bold">Fig. 7. Curve of p<sub>n</sub><sup>1/2<sup>n</sup></sup>(p<sub>n</sub>=0.5)</center>

<h1 id="Beam-search"><a href="#Beam-search" class="headerlink" title="Beam search"></a>Beam search</h1><p>When predicting a token using softmax, we are actually using <em>Greedy search</em>. That is, for each timestep, the token we choose is the one with the highest probability.</p>
<p><img src="/2023/05/31/EncoderDecoder/8.png" alt="8"></p>
<center style="font-size:12px; font-weight:bold">Fig. 8. Greedy search</center> <br>

<p>However, local optimum does not necessarily lead to global optimum. If we choose B in timestep1 and feed it to decoder in timestep2, maybe we will get higher accuracy for the prediction in timestep2. <em>Exhaustive search</em> can solver this problem but it is time-consuming.</p>
<p>Beam search is a compromise between greedy search and Exhaustive search. In the first timestep, beam search will choose <em>k</em> tokens with high probability and finally generate <em>k</em> condidate sequence. Then, sequence with the highest probability is chosen. <em>k</em> is a hyperparameter called <em>beam size</em>.</p>
<p><img src="/2023/05/31/EncoderDecoder/9.png" alt="9"></p>
<center style="font-size:12px; font-weight:bold">Fig. 9. Beam search</center>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zclzcl0223.github.io/2023/05/31/EncoderDecoder/" data-id="clzik0lc90024v47k69bibu6b" data-title="The Encoder-Decoder Architecture" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/CNN/" rel="tag">CNN</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Deep-Learning/" rel="tag">Deep Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/RNN/" rel="tag">RNN</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/06/02/OptimizationAlgorithms/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Optimization Algorithms
        
      </div>
    </a>
  
  
    <a href="/2023/05/29/CommonRNNModels/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Common RNN Models</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Advanced-Model/">Advanced Model</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/CS7313-Computational-Complexity/">CS7313: Computational Complexity</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Configuration/">Configuration</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Dive-Into-Deep-Learning/">Dive Into Deep Learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/GNN/">GNN</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Kernel-Method/">Kernel Method</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/MATH6005-Matrix-Theory/">MATH6005: Matrix Theory</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning-by-AndrewNg/">Machine Learning by AndrewNg</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Math/">Math</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Paper/">Paper</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Programming-Language/">Programming Language</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Tool/">Tool</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Attention-Mechanism/" rel="tag">Attention Mechanism</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CNN/" rel="tag">CNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Configuration/" rel="tag">Configuration</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Data-Analysis/" rel="tag">Data Analysis</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Dataset-Distillation/" rel="tag">Dataset Distillation</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Decision-Trees/" rel="tag">Decision Trees</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Deep-Learning/" rel="tag">Deep Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GCN/" rel="tag">GCN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GNN/" rel="tag">GNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Generative-AI/" rel="tag">Generative AI</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hexo/" rel="tag">Hexo</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Lab/" rel="tag">Lab</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux/" rel="tag">Linux</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-Learning/" rel="tag">Machine Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Markup-Language/" rel="tag">Markup Language</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Math/" rel="tag">Math</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Paper/" rel="tag">Paper</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/" rel="tag">Python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RNN/" rel="tag">RNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Recommender-System/" rel="tag">Recommender System</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Reinforcement-Learning/" rel="tag">Reinforcement Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SVM/" rel="tag">SVM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Supervised-Learning/" rel="tag">Supervised Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Theoretical-Computer-Science/" rel="tag">Theoretical Computer Science</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Tool/" rel="tag">Tool</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Unsupervised-Learning/" rel="tag">Unsupervised Learning</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Attention-Mechanism/" style="font-size: 11.82px;">Attention Mechanism</a> <a href="/tags/CNN/" style="font-size: 12.73px;">CNN</a> <a href="/tags/Configuration/" style="font-size: 10px;">Configuration</a> <a href="/tags/Data-Analysis/" style="font-size: 10px;">Data Analysis</a> <a href="/tags/Dataset-Distillation/" style="font-size: 13.64px;">Dataset Distillation</a> <a href="/tags/Decision-Trees/" style="font-size: 10px;">Decision Trees</a> <a href="/tags/Deep-Learning/" style="font-size: 20px;">Deep Learning</a> <a href="/tags/GCN/" style="font-size: 11.82px;">GCN</a> <a href="/tags/GNN/" style="font-size: 15.45px;">GNN</a> <a href="/tags/Generative-AI/" style="font-size: 10.91px;">Generative AI</a> <a href="/tags/Hexo/" style="font-size: 10.91px;">Hexo</a> <a href="/tags/Lab/" style="font-size: 10.91px;">Lab</a> <a href="/tags/Linux/" style="font-size: 16.36px;">Linux</a> <a href="/tags/Machine-Learning/" style="font-size: 17.27px;">Machine Learning</a> <a href="/tags/Markup-Language/" style="font-size: 10px;">Markup Language</a> <a href="/tags/Math/" style="font-size: 19.09px;">Math</a> <a href="/tags/Paper/" style="font-size: 14.55px;">Paper</a> <a href="/tags/Python/" style="font-size: 18.18px;">Python</a> <a href="/tags/RNN/" style="font-size: 10.91px;">RNN</a> <a href="/tags/Recommender-System/" style="font-size: 10.91px;">Recommender System</a> <a href="/tags/Reinforcement-Learning/" style="font-size: 10.91px;">Reinforcement Learning</a> <a href="/tags/SVM/" style="font-size: 10px;">SVM</a> <a href="/tags/Supervised-Learning/" style="font-size: 11.82px;">Supervised Learning</a> <a href="/tags/Theoretical-Computer-Science/" style="font-size: 15.45px;">Theoretical Computer Science</a> <a href="/tags/Tool/" style="font-size: 18.18px;">Tool</a> <a href="/tags/Unsupervised-Learning/" style="font-size: 11.82px;">Unsupervised Learning</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/06/">June 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/01/">January 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/12/">December 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/06/">June 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/05/">May 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/04/">April 2023</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2024/06/17/Diffusion/">Diffusion</a>
          </li>
        
          <li>
            <a href="/2024/01/20/VAE/">VAE</a>
          </li>
        
          <li>
            <a href="/2023/12/02/CountingComplexity/">Computational Complexity: Complexity of Counting</a>
          </li>
        
          <li>
            <a href="/2023/11/24/MatrixTheory5/">MatrixTheory: 特殊矩阵与矩阵分解</a>
          </li>
        
          <li>
            <a href="/2023/11/13/RandomizedComputation/">Computational Complexity: Randomized Computation</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 ChaosTsang<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/" class="mobile-nav-link">home</a>
  
    <a href="/about/" class="mobile-nav-link">about</a>
  
    <a href="/tags/" class="mobile-nav-link">tags</a>
  
    <a href="/categories/" class="mobile-nav-link">categories</a>
  
    <a href="/archives/" class="mobile-nav-link">archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>





<script src="/js/script.js"></script>





  </div>
</body>
</html>