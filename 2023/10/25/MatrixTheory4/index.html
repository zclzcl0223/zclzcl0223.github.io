<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=[object Object]"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', '[object Object]');
</script>
<!-- End Google Analytics -->

  
  <title>MatrixTheory: 特征值、特征向量 | JourneyToCoding</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="矩阵理论第五课，主要讲特征值、特征向量与内积。">
<meta property="og:type" content="article">
<meta property="og:title" content="MatrixTheory: 特征值、特征向量">
<meta property="og:url" content="https://zclzcl0223.github.io/2023/10/25/MatrixTheory4/index.html">
<meta property="og:site_name" content="JourneyToCoding">
<meta property="og:description" content="矩阵理论第五课，主要讲特征值、特征向量与内积。">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-10-25T04:52:41.000Z">
<meta property="article:modified_time" content="2023-12-17T10:51:12.000Z">
<meta property="article:author" content="ChaosTsang">
<meta property="article:tag" content="Math">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="JourneyToCoding" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/%5Bobject%20Object%5D">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">JourneyToCoding</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">Code for fun</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/">home</a>
        
          <a class="main-nav-link" href="/about/">about</a>
        
          <a class="main-nav-link" href="/tags/">tags</a>
        
          <a class="main-nav-link" href="/categories/">categories</a>
        
          <a class="main-nav-link" href="/archives/">archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://zclzcl0223.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-MatrixTheory4" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/10/25/MatrixTheory4/" class="article-date">
  <time class="dt-published" datetime="2023-10-25T04:52:41.000Z" itemprop="datePublished">2023-10-25</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/MATH6005-Matrix-Theory/">MATH6005: Matrix Theory</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      MatrixTheory: 特征值、特征向量
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <span id="more"></span>

<h1 id="不变子空间"><a href="#不变子空间" class="headerlink" title="不变子空间"></a>不变子空间</h1><p>$V: \mathbb{F}上$线性空间，$T \in L(V)$，$U$是$V$的子空间。若$\forall \alpha \in U$，$T(\alpha) \in U$，即$T(U) \subseteq U$，则称$U$是$V$的关于$T$的不变子空间。</p>
<blockquote>
<p>平凡子空间$\{0\}$和$V$是不变子空间。线性变换的零空间和像空间也是不变子空间。</p>
</blockquote>
<h2 id="特征值、特征向量"><a href="#特征值、特征向量" class="headerlink" title="特征值、特征向量"></a>特征值、特征向量</h2><p>若$V &#x3D; U _1 \oplus U _2 \oplus U _3$，其中$U _1...U _3$均为不变子空间，则以不变子空间的基为基向量的线性变换矩阵为分块对角矩阵。进一步地，若存在$\text{dim}V$个不变子空间且这些子空间的直和为$V$，则线性变换矩阵就是对角矩阵，对角线的值为特征值，不变子空间的基为特征向量。</p>
<p>$T(\alpha _i) &#x3D; \lambda _i \alpha _i$，$\lambda _i$为特征值，$\alpha _i$为特征向量，其中$\lambda \in \mathbb{F}$，$\alpha \not &#x3D;{0}$。</p>
<blockquote>
<p>这和$Ax &#x3D; \lambda x$是等价的，只不过矩阵的只能是有限维的。</p>
<p>不同特征值对应的特征向量线性无关。</p>
<p>$T\in L(V)$，$\mathbb{F}&#x3D;C$，$\text{dim}V&#x3D;n$，则$T$在<em>复数域</em>内一定有特征值。</p>
</blockquote>
<p>$T \in L(V)$，$U$是不变子空间</p>
<ol>
<li>$T | _U \in L(U)$，缩小算子$T$的定义域。</li>
<li>$T &#x2F; U: V&#x2F;U \to V&#x2F;U$，商空间到商空间的映射，只有在不变子空间下才合理。如$(T&#x2F;U)(\alpha + U) &#x3D; T(\alpha) + U$</li>
</ol>
<h2 id="简单的基下矩阵"><a href="#简单的基下矩阵" class="headerlink" title="简单的基下矩阵"></a>简单的基下矩阵</h2><p>对$T \in L(V)$，$\mathbb{F}&#x3D;C$，$\text{dim}V &#x3D; n$，一定存在一组基，使得基下的矩阵是上三角。</p>
<blockquote>
<p>证明：即证$T(\alpha _i) \in \text{span}[\alpha _1, ...,\alpha _i]$。要用到商空间和商空间的线性变换。</p>
</blockquote>
<p>等价地，复数域上的任意一个方阵一定相似于一个上三角。</p>
<h2 id="特征子空间"><a href="#特征子空间" class="headerlink" title="特征子空间"></a>特征子空间</h2><p>特征子空间$E(T, \lambda) &#x3D; \text{null}(T - \lambda I)$，内部元素为所有的特征向量+零向量。不同特征值的特征子空间的和是直和。</p>
<p>$$<br>E(T,\lambda _1) \oplus E(T, \lambda _2) \oplus...\oplus E(T, \lambda _m) \le V<br>$$</p>
<blockquote>
<p>取等当且仅当存在一组基使得基下的矩阵为对角矩阵；<br>也当且仅当$V$可以分成$n$个一维不变子空间的直和。</p>
</blockquote>
<h1 id="内积空间"><a href="#内积空间" class="headerlink" title="内积空间"></a>内积空间</h1><p>内积空间：定义了内积的线性空间。其中，内积满足：</p>
<ol>
<li>$(\alpha, \beta)&#x3D;(\beta, \alpha)$</li>
<li>$(\alpha+\beta,\gamma)&#x3D;(\alpha, \gamma)+(\beta,\gamma)$</li>
<li>$(k\cdot\alpha, \beta)&#x3D;k(\beta, \alpha)$</li>
<li>$(\alpha,\alpha)\ge 0,\text{iff}\space \alpha&#x3D;0\space \text{取等}。$</li>
</ol>
<p>对复数域，坐标相乘相加的内积要对第二项取共轭，即：</p>
<p>$$<br>(\alpha, \beta)&#x3D;\alpha \overline{\beta}<br>$$</p>
<blockquote>
<p>实数域的内积空间称<em>欧式空间</em>，复数域上的称<em>酉空间</em>。</p>
</blockquote>
<h2 id="内积的性质"><a href="#内积的性质" class="headerlink" title="内积的性质"></a>内积的性质</h2><ol>
<li>设$\beta \in V$，$\beta$固定。定义$T: V \to \mathbb{F}$，$\forall \alpha \in V$，$T(\alpha) &#x3D; (\alpha, \beta)$，$T \in L(V,\mathbb{F})$</li>
<li>$(0,\beta) &#x3D; (\beta,0)&#x3D;0$</li>
<li>$(\alpha, \beta + \gamma)&#x3D;(\alpha, \beta)+(\alpha,\gamma)$</li>
<li>$(\alpha, k\cdot\beta)&#x3D;\overline{k}(\alpha, \beta)$</li>
<li>$(\sum _{i&#x3D;1} ^m x _i \alpha _i,\sum _{j&#x3D;1} ^n \gamma _j \beta _j )&#x3D;\sum\sum x _i \overline{y _j}(\alpha _i, \beta _j)$</li>
</ol>
<h2 id="向量的长度"><a href="#向量的长度" class="headerlink" title="向量的长度"></a>向量的长度</h2><p>$$<br>||\alpha|| &#x3D; \sqrt{(\alpha, \alpha)}<br>$$</p>
<ol>
<li>$||k\alpha|| &#x3D; |k|\sqrt{(\alpha, \alpha)}$</li>
<li>$|(\alpha, \beta)|\le ||\alpha||||\beta||$（柯西不等式）。当且仅当两者线性相关等号成立。</li>
<li>$||\alpha + \beta||\le ||\alpha||+||\beta||$（三角不等式）。当且仅当两者共线等号成立。</li>
</ol>
<h1 id="正交向量组、标准正交向量组、标准正交基"><a href="#正交向量组、标准正交向量组、标准正交基" class="headerlink" title="正交向量组、标准正交向量组、标准正交基"></a>正交向量组、标准正交向量组、标准正交基</h1><p>正交向量组：向量间两两正交。正交向量组一定线性无关。</p>
<p>校准正交组：单位化的正交向量组。</p>
<p>标准正交基：可由一组线性无关的基经过斯密特正交化得到。</p>
<blockquote>
<p>斯密特正交化前后的基张成的空间是相同的空间。所以$\exists$标准正交基，使得基下的矩阵为上三角。</p>
<p>实数域的转置等价于复数域的共轭转置。</p>
<p>向量$\alpha$在标准正交基下的线性表示系数为$(\alpha, \gamma _i)$。</p>
</blockquote>
<h2 id="正交补空间"><a href="#正交补空间" class="headerlink" title="正交补空间"></a>正交补空间</h2><p>$V$内积空间，$U$子空间，定义：</p>
<p>$$<br>U ^{\perp} &#x3D; \{\alpha \in V: \forall \beta \in U, (\alpha,\beta)&#x3D;0\}<br>$$</p>
<p>$U ^{\perp}$是$V$的子空间，是$U$的正交不空间，且$V&#x3D;U \oplus U ^{\perp}$。</p>
<h3 id="度量矩阵"><a href="#度量矩阵" class="headerlink" title="度量矩阵"></a>度量矩阵</h3><p>$$<br>G &#x3D;<br>\begin{bmatrix}<br>    (\alpha _1, \alpha _1)...(\alpha _n, \alpha _1)\\<br>    ... &amp;\\\<br>    (\alpha _1, \alpha _n)...(\alpha _n, \alpha _n)<br>\end{bmatrix}<br>$$</p>
<p>则任意两个向量在指定基$\alpha _1,...,\alpha _n$下的内积为：</p>
<p>$$<br>(\alpha, \beta)&#x3D;\overline{y}Gx<br>$$</p>
<p>其中$y$是$\beta$在基下的坐标，$x$是$\alpha$在基下的坐标。</p>
<blockquote>
<p>实数域的$G$是个对称的正定矩阵。<br>复数域的$G$是个共轭对称的正定矩阵。（共轭转置：$G ^*&#x3D;G$）<br>正定一定对称。</p>
</blockquote>
<p>一个正定矩阵定义一个内积。不同基下的$G$矩阵合同。</p>
<h3 id="投影变换"><a href="#投影变换" class="headerlink" title="投影变换"></a>投影变换</h3><p>$V &#x3D; U \oplus U ^\perp,\forall \alpha \in V,\alpha &#x3D; \beta + \gamma,\beta \in U, \gamma \in U ^\perp$。定义$P _U: V \to V,\forall \alpha \in V, \alpha &#x3D; \beta + \gamma,P _U(\alpha)&#x3D;\beta$。$P _U$即为$V$中的向量$\alpha$到$U$的投影变换。投影变换有性质：</p>
<ol>
<li>$\text{null}P _U &#x3D; U ^\perp$</li>
<li>$\text{range}P _U &#x3D; U$</li>
<li>$P ^2 _U &#x3D; P _U$</li>
<li>设$\epsilon _1,...,\epsilon _k$为$U$的一组标准正交基，则$P _U (\alpha) &#x3D; (\alpha, \epsilon _1) \epsilon _1+...+(\alpha, \epsilon _k) \epsilon _k$</li>
</ol>
<h3 id="最佳近似向量"><a href="#最佳近似向量" class="headerlink" title="最佳近似向量"></a>最佳近似向量</h3><p>内积空间$V$，子空间$U$，$\beta \in V$，$\beta \notin U$，若有$\alpha \in U$，使得$\forall \gamma \in U$，$||(\beta - \alpha)|| \le ||\beta -\gamma||$，则称$\alpha$为$\beta$在$U$的最佳近似向量。</p>
<blockquote>
<p>即距离最小的。实际上$\alpha$就是$\beta$的正交投影向量。</p>
<p>用途：求矛盾方程的最佳近似解（又称最小二乘解）。如$Ax&#x3D;b$没有解，则$A ^TAx&#x3D;A^Tb$求出来的$x$就是最佳近似解。因为没有解，所以$b$不在列空间$C$中，列空间的正交补空间为$N(A ^T)$，设$A x _0$为最佳近似向量，则b在正交补空间的投影可表示为$b - A x _0$，正交补空间为$N(A ^T)$，所以有$A ^T(b - Ax _0)&#x3D;0$。</p>
</blockquote>
<h2 id="内积空间的线性变换"><a href="#内积空间的线性变换" class="headerlink" title="内积空间的线性变换"></a>内积空间的线性变换</h2><p>内积空间$V$，算子$T \in L(V)$。若$\forall \alpha,\beta \in V,(\alpha,\beta)&#x3D;(T(\alpha),T(\beta))$，则称该变换为等积变换。对应地，有：</p>
<ul>
<li>$||\alpha||&#x3D;||T(\alpha)||$，等长变换；</li>
<li>$||\alpha-\beta||&#x3D;||T(\alpha)-T(\beta)||$，等距变换；</li>
<li>$\epsilon _1,...,\epsilon _k$为$V$的标准正交基，$T(\epsilon _1),...,T(\epsilon _k)$也是标准正交基；</li>
<li>$T(\epsilon _1,..,\epsilon _n)&#x3D;(\epsilon _1,..,\epsilon _n)A _{n\times n},A ^* A &#x3D; A A ^* &#x3D;E$，这种变换称正交变换，即保持内积的变换。</li>
</ul>
<p>这几个都是等价的，也称正交变换。</p>
<blockquote>
<p>伴随变换：$(T(\alpha),\beta)&#x3D;(\alpha,S(\beta))$，其基下矩阵满足$A &#x3D; B ^*$。</p>
<p>自伴随$T&#x3D;S$，此时$A ^*&#x3D;A$</p>
</blockquote>
<h1 id="广义特征向量"><a href="#广义特征向量" class="headerlink" title="广义特征向量"></a>广义特征向量</h1><p>对$T\in L(V)$，$\lambda$为特征值，$(T -\lambda I) ^j (\alpha) &#x3D; 0$，$j$为正整数，$\alpha \ne 0$，则称$\alpha$为特征值$\lambda$的广义特征向量。$G(T,\lambda)$为广义特征子空间。</p>
<ol>
<li>$\{0\}&#x3D;\text{null}T ^0 \subseteq \text{null}T _1\subseteq ... \subseteq \text{null}T ^k$</li>
<li>若$\text{null}T ^{k+1} &#x3D;\text{null} ^k$，则$\text{null} ^{n+k+1}&#x3D;\text{null} ^{n+k}$</li>
<li>$\text{dim}V&#x3D;n$，则$\text{null}T ^n&#x3D;\text{null} T ^{n+1}$</li>
<li>$\text{dim}V&#x3D;n$，则$V &#x3D; \text{null} T ^n \oplus \text{range}T ^n$</li>
</ol>
<ul>
<li>$G(T,\lambda)&#x3D;\text{null}(T-\lambda I) ^{\text{dim}V}$，$T\in L(V)$，$\lambda _1,...,\lambda _m$是不同特征值，其对应的不同广义特征向量$\alpha _1,...,\alpha _m$线性无关。</li>
</ul>
<blockquote>
<p>证明：分别作用线性变换使得只有一项留下。如，定义$k$为使得$(T-\lambda _1 I) ^k (\alpha _1)\ne 0$的最大整数。</p>
</blockquote>
<h2 id="幂零变换"><a href="#幂零变换" class="headerlink" title="幂零变换"></a>幂零变换</h2><p>$N \in L(V)$，若$N ^k &#x3D; 0$，则称$N$为幂零变换，类似于幂零矩阵。对幂零变换，一定有$N ^{\text{dim}V}&#x3D;0$。</p>
<p>存在某个基，使得幂零变换在基下的上三角矩阵的对角线元素全为0，即幂零变换的特征值都是0。（因为上三角矩阵的对角线元素就是特征值）</p>
<p>$T \in L(V)$，$\lambda _1,...,\lambda _m$为不同特征值，则：</p>
<ol>
<li>$V &#x3D; G(T,\lambda _1)\oplus...\oplus G(T,\lambda _m)$;</li>
<li>$G(T,\lambda _i)$是$T$的不变子空间；</li>
<li>$(T - \lambda _i I) | _{G(T,\lambda _i)}$是幂零变换。</li>
</ol>
<blockquote>
<p>对2：$\forall \alpha \in G(T, \lambda _i) &#x3D; \text{null}(T - \lambda _i I) ^k$，有$(T - \lambda _i I) ^k (\alpha) &#x3D; 0$，$(T - \lambda _i I) ^k (T(\alpha)) &#x3D; T((T - \lambda _i I) ^k(\alpha))&#x3D;0$<br>对3：广义特征子空间的定义；</p>
</blockquote>
<blockquote>
<p>不变子空间的基下矩阵为分块对角阵。<br>特征值数、特征子空间维数：几何重数<br>广义特征向量、广义特征子空间维数：代数重数</p>
</blockquote>
<p>$V$存在由广义特征向量构成的基，基下的矩阵为分块对角，块数为特征值数，块的维数该特征值下广义特征子空间的维数。进一步地，该分块对角矩阵可被优化为上三角矩阵。</p>
<p>$$<br>T | _{G(T,\lambda _i)} &#x3D; (T - \lambda _i I) | _{G (T, \lambda _i)} + \lambda _i I | _{G (T, \lambda _i)}<br>$$</p>
<p>即，拆分为一个幂零变换和恒等变换的和。</p>
<h1 id="Jordan标准型"><a href="#Jordan标准型" class="headerlink" title="Jordan标准型"></a>Jordan标准型</h1><p>基下矩阵的进一步简化：只有对角线有值，且次对角线全为1。</p>
<p>$$<br>\begin{bmatrix}<br>    \lambda _1, 1, ...,...\\<br>    0, \lambda _2, 1,...\\<br>    0, 0, \lambda _3, 1<br>\end{bmatrix}<br>$$</p>
<blockquote>
<p>同一个特征值的Jordan块的数目取决于线性无关的特征向量的个数。</p>
</blockquote>
<p>$V$存在由广义特征向量构成的基，基下的矩阵为Jordan标准型。</p>
<blockquote>
<p>先研究幂零变换的Jordan标准型，其他可由幂零变换+恒等变换得到。</p>
</blockquote>
<p>若$N \in L(V)$为幂零变换，则$V$中存在一组向量$\alpha _1, \alpha _2,...,\alpha _m$，及一组非负整数$k _1, k _2,...,$，使得：</p>
<p>$$<br>\begin{align*}<br>    &amp;N ^{k _1}(\alpha _1), ... , N(\alpha _1), \alpha _1 \\<br>    &amp;N ^{k _2}(\alpha _2), ... , N(\alpha _2), \alpha _2 \\<br>    &amp;...\\<br>    &amp;N ^{k _m}(\alpha _m), ... , N(\alpha _m), \alpha _m \\<br>    &amp;N ^{K _i + 1}(\alpha _i)&#x3D;0<br>\end{align*}<br>$$</p>
<p>化为$V$的一组基。其中$m$为Jordan块个数。</p>
<blockquote>
<p>同一个特征值的Jordan块个数等于几何重数。<br>相似于Jordan标准型的特征向量为广义特征向量。</p>
</blockquote>
<h2 id="Hamilton-Cayley-Them"><a href="#Hamilton-Cayley-Them" class="headerlink" title="Hamilton-Cayley Them"></a>Hamilton-Cayley Them</h2><p>线性变换的特征多项式：</p>
<p>$$<br>f _{\tau}(\lambda) &#x3D; (\lambda - \lambda _1) ^{n _1}...(\lambda - \lambda _m) ^{n _m}<br>$$</p>
<p>其中$\sum n _i &#x3D; n$，若将该线性变换带入，即将$\lambda$换为$T$，则$f _\tau (T)$是一个零变换。该多项式称为零化多项式。</p>
<p>由该定理，$A ^n$以及更高次的$A _{n+1}$都可由$A ^{n-1},...,E$线性表示。若$A$可逆，则逆也可以由$A ^{n-1},...,E$线性表示。</p>
<h3 id="最小多项式"><a href="#最小多项式" class="headerlink" title="最小多项式"></a>最小多项式</h3><p>$A$的零化多项式中，次数$n$最低的首一多项式（最高项系数为1），记作$m _A(\lambda)$。</p>
<ol>
<li>最小多项式是唯一的；</li>
<li>设$f(\lambda)$为$A$的任意零化多项式，则$m _A(\lambda) | f(\lambda)$，即$f(\lambda)$一定可以被$m _A (\lambda)$整除。特别的，的任意零化多项式，则$m _A(\lambda) | f _A(\lambda)$，因为特征多项式是零化多项式；</li>
<li>$P ^{-1}A P &#x3D; B$，则$m _A(\lambda)&#x3D;m _B(\lambda)$；</li>
<li>$A$的任一特征值为$m _A (\lambda)$（对任意零化多项式也成立）的根；</li>
<li>若$A$是分块对角，则每个块的最小多项式$m _{A _i}(\lambda) | f(\lambda)$，即$A$的最小多项式是每个块最小多项式的最小公倍式；</li>
<li>$A$可对角化当且仅当$m _A(\lambda)$无重根（复数域上）。</li>
</ol>
<blockquote>
<p>Jordan标准型的最小多项式就是最大次数的几个不同Jordan块的最小多项式的乘积。<br>$\lambda ^n &#x3D; 1$无重根。</p>
</blockquote>
<blockquote>
<p>Jordan型求法：a. 求特征值：若特征值均不同，则对角化；否则求几何重数。对三阶矩阵，一定能求；对四阶，若所有特征值相同，几何重数为2，则可能的Jordan块组合为2+2和3+1，用最小多项式检验哪个是对的即可。（更高阶的不需要掌握）</p>
</blockquote>
<h1 id="圆盘定理"><a href="#圆盘定理" class="headerlink" title="圆盘定理"></a>圆盘定理</h1><p>近似地计算特征值 &amp; 确定特征值的范围：</p>
<p>$$<br>|\lambda - a _{ii}| \le \sum\limits _{i \ne j} ^n |a _{ij}|<br>$$</p>
<h2 id="第一圆盘定理"><a href="#第一圆盘定理" class="headerlink" title="第一圆盘定理"></a>第一圆盘定理</h2><p>$A$的任意一个特征值一定会落在某个圆盘内。</p>
<h2 id="第二圆盘定理"><a href="#第二圆盘定理" class="headerlink" title="第二圆盘定理"></a>第二圆盘定理</h2><p>几个圆盘叠在一起，那么这个叠加后的区域就有几个特征值。由此推知，若圆盘互相分离，则$A$一定有$n$个不同的特征值，$A$一定可以对角化，这是对复数域，若是实数域，则还可以说明特征值都是实数（因为此时圆盘是关于$x$轴对称的，根都是成对出现的，除了$x$轴上的根）。</p>
<h1 id="谱-amp-谱半径"><a href="#谱-amp-谱半径" class="headerlink" title="谱 &amp; 谱半径"></a>谱 &amp; 谱半径</h1><p>谱：所有特征值构成的集合。</p>
<p>谱半径$\rho (A)$：特征值模的最大值。</p>
<p>$$<br>\begin{align*}<br>    \mu&#x3D;&amp;\max \{\sum \limits _{j&#x3D;1} ^n |a _{ij}|\} _{1\le i \le n} \\<br>    \mu &#39;&#x3D;&amp;\max \{\sum \limits _{i&#x3D;1} ^n |a _{ij}|\} _{1\le j \le n} \\<br>    \rho (A) &amp;\le \min \{\mu,\mu &#39;\}<br>\end{align*}<br>$$</p>
<p>事实上：$\rho (A) \le ||A||$。</p>
<blockquote>
<p>$||A||$是矩阵范数。</p>
</blockquote>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zclzcl0223.github.io/2023/10/25/MatrixTheory4/" data-id="clzik1qtr0057m07ka0vt42ss" data-title="MatrixTheory: 特征值、特征向量" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Math/" rel="tag">Math</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/10/31/CircuitComplexity/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Computational Complexity: Circuit Complexity
        
      </div>
    </a>
  
  
    <a href="/2023/10/25/PolynomialHierarchy/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Computational Complexity: Polynomial Hierarchy</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Advanced-Model/">Advanced Model</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/CS7313-Computational-Complexity/">CS7313: Computational Complexity</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Configuration/">Configuration</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Dive-Into-Deep-Learning/">Dive Into Deep Learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/GNN/">GNN</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Kernel-Method/">Kernel Method</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/MATH6005-Matrix-Theory/">MATH6005: Matrix Theory</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning-by-AndrewNg/">Machine Learning by AndrewNg</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Math/">Math</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Paper/">Paper</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Programming-Language/">Programming Language</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Tool/">Tool</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Attention-Mechanism/" rel="tag">Attention Mechanism</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CNN/" rel="tag">CNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Configuration/" rel="tag">Configuration</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Data-Analysis/" rel="tag">Data Analysis</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Dataset-Distillation/" rel="tag">Dataset Distillation</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Decision-Trees/" rel="tag">Decision Trees</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Deep-Learning/" rel="tag">Deep Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GCN/" rel="tag">GCN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GNN/" rel="tag">GNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Generative-AI/" rel="tag">Generative AI</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hexo/" rel="tag">Hexo</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Lab/" rel="tag">Lab</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux/" rel="tag">Linux</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-Learning/" rel="tag">Machine Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Markup-Language/" rel="tag">Markup Language</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Math/" rel="tag">Math</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Paper/" rel="tag">Paper</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/" rel="tag">Python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RNN/" rel="tag">RNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Recommender-System/" rel="tag">Recommender System</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Reinforcement-Learning/" rel="tag">Reinforcement Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SVM/" rel="tag">SVM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Supervised-Learning/" rel="tag">Supervised Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Theoretical-Computer-Science/" rel="tag">Theoretical Computer Science</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Tool/" rel="tag">Tool</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Unsupervised-Learning/" rel="tag">Unsupervised Learning</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Attention-Mechanism/" style="font-size: 11.82px;">Attention Mechanism</a> <a href="/tags/CNN/" style="font-size: 12.73px;">CNN</a> <a href="/tags/Configuration/" style="font-size: 10px;">Configuration</a> <a href="/tags/Data-Analysis/" style="font-size: 10px;">Data Analysis</a> <a href="/tags/Dataset-Distillation/" style="font-size: 13.64px;">Dataset Distillation</a> <a href="/tags/Decision-Trees/" style="font-size: 10px;">Decision Trees</a> <a href="/tags/Deep-Learning/" style="font-size: 20px;">Deep Learning</a> <a href="/tags/GCN/" style="font-size: 11.82px;">GCN</a> <a href="/tags/GNN/" style="font-size: 15.45px;">GNN</a> <a href="/tags/Generative-AI/" style="font-size: 10.91px;">Generative AI</a> <a href="/tags/Hexo/" style="font-size: 10.91px;">Hexo</a> <a href="/tags/Lab/" style="font-size: 10.91px;">Lab</a> <a href="/tags/Linux/" style="font-size: 16.36px;">Linux</a> <a href="/tags/Machine-Learning/" style="font-size: 17.27px;">Machine Learning</a> <a href="/tags/Markup-Language/" style="font-size: 10px;">Markup Language</a> <a href="/tags/Math/" style="font-size: 19.09px;">Math</a> <a href="/tags/Paper/" style="font-size: 14.55px;">Paper</a> <a href="/tags/Python/" style="font-size: 18.18px;">Python</a> <a href="/tags/RNN/" style="font-size: 10.91px;">RNN</a> <a href="/tags/Recommender-System/" style="font-size: 10.91px;">Recommender System</a> <a href="/tags/Reinforcement-Learning/" style="font-size: 10.91px;">Reinforcement Learning</a> <a href="/tags/SVM/" style="font-size: 10px;">SVM</a> <a href="/tags/Supervised-Learning/" style="font-size: 11.82px;">Supervised Learning</a> <a href="/tags/Theoretical-Computer-Science/" style="font-size: 15.45px;">Theoretical Computer Science</a> <a href="/tags/Tool/" style="font-size: 18.18px;">Tool</a> <a href="/tags/Unsupervised-Learning/" style="font-size: 11.82px;">Unsupervised Learning</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/06/">June 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/01/">January 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/12/">December 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/06/">June 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/05/">May 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/04/">April 2023</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2024/06/17/Diffusion/">Diffusion</a>
          </li>
        
          <li>
            <a href="/2024/01/20/VAE/">VAE</a>
          </li>
        
          <li>
            <a href="/2023/12/02/CountingComplexity/">Computational Complexity: Complexity of Counting</a>
          </li>
        
          <li>
            <a href="/2023/11/24/MatrixTheory5/">MatrixTheory: 特殊矩阵与矩阵分解</a>
          </li>
        
          <li>
            <a href="/2023/11/13/RandomizedComputation/">Computational Complexity: Randomized Computation</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 ChaosTsang<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/" class="mobile-nav-link">home</a>
  
    <a href="/about/" class="mobile-nav-link">about</a>
  
    <a href="/tags/" class="mobile-nav-link">tags</a>
  
    <a href="/categories/" class="mobile-nav-link">categories</a>
  
    <a href="/archives/" class="mobile-nav-link">archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>





<script src="/js/script.js"></script>





  </div>
</body>
</html>