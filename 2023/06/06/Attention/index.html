<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=[object Object]"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', '[object Object]');
</script>
<!-- End Google Analytics -->

  
  <title>Attention Mechanisms: More Targeted Information Extraction | JourneyToCoding</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="Attention mechanisms is a layer of neural networks added to deep learning models to focus their attention to specific parts of data, based on different weights assigned to different parts. Just as the">
<meta property="og:type" content="article">
<meta property="og:title" content="Attention Mechanisms: More Targeted Information Extraction">
<meta property="og:url" content="https://zclzcl0223.github.io/2023/06/06/Attention/index.html">
<meta property="og:site_name" content="JourneyToCoding">
<meta property="og:description" content="Attention mechanisms is a layer of neural networks added to deep learning models to focus their attention to specific parts of data, based on different weights assigned to different parts. Just as the">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://zclzcl0223.github.io/2023/06/06/Attention/1.png">
<meta property="og:image" content="https://zclzcl0223.github.io/2023/06/06/Attention/2.png">
<meta property="og:image" content="https://zclzcl0223.github.io/2023/06/06/Attention/3.png">
<meta property="og:image" content="https://zclzcl0223.github.io/2023/06/06/Attention/4.png">
<meta property="og:image" content="https://zclzcl0223.github.io/2023/06/06/Attention/5.png">
<meta property="og:image" content="https://zclzcl0223.github.io/2023/06/06/Attention/6.png">
<meta property="og:image" content="https://zclzcl0223.github.io/2023/06/06/Attention/7.png">
<meta property="article:published_time" content="2023-06-05T21:29:23.000Z">
<meta property="article:modified_time" content="2023-08-09T05:47:12.000Z">
<meta property="article:author" content="ChaosTsang">
<meta property="article:tag" content="Attention Mechanism">
<meta property="article:tag" content="Deep Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://zclzcl0223.github.io/2023/06/06/Attention/1.png">
  
    <link rel="alternate" href="/atom.xml" title="JourneyToCoding" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/%5Bobject%20Object%5D">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">JourneyToCoding</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">Code for fun</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/">home</a>
        
          <a class="main-nav-link" href="/about/">about</a>
        
          <a class="main-nav-link" href="/tags/">tags</a>
        
          <a class="main-nav-link" href="/categories/">categories</a>
        
          <a class="main-nav-link" href="/archives/">archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://zclzcl0223.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-Attention" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/06/06/Attention/" class="article-date">
  <time class="dt-published" datetime="2023-06-05T21:29:23.000Z" itemprop="datePublished">2023-06-06</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Dive-Into-Deep-Learning/">Dive Into Deep Learning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      Attention Mechanisms: More Targeted Information Extraction
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <span id="more"></span>

<h1 id="Attention-in-Biology"><a href="#Attention-in-Biology" class="headerlink" title="Attention in Biology"></a>Attention in Biology</h1><p>In biology, there are two kinds of cues that affect our attention: volitional cue (自主性提示) and nonvolitional cue (非自主性提示).</p>
<p><img align="left" src="/2023/06/06/Attention/1.png" style=" width:380px; padding: 0px 0px; "> <img align="mid" src="/2023/06/06/Attention/2.png" style=" width:380px; padding: 0px 0px; "></p>
<center style="font-size:12px; font-weight:bold">Fig. 1. Nonvolitional cue (left) and Volitional cue (right)</center><br>

<p>The nonvolitional cue is based on the saliency of objects in the environment. It&#39;s unconscous. As the above picture shows, when there is a red cup among several white objects, we will unconsciously choose the red cup as its color is more prominent. The volitional cue is controlled by our awareness and our decisions are led by our consciousness. For example, after drinking a cup of coffee, we may want to relax, because of which we choose a book to read.</p>
<h1 id="Attention-Mechanisms"><a href="#Attention-Mechanisms" class="headerlink" title="Attention Mechanisms"></a>Attention Mechanisms</h1><p>The nonvolitional cue and volitional cue inspire us to take the environment as input and use volitional cue to focus the attention to a certain object or feature.</p>
<h2 id="Queries-Keys-and-Values"><a href="#Queries-Keys-and-Values" class="headerlink" title="Queries, Keys and Values"></a>Queries, Keys and Values</h2><p>In attention mechanisms, the volitional cue is also called <strong>query</strong>. It reflects the tendency of feature extraction. The nonvolitional cue is actually our perception of the environment. The characteristics of the environment can be represented by several <strong>key-value pairs</strong>. These three things consist of the input of the attention mechanism. Giving queries and keys, the attention mechanism will aggregate values and choose the most prominent and suitable value as output. Such a process is also called <em>attention pooling</em>. It is queries that distinguish attention pooling from dense layers.</p>
<p><img src="/2023/06/06/Attention/3.png" alt="3"></p>
<center style="font-size:12px; font-weight:bold">Fig. 2. Attention pooling</center>

<h2 id="Mathematical-Representation-of-Attention"><a href="#Mathematical-Representation-of-Attention" class="headerlink" title="Mathematical Representation of Attention"></a>Mathematical Representation of Attention</h2><p>Our attention to a certain object reflects how much we care about it, or, mathematically, the weight we give it. When a certain query is offered to us, it is natural for us to think of finding something similar to this query to get the answer. That is, for a key-value pair, the more similar the key is to the query, the higher proportion its value should take up in the answer. This is how <em><strong>Nadaraya-Watson kernel regression</strong></em> works:</p>
<p>$$<br>f(\mathbf{q})&#x3D;\sum\limits _{i&#x3D;1} ^n \frac{K(\mathbf{q}-\mathbf{k}_i)}{\sum\limits _{j&#x3D;1} ^n K(\mathbf{q}-\mathbf{k}_j)}\mathbf{v}_i<br>$$</p>
<p>where $K$ is a kernel. Such an estimator assigns different weight to different keys for a certain query. More generally, attention is defined as:</p>
<p>$$<br>\text{Attention}(\mathbf{q},\mathbf{k},\mathbf{v})&#x3D;\sum\limits _{i&#x3D;1} ^n\alpha(\mathbf{q},\mathbf{k}_i)\mathbf{v}_i<br>$$</p>
<p>where $\alpha(\mathbf{q},\mathbf{k}_i)$ is <em>attention weight</em>. It is non-negative and its sum is $1$. The output is a weighted sum of the values for each key-value pair.</p>
<p><img src="/2023/06/06/Attention/4.png" alt="4"></p>
<center style="font-size:12px; font-weight:bold">Fig. 3. Mathematical representation of attention pooling</center> <br>

<p>It is noteworthy that Nadaraya-Watson kernel regression is a nonparametric model (<em>nonparametric attention pooling</em>). Therefore, it is consistent. With enough data, it will converge to the optimum result. However, in machine learning, the agent needs parameters to learn so that it could improve its performance. For instance, if we set $K$ as a gaussian kernel and add learned parameter $\mathbf{w}$, we get a formula that looks like softmax:</p>
<p>$$<br>\begin{align*}<br>K(\mathbf{u})&amp;&#x3D;\frac{1}{\sqrt{2\pi}}\exp(-\frac{\mathbf{u}^2}{2})\\<br>f(\mathbf{q})<br>    &amp;&#x3D;\sum\limits _{i&#x3D;1} ^n\alpha(\mathbf{q},\mathbf{k}_i)\mathbf{v}_i\\<br>    &amp;&#x3D;\sum\limits _{i&#x3D;1} ^n\frac{\exp[-\frac{1}{2}(\mathbf{q}-\mathbf{k}_i)^2\mathbf{w}^2]}{\sum\limits _{j&#x3D;1} ^n\exp[-\frac{1}{2}(\mathbf{q}-\mathbf{k}_i)^2\mathbf{w}^2]}\mathbf{v}_i\\<br>    &amp;&#x3D;\sum _{i&#x3D;1} ^n\text{softmax}[-\frac{1}{2}(\mathbf{q}-\mathbf{k}_i)^2\mathbf{w}^2]\mathbf{v}_i<br>\end{align*}<br>$$</p>
<h2 id="Attention-Scoring-Function"><a href="#Attention-Scoring-Function" class="headerlink" title="Attention Scoring Function"></a>Attention Scoring Function</h2><p>The exponent of the gaussian kernel above ($-{\mathbf{u}^2}&#x2F;2$) is actually an attention scoring function ($a$) which evaluates the similarity between $\mathbf{q}$ and $\mathbf{k}_i$. Hence, the attention pooling can be also represented as:</p>
<p>$$<br>\begin{align*}<br>    \text{Attention}(\mathbf{q},\mathbf{k},\mathbf{v})&amp;&#x3D;\sum\limits _{i&#x3D;1} ^n\alpha(\mathbf{q},\mathbf{k}_i)\mathbf{v}_i\\<br>    &amp;&#x3D;\sum\limits _{i&#x3D;1} ^n\text{softmax}[a(\mathbf{q},\mathbf{k}_i)]\mathbf{v}_i<br>\end{align*}<br>$$</p>
<p><img src="/2023/06/06/Attention/5.png" alt="5"></p>
<center style="font-size:12px; font-weight:bold">Fig. 4. Computing the output of attention pooling</center> <br>

<p>where $\text{softmax}$ generates attention weights. The more similar the key $\mathbf{k}_i$ is to the query $\mathbf{q}$, the higher weight its value $\mathbf{v}_i$ gets. The output is the weighted sum of values.</p>
<p>More generally, query $\mathbf{q}$ and key $\mathbf{k}_i$ are both <strong>vectors</strong> of different length. The attention scoring function maps these two vectors to a scalar</p>
<h3 id="Additive-Attention"><a href="#Additive-Attention" class="headerlink" title="Additive Attention"></a>Additive Attention</h3><p>Additive attention uses MLP to evaluate the similarity between $\mathbf{q}$ and $\mathbf{k}$, where $\mathbf{q}\in\mathbf{R}^q$ and $\mathbf{k}\in\mathbf{R}^k$:</p>
<p>$$<br>a(\mathbf{q,k})&#x3D;[\tanh(\mathbf{q}\mathbf{W}_q+\mathbf{k}\mathbf{W}_k)]\mathbf{w}_v\in\mathbf{R}<br>$$</p>
<p>where $\mathbf{W}_q\in\mathbf{R} ^{q\times h}$, $\mathbf{W}_k\in\mathbf{R} ^{k\times h}$ and $\mathbf{w}_v\in\mathbf{R} ^{h}$. All of them are learned parameters (here we ignore the bias) in one dense layer.</p>
<blockquote>
<p>In practice, each batch may have several queries and key-value pairs. And there are always more than one batches. In this case, the shape of $\mathbf{q}$ is <code>(batch_size, num_queries, q)</code> and the shape of $\mathbf{k}$ is <code>(batch_size, num_kv_pairs, k)</code>. The shape of $\mathbf{q}\mathbf{W}_q$ is <code>(batch_size, num_queries, h)</code> and the shape of $\mathbf{k}\mathbf{W}_k$ is <code>(batch_size, num_kv_pairs, h)</code>. <code>num_queries</code> and <code>num_kv_pairs</code> are always different, which means that we can&#39;t add $\mathbf{q}\mathbf{W}_q$ and $\mathbf{k}\mathbf{W}_k$ directly. </p>
<p>To solve this, we always use <code>queries.unsqueeze(2)</code> and <code>keys.unsqueeze(1)</code> (<code>queries</code> and <code>keys</code> are both tensor) to expand their dimensions. After <code>unsqueeze</code>, the shape of <code>queries</code> is <code>(batch_size, num_queries, 1, h)</code> and the shape of <code>keys</code> is <code>(batch_size, 1, num_kv_pairs, h)</code>. Now broadcasting can make $\mathbf{q}\mathbf{W}_q+\mathbf{k}\mathbf{W}_k$ legal. The nature of this operation is that each query should take all the keys into account so the $2$-th dimension of <code>queries</code> is expanded to <code>num_kv_pairs</code>, so is <code>keys</code>. </p>
</blockquote>
<h3 id="Scaled-Dot-Product-Attention"><a href="#Scaled-Dot-Product-Attention" class="headerlink" title="Scaled Dot-Product Attention"></a>Scaled Dot-Product Attention</h3><p>If the shape of $\mathbf{q}$ and $\mathbf{k}$ are the same (both are $d$), we could just use dot-product to evaluate their similarity:</p>
<p>$$<br>a(\mathbf{q,k})&#x3D;\mathbf{q}^\text{T}\mathbf{k}&#x2F;\sqrt{d}<br>$$</p>
<p>where $\sqrt{d}$ is to make sure that the value of $a(\mathbf{q,k})$ will not change significantly when the shape changes.</p>
<p>If considering SGD, the output of attention pooling is:</p>
<p>$$<br>\text{softmax}(\mathbf{Q}\mathbf{K}^\text{T}&#x2F;\sqrt{d})\mathbf{V}<br>$$</p>
<p>where $d$ is still the shape of each query or each key.</p>
<h1 id="Different-Types-of-Attention"><a href="#Different-Types-of-Attention" class="headerlink" title="Different Types of Attention"></a>Different Types of Attention</h1><p>Depending on the number of attention pooling and the relationship among queries, keys and values, there are various forms of attention mechanisms.</p>
<h2 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h2><p>Just as different convolutional output channels can extract different types of features, different attention pooling will also learn various behaviors. Hence, we can use multi-head attention to generate multiple attention pooling outputs $\mathbf{h}_1...\mathbf{h}_i$ in parallel and concatenate them together to form $(\mathbf{h}_1,...,\mathbf{h}_i)$.</p>
<p><img src="/2023/06/06/Attention/6.png" alt="6"></p>
<center style="font-size:12px; font-weight:bold">Fig. 5. Multi-head attention</center>

<h2 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h2><p>Self-attention (intra-attention) is a special kind of attention, whose queries, keys and values are all the same things.</p>
<h1 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h1><p>When dealing with sequence information, RNNs operate each token in sequence. Therefore, the location information of the sequence is used. However, the attention pooling operates all the inputs in parallel, because of which, the location information is wasted.</p>
<p>Positional encoding is the location information added to the keys, values and queries. In computer science, we use binary string to represent position (e.g. 0: 000, 1: 001, ..., 7: 111). As can be seen, the bit on each position alternates at diferent frequencies respectively. Higher bits alternate less frequently than lower bits. Such alternating changes can represent discrete positional information.</p>
<h2 id="Absolute-Positional-Information"><a href="#Absolute-Positional-Information" class="headerlink" title="Absolute Positional Information"></a>Absolute Positional Information</h2><p>For the input $\mathbf{X}\in\mathbf{R} ^{n\times d}$ which contains $n$ tokens and $d$ features for each token, we can use positional embedding matrix $\mathbf{P}\in\mathbf{R} ^{n\times d}$ to offer it positional information:</p>
<p>$$<br>\begin{align*}<br>    \mathbf{X&#39;}&amp;&#x3D;\mathbf{X}+\mathbf{P}\\<br>    \mathbf{P} _{i,2j}&amp;&#x3D;\sin(\frac{i}{10000 ^{2j&#x2F;d}})\\<br>    \mathbf{P} _{i,2j+1}&amp;&#x3D;\cos(\frac{i}{10000 ^{2j&#x2F;d}})<br>\end{align*}<br>$$</p>
<p>It works because different columns, which are similar to the bit position in binary string, oscillate at different frequencies. It is just like the binary string but the value is continuous.</p>
<p><img src="/2023/06/06/Attention/7.png" alt="7"></p>
<center style="font-size:12px; font-weight:bold">Fig. 6. Oscillating frequency of different columns</center>

<h2 id="Relative-Positional-Information"><a href="#Relative-Positional-Information" class="headerlink" title="Relative Positional Information"></a>Relative Positional Information</h2><p>The positional information we add to $\mathbf{X}$ is called absolute positional information. What&#39;s more, we can also get the relative positional information for different rows in the same column:</p>
<p>$$<br>\begin{align*}<br>    (\mathbf{P} _{i,2j}, \mathbf{P} _{i,2j+1})&amp;&#x3D;(\sin\frac{i}{10000 ^{2j&#x2F;d}},\cos\frac{i}{10000 ^{2j&#x2F;d}})\\<br>    (\mathbf{P} _{i+\delta,2j}, \mathbf{P} _{i+\delta,2j+1})&amp;&#x3D;(\sin\frac{i+\delta}{10000 ^{2j&#x2F;d}},\cos\frac{i+\delta}{10000 ^{2j&#x2F;d}})\\<br>    &amp;&#x3D;({\begin{bmatrix}<br>           \cos\delta\omega_j\space\space \sin\delta\omega_j\\<br>           -\sin\delta\omega_j\space\space \cos\delta\omega_j<br>       \end{bmatrix}}<br>      {\begin{bmatrix}<br>           \mathbf{P} _{i,2j}\\<br>           \mathbf{P} _{i,2j+1}<br>       \end{bmatrix}})^\text{T}\\<br>    &amp;&#x3D;[\sin(i+\delta)\omega_j,\cos(i+\delta)\omega_j]\\<br>\end{align*}<br>$$</p>
<p>where $\omega_j&#x3D;1&#x2F;10000^{2j&#x2F;d}$.</p>
<blockquote>
<p>Positional information is information added to the input $\mathbf{X}$, including keys, values and queries, it could be fixed positional encoding like what we do above or learned one.</p>
</blockquote>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zclzcl0223.github.io/2023/06/06/Attention/" data-id="clzik0lbw0009v47k49efh3tk" data-title="Attention Mechanisms: More Targeted Information Extraction" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Attention-Mechanism/" rel="tag">Attention Mechanism</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Deep-Learning/" rel="tag">Deep Learning</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/06/07/Transformer/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Transformer: Self-Attention and Parallelization
        
      </div>
    </a>
  
  
    <a href="/2023/06/02/OptimizationAlgorithms/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Optimization Algorithms</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Advanced-Model/">Advanced Model</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/CS7313-Computational-Complexity/">CS7313: Computational Complexity</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Configuration/">Configuration</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Dive-Into-Deep-Learning/">Dive Into Deep Learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/GNN/">GNN</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Kernel-Method/">Kernel Method</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/MATH6005-Matrix-Theory/">MATH6005: Matrix Theory</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning-by-AndrewNg/">Machine Learning by AndrewNg</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Math/">Math</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Paper/">Paper</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Programming-Language/">Programming Language</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Tool/">Tool</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Attention-Mechanism/" rel="tag">Attention Mechanism</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CNN/" rel="tag">CNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Configuration/" rel="tag">Configuration</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Data-Analysis/" rel="tag">Data Analysis</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Dataset-Distillation/" rel="tag">Dataset Distillation</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Decision-Trees/" rel="tag">Decision Trees</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Deep-Learning/" rel="tag">Deep Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GCN/" rel="tag">GCN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GNN/" rel="tag">GNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Generative-AI/" rel="tag">Generative AI</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hexo/" rel="tag">Hexo</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Lab/" rel="tag">Lab</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux/" rel="tag">Linux</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-Learning/" rel="tag">Machine Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Markup-Language/" rel="tag">Markup Language</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Math/" rel="tag">Math</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Paper/" rel="tag">Paper</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/" rel="tag">Python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RNN/" rel="tag">RNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Recommender-System/" rel="tag">Recommender System</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Reinforcement-Learning/" rel="tag">Reinforcement Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SVM/" rel="tag">SVM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Supervised-Learning/" rel="tag">Supervised Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Theoretical-Computer-Science/" rel="tag">Theoretical Computer Science</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Tool/" rel="tag">Tool</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Unsupervised-Learning/" rel="tag">Unsupervised Learning</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Attention-Mechanism/" style="font-size: 11.82px;">Attention Mechanism</a> <a href="/tags/CNN/" style="font-size: 12.73px;">CNN</a> <a href="/tags/Configuration/" style="font-size: 10px;">Configuration</a> <a href="/tags/Data-Analysis/" style="font-size: 10px;">Data Analysis</a> <a href="/tags/Dataset-Distillation/" style="font-size: 13.64px;">Dataset Distillation</a> <a href="/tags/Decision-Trees/" style="font-size: 10px;">Decision Trees</a> <a href="/tags/Deep-Learning/" style="font-size: 20px;">Deep Learning</a> <a href="/tags/GCN/" style="font-size: 11.82px;">GCN</a> <a href="/tags/GNN/" style="font-size: 15.45px;">GNN</a> <a href="/tags/Generative-AI/" style="font-size: 10.91px;">Generative AI</a> <a href="/tags/Hexo/" style="font-size: 10.91px;">Hexo</a> <a href="/tags/Lab/" style="font-size: 10.91px;">Lab</a> <a href="/tags/Linux/" style="font-size: 16.36px;">Linux</a> <a href="/tags/Machine-Learning/" style="font-size: 17.27px;">Machine Learning</a> <a href="/tags/Markup-Language/" style="font-size: 10px;">Markup Language</a> <a href="/tags/Math/" style="font-size: 19.09px;">Math</a> <a href="/tags/Paper/" style="font-size: 14.55px;">Paper</a> <a href="/tags/Python/" style="font-size: 18.18px;">Python</a> <a href="/tags/RNN/" style="font-size: 10.91px;">RNN</a> <a href="/tags/Recommender-System/" style="font-size: 10.91px;">Recommender System</a> <a href="/tags/Reinforcement-Learning/" style="font-size: 10.91px;">Reinforcement Learning</a> <a href="/tags/SVM/" style="font-size: 10px;">SVM</a> <a href="/tags/Supervised-Learning/" style="font-size: 11.82px;">Supervised Learning</a> <a href="/tags/Theoretical-Computer-Science/" style="font-size: 15.45px;">Theoretical Computer Science</a> <a href="/tags/Tool/" style="font-size: 18.18px;">Tool</a> <a href="/tags/Unsupervised-Learning/" style="font-size: 11.82px;">Unsupervised Learning</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/06/">June 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/01/">January 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/12/">December 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/06/">June 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/05/">May 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/04/">April 2023</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2024/06/17/Diffusion/">Diffusion</a>
          </li>
        
          <li>
            <a href="/2024/01/20/VAE/">VAE</a>
          </li>
        
          <li>
            <a href="/2023/12/02/CountingComplexity/">Computational Complexity: Complexity of Counting</a>
          </li>
        
          <li>
            <a href="/2023/11/24/MatrixTheory5/">MatrixTheory: 特殊矩阵与矩阵分解</a>
          </li>
        
          <li>
            <a href="/2023/11/13/RandomizedComputation/">Computational Complexity: Randomized Computation</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 ChaosTsang<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/" class="mobile-nav-link">home</a>
  
    <a href="/about/" class="mobile-nav-link">about</a>
  
    <a href="/tags/" class="mobile-nav-link">tags</a>
  
    <a href="/categories/" class="mobile-nav-link">categories</a>
  
    <a href="/archives/" class="mobile-nav-link">archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>





<script src="/js/script.js"></script>





  </div>
</body>
</html>