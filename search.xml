<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hyper Connections</title>
    <url>/2026/01/23/Hyper-Connections/</url>
    <content><![CDATA[<span id="more"></span>

<p>以Transformer为根基的LLMs能发展至今，离不开Transformer设计的规整、对称性。对Transformer来说，经过Embedding层映射到向量空间的文本序列，成为了一个很规整的$T\times D$的矩阵，其中$T$是序列长度，$D$是Embedding维度。Attention生成动态权重进行<strong>序列维度</strong>的变换映射；FFN以固定的权重进行<strong>特征维度</strong>的变换映射，位置编码PositionEncoding赋予模型识别顺序、外延能力（当然Causal Attention本身就不具备置换不变性，内在地有顺序感知能力，但加上位置编码总归是更直接点），残差Residue以及正则Norm作为深度神经网络的基本组件保证训练的可持续与稳定性。</p>
<p>若再从Attention和FFN统一性的角度来看：Attention的QK内积生成权重，对V在序列维度加权求和；FFN（两层Linear维度分别为$D\times M$，$M\times D$）的输入与第一层Linear生成权重，对第二层的Linear在$M$维度加权求和。而稀疏性是普通存在的，因此很直接地，在FFN，对某个Token，在第二层的Linear中起到作用的肯定只有少数的行，因此有了MoE；同样地，与当前Token相关的历史Token也是少数的，即有用的V也就几个，因此有了稀疏注意力，如<a href="https://arxiv.org/pdf/2512.02556">DSA</a>。</p>
<p>对其他的组件，如PositionEncoding，从Sinusoidal到<a href="https://arxiv.org/pdf/2104.09864">RoPE</a>到<a href="https://arxiv.org/pdf/2309.00071">YaRN</a>等，一直在发展；Norm也由LayerNorm到<a href="https://arxiv.org/pdf/1910.07467">RMSNorm</a>（当然还有<a href="https://arxiv.org/pdf/2503.10622">DyT</a>）。现在终于轮到Residue以及TokenEmbedding了（悲QAQ）。本篇主要围绕字节的<a href="https://arxiv.org/pdf/2409.19606">HC</a>以及DeepSeek的<a href="https://arxiv.org/pdf/2512.24880">mHC</a>看看米娜桑都对残差整了些什么花活儿。</p>
<h2 id="HC"><a href="#HC" class="headerlink" title="HC"></a>HC</h2><p>emmm...怎么说呢，我十分认同openreview给5分的审稿人的意见，这篇文章的写作确实有点一言难尽。本来应该是很简单、直觉的方法，论文把它描述地过于复杂了，而且伪代码给得也不好。因此我决定跳过这篇文章，直接看mHC。</p>
<p><img src="/2026/01/23/Hyper-Connections/1.png" alt="hc"></p>
<center style="font-size:12px; font-weight:bold">Fig. 1. Hyper-connections</center><br>

<h2 id="mHC"><a href="#mHC" class="headerlink" title="mHC"></a>mHC</h2><p>确实写得好不少，两个公式就概括了HC和普通的残差的区别，绘图也更为清晰：</p>
<p><img src="/2026/01/23/Hyper-Connections/3.png" alt="residue"></p>
<center style="font-size:12px; font-weight:bold">Fig. 2. Residue</center><br>

<p><img src="/2026/01/23/Hyper-Connections/4.png" alt="residue"></p>
<center style="font-size:12px; font-weight:bold">Fig. 3. Hyper-connections</center><br>

<p><img src="/2026/01/23/Hyper-Connections/2.png" alt="mhc"></p>
<center style="font-size:12px; font-weight:bold">Fig. 4. Manifold-constrained hyper-connections</center><br>

<p>即相比于残差，HC人为地将恒等流由原来的一条增加为了n条，相应地输入$x _l$的维度也由$Batch\times SeqLen\times Dim$变为了$Batch\times SeqLen\times n\times Dim$（当然，在第一层的时候应该只是将原本的$x _l$复制了n份）。因此为了融合，会需要多个整合维度的投影矩阵，因此有了：$\mathcal{H} _l ^{pre}\in \mathbb{R} ^{n\times 1}$将输入的多流整合为一条流，输入Attention或FFN模块；$\mathcal{H} _l ^{post}\in \mathbb{R} ^{1\times n}$将Attention或FFN的输出重新转为n条流；$\mathcal{H} _l ^{res}\in \mathbb{R} ^{n\times n}$进行n条恒等流的交互。</p>
]]></content>
      <categories>
        <category>Advanced Model</category>
      </categories>
      <tags>
        <tag>LLM</tag>
      </tags>
  </entry>
  <entry>
    <title>Retrievers: Indexing and Ranking</title>
    <url>/2026/01/18/Retrievers-Indexing-and-Ranking/</url>
    <content><![CDATA[<span id="more"></span>

<p>每次临近毕业都是很闲的时候，因此如本科毕业般，这次也想学点之前没认真学的东西。召回、各类大模型技术报告、新的语言......那么先从召回开始。</p>
<p>从盘古开天地看起也不太现实，所以主要是围绕几篇主要的论文展开：</p>
<ul>
<li><a href="https://arxiv.org/pdf/1603.09320">Hierarchical Navigable Small World (HNSW)</a></li>
<li><a href="https://arxiv.org/pdf/1801.02294">Tree-based Deep Models (TDM)</a></li>
<li><a href="https://arxiv.org/pdf/2202.10226">NANN (二向箔)</a></li>
<li><a href="https://arxiv.org/pdf/2007.07203">Deep Retrieval (DR)</a></li>
<li><a href="https://arxiv.org/pdf/2501.08695">Streaming VQ</a></li>
</ul>
<h2 id="Streaming-VQ"><a href="#Streaming-VQ" class="headerlink" title="Streaming VQ"></a>Streaming VQ</h2><p>因为先看的Streaming VQ，所以就倒过来写了。</p>
<p>VQ（Vector Quantisation），顾名思义是将一坨向量映射到一个索引，实际就是将向量分组。于推荐而言，召回中的VQ可以理解为由反向传播优化得到的动态分组方法。为什么召回需要分组，或者说需要索引？因为召回的候选是视频的全集，我们不可能在第一个阶段就对每个用户进行全候选计算，因此需要缩小范围。那么如何缩小范围？最简单的就是为用户以及视频打上标签，如我喜欢看日漫，那就只召回带日漫标签的。直接用标签分组缺乏灵活性，而VQ就是这么一个动态分组。</p>
<h3 id="VQ-VAE"><a href="#VQ-VAE" class="headerlink" title="VQ-VAE"></a>VQ-VAE</h3><p>首先，VQ这种方式在Neurips2017的文章<a href="https://arxiv.org/pdf/1711.00937">VQ-VAE</a>中首次被用到了生成式任务中：</p>
<p><img src="/2026/01/18/Retrievers-Indexing-and-Ranking/1.png" alt="vqvae"></p>
<center style="font-size:12px; font-weight:bold">Fig. 1. VQ-VAE</center><br>

<p>其对VAE Encoder的输出$z$进行了分组（后面都称索引），由于现实的图片分布可以理解为无数个高斯分布的叠加，因此这些索引我认为最终是实现了一个索引对应一个或者几个高斯分布。整个流程还是VAE那一套，量化体现在Encoder-&gt;Decoder的部分，即每个索引有其自身的Embedding，选取与Encoder输出欧式距离最近的组的Embedding作为Decoder的输入：</p>
<p><img src="/2026/01/18/Retrievers-Indexing-and-Ranking/2.png" alt="quantisation"></p>
<center style="font-size:12px; font-weight:bold">Fig. 2. Quantisation</center><br>

<p>Loss由三部分组成（加号分割）：</p>
<ul>
<li>$\mathcal{L} _1$即重构MSE，生成梯度用于更新Decoder，比较特别的是，由于输入Decoder的是索引Embedding，因此梯度不会传到到Encoder，作者的解决方式是：Decoder和Encoder的结构是一模一样的，因此直接用Decoder的梯度更新Encoder;</li>
<li>$\mathcal{L} _2$中sg指stop gradient，用于更新索引Embedding。不难看出，最终每个索引的Embedding其实一定程度上等于该索引所包含的向量的均值，因此也可以采用指数移动平均EMA的更新方法，Streaming VQ用的就是这种方式；</li>
<li>$\mathcal{L} _3$更新Encoder，起到正则的效果。</li>
</ul>
<p><img src="/2026/01/18/Retrievers-Indexing-and-Ranking/3.png" alt="loss"></p>
<center style="font-size:12px; font-weight:bold">Fig. 3. Loss of VQ-VAE</center><br>

<p>以上都是训练，实际Serving的时候就和VQ没什么关系了，还是从某个先验中采样出Embedding输入Decoder，而不是用Codebook（即索引表）的Embedding。</p>
<h3 id="Streaming-VQ-1"><a href="#Streaming-VQ-1" class="headerlink" title="Streaming VQ"></a>Streaming VQ</h3><p>在VQ-VAE中，VQ实际上只是起到了一个辅助VAE训练的作用，最终的Codebook在Serving的时候并没有起到作用，而在Streaming VQ中，Codebook不仅有索引，每个索引下还有着对应的归属于该索引的视频，因而是有着实际作用的。</p>
<h4 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h4><p>整体架构如大的黑色虚线上面的部分所示：</p>
<p><img src="/2026/01/18/Retrievers-Indexing-and-Ranking/4.png" alt="streaming_vq"></p>
<center style="font-size:12px; font-weight:bold">Fig. 4. Streaming VQ</center><br>

<p>相比VQ-VAE，下游任务不再是重构，而是对应的推荐系统二分类任务，并且Codebook的更新使用了<a href="https://zhuanlan.zhihu.com/p/68748778">EMA</a>，因此最终的损失函数实际只有一项，对应VQ-VAE的重构Loss，但细拆下来会有两项，分别是Item（视频）本身与User的分类Loss，以及Codebook与User的分类Loss：</p>
<ul>
<li><p>Item_User：更新G侧和U侧。<br><img src="/2026/01/18/Retrievers-Indexing-and-Ranking/5.png" alt="item_user"></p>
<center style="font-size:12px; font-weight:bold">Fig. 5. Item-User</center><br>
</li>
<li><p>Codebook_User：Codebook的Loss回传更新G侧，Codebook本身由G侧Embedidng EMA更新，U侧是否更新不清楚。<br><img src="/2026/01/18/Retrievers-Indexing-and-Ranking/6.png" alt="codebook_user"></p>
<center style="font-size:12px; font-weight:bold">Fig. 6. Codebook-User</center><br></li>
</ul>
<p>以上两者都采用In-batch softmax，即InfoNCE（无温度系数，定义见<a href="https://arxiv.org/pdf/1911.05722">MoCo</a>）。</p>
<blockquote>
<p>In-batch softmax，本质上是一种负采样方法。对于一个Batch内的U-G对，将匹配的$U _i$与$G _i$作为正样本，而将$U _i$与$G _j$作为负样本，于是对一个大小为n的batch，就有n个正样本，n(n-1)个负样本。但这会导致视频成为负样本的概率与其出现次数成正比，导致对高热视频的打压，<a href="https://dl.acm.org/doi/10.1145/3298689.3346996">Sampling Bias</a>即用于解决这个问题。</p>
</blockquote>
<h4 id="Serving"><a href="#Serving" class="headerlink" title="Serving"></a>Serving</h4><p>由于G侧塔和U侧塔的特征是完全独立的，因此在同一个时刻每个Item都会唯一地被分配一个索引，那么实际我们只要计算U侧Embedidng和Codebook各个索引Embedding的内积，取靠前的几个作为我们的候选即可。但是，用户的兴趣是多元的（或者说长尾的），分低的索引中也可能有用户很感兴趣的视频，前面的处理会导致过于注重头部兴趣，而忽略了尾部兴趣。Streaming VQ的解决方式是为每个Item引入单独的Bias项，起到Embedding表征个性化、Bias表征视频热门程度的作用：</p>
<p><img src="/2026/01/18/Retrievers-Indexing-and-Ranking/9.png" alt="personality_formula"></p>
<center style="font-size:12px; font-weight:bold">Fig. 7. Item bias for personality and popularity: formula</center><br>

<p><img src="/2026/01/18/Retrievers-Indexing-and-Ranking/8.png" alt="personality"></p>
<center style="font-size:12px; font-weight:bold">Fig. 8. Item bias for personality and popularity: model</center><br>

<p>如此，Codebook中不同索引的Item也变得可比了，于是我们可以直接k路归并地召回：</p>
<p><img src="/2026/01/18/Retrievers-Indexing-and-Ranking/7.png" alt="K-merge"></p>
<center style="font-size:12px; font-weight:bold">Fig. 9. K-merge sort</center><br>

<p>还有一个问题是，如何为所有的样本打上索引标签？文中的解决方式是两个数据流：</p>
<ul>
<li>曝光样本组成的实时数据流：即正常的流式更新，有梯度回传，如Fig4中item feature的蓝线所示；</li>
<li>所有候选数据流：仅生成G侧塔的Embedding以及对应的索引，无梯度，如Fig4中item feture和黑色虚线所示。</li>
</ul>
<h4 id="Trick"><a href="#Trick" class="headerlink" title="Trick"></a>Trick</h4><p>总的框架就如上面所述，对一个用户，Serving时：User Embedding和Codebook Embedding内积求相似度-&gt;k路归并召回一定数量的视频-&gt;Ranking模型打分将前面的视频送入粗排。</p>
<p>但实际要真能在线上有用，还是需要一些小技巧。</p>
<h5 id="Index-Reparability"><a href="#Index-Reparability" class="headerlink" title="Index Reparability"></a>Index Reparability</h5><p>即视频对应索引的可变性。对比VQ-VAE和Streaming VQ的Loss可发现，除去Streaming VQ使用EMA代替梯度回传外，Streaming VQ还少了$\mathcal{L} _3$，即用于让Item Embedding不会过多偏移其所属的Codebook Embedding的L2正则项，起到的作用为让Item的对应的索引保持稳定。</p>
<p>如文中所述，图片的特征是静态的，其所属的索引保持稳定没有任何问题；而对于推荐的视频，其不少特征是动态变化的，如互动、播放量等，因此其对应的索引也应该是可变化的。故而Streaming VQ去掉了这一项正则，而让Item_User的Loss去更新Item。</p>
<h5 id="Index-Balancing"><a href="#Index-Balancing" class="headerlink" title="Index Balancing"></a>Index Balancing</h5><p>即每个索引包含的视频个数要尽可能地均衡。由于VQ的映射是多对一的，因此Codebook_User Loss这一项的存在会天然地要求每个索引所包含的视频个数尽可能一致，如5个视频5个索引，那肯定是一对一的Loss最小，因为这时Codebook Embedding就等于Item Embedding。</p>
<p>为了进一步提升均衡性，文中还有两个额外的改进：</p>
<ol>
<li>EMA时，用视频的在当前时间步的出现周期$\delta ^t$作为放缩项，减小高热Item的权重；同时维护一个<strong>索引k</strong>出现次数的计数$c _k$，打压高热Cluster的更新。<br>$$<br>\begin{align*}<br>\mathbf{w} ^{t+1} _k&amp;&#x3D;\alpha \mathbf{w} _k ^t+(1-\alpha)(\delta ^t) ^{\beta}v _j ^t,\\<br>c _k ^{t+ 1}&amp;&#x3D;\alpha c _k ^t+(1-\alpha)(\delta ^t) ^{\beta},\\<br>\mathbf{e} _k ^{t+1}&amp;&#x3D;\frac{\mathbf{w} _k ^{t+1}}{c _k ^{t+ 1}}<br>\end{align*}<br>$$</li>
<li>复用前面的$c _k$，对出现次数少的Cluster做一个Boost。<br><img src="/2026/01/18/Retrievers-Indexing-and-Ranking/10.png" alt="boost"><center style="font-size:12px; font-weight:bold">Fig. 10. Low loading cluster boost</center><br></li>
</ol>
<p>即，在分布均匀可由VQ自身保证的情况下，分别从Item侧（防止Cluster被高热Item主导）和Cluster侧（防止训练被高热Cluster主导）做了平衡措施。</p>
<blockquote>
<p>$\delta$定义源自<a href="https://dl.acm.org/doi/10.1145/3298689.3346996">Sampling Bias</a>。按论文中的方法，为每个GID分配一个$\delta _i$，初始化为0，以及最近一次出现的epoch$t _i$，然后当其在本epoch出现时，移动平均地更新：$\delta _i&#x3D;(1-\alpha) \delta _{i-1} + \alpha (t - t _i)$；$t _i &#x3D; t$，不难看出，当更新步数足够大时，$\delta _i$即为对应视频出现的周期，那么$1&#x2F;\delta _i$即为其出现的频率。原文用这个频率为In-batch softmax的双塔内积消偏，即：$s(x _i, y _j) - \log(p _j)$，最终效果为对热门物品，放大其打分。</p>
</blockquote>
<h5 id="Multi-task-Streaming-VQ"><a href="#Multi-task-Streaming-VQ" class="headerlink" title="Multi-task Streaming VQ"></a>Multi-task Streaming VQ</h5><p>仅针对Indexing时的多目标，因为Ranking的多目标和粗精排没什么区别。</p>
<p>一言以蔽之：仅U侧塔个数变为多目标个数，前面的负载均衡公式引入每个目标的Reward项$h$（即0&#x2F;1，是否为正样本，最终累乘的结果实际上就是一个多项式相乘的融分）调节多目标的更新权重：</p>
<p><img src="/2026/01/18/Retrievers-Indexing-and-Ranking/11.png" alt="boost"></p>
<center style="font-size:12px; font-weight:bold">Fig. 11. Multi-task</center><br>

<h2 id="DR"><a href="#DR" class="headerlink" title="DR"></a>DR</h2><h2 id="NANN"><a href="#NANN" class="headerlink" title="NANN"></a>NANN</h2><h2 id="TDM"><a href="#TDM" class="headerlink" title="TDM"></a>TDM</h2><h2 id="HNSW"><a href="#HNSW" class="headerlink" title="HNSW"></a>HNSW</h2>]]></content>
      <categories>
        <category>Recommender System</category>
      </categories>
      <tags>
        <tag>Recommender System</tag>
        <tag>Recall</tag>
      </tags>
  </entry>
  <entry>
    <title>LLM</title>
    <url>/2025/04/06/LLM/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h1><h2 id="Entropy-amp-Cross-entropy-amp-KL-Divergence-amp-Perplexity"><a href="#Entropy-amp-Cross-entropy-amp-KL-Divergence-amp-Perplexity" class="headerlink" title="Entropy &amp; Cross-entropy &amp; KL Divergence &amp; Perplexity"></a>Entropy &amp; Cross-entropy &amp; KL Divergence &amp; Perplexity</h2><p>对于随机事件 $X$, 其有 $n$ 种可能的具体事件, 每个具体事件发生的概率为 $p(x _i)$. 从信息论的角度, 当我们观测到具体事件时, 该具体事件所蕴含的信息量 $H(x _i)$ 应满足:</p>
<ol>
<li>越小概率的事件发生了产生的信息量越大, 即<strong>事件的包含的信息量与其发生的概率成反比</strong>;</li>
<li>两个<strong>独立</strong>随机事件同时发生时, 其产生的信息量应等于两者分别发生时产生信息量的<strong>和</strong>, 即 $H(x _i\cdot y _j)&#x3D;H(x _i) + H(y _j)$.</li>
</ol>
<p>$-\log(x)$ 函数正好满足这一性质. 因此 $-\log(x _i)$ 即表示具体事件 $x _i$ 发生时产生的信息量. 当以 2 作为底数时, 其又可被解释为&quot;表征所这一信息需要的<strong>比特数</strong>&quot;. 对于整个随机事件 $X$, 其信息量的期望即为该随机事件 $X$ 的 <strong>信息熵</strong> (简称熵, Entropy):</p>
<p>$$<br>Entropy&#x3D;H(p, X)&#x3D;-\sum\limits _{i&#x3D;0}^{n-1}p(x _i)\log p(x _i)<br>$$</p>
<p>熵同时也代表着系统的混乱程度, 当我们不得不用近似的随即事件分布 $q$ 去拟合真实的随机事件分布 $p$ 时, 其熵的期望称为<strong>交叉熵</strong> (Cross-entropy):</p>
<p>$$<br>Cross-entropy&#x3D;H(p, q, X)&#x3D;-\sum\limits _{i&#x3D;0}^{n-1}p(x _i)\log q(x _i), H(p, q, X) \ge H(p, X)<br>$$</p>
<p>真实熵与交叉熵的差距即 <strong>KL散度</strong> (Kullback–Leibler Divergence)：</p>
<p>$$<br>D _{KL}(p||q)&#x3D;H(p, q, X)-H(p, X)&#x3D;-\sum\limits _{i&#x3D;0}^{n-1}p(x _i)\log\frac{q(x _i)}{p(x _i)}&#x3D;\sum\limits _{i&#x3D;0}^{n-1}p(x _i)\log\frac{p(x _i)}{q(x _i)}&#x3D;\mathbb{E} _{x\sim p}\log\frac{p(x _i)}{q(x _i)}<br>$$</p>
<p>KL散度衡量了真实分布和近似分布的差距但不是距离, 因为 $KL(p, q) !&#x3D; KL (q, p)$, 除非两个分布完全一样. </p>
<blockquote>
<p>困惑度 (Perplexity) 是熵 (或交叉熵) 的指数, 取决于不同的场景, 对概率分布来说, 指熵的指数; 对模型来说, 指交叉熵的指数 (即 loss 的指数).</p>
</blockquote>
<h3 id="问答"><a href="#问答" class="headerlink" title="问答"></a>问答</h3><ol>
<li>为什么分类问题用 Cross-entropy 而不用 MSE?</li>
</ol>
<p><strong>从损失函数自身来说, CE 是有倾向性的, 因为我们只对 Label 会有熵, 因此 CE 就倾向于优化 Label 的概率趋近于1; 而 MSE 是没有倾向性的, 比如对于 [1, 0, 0] 这个 Label, MSE 会认为 [0.8, 0.1, 0.1] 比 [0.8, 0.15, 0.1] 好, 即平均会比有倾向性好, 但对于分类来说这样是没有必要的; 然后从梯度的传递来说, 对 softmax 后的 MSE 来说, 其梯度是 $(-y _{hat}(1-y _{hat})^2)$, 这会导致其在我们分类完全错误或正确的时候没有梯度, 而越是分类错误我们反而越需要梯度, CE 则避免了这个问题, 因为它的梯度是 $y _{hat}-1$, 即在分类错误的时候会有最大的梯度.</strong></p>
<ol start="2">
<li>Label Smoothing 公式以及作用原理?</li>
</ol>
<p><strong>one-hot 编码时 softmax 后交叉熵梯度为零时的理论情况为: $y _{hat}-y$, 即此时正确类 softmax 后的概率为 1, 相应地, 错误类的概率都为 0, 进而可推导出来正确类的 logit 是一个常数, 而错误类的 logit 是负无穷. 这样会导致正确类和错误类的logit输出误差很大, 网络的泛化能力不强. 并且因为网络训练时会有一些正则化的存在, logit的输出很难是负无穷. label smoothing 的编码方式只要正确类和错误类有一定的数值误差即可，这个取决于分类的类别数量和 $\epsilon$. 网络极使在正则化的情况下也比one-hot容易学习到最优情况.</strong>:</p>
<p>$$<br>y _i&#x3D;<br>\begin{cases}<br>1-\epsilon, i&#x3D;true,\<br>\frac{\epsilon}{K-1}, otherwise, \text{K is the number of class}<br>\end{cases}<br>$$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># code implementation</span></span><br><span class="line"><span class="comment"># (B, T, V)</span></span><br><span class="line">logits = model(...)</span><br><span class="line">log_probs = -F.log_softmax(logits, dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># (B, T, V) -&gt; (B, T), 相当于给其他的 label 均分了一些数值</span></span><br><span class="line">ce_loss = log_probs.gather(dim=-<span class="number">1</span>, index=labels)</span><br><span class="line">smoothed_loss = log_probs.<span class="built_in">sum</span>(dim=-<span class="number">1</span>) / V</span><br><span class="line"></span><br><span class="line">loss = (<span class="number">1</span> - epsilon) * ce_loss + epsilon * smoothed_loss</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>top-k、top-p, beam search</li>
</ol>
<p><strong>top-k: 选取前 k 个概率最高的词, 将这 k 个词的概率重新归一化形成新的分布, 从新的分布中随机采样; top-p: 选取累计概率超过 p 的词集 S, 在 S 中进行随机采样, 比 top-k 更灵活; beam-search: 与前两者不同, 最终会生成 B 个候选序列, 每次生成, 选取概率最高的 B 个词, 然后用这 B 个词独立接着生成, 重复上述过程 (注: 第一步以后, 是所有的生成结果中选 B 个), 最终得到 B 个生成结果.</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">beam_search</span>(<span class="params">decoder, k, max_time_steps</span>):</span><br><span class="line">    sequences = [[[<span class="string">&#x27;&lt;start&gt;&#x27;</span>], <span class="number">1.0</span>]]</span><br><span class="line">    ret = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> k <span class="keyword">and</span> max_time_steps:</span><br><span class="line">        all_candidates = []</span><br><span class="line">        <span class="keyword">for</span> sequence <span class="keyword">in</span> sequences:</span><br><span class="line">            seq, score = sequence[<span class="number">0</span>], sequence[<span class="number">1</span>]</span><br><span class="line">            token_list, scores = decoder(seq)</span><br><span class="line">            <span class="keyword">for</span> token, s <span class="keyword">in</span> <span class="built_in">zip</span>(token_list, score):</span><br><span class="line">                candidate = [seq + token, score * s]</span><br><span class="line">                all_candidates.append(candidate)</span><br><span class="line">        <span class="comment"># 是在所有的生成结果中选前 k 个, 而不是每条分支选 k 个</span></span><br><span class="line">        ordered = <span class="built_in">sorted</span>(all_candidates, key=<span class="keyword">lambda</span> tup: tup[<span class="number">1</span>])  <span class="comment"># 按 score 升序排序</span></span><br><span class="line">        sequences = ordered[-k:]  <span class="comment"># 选分数最高的 k 个</span></span><br><span class="line">        tmp = []</span><br><span class="line">        <span class="keyword">for</span> seq <span class="keyword">in</span> sequences:</span><br><span class="line">            <span class="keyword">if</span> seq[<span class="number">0</span>][-<span class="number">1</span>] == <span class="string">&#x27;&lt;end&gt;&#x27;</span>:</span><br><span class="line">                ret.append(seq)</span><br><span class="line">                k -= <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                tmp.append(seq)</span><br><span class="line">        sequences = tmp</span><br><span class="line">        max_time_steps -= <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(ret) &lt; k:</span><br><span class="line">        ret.append(sequences[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">return</span> ret</span><br></pre></td></tr></table></figure>


<ol start="4">
<li>accuracy, precision, recall, roc, auc</li>
</ol>
<p><strong>Acc: (TP + TN) &#x2F; (TP + FP + TN + FN), 判断正确的比例; Precision: (TP) &#x2F; (TP + FP), 预测为正类的中真正正类的比率; Recall: (TP) &#x2F; (TP + FN), 真正为正类样本被预测正确的比例. <code>记忆方法: y(预)p z(真)r</code>. F1 score 是 PR 的调和平均值. ROC: 二分类下, 正类被预测正确的准确率 (Y 轴) 和负类被预测为正类的比例 (X 轴) 随阈值的分布. ROC 下的面积为 AUC, 越大表模型分类效果越好.</strong></p>
<ol start="5">
<li>ROC 和 AUC 的进一步讨论</li>
</ol>
<p><strong>AUC 一种直观的解释是给定一个正负样本对, 模型对正样本打分高于负样本的概率, 因此 AUC 很适合作为排序场景的指标 (如用户是否会点击、是否会购买等). 也正因如此, AUC 对正负样本的比例不敏感, 因为 AUC 是对正负样本分别考虑的, 是一个相对客观地分别评估模型在正样本和负样本中能力的指标, 只要正负样本自身的分布不发生大的变化, 两者的比例对 AUC 几乎没影响. 同样地, 由于这个性质, AUC 只考虑相对性, 当场景需要考虑绝对数量的时候, 即场景需要正例的概率很高时, AUC 可能不太合适, 如广告, 此时logloss可能更适合. 实际上就是 Bradley-Terry (对 ROC 的近似) 和 CE loss. AUC 有三种计算方法：1）取定不同阈值画图；2）正样本大于负样本的概率，太慢；3）对二的改进，即先排序。</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">BCE</span>(<span class="params">self, target</span>):</span><br><span class="line">    loss = (target - <span class="number">1</span>) * torch.log(<span class="number">1</span>-self) - target * torch.log(self)</span><br><span class="line">    <span class="keyword">return</span> loss.mean()</span><br><span class="line"></span><br><span class="line">label = torch.randint(low=<span class="number">0</span>, high=<span class="number">2</span>, size=(<span class="number">10</span>,), dtype=torch.float32)</span><br><span class="line">y = torch.nn.functional.sigmoid(torch.rand((<span class="number">10</span>,)))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(BCE(y, label))</span><br><span class="line"><span class="built_in">print</span>(torch.nn.functional.binary_cross_entropy(y, label))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">CE</span>(<span class="params">self, target</span>):</span><br><span class="line">    self = torch.nn.functional.softmax(self, dim=-<span class="number">1</span>)</span><br><span class="line">    loss = -torch.log(torch.gather(self, dim=<span class="number">1</span>, index=target.unsqueeze(-<span class="number">1</span>)))</span><br><span class="line">    <span class="keyword">return</span> loss.mean()</span><br><span class="line"></span><br><span class="line">label = torch.randint(low=<span class="number">0</span>, high=<span class="number">5</span>, size=(<span class="number">10</span>,))</span><br><span class="line">y = torch.rand((<span class="number">10</span>, <span class="number">5</span>))</span><br><span class="line">loss = torch.nn.CrossEntropyLoss()</span><br><span class="line"><span class="built_in">print</span>(CE(y, label))</span><br><span class="line"><span class="built_in">print</span>(loss(y, label))</span><br></pre></td></tr></table></figure>

<ol start="6">
<li>softmax 温度系数的影响</li>
</ol>
<p><strong>温度系数大于 1 时, 会软化分布, 使得输出的分布更加平滑; 温度系数小于 1 时, 输出的概率分布会更加接近 one-hot 的形式, 模型对结果的置信度会更高, 所以在推理的时候往往会设置小于 1 的温度系数.</strong></p>
<ol start="7">
<li>大模型幻觉</li>
</ol>
<p><strong>就是大模型会胡说八道, 输出一些不符合事实的内容. 本质上因为 Causal Transformer 是一个概率模型, 所以在 next token prediction 的时候是根据只是根据计算结果选择了概率大的 Token 输出. 解决方法有, 比如修改训练数据, 让大模型性能够说<code>不</code>, 这种方式的依据是大模型参数那么多, 应该有些参数是和模型自身的<code>困惑度</code>之类相关的, 所以只要训练这些参数, 让模型能在在这个激活值比较大时回答<code>不</code>即可. 还有就是增加模型的上下文, 比如 RAG, 对推理问题还有 CoT 等等.</strong></p>
<ol start="8">
<li><code>repetition_penalty</code></li>
</ol>
<p><strong>将已生成词的概率除以 <code>repetition_penalty</code>. 大于1: 减少重复词概率; 小于1: 增加重复词概率. 现在一般都设成 1 了, 即不考虑这个东西了.</strong></p>
<ol start="9">
<li>长文本问题: 与推理结果相关的内容在文本中的位置会怎样影响模型的推理结果?</li>
</ol>
<p><strong>尾&gt;首&gt;中&gt;随机. 因为整体而言, 模型的 attention 会集中分布在头部和尾部, 中间的 attention 会少.</strong></p>
<ol start="10">
<li>什么是压缩即智能?</li>
</ol>
<p><strong>一种理解是模型在训练集上的 perplexity, 越小说明模型对数据记忆地越好, 也就是模型把数据集很好地压缩到了模型中. 而 perplexity 其实就是训练的 loss, 所以就是 loss 越低, 模型的泛化性越好, 完成各种人类任务的能力越好.</strong></p>
<ol start="11">
<li>内存不够时, batch_size 和 lr 的调节关系.</li>
</ol>
<p><strong>lr 与 batch_size 同向变化, 但倍数为 batch_size 变化的根号.</strong></p>
<ol start="12">
<li>LLM 在数学问题上为什么 majority voting 总是有效?</li>
</ol>
<p><strong>因为 LLM 是一个概率模型, 假设 LLM 做对某道题的概率是 p, 如果只 sample 一次, 那就是 p, 但是如果 sample 多次, 那么正确答案出现次数大于一半的概率就可用一个二项分布来算, 而当 p &gt; 0.5 的时候, 这个结果都是严格大于 p 的. 反过来, 如果 p &lt; 0.5 那投票也不会有效.</strong></p>
<ol start="13">
<li>Adam 和 AdamW 的区别?</li>
</ol>
<p><strong>Adam 的 weight decay 是应用在梯度上的, 因此会参与一阶和二阶矩的计算; AdamW 将其移除, 直接放在最后更新上. 即: 对 Adam $g &#x3D; \delta f&#x2F;\delta w + w$, 而对 AdamW: $w &#x3D; w - lr(动量+w)$ (此处省略了 weight decay参数)</strong></p>
<ol start="14">
<li>二分类如何处理多分类?</li>
</ol>
<p><strong>两种策略: ovo, 训练 n(n-1) &#x2F; 2 个二分类器; ovr 训练 n 个二分类器. 相应地, 混淆矩阵也有 n(n-1) - 2 或 n 个. F1-score 也有两种计算方法: 对每个分类器的 P 和 R 求平均, 再算 F1; 对每个分类器的 TP、TN、FP、FN 求平均, 再求 P R F1.</strong></p>
<ol start="15">
<li>k-means 的过程</li>
</ol>
<p><strong>选择 k 个初始聚类中心, 每个样本计算到 k 个中心的距离, 选择最小的一个加入那个聚类中, 每个聚类重新计算聚类中心. 重复直至收敛.</strong></p>
<ol start="16">
<li>LSTM</li>
</ol>
<p><strong>忘记门、输入门、输出门. 两个 memory $C _{t-1}$ 和 $H _{t-1}$.</strong></p>
<ol start="17">
<li>GRU</li>
</ol>
<p><strong>更新门 (加入)、重置门 (忘记).</strong></p>
<ol start="18">
<li>attention 和 LSTM 的 GATE的不同</li>
</ol>
<p><strong>首先两者有些相同点, 都是用加权的方式控制过去信息对当前信息的影响, 使得当前的输出依赖于历史. 但是 LSTM或者GRU 的 GATE 是自回归, 所以它后一个时刻如果想对之前某个时刻的信息进行增强是没有办法的, 因为它自回归的特性决定了前面的信息到后面只会越来越少, 所以不可避免的会有长时遗忘的问题. Attention 则</strong></p>
<ol start="19">
<li>分组卷积 和 普通卷积的区别</li>
</ol>
<p><strong>分组卷积分组融合, 所以如果组的个数等于通道个数那就是不融合.</strong></p>
<ol start="20">
<li>xgboost</li>
</ol>
<p><strong>叶子的值作为参数, 叶子的个数可作为模型的复杂度. 核心理念为为多个模型 (多棵树) 去进行函数拟合, 后一棵树拟合前一棵树的残差. 按顺序训练, 因此后一棵树训练的时候前一个树的参数以及复杂度均为常数 (树的复杂度除了叶子个数以外, 还会加入权重的 L2 正则, 因为 w 越大越容易过拟合. 即多个弱分类器要比一个强分类器泛化性好). 因为 xgboost 用二阶泰勒展开近似了损失函数, 所以对 xgboost 的每一棵树, 在划分一定时, 其最优权重是个解析解 (对二阶泰勒展开的) (即开口向上的二次函数), 因此没棵 Tree 优化的实际上是是否要增加开新分支增加新的叶子结点. GDBT 对权重的求解则用的梯度下降的方式.</strong></p>
<ol start="21">
<li>对比学习的温度和大模型推理时的温度有什么区别?</li>
</ol>
<p><strong>对比学习的温度是用来稳定训练的, 一般是设置为0-1, 好处在于, 当模型分类正确时, 因为小于 1 的温度会使得概率分布趋于 one-hot, 所以产生的梯度就会偏小, 对模型起着稳定的作用; 而当模型分类错误的时候, 也是因为概率分布会趋于 0-1, 所以此时正确类的概率会比没有温度的时候更小, 于是就能够产生更大的梯度, 从而加速模型的收敛.</strong></p>
<h2 id="一些数学上的东西"><a href="#一些数学上的东西" class="headerlink" title="一些数学上的东西"></a>一些数学上的东西</h2><ul>
<li>蒙特卡洛方法: 用估计值 (多数情况为均值) 近似理论值. 当采样次数够多, 估计值就会收敛到理论值, 如强化学习中对状态价值的估计、对 KL 散度的估计.</li>
<li>二项分布: n 次伯努利重复实验正例出现个数的分布.</li>
<li>泊松分布: 单位时间随机事件发生次数的分布, 如一个小时加油站进站车的个数. 当 n 很大时, 二项分布可用近似.</li>
<li>B分布: 是定义在 (0, 1) 的概率分布. 有 $\alpha$ 和 $\beta$ 两个参数, 两者相等时概率密度关于 0.5 对称. 两者均为 1 则退化为均匀分布.</li>
</ul>
<h2 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h2><p>因为自注意力不感知位置信息, 所以需要位置编码来辅助其学习文本的语序 (但其实因果注意力是能够感知位置的.). 当下的大模型基本用的都是 RoPE.</p>
<ol>
<li>绝对位置编码: 编码 Token 的绝对位置.<ul>
<li>正弦位置编码 (Sinusoidal): &quot;Attention is all you need&quot; 原文所采用的.</li>
<li>可学习位置编码: GPT2 采用的. 与正弦位置编码一样直接和输入 Token 相加.</li>
</ul>
</li>
<li>相对位置编码: 编码 q 和 k 的相对位置.<ul>
<li>旋转位置编码 (RoPE): 用向量的角度来代表位置, 因而施加的方式是与投影后的 q, k 相乘.</li>
<li>ALiBi: 直接加在注意力矩阵上, (-n, ..., -1, 0) 表示相对当前 Token 的位置.</li>
</ul>
</li>
</ol>
<h3 id="问答-1"><a href="#问答-1" class="headerlink" title="问答"></a>问答</h3><ol>
<li>频率项的计算?</li>
</ol>
<p>RoPE 和 Sinusosial 都有统一的频率 (角度) 计算方式:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># k / (base)^&#123;2i/d&#125;, 其中 k 是 Token 的位置, i 是 Embedding 的位置, d 对 正弦位置编码是 Token 维度, 但对 RoPE 是 Head 的维度</span></span><br><span class="line">base, seq_len, n_embd = <span class="number">10000</span>, <span class="number">1024</span>, <span class="number">768</span></span><br><span class="line">t = torch.arange(seq_len)</span><br><span class="line">freqs = <span class="number">1.0</span> / (base**(torch.arange(<span class="number">0</span>, n_embd, <span class="number">2</span>)/n_embd))</span><br><span class="line"><span class="comment"># (seq_len, n_embd//2)</span></span><br><span class="line">freqs = torch.outer(t, freqs)</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>RoPE 是如何处理训练长度以外的长度的? 即 RoPE 如何进行长度外推?</li>
</ol>
<p><strong>有两种可以结合的方式. 第一种是对 RoPE 的旋转弧度进行放缩. 最开始是根据新的序列长度和预训练长度的比值去缩小旋转弧度. 后来有文章指出, 因为 RoPE 对于位置越靠前的 embedding 旋转角度越大, 频率越高, 所以越靠前的特征对位置信息的训练越足, 外推能力越强, 因为见到的角度变化越多, 而越靠后的受到的训练越少, 外推能力越弱, 所以对不同的特征进行相同程度的放缩会导致高频信息的丢失. 因此后续的方法有像 ntk-aware 的在进行长推理的时候把 base 扩大; 也有像 ntk-by-parts 的对靠前的特征的角度不做放缩, 只对靠后的进行放缩. 现在用得更多的应该是 dynamic ntk, 就是对推理长度等于训练长度的部分不进行操作, 当推理长度大于训练长度时, 动态地根据当前的长度放大 base. 第二种是对注意力分数进行放缩, 因为 RoPE 在 qk 相乘的时候转换为相对位置信息的, 所以在缩小了旋转角度之后, qk 之间的旋转弧度就变小了, 两者内积得到的注意力分数也相应的增大了, 最终会导致注意力分数分布的变化. 所以就要对注意力分数进行放缩, 方式可以是像 YaRN 用一个用推理长度和训练长度计算出来的问题系数放缩, 也可以是 LogN-Scaling. 此外 RoPE 的远程衰变性对最远的 Token 也不是直接衰变到 0 的, 而是会振荡, 这使得模型在长窗口也能够召回一些相关的 Token.</strong></p>
<p>$$<br>(\frac{l&#39;}{l})^{d&#x2F;(d-2)}<br>$$</p>
<ol start="3">
<li>RoPE 的远程衰减特性是什么?</li>
</ol>
<p><strong>远程衰减性是指 qv 的内积随着它们相对距离的增加会有衰减的趋势, 这就给模型加入了越远的 Token 与当前 Token 越不相关的先验. (theta 要随着位置单调递增才行, sinusosial 就可满足). (本质上是旋转角度的余弦 $\cos n _{relative}\theta$ 在 $(0, 1)$ 单调递减, 因此实际上远程衰变性是由低频的、旋转角度小的、位于后方的特征保持的. 高频的、旋转角度大的因为转动较快, 所以呈现出的是周期性, 而低频的才呈现出单调性, 因此远程衰减性是由低频部分维持的.)</strong></p>
<ol start="4">
<li>LogN-Scaling and YaRN 中 缩放公式的区别.</li>
</ol>
<p><strong>LogN</strong>:<br>$$<br>\text{softmax}(\frac{\mathcal{\kappa\log n}}{d}QK^T)V<br>$$</p>
<p>其中 $\kappa$ 是超参数, $n$ 是新的序列长度, $d$ 是注意力头的维度, 若要使得对预训练长度内的性能不变, 则 $\kappa$ 可设为 $\sqrt{d}&#x2F;\log n _{pre}$, 最终得到:<br>$$<br>\text{softmax}(\frac{\mathcal{\log _{n^{pre}} n}}{\sqrt{d}}QK^T)V<br>$$</p>
<p><strong>YaRN</strong>:<br>$$<br>\text{softmax}(\frac{1}{t\sqrt{d}}QK^T)V<br>$$</p>
<p>其中:</p>
<p>$$<br>\sqrt{\frac{1}{t}}&#x3D;0.1*\ln(\frac{n}{n^{pre}}) + 1<br>$$</p>
<blockquote>
<p>LogN 的推导原理: 熵不变性, 即新长度下和旧长度下, 同一个 Token 注意到的 Token 不会发生变化. 见苏剑林文章.</p>
</blockquote>
<ol start="5">
<li>还有其他的外推方式吗?</li>
</ol>
<p><strong>Window Attention, 就是把注意力往前看的对象限制在一个窗口内, 低层用较短的窗口, 高层用更长的窗口. Qwen 用的是 dynamic ntk, logN-Scaling, 和 window attention.</strong></p>
<ol start="6">
<li>Causal Attention 不具有置换不变性, 为什么现在的 Decoder-only 架构还需要位置编码?</li>
</ol>
<p><strong>置换不变性指 Token 的次序不影响输出结果, 双向注意力是具有置换不变性的, 但单向注意力没有. 有研究指出, 单向注意力不需要位置编码, 即 NoPE 也能取得很好的效果, 一种解释是单向注意力因为不同位置的 Token 注意力加权求和的元素个数不同, 即第一个位置只和其自身, 第二个位置是前一个位置和自身... 所以输出结果的方差也不同 (假设输入都是同分布的情况下), 而位置信息就编码在这些方差中. 但是 NoPE 实现的是绝对位置编码, 而实验上的结果显示相对位置编码更适合自然语言, 并且 NoPE 也没有 RoPE 等带来的远程衰减的先验, 即对距离越远的信息应该给予给少的关注, 所以目前来说加上相对位置编码还是更好的.</strong></p>
<h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><h3 id="问答-2"><a href="#问答-2" class="headerlink" title="问答"></a>问答</h3><ol>
<li>为什么 q, k 转置相乘后还要除 $\sqrt{d}$ ($d$ 为注意力头的维度) 再做 softmax?</li>
</ol>
<p><strong>这和模型的初始化以及标准化想要解决的问题是类似的. 为了神经网络的训练稳定, 防止梯度爆炸和消失, 我们会希望输入和输出的某些统计特性是一致的, 比如都是均值为 0, 方差为 1, 或者二阶矩都是 1. (括号内的不说. 基于此, Xavier 将 Linear Weight 初始化为均值为 0, 方差为 1&#x2F;m (因为输出的方差是mVar(输入), m 是输入维度, 也好理解 m 个相加嘛); kaiming初始化针对 relu 会让一半的数值归零, 将带有 relu 的网络的初始化方差调整为 2&#x2F;m. 更直接的方法是标准化 (LayerNorm, BatchNorm...), 即强行让输出为正太分布, 或者直接将输出除以 1&#x2F;$\sqrt{\text{m}}$.) 假设 q, k 都服从标准正太分布, 那么其内积的方差 (二阶矩也是) 就是 $d$, 这会导致注意力分数的极差很大$(-3\sqrt{d},3\sqrt{d})$, 那么 softmax 之后权重最大的就会接近 1, 最小的接近 0, 整体趋于一个 one-hot 的分布, 所以会带来较严重的梯度消失问题. 解决办法就是除 $\sqrt{d}$, 让 qk 内积的方差变回 1, 或者初始化 qk 的线性层时让方差多除个 $\sqrt{d}$, 让 qk 输出的方差变成 1&#x2F;$\sqrt{d}$, 这样最后内积的方差就也还是 1 了.</strong></p>
<ol start="2">
<li>为什么现在多数模型用 PreNorm 而不是 PostNorm (原版 Transformer 和 BERT 采用)? 两者各有什么特点?</li>
</ol>
<p><strong>PostNorm 是对残差连接后的结果进行标准化, 带来的结果是前面层的通过恒等分支传递来的残差的效果随着层数的推进, 因为 Norm 而被削弱了; PreNorm 则恰恰相反, 它是对残差连接前的结果进行标准化的, 所以每一层的残差都会平等地传播到最后. 这带来的效果是, 因为残差是为了更好地训练, 所以 PreNorm 的模型更好训练. 但实际上有的文章指出, 在预训练效果一致的情况下, PostNorm 微调的效果会更好. 因为 PreNorm 对过往的残差都是等权相加的, 所以当层数很深的时候, 最深几层的输出结果会很相似, 体现在梯度上就是 PreNorm 模型低层的梯度会比高层的梯度大, 最终呈现的效果就是 PreNorm 有效深度降低. 而 PostNorm 则因为对前面层的残差会进行削弱, 所以虽然训练更难了, 但最终对高层的训练也会更加充足.</strong></p>
<ol start="3">
<li>为什么在最后的投影层前要对输出进行标准化?</li>
</ol>
<p><strong>因为现在多数大模型都采用的是 PreNorm 的架构, 即只标准化残差分支, 而不标准化恒等分支, 所以最后传递到投影层的结果是很多个残差相加的结果, 这就导致输出的方差比较大, 所以要通过标准化缩小方差.</strong></p>
<ol start="4">
<li>为什么现在都是 Decoder-only 架构?</li>
</ol>
<p><strong>Decoder-only 用的是 Causual-Attention, 即 Token 只对前面的位置求注意力. 相比于双向的注意力, 其好处在于生成的注意力矩阵是满秩的, 因为注意力矩阵是由 qk 内积得到的, 而 qk 其每个 head 的维度是远小于序列长度的, 所以双向注意力生成的是一个低秩的矩阵, 而单向注意力因为是上三角或下三角的, 而 softmax 又保证元素非 0, 所以其行列式非零, 进而是满秩的, 满秩矩阵的表达能力要强于低秩矩阵, 所以理论上来说 Causual-Attention 的表达能力更强.</strong></p>
<h2 id="Pytorch"><a href="#Pytorch" class="headerlink" title="Pytorch"></a>Pytorch</h2><h3 id="问答-3"><a href="#问答-3" class="headerlink" title="问答"></a>问答</h3><ol>
<li><p><code>model.eval()</code>, <code>with torch.no_grad()</code> 的不同</p>
<ul>
<li><code>model.eval()</code> 只用于将 <code>dropout</code> 和 <code>batchnorm</code> 设置为推理模式 (即让所有单元激活, 用学习到的全局 mean 和 var 计算 <code>batchnorm</code>), 其他行为与 <code>model.train()</code> 一模一样 (仍可梯度回传).</li>
<li><code>with torch.no_grad()</code>: 创造一个空间, 将生成的中间变量的 <code>requires_grad</code> 设置为 <code>False</code> (不改变已有参数的梯度情况), 因此不再会保留计算图, 这些中间变量在后续阶段的 <code>requires_grad</code> 仍是 <code>False</code> , 一般用在推理或不需要梯度的中间变量计算.</li>
</ul>
</li>
</ol>
<h2 id="资源、硬件、分布式相关"><a href="#资源、硬件、分布式相关" class="headerlink" title="资源、硬件、分布式相关"></a>资源、硬件、分布式相关</h2><h3 id="问答-4"><a href="#问答-4" class="headerlink" title="问答"></a>问答</h3><ol>
<li>FlashAttention 的原理?</li>
</ol>
<p><strong>核心目的是减少 GPU 高速的 SRAM 和 较为慢的 HBM 间通信的次数, 因为当前 GPU 的计算能力是大于通信能力的, 所以 IO 是瓶颈. 具体做法是: 原本的 softmax, 我们需要先找到注意力分数中的最大项, 然后把所有的注意力分数都减去这项, 防止算指数的时候溢出, 这需要一轮的 IO, 然后计算 softmax 的分母要一轮 IO, 最后得到注意力权重然后和 value 相乘要一轮 IO, 共 3 轮. Flash Attention 的做法是, 把求最大注意力分数、计算 softmax 分母、以及注意力权重和 value 相乘都转换为一个递归的等比数列的形式, 以此实现在一轮 IO 中完成所有的计算.</strong></p>
<ol start="2">
<li>如何根据模型的参数量估计<strong>推理</strong>和<strong>全量训练</strong>的显存?</li>
</ol>
<p><strong>模型占用的显存有 3 个考量: 1) 模型自身占用的空间. 这取决于模型的精度以及参数量, 每 B 字节的参数一般对应 1G 的内存, 因此 FP16 的 7B 模型占用 14 G. 2) 优化器状态参数占用的空间. 对于 AdamW, 因为要为每个参数同时维护一阶和二阶动量, 所以占用空间是模型参数的 2 倍; 对于带一阶动量的 SGD, 占用空间为模型参数 1 倍; 对于不带动量的 SGD, 则不占用内存. 3) 动态的显存开销. 包括 a). 模型自身的梯度. 因为和模型精度相同, 所以占用内存与模型相同; b) 优化器的中间状态. 一般也和模型精度相同; c) 前向传播激活值. 通用公式为 $$(4.6894\times 10^{-4}\times N+1.8494\times 10^6)\times B\times L \times precision$$ 其中, $N$ 为参数量, 梯度和优化器中间状态一般在反向传播出现, 激活值一般在前向传播出现. 因此, 推理时只需要考虑 模型以及激活值, 而训练时则需考虑 模型、优化器以及max(梯度+中间状态, 激活值). 若只考虑 fp16, 则粗略地估计为: 推理显存约为 2<del>3N G, 训练约为 8</del>10N G (混合精度则还要加，大概16 N). 此外还要考虑框架开销和显存碎片, 因此要多预留 30% 左右.</strong></p>
<p><img src="/2025/04/06/LLM/mem.jpg" alt="mem"></p>
<center style="font-size:12px; font-weight:bold">Fig. 1. Memory Usage</center><br>

<ol start="3">
<li>混合精度 (fp32&#x2F;bf16) 训练时, 参数、梯度和优化器状态的精度各是怎么样的?</li>
</ol>
<p><strong>混合精度训练时, 显存的节省与参数、梯度和优化器无关, 仅来源于激活值. 因为: fp32 训练时, 参数+梯度+优化器&#x3D;4+4+4*2&#x3D;16; fp32&#x2F;bf16 时, 参数+梯度+优化器&#x3D;2+2+4+4*2&#x3D;16, 其中额外的 4 来自原始 fp32 参数的备份. 因此除激活值以外的开销是不变的.</strong></p>
<ol start="4">
<li>混合精度训练时, 前向、反向传播以及优化器的精度变化各是怎样的?</li>
</ol>
<p><strong>前向传播时, 输入、权重以及输出的精度都是 bf16, 部分归一化和累加计算 (e.g., softmax) 为了防止溢出会用 fp32; 反向传播时, 梯度的计算用的是 bf16, 梯度的累加用的是 fp32, 防止溢出; 优化器更新时, 梯度转为 fp32, 更新 fp32 的权重副本、优化器状态, 然后 fp32 权重转为 fp16 进行下一轮的前向、反向.</strong></p>
<p><img src="/2025/04/06/LLM/mix.jpg" alt="mix"></p>
<center style="font-size:12px; font-weight:bold">Fig. 2. Mixed Precision Training</center><br>

<ol>
<li>介绍一下模型并行 (MP) 和 数据并行 (DP).</li>
</ol>
<p><strong>模型并行是在不同的 GPU 中对相同的数据做部分模型的计算, 现在用的更多的是 <code>megatron</code> 的张量并行, 即把一个模型的线性层、attention head 分块放在不同的 GPU 中; 数据并行是在不同的 GPU 中对不同的数据做全模型的计算, 最简单的有 <code>DDP</code>, 每张卡都装相同的整个模型, 接受不同的输入, 还有 <code>ZeRO</code> 把优化器、梯度、参数都切分开, 放在不同的卡, 在需要用到的时候再通过 GPU 通信传输过来.</strong></p>
<ol start="6">
<li>介绍一下 <code>megatron</code> 以及分析一次前向&#x2F;反向传播 <code>megatron</code> 的通信量.</li>
</ol>
<p><strong><code>megatron</code> 是针对 <code>Transformer</code> 的模型并行策略 (但也可以拓展到其他架构). 假设有 k 张卡, 则基本的做法为: 对于 Embedding 层, 每张卡只存有 1&#x2F;k 的 Token 的 Embedding, 因此对本卡查不到的直接返回零向量, 然后通过一次 <code>allreduce(sum)</code> 从其他卡中拿到没有的 Token 的 Embedding; 对 Attention 层, 每张卡只有 qkv 投影层的 1&#x2F;k 列, 等价于每张卡各处理不同的头, 然后每张卡只有 o 的 1&#x2F;k 行, 最后还是通过一次 <code>allreduce(sum)</code> 得到最终的输出; 对 FFN 层, 每张卡只有上投影层的 1&#x2F;k 列, 下投影层的 1&#x2F;k 行, 最后还是通过一次 <code>allreduce(sum)</code> 得到最终的输出; 对最后的投影层 (n_embd x vocab_size), 每张卡只有 1&#x2F;k 列 (无论是否与 Embedding 层共享参数), 此时因为 vocab_size 很大, 因而通讯量巨大, 故会将其与 softmax 一起处理, 即本卡计算 softmax 的分母, 然后只 <code>allreduce(sum)</code>分母, 这样对一个batch和一个token通信量就只是一个标量. 前向传播时, 每个 Layer 两次通信, 此外头尾各一次, 因此通信个数为 $2\cdot(n _{layers}+1)$, 通信量为 $2\cdot(n _{layers}+1)\cdot b\cdot l\cdot d$. 反向传播通信开销与前向传播一致 (因为对每一层, 输入 x 是 repeat 的, 所以反传时要在此处 <code>allreduce</code>).</strong></p>
<ul>
<li><code>allreduce</code>: reduce (求和、求平均) + broadcast (广播到所有设备).</li>
</ul>
<ol start="7">
<li>介绍一下 <code>ZeRO</code>.</li>
</ol>
<p><strong>zero1: 分割优化状态参数. zero2: zero1的基础上还分割梯度. zero3: zero2的基础上还分割参数. 每张卡接受不同的 minibatch, 前向传播时, allreduce 参数, 反向传播时, allreduce 参数、梯度和优化器状态.</strong></p>
<ol start="8">
<li>有用过 <code>DeepSpeed</code> 吗, 介绍一下?</li>
</ol>
<p><strong>用过 accelerate + deepspeed. 基于 ZeRO 的 DP 框架.</strong></p>
<ol start="9">
<li>流水线并行 (PP)、序列并行 (SP) 和 专家并行 (EP) 各是什么?</li>
</ol>
<p><strong>PP: 按 Layer 切; EP: 不同专家放不同 GPU; SP: Sequence 维度的数据并行, 即把序列的分块放在不同的 GPU.</strong></p>
<ol start="10">
<li>NCCL (NVIDIA Collective Communications Library) 的通信方式:</li>
</ol>
<p><strong>NVLinK (GPU 直连) &gt; P2P (GPU 通过 PCI-E 直连) &gt;  PCI-E + CPU RAM (GPU1 -&gt; CPU -&gt; 内存 (通过 PCI-E) -&gt; CPU -&gt; GPU2)</strong></p>
<blockquote>
<p>以下通信量分析均把收发视作一次通信.</p>
</blockquote>
<ol start="11">
<li>分析 <code>Pytorch</code> 中 <code>DP</code> 和 <code>DDP</code> 的通信量 (设 $N$ 为 GPU 个数, $\psi$ 为参数量)</li>
</ol>
<p><strong><code>DP</code> 是单进程多线程的模式, 因而有一个主线程完成所有的收发工作. 主线程接收 $(N-1)\psi$ 的其他线程的梯度, 在本地更新参数后, 将 $(N-1)\psi$ 的参数发给其他线程, 其他线程各只用收发 $\psi$ 的参数. <code>DP</code> 的负载是极度不均衡的, 已被弃用. <code>DDP</code> 是多进程的模型, 进程间使用 Ring-AllReduce 的技术进行通信, 即进程间环状地向相邻进程传输不同的数据 (e.g., GPU0 -a-&gt; GPU1 -b-&gt; GPU2 -c-&gt; GPU3), 这样经过 $(N-1)$ 轮, 每个 GPU 上都有 $\psi&#x2F;N$ 的参数是通信好的, 该阶段称 Scatter-Reduce; 然后通过 AllGather 阶段, 同样地环状传递通信好的参数, 经过 $(N-1)$轮, 所有参数同步完毕, 总的通信量近似为 $2\psi$. 需要注意, DDP 只通信梯度, 而梯度后传是串行的, 所以已经计算好的梯度的参数积累到一定程度后就会进行通信.</strong></p>
<ol start="12">
<li>分析各个 <code>ZeRO</code> 的通信量</li>
</ol>
<p><strong>zero1: 前向传播时无需通信, 反向传播时, 每个 GPU 向拥有那部分优化器的 GPU 发送梯度, 该 GPU 同时接收其他两个 GPU 传来的梯度, 通信量为 $(N-1)\psi&#x2F;N$, 此外, 梯度更新后的参数要广播给其他 GPU 也要接收其他 GPU 广播给自己的参数, 通信量 $(N-1)\psi&#x2F;N$, 总通信量 $2\psi$, 与 DDP 一致; zero2: 通信量亦为 $2\psi$, 因为没有优化器本来也没必要存储梯度了; zero3: 前向传播要参数广播, 通信量 $(N-1)\psi&#x2F;N$, 反向传播计算梯度要参数广播, 通信量 $(N-1)\psi&#x2F;N$, 梯度要广播, $(N-1)\psi&#x2F;N$, 因为参数只在本地, 所以此时更新参数无需再广播, 故总的通信量为 $3\psi$. 可见, zero1 和 zero2 均不增加通信量, 所以一般用到 zero2. 但如果显存吃紧, 可用上 zero3.</strong></p>
<ol start="13">
<li>vllm 原理</li>
</ol>
<p><strong>PageAttention + shared kv cache (有同样上文的共享 kv cache) 加速推理.</strong></p>
<ol start="14">
<li>为什么 <code>bf16</code> 更受欢迎?</li>
</ol>
<p><strong>一方面是 <code>bf16</code> 相比于 <code>fp32</code> 只裁剪了精度, 指数位都是 8 位, 而 <code>fp16</code> 的指数位是 5 位, 因此 <code>bf16</code> 和 <code>fp32</code> 的数值范围是一致的, 能够较好的保持反传梯度的稳定性, 不太需要 loss scaling; 然后就是 <code>bf16</code> 的效率要稍微高一点, 同样是因为它保持了和 <code>fp32</code> 一样的指数位, 所以转化的时候只要最后补零就好了.</strong></p>
<p><img src="/2025/04/06/LLM/bf16.png" alt="bf16"></p>
<center style="font-size:12px; font-weight:bold">Fig. 3. Nvidia BF16</center><br>

<h1 id="各类大模型架构"><a href="#各类大模型架构" class="headerlink" title="各类大模型架构"></a>各类大模型架构</h1><h2 id="GPT"><a href="#GPT" class="headerlink" title="GPT"></a>GPT</h2><p>传统派. GPT2 是 <strong>可学习的绝对位置编码</strong>、<strong>Pre-layernorm</strong>、<strong>gelu</strong>、<strong>最后的解码层和 Embedding 共享参数</strong>, 以及除了最后的解码层, 其他的 <code>Linear</code> 都有 <code>bias</code>. 后续的版本应该都用上了 RoPE、silu 等.</p>
<h2 id="Qwen"><a href="#Qwen" class="headerlink" title="Qwen"></a>Qwen</h2><p>不同点:</p>
<ol>
<li>注意力层<ul>
<li>只有注意力层的 <code>q_proj</code>、<code>k_proj</code>、<code>v_proj</code> 有 <code>bias</code>, 说是注意力层的 <code>bias</code> 能增强模型外推能力.</li>
<li><strong>GQA</strong> 代替 <strong>MHA</strong>, 减少 k, v 注意力头的个数, 相当于降低 k, v 的维度, 进而减少 kv-cache 需要的内存. 最后求注意力还是要通过 repeat 让 kv 头的个数和 q 对齐.</li>
</ul>
</li>
<li>前馈层<ul>
<li>激活函数用的 silu.</li>
<li>用的 <code>gated-mlp</code>, 即有三个 <code>Linear</code> 层, 相应地为了保持参数一致将隐藏层的扩大系数降低为 8&#x2F;3.</li>
</ul>
</li>
<li>位置编码<ul>
<li>用的 RoPE, 施加在 <code>q_proj</code>、<code>k_proj</code> 后的 qk 上.</li>
<li>theta&#x3D;1000000</li>
</ul>
</li>
<li>其他<ul>
<li>最后的解码层和 Embedding 不再共享参数</li>
<li>LayerNorm 换为 RMSNorm, 降低计算量</li>
</ul>
</li>
</ol>
<h3 id="问答-5"><a href="#问答-5" class="headerlink" title="问答"></a>问答</h3><ol>
<li>MHA, MQA, 和 GQA 的区别</li>
</ol>
<p><strong>MHA 的 qkv 头个数相等; MQA kv 只有一个头, 等价于所有头共享一组 kv; GQA 是 MHA 和 MQA 的折中, 即 q 的头是 kv 头的倍数, 等价于多个头共享一组 kv. MHA 能力最强, 开销最大; MQA 能力最弱, 开销最小; GQA 居中.</strong></p>
<ol start="2">
<li>实现 RMSNorm</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RMSNorm</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init</span> __(self, dim, eps=<span class="number">1e-6</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dim = dim  <span class="comment"># token 维度</span></span><br><span class="line">        self.eps = eps  <span class="comment"># 防除0</span></span><br><span class="line">        self.w = nn.Parameter(torch.ones(dim))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        rms = torch.rsqrt(torch.mean(x.<span class="built_in">pow</span>(<span class="number">2</span>), dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>) + self.eps)  <span class="comment"># 等价于除以均方根的倒数</span></span><br><span class="line">        <span class="keyword">return</span> x * rms * self.w</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>为什么用 RMSNorm?</li>
</ol>
<p><strong>RMSNorm 去掉了过去标准化方法都有的去中心化操作, 就是 RMSNorm 只是除以了所有特征的均方根. 原文的实验表明模型 RMSNorm 和 LayerNorm 的表现差不多, 但是 RMSNorm 因为要的计算更少所以效率更高.</strong></p>
<ol start="4">
<li>对比各种激活函数 <code>relu</code>, <code>gelu</code>, <code>silu</code>, 为什么现在的模型多用 <code>silu</code>?</li>
</ol>
<p><strong><code>relu=max(0, x)</code>, 能够有效地为模型提供非线性性, 但对负输入永远为 0, 可能导致一些神经元在训练过程中一直不更新; <code>gelu=x*f(x)</code>, 其中 <code>f(x)</code> 标准正态分布的累积分布函数 (即小于x的比例), gelu的输出接近正态分布, 和神经网络的权重分布较为一致, 且正负值的过渡更为平滑, 但是计算较复杂; <code>silu=x*sigmoid(x)=x/(e^(-x)+1)</code>, 与 <code>gelu</code> 一样的平滑作用, 计算比 <code>gelu</code> 简单. 实验结果显示 <code>silu</code> 更好.</strong></p>
<h2 id="LLAMA"><a href="#LLAMA" class="headerlink" title="LLAMA"></a>LLAMA</h2><p>和 Qwen 的架构差不多 (或者说 Qwen 和 LLAMA 的架构差不多)</p>
<h2 id="DeepSeek"><a href="#DeepSeek" class="headerlink" title="DeepSeek"></a>DeepSeek</h2><p>MLA 优化的点主要在 kv cache, 因为目前推理阶段的瓶颈是 IO, 所以单卡&#x2F;单机能缓存更多的 kv 就能显著地提升推理的速度. 至于训练部分, MLA 并不会变快, 因为实际上计算量还上去了.</p>
<h3 id="问答-6"><a href="#问答-6" class="headerlink" title="问答"></a>问答</h3><h2 id="MLLMs"><a href="#MLLMs" class="headerlink" title="MLLMs"></a>MLLMs</h2><p>多模态模型, 当前一般指图片+文本的模型. 通常由两大模块组成: <strong>LLMs: 通常为用 Instruction SFT 后的大语言模型而不是 PLM</strong>; <strong>Vision encoder: CLIP、ViT 等. 一般冻住.</strong> 常见的融合方式有两种:</p>
<p><img src="/2025/04/06/LLM/vlm.jpg" alt="vlm"></p>
<center style="font-size:12px; font-weight:bold">Fig. 4. VLM</center><br>

<ol>
<li>Decoder-only: 仅在序列维度将编码、投影后的 Image Patch 与文本拼接. <strong>优势</strong>为不需要改变 LLMs 的结构, 且在 OCR 相关的问题上有较高的精度. <strong>劣势</strong>为计算复杂度过高, 因为拼接后序列长度的增长将导致复杂度<strong>平方级</strong>地增长. 模型有 LLaVA (MLP将视觉投影到文本)、Qwen-VL (类似Flamingo的 perceiver resampler 作为桥梁). 典型的训练流程为:<ul>
<li>Pretrain: LLM 被冻住, 只训练投影模块 (如 MLP), 有些模型也会同时训练 Vision encoder;</li>
<li>SFT: 投影模块与 LLM 同时训练, 有些也会训练 Vision encoder.</li>
</ul>
</li>
<li>Cross-attention-based: 在原始的 LLM 的 Decoder 中额外增加 Cross-attention 模块 (每层或者每隔 k 层), 将 Text 作为 q, 编码、投影后的 Image Patch 作为 kv. <strong>优势</strong>为计算效率高. 代表有 Flamingo、LLAMA 3.2-V. 典型的训练流程为:<ul>
<li>Pretrain &amp; SFT: 一般都只训练投影模块 (Flamingo 的叫 perceiver resampler, NVLM 觉得这会搅浑空间信息, 所以将其简单地替换为一层 Linear, 用以对齐 Image Token 和 Text Token 的维度) 和 Cross-attention 模块. 此外, Cross-attention 模块也可视图片的有无进行激活, 因而其对 LLM 原本的文本处理能力是无损的 (当然也不会带来提升).</li>
</ul>
</li>
</ol>
<p>NVIDIA 有个混合两种方式的模型 (Hybrid), 效果比 Cross 的好, 比 Decoder 的差, 效率也介于两者之间.</p>
<p><img src="/2025/04/06/LLM/nvlm.png" alt="nvlm"></p>
<center style="font-size:12px; font-weight:bold">Fig. 5. NVLM</center><br>

<h3 id="问答-7"><a href="#问答-7" class="headerlink" title="问答"></a>问答</h3><ol>
<li>目前的多模态模型中, 视觉和文本结合的方式是怎样的?</li>
</ol>
<p><strong>见上.</strong></p>
<ol start="2">
<li>低分辨率训练的 Vision encoder 如何处理高分辨率的图片?</li>
</ol>
<p><strong>把高分辨率的图片用低分辨率的比例切分为多个 tiles, 如对 896x672 的图片, 对用 224x224 训练的 Vision encoder, 会先把高分辨率图片切分为 (896&#x2F;224 x 672&#x2F;224) 共 12 个 tiles, 然后分别送进 encoder, 最后得到的 tokens 拼接起来. 比如若 patch size 为 14x14, 那么每个 tile 就有 256 个 tokens, 12 个就总共有 3072 个 tokens. 同时加入一个 <tile_i> token 切分每个 tile (NVLM 的做法).</tile_i></strong></p>
<ol start="3">
<li>如何解决 VLM 训练后模型的纯文本处理能力下降的问题?</li>
</ol>
<p><strong>使用 Cross-attention 的架构而不是 Decoder-only 的架构; 在 SFT 阶段加入高质量的纯文本数据.</strong></p>
<ol start="4">
<li>BLIP &amp; BLIP2</li>
</ol>
<p><strong>BLIP 类的模型是想用一个模型来同时完成图文匹配和根据图片的文本生成任务. 因此, 除了图片编码器以外, BLIP 还三个并行的 Attention 架构的文本编码或解码器. 其中一个解码器负责的是和 CLIP 一样的任务, 即通过图文对比学习来更新, 另一个解码器是带 cross-attention 的, 其中cross attention 用于输入图像信息, 负责的是图文匹配任务, 即完成一个二分类的任务, 最后一个解码器是带 cross-attention 的 因果子注意力架构, 完成根据图片的文本生成任务. 三个 attention 大部分参数都是共享的, 除了 causual self-attention 是单独的之外. BLIP2 论文中对标的主要是 FLAMINGO 架构, 他认为 FLAMINGO 用来对齐图文的 perceiver resampler 太弱了, 所以是先用和 BLIP1 一样的任务对用BERT初始化的 Image Transformer 和 Text Transformer 进行第一阶段的训练, 其中 Image Transformer 加入了 cross-attention 层, 以及可学习的 Query Token, 来压缩图片信息, 最后图文生成的时候是只用 Image Transformer 生成 可学习的 Token 在和 Image 交互后的编码, 投影, 然后直接和文本拼接在一起输入冻结的 LLM中.</strong></p>
<p><img src="/2025/04/06/LLM/blip2.png" alt="blip2"></p>
<center style="font-size:12px; font-weight:bold">Fig. 6. BLIP2</center><br>

<h1 id="Pre-training"><a href="#Pre-training" class="headerlink" title="Pre-training"></a>Pre-training</h1><h3 id="问答-8"><a href="#问答-8" class="headerlink" title="问答"></a>问答</h3><ol start="0">
<li>什么是 BPE, 为什么要 BPE?</li>
</ol>
<p><strong>Byte-Pair Encoding. 就是不断地把一起出现次数多的词 merge 到一起加入词汇表中的算法. 一方面是人类是不断地在创造新词的, 我们不能每创建一个词就把 Token 加一, 因此要把单词拆分, 因为词汇的创造往往是基于已有词汇的; 另一方面字母 Level 的拆分没什么意义, 所以需要 BPE.</strong></p>
<ol>
<li>为什么自回归是最主流的预训练方法? 此外还有什么预训练方法?</li>
</ol>
<p><strong>和下游任务契合度高, 只需要调整 Prompt 就可以处理各种任务, 而像 Bert 这类的掩码填空则还要多加一个投影头. 还有像 Bert 这种双向注意力模型的掩码后填空的预训练方式 (MLM). BERT: 掩码的方式是 80% 掩, 10% 替换为别的 Token, 10% 啥也不干. 还有一个任务是 Next Sentence Prediction, 给两个句子, 判断两者是否相邻. BERT 的三个 Embedding (Token, Position, Sentence, 都是 Learnable 的). BERT 处理多任务: 分类-用头的[CLS]+额外的投影头, &quot;海底捞针&quot;: 额外的 [S] 和 [E] 分别表示句子的开始和结尾, 每个 Token 和 [S]、[E] 做点积, 然后分别 SOFTMAX.</strong></p>
<ol start="2">
<li>增量预训练 (Continued Pretraining) 的方式和技巧</li>
</ol>
<p><strong>用和最初预训练时相同的学习率 warmup-decay 方式, 在新的数据集中加入少量 (5%, 10%) (当分布差异不是特别大的时候replay 5%原始数据，当分布差异特别大的时候10%-20%原始数据) 的最初预训练数据, 避免遗忘.</strong></p>
<ol start="3">
<li>什么是推理计算 (test-time compute)? 常用的方法有哪些?</li>
</ol>
<p><strong>test-time compute 即通过让模型在推理时输出更多的 Token 来增强模型表现. 想法的来源是人在解决困难问题时往往会需要更多的思考时间, 对于语言模型来说也应当是成立的. 常用的方法让模型随机生成多个回答或者 Beam-search 生成多个回答, 然后 Best-of-N 选择最好的; 还有让模型自己对回答进行多次的修改. 通常简单的问题让模型自己修改即可, 而更困难的让模型生成多个回答会更有优势.</strong></p>
<ol start="4">
<li>LLM 训练中数据质量的关键是什么?</li>
</ol>
<p><strong>多样性、涉及面广、去重复性、无风险性、正确以及结构化. 其中在结构化方面, json 格式的数据表现最好.</strong></p>
<ol start="5">
<li>cosine 退火的周期要怎么选?</li>
</ol>
<p><strong>一般 warmup 后有多少步就退火多少步, 即最后一个 epoch 退火到 min_lr, 这样能在该步数下取得最小的loss. 也有研究表明退火阶段loss降得很快, 所以可以在 warmup 之后以最大学习率训练, 让 loss 缓慢下降, 等到要结束的时候再用约 10% 的最大学习率训练步数退火即可 (MiniCPM 的方法), 这样的好处是可以一直训练, 不像原始的 cosine 退火到 min_lr 即便再接着训练 loss 也变化不大.</strong></p>
<ol start="6">
<li>LongContext 训练时, 短文本能力损失?</li>
</ol>
<p><strong>1. RoPE base 变化的影响, 进而导致 attention score 的变化, 具体就是远程衰变变慢, 模型对远近的感知变弱. 补救: 长短混合训练, 最好的短文本是 数学、代码, 因为其短程依赖性强.</strong></p>
<ol start="7">
<li>Chinchilla scaling law 和 kaplan 的区别?</li>
</ol>
<p><strong>scaling law 是指在限定计算资源的情况下, 达到最低 train_loss 时模型参数和训练 token 的配比. chichilla 认为参数x2, token也x2, 而 kaplan 则认为是参数主导. 结论是: 训练的 Token 个数大约是模型参数的 20 倍时, 能到达最低的 Loss. (C&#x3D;6ND (前向2, 反向4, 因为 1. 计算梯度更新本层参数 2. 计算输入梯度传导到前一层))</strong></p>
<ol start="8">
<li>数据集的构建方法</li>
</ol>
<p><strong>URL (过滤有害网站) -&gt; 文本提取 (从 HTML 中提取纯文本) -&gt; 语言过滤 -&gt; 去重 (LocalMinHash) -&gt; C4 filter (删掉 cookie 声明、太短的等等)-&gt; 自定义的filter -&gt; 删除个人信息.</strong></p>
<h1 id="Supervised-Fine-tuning"><a href="#Supervised-Fine-tuning" class="headerlink" title="Supervised Fine-tuning"></a>Supervised Fine-tuning</h1><h3 id="问答-9"><a href="#问答-9" class="headerlink" title="问答"></a>问答</h3><ol>
<li>有哪些高效微调的方法? 和全量微调比, 两者各有什么特点?</li>
</ol>
<p><img src="/2025/04/06/LLM/peft.png" alt="peft"></p>
<center style="font-size:12px; font-weight:bold">Fig. 7. PEFT</center><br>

<p><strong>常见的高效微调的方法有像 Adapter, 在每层的残差连接前的 self-attention 和 ffn 后添加一个小的低秩参数矩阵; 也有像 prefix tuning 在投影后的 kv 前拼接几个可学习的 Token, prompt tuning 和 prefix tuning 类似, 只不过它不在每一层都添加 Token, 而是直接在Embedding 层后就把 可学习的 Token 拼接上去; LoRA 则是通过设置一个和线性层并行的低秩参数矩阵, 通过只微调这部分的参数达到轻量化微调的目的. 这几种方式都可以统一为通过特定的方式修改 Token 的hidden state, 即让模型的输出由 h 变为 h + $\Delta$ h, 原理上来说就是让模型的参数在一个较小的偏移空间内调整, 从而实现微调参数的同时避免灾难性遗忘现象. 不过前两者都增加了微调后模型的参数量, 所以会增加推理开销, 而 LoRA 的旁路矩阵可以直接并入原始参数中, 所以不会带来额外的推理开销. 和全量微调相比: 全量微调在微调任务下的表现要更好, 但 LoRA 等高效微调方式有着更好的泛化性, 即其遗忘现象要更弱, 因而能更好地保持微调前的模型在非微调任务下的表现.</strong></p>
<p><strong>微调参数的比例较大时, 微调 FFN 比微调 qkv 效果更好. (LoRA 微调 Self-attention 时, 调 qk 或 qv 的都有.)</strong></p>
<ol start="2">
<li>谈谈你对 LoRA 的理解, 以及最新的关于 LoRA 的研究有哪些?</li>
</ol>
<p><strong>LoRA 是一种高效微调的方法, 它通过在模型的参数层, 即 Linear 层旁增加一个参数量小的、低秩的矩阵来实现轻量化的微调. 这个低秩矩阵由一个下投影矩阵 A 和上投影矩阵 B 相乘得到, 其中 A 随机初始化, B 初始化为全零矩阵, 使得在训练初期 LoRA 不会干预模型的表现, 让模型能够渐进式地学习新的知识, 提升训练的稳定性. LoRA 的轻量化主要体现在反向传播上, 因为有很小的参数是需要梯度的, 所以反向传播的计算量和优化器需要的显存都会远小于全量微调. 同时, 因为 LoRA 可微调的参数空间较小, 所以最终的效果可能不如全量微调, 但胜在轻量化和遗忘现象不明显.</strong></p>
<p><strong>有像 DoRA, DoRA 把全连接层的向量用模长和方向表示, 然后单独优化模长, 而方向则用 LoRA 来优化. 这一方法取得了比 LoRA 更接近全量微调的效果. 还有像 QLoRA, QLoRA 把预训练模型用 4 bit 量化, 然后在这个量化的模型上进行 LoRA, 从而进一步节省微调时的内存开销.</strong></p>
<ol start="3">
<li>LoRA 和全量微调该怎么选择?</li>
</ol>
<p><strong>如果都是对所有模块都进行微调, LoRA 因为是一个低秩的近似, 可学习的参数会比全量微调少, 所以开销, 但效果上多数不如全量微调. 其中对于 Instruction Tuning 类的, LoRA 可以通过增大 rank 来接近全量微调的效果; 而对于 Continued Pretraining, LoRA 不如全量微调. 所以 Instruction Tuning 用 LoRA 更好, 因为它一方面开销少, 另一方面遗忘少, 而增量预训练则还是全量的比较好.</strong></p>
<ol start="4">
<li>SFT packing 是什么?</li>
</ol>
<p><strong>把多个 sft 样本打包到一个样本内进行训练, 可以加快模型的训练速度, 因为如果不 pack 的话短的 sft 样本就会有很多 padding token, 这对计算资源是极大的浪费. 计算的时候可以加 mask 让每个样本只关注自己的, 也可以直接 causal attention, 效果差不多.</strong></p>
<ol start="5">
<li>SFT 常见的加速方法</li>
</ol>
<p><strong>多轮合并 (将多轮对话的合并为一个样本); packing.</strong></p>
<ol start="6">
<li>多轮合并和 packing 会带来什么问题?</li>
</ol>
<p><strong>可能会导致短的回答没有得到充分训练. 因为计算loss的时候有求 mean 的操作, 所合并或者packing后, 短回答的梯度相当于被长回答给稀释掉了, 所以相应地模型短回答的能力就变弱了. 解决办法就是让每个 Token 的loss 都是等权的, 就是有点麻烦. (似乎在大数据集上影响不大)</strong></p>
<ol start="7">
<li>SFT 能学新知识吗?</li>
</ol>
<p><strong>很少. SFT 更多是对齐. 大模型的很多能力都是在预训练的时候获得的. 且太多知识的注入会影响模型遵循指令的能力.</strong></p>
<ol start="8">
<li>SFT 的效果取决于什么?</li>
</ol>
<p><strong>LIMA (less is more for alignment) 指出, SFT 不需要堆数据量, 更重要的是 Prompt 的多样性和回答的质量.</strong></p>
<ol start="9">
<li>如何提升 sft prompt 的多样性?</li>
</ol>
<p><strong>给不同的 prompt 打 tag, 然后通过 tag 对分布进行调整; 用模型最后一层的 embedding 对 prompt 进行表示, 相似度高的 prompt 删除; 对简单的 prompt 进行难度升级, 如正着问 反着问.</strong></p>
<ol start="10">
<li>In-context Learning 和 SFT 的关系?</li>
</ol>
<p><strong>ICL 是特殊的 SFT. 有研究表明, 对于相同 instruct 的最后一个 token 的最后一层的 embedding, ICL 和 SFT 后模型的差不多.</strong></p>
<ol start="11">
<li>如何在 SFT 中做 RLHF?</li>
</ol>
<p><strong>对数据进行处理, 给高质量回答加标签; 引入 DPO Loss.</strong></p>
<ol start="12">
<li>SFT 更难改变哪些数据的模式?</li>
</ol>
<p><strong>代码等结构化、确定性强的数据. 因为对这些数据, 模型的困惑度低, loss 低, 所以少样本的情况下很难扭转.</strong></p>
<ol start="13">
<li>SFT 学习率设置</li>
</ol>
<p><strong>一般为退火后的.</strong></p>
<ol start="14">
<li>SFT 后复读机的原因</li>
</ol>
<p><strong>本质上是新生成的 Token 没什么用, 注意力权重很低, 所以下一个 Token 的预测结果还是它, 一直重复. 原因有: 1. 预训练没做好. 模型规模不够、数据不够多样, 导致 sft 数据超出了模型能力本身, 模型为了记住这部分数据, 就会打乱原始的 attention 分布, 就导致模型整个就不行了. 2. sft 数据虽然不需要过多, 但太少也不行, 因为 pretrain 的时候都是把文本 packing 到一起的, 所以模型没怎么学会停止输出.</strong></p>
<ol start="15">
<li>如何利用 ICL 增强预训练?</li>
</ol>
<p><strong>ICL 告诉我们, 模型可以从相关的知识中学习到通用的模式, 因此, 在预训练过程中, 与其简单地将不相关的文本拼接在一起组成长文本, 不如将相互关联的文本拼接在一起. 当然, 这会需要重新组织数据格式.</strong></p>
<ol start="16">
<li>模型如何获得 ICL 的能力?</li>
</ol>
<p><strong>是在预训练中获得的. 模型在预训练时, 会潜在地将具有相似格式的上文编码到一个特殊的隐空间中.</strong></p>
<ol start="17">
<li>Function call 是怎么训练的?</li>
</ol>
<p><strong>大模型在 Function call 中的作用是根据用户的输入选取合适的 Function 和提取 Function 的输入, 然后是由实际的程序来执行这个 Function, 最后将响应发回给模型, 再让模型继续输出. 所以训练 Function call 大模型和普通的 SFT 差不多, 主要是数据格式上的区别, system prompt 里面会有能使用工具的具体描述, 包括函数名、函数描述、参数名、参数描述等. 训练的时候的 Ground Truth 就是 User Prompt 中提取到的参数, 按指定的格式, 一般是 json 那样的, 放到指定的 box 里面. 然后一个 function 框, 里面是 function 执行结果, 模型再从中提取输出, 按指定的格式输出. 类似下面这样.</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&quot;&quot;&quot;</span><br><span class="line">&lt;|im_start|&gt;system</span><br><span class="line">You are a helpful assistant with access to the following functions. Use them if required -</span><br><span class="line">&#123;</span><br><span class="line">    &quot;name&quot;: &quot;generate_invoice&quot;,</span><br><span class="line">    &quot;description&quot;: &quot;Generate an invoice with specified details&quot;,</span><br><span class="line">    &quot;parameters&quot;: &#123;</span><br><span class="line">        &quot;type&quot;: &quot;object&quot;,</span><br><span class="line">        &quot;properties&quot;: &#123;</span><br><span class="line">            &quot;customer_name&quot;: &#123;</span><br><span class="line">                &quot;type&quot;: &quot;string&quot;,</span><br><span class="line">                &quot;description&quot;: &quot;The name of the customer&quot;</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;items&quot;: &#123;</span><br><span class="line">                &quot;type&quot;: &quot;array&quot;,</span><br><span class="line">                &quot;items&quot;: &#123;</span><br><span class="line">                    &quot;type&quot;: &quot;object&quot;,</span><br><span class="line">                    &quot;properties&quot;: &#123;</span><br><span class="line">                        &quot;name&quot;: &#123;</span><br><span class="line">                            &quot;type&quot;: &quot;string&quot;,</span><br><span class="line">                            &quot;description&quot;: &quot;The name of the item&quot;</span><br><span class="line">                        &#125;,</span><br><span class="line">                        &quot;quantity&quot;: &#123;</span><br><span class="line">                            &quot;type&quot;: &quot;integer&quot;,</span><br><span class="line">                            &quot;description&quot;: &quot;The quantity of the item&quot;</span><br><span class="line">                        &#125;,</span><br><span class="line">                        &quot;price&quot;: &#123;</span><br><span class="line">                            &quot;type&quot;: &quot;number&quot;,</span><br><span class="line">                            &quot;description&quot;: &quot;The price of the item&quot;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;,</span><br><span class="line">                    &quot;required&quot;: [</span><br><span class="line">                        &quot;name&quot;,</span><br><span class="line">                        &quot;quantity&quot;,</span><br><span class="line">                        &quot;price&quot;</span><br><span class="line">                    ]</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;required&quot;: [</span><br><span class="line">            &quot;customer_name&quot;,</span><br><span class="line">            &quot;items&quot;</span><br><span class="line">        ]</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">&lt;|im_end|&gt;</span><br><span class="line">&lt;|im_start|&gt;user</span><br><span class="line">I need to generate an invoice for a customer named John Doe. He bought 2 apples for $1 each and 3 oranges for $2 each.&lt;|im_end|&gt;</span><br><span class="line">&lt;|im_start|&gt;assistant</span><br><span class="line">&lt;functioncall&gt; &#123;&quot;name&quot;: &quot;generate_invoice&quot;, &quot;arguments&quot;: &#x27;&#123;&quot;customer_name&quot;: &quot;John Doe&quot;, &quot;items&quot;: [&#123;&quot;name&quot;: &quot;apple&quot;, &quot;quantity&quot;: 2, &quot;price&quot;: 1&#125;, &#123;&quot;name&quot;: &quot;orange&quot;, &quot;quantity&quot;: 3, &quot;price&quot;: 2&#125;]&#125;&#x27;&#125; &lt;|endoftext|&gt;&lt;|im_end|&gt;</span><br><span class="line">&lt;|im _start|&gt;function</span><br><span class="line">&#123;&quot;invoice_id&quot;: &quot;INV12345&quot;, &quot;customer_name&quot;: &quot;John Doe&quot;, &quot;items&quot;: [&#123;&quot;name&quot;: &quot;apple&quot;, &quot;quantity&quot;: 2, &quot;price&quot;: 1, &quot;total&quot;: 2&#125;, &#123;&quot;name&quot;: &quot;orange&quot;, &quot;quantity&quot;: 3, &quot;price&quot;: 2, &quot;total&quot;: 6&#125;], &quot;total&quot;: 8, &quot;status&quot;: &quot;Generated&quot;&#125;&lt;|im_end|&gt;</span><br><span class="line">&lt;|im_start|&gt;assistant</span><br><span class="line">The invoice has been successfully generated. The invoice ID is INV12345. The total amount for 2 apples and 3 oranges is $8. &lt;|endoftext|&gt;&lt;|im_end|&gt;</span><br><span class="line">&quot;&quot;&quot;</span><br></pre></td></tr></table></figure>

<ol start="18">
<li>QLoRA</li>
</ol>
<p><strong>对 Base 的参数 NF4 量化, 即按正态分布的累计概率为每个概率区间赋予一个 NF4 中的 15 位 (0 单独). 量化前用最大的参数值作为scale进行归一化. 为了进一步减少内存, 对 scale 也进行量化. 即两阶段的量化. FT 时, 前向、反向传播时, 对 scale 和 weight 依次反量化计算激活值, LoRA 的参数是 BF16 的, 梯度下降只对 LoRA 做. 在资源受限时, 增加参数、降低精度是有利的.</strong></p>
<h1 id="Reinforcement-Learning"><a href="#Reinforcement-Learning" class="headerlink" title="Reinforcement Learning"></a>Reinforcement Learning</h1><p>强化学习, 策略函数 (Policy), 奖励函数 (Reward), (状态)价值函数 (Value):</p>
<ul>
<li>策略函数 ($\pi$), 根据当前状态 (State) 生成下一步动作 (Action) 的概率分布;</li>
<li>奖励函数, 根据当前状态 (State) 生成下一步动作 (Action) 的奖励;</li>
<li>价值函数, 根据当前状态 (State) 以及策略 (Policy) 输出按当前策略奖励的期望.</li>
</ul>
<p>常见的三类基于深度学习的强化学习方法:</p>
<ol>
<li>基于价值函数的方法 (e.g., DQN): 通过神经网络学习价值函数, 输入当前状态, 直接输出所有动作最终能够得到的价值 (离散情况; 连续情况则输入还要输入动作), 选择价值最大的作为下一步动作;</li>
<li>基于策略函数的方法 (e.g., REINFORCE): 通过神经网络学习策略函数, 输入当前状态, 输出下一步动作的概率分布, 通过最大化价值优化动作分布, 在优化过程中价值以估计的方法计算 (e.g., 蒙特卡洛方法);</li>
<li>Actor-Critic方法 (e.g., PPO): 同时学习策略和价值函数, 本质上是基于策略的方法, 不过是用可学习的网络代替了估计方法来计算价值而已. 策略和价值网络产生 Loss 的方式不同:<ul>
<li><strong>价值网络</strong>: 一般采用时序差分算法, 该算法的核心思想在于, 在最优策略下, (当前状态的价值) 与 (当前状态下采用行动的奖励+衰减因子*下一状态的价值) (即时序差分项) 是相等的, 于是价值函数就是要优化网络, 使得:$$\mathcal{L} _{v}&#x3D;\argmin[r _t+\gamma V _{\theta}(s _{t+1})-V _{\theta}(s _{t})]$$最小. 其中 $r _t$ 是当前状态采用的行动的奖励, $\gamma$ 是衰减因子. 注意, 此时时序差分项不会产生梯度, 是被作为 Ground Truth 的.</li>
<li><strong>策略网络</strong>: 策略网络的核心诉求是最大化价值, 但实际计算起来可视为一种特殊的交叉熵损失:$$\mathcal{L} _p&#x3D;\argmin[-(\log\pi _\theta(s _t)) _{a _i}\cdot\mathcal{L} _v.detach()]$$ 其中前项是采取动作 $a _i$ 的熵. 与普通交叉熵不同的是, $\mathcal{L} _v$ 控制了交叉熵梯度更新的方向, 即: 当采取行动 $a _i$ 会使得下一个状态不是最优状态 ($\mathcal{L} _v$ 为负) 时, 说明应该降低 $a _i$ 发生的概率, 因此进行的是<strong>梯度上升</strong>; 反之, 即为普通交叉熵的情况, 执行<strong>梯度下降</strong>.</li>
</ul>
</li>
</ol>
<h2 id="PPO-Proximal-Policy-Optimization"><a href="#PPO-Proximal-Policy-Optimization" class="headerlink" title="PPO (Proximal Policy Optimization)"></a>PPO (Proximal Policy Optimization)</h2><p>PPO 属于 Actor-Critic 类的强化学习算法. 前面描述的 Actor-Critic 用的是最经典的策略梯度算法, 其缺点在于训练不稳定、策略梯度的更新不可控, 即我们无法保证每次的梯度更新得到的都是更好的一个策略. 对此, TRPO (Trust Region Policy Optimization) 提出了信任区域 (Trust Region), 以确保在该区域更新策略能带来正向优化. PPO 是 TRPO 的简化版, 其以<strong>惩罚</strong>或<strong>截断</strong>的方式代替了信任区域, 以保证新参数与旧参数差距不会过大. PPO 在绝大多数情况下比 TRPO 快、效果好.</p>
<p>OAI 的 RLHF (Reinforcement Learning with Human Feedback) 用的是 PPO-截断:</p>
<p>$$<br>\mathcal{L} _p^{PPO}&#x3D;\argmax[\mathbb{E} _{q\sim P(Q), o\sim\pi _{\theta _{old}}(O|q)}\frac{1}{|o|}\sum _{t&#x3D;1}^{|o|}\min[\frac{\pi _\theta(o _t|q,o _{&lt;t})}{\pi _{\theta _{old}}(o _t|q,o _{&lt;t})}A _t,clip(\frac{\pi _\theta(o _t|q,o _{&lt;t})}{\pi _{\theta _{old}}(o _t|q,o _{&lt;t})},1-\epsilon, 1+\epsilon)A _t]]<br>$$</p>
<p>其中, $\pi _\theta$ 和 $\pi _{\theta _{old}}$ 分别是当前策略和过去的策略, 添加这一项可以保证无偏 (期望不变) 的同时采用过去策略与环境互动的数据进行训练, 从而避免每次训练前都要先用当前策略与环境互动得到训练数据, 即由 (On-Policy -&gt; Off-Policy). <strong>无偏</strong>是因为:</p>
<p>$$<br>\mathbb{E} _{(o,q)\sim\pi _\theta}f(o,q)&#x3D;\int\pi _\theta(o,q)f(o,q)d(o,q)&#x3D;\int\frac{\pi _\theta(o,q)}{\pi _{\theta _{old}}(o,q)}f(o,q)\pi _{\theta _{old}}(o,q)d(o,q)&#x3D;\mathbb{E} _{(o,q)\sim\pi _{\theta _{old}}}\frac{\pi _\theta(o,q)}{\pi _{\theta _{old}}(o,q)}f(o,q)<br>$$</p>
<p>而:</p>
<p>$$<br>\frac{\pi _\theta(o,q)}{\pi _{\theta _{old}}(o,q)}&#x3D;\frac{\pi _\theta(o|q)\pi _\theta(q)}{\pi _{\theta _{old}}(o|q)\pi _{\theta _{old}}(q)}\approx\frac{\pi _\theta(o|q)}{\pi _{\theta _{old}}(o|q)}<br>$$</p>
<p>上式成立因为假设不同策略看到的状态 (对 LLMs 即 q) 分布相似. 另一种是 PPO-惩罚, 即将前后策略输出分布的KL散度加入损失项中, 不常用.</p>
<h3 id="A-t-的计算"><a href="#A-t-的计算" class="headerlink" title="$A _t$ 的计算"></a>$A _t$ 的计算</h3><p>$A$, 即优势 (Advantage), 代表当前动作优于平均水平的程度. 如果只看一步的优势, 那么 $A _t$ 即等于时序差分误差项:</p>
<p>$$<br>A _t &#x3D; \delta _t&#x3D; r _t+\gamma V _{\theta}(s _{t+1})-V _{\theta}(s _{t})<br>$$</p>
<p>若考虑未来所有步的优势, 则采用广义优势估计 (Generalized Advantage Estimation，GAE):</p>
<p>$$<br>\begin{align*}<br>A _t^{GAE}&amp;&#x3D;(1-\lambda)(A _t^{(1)}+\lambda A _t^{(2)}+\lambda^2 A _t^{(3)}+...)\<br>&amp;&#x3D;(1-\lambda)(\delta _t+\lambda (\delta _t+\gamma\delta _{t+1})+\lambda^2 (\delta _t+\gamma\delta _{t+1}+\gamma^2\delta _{t+2})+...)\<br>&amp;&#x3D;\sum _{i&#x3D;0}(\gamma\lambda)^i\delta _{t+l}<br>\end{align*}<br>$$</p>
<p>即越后面的动作产生的优势受当前动作的影响越小, 因此在计算从当前开始到最后的优势时, 要对后面的优势进行衰减. 注意, $A$ 是用旧策略计算的.</p>
<blockquote>
<p>$\gamma$ 的取值, 过大会带来高方差, 但是是 $\gamma$ 的准确估计; 相反, 过小 方差低, 但不准; $\lambda$ 一样</p>
</blockquote>
<h3 id="PPO-与-RLHF"><a href="#PPO-与-RLHF" class="headerlink" title="PPO 与 RLHF"></a>PPO 与 RLHF</h3><p><img src="/2025/04/06/LLM/ppo.png" alt="r1"></p>
<center style="font-size:12px; font-weight:bold">Fig. 8. PPO</center><br>

<p>对于一些难以明确定义 Reward 的文本生成问题, 如写诗、写小说等, 为了免除繁重的人工评判, 需要预先训练一个奖励模型, 因此 PPO feat. RLHF 用到的模型有四类:</p>
<ul>
<li>Policy Model: 即需要训练的LLMs;</li>
<li>Value Model: 即价值模型;</li>
<li>Reward Model: 即用 Human Feedback 数据训练的奖励模型;</li>
<li>Reference Model: 即参数冻结的LLMs (Pretrain+SFT 后的, 即 Policy 的初始参数), 保证 RLHF 后的参数不会偏离初始参数太远.</li>
</ul>
<p>虽然表面上有 4 个模型, 但实际上用 Human Feedback 数据训练的奖励模型会在 PPO 的过程中同时充当 Reward 和 Value Model (好像不一定? 以前的 TRL 的 Value Model 是在 Policy 加个投影头), 只不过 Reward Model 被冻结而 Value Model 会在 PPO 过程中被更新. 类似地, Policy 和 Reference 也是同一个模型, 只不过 Reference 被冻结而 Policy 可训练 (Policy_old 直接记录以前的输出即可). 整个过程中, 最重要的一点就是: <strong>我们只优化两个模型-- Policy and Value. Policy 本质是在做一个分类任务, Value 本质是在做一个回归任务, 因此, 只有将旧有策略交互得到的 Prompts+Responses 输入的那一步出来的值需要梯度, 即分别是 Policy 和 Value 的输出</strong>. 基本的流程如下:</p>
<p><img src="/2025/04/06/LLM/ppopipe.png" alt="r1"></p>
<center style="font-size:12px; font-weight:bold">Fig. 9. PPO Pipeline</center><br>

<ol>
<li><p>Training Reward Model with Human Feedback. RLHF 的第一步, Reward Model 接受 Prompt+Response (B, L, D), 输出 (B, L) 的向量, 其中在 L 维度上, 第 i 位置表示 i 这个状态 (token) 能获得的奖励和 (即价值), 供 Value Model 使用; 最后一个位置的值则表示对<strong>整个句子</strong>的打分, 供 Reward Model使用. 训练时采用 <strong>Pair-wise Loss</strong>, 即同时接受人工排序好的两个句子, 提高好句子 (chosen) 的打分, 而降低差句子 (rejected) 的打分:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># code</span></span><br><span class="line">chosen_reward = reward_model(chosen)[:, last_non_pad_token_chosen]</span><br><span class="line">rejected_reward = reward_model(rejected)[:, last_non_pad_token_rejected]</span><br><span class="line">loss = -torch.log(F.sigmoid(chosen_reward - rejected_reward)).mean()  <span class="comment"># 将 chosen 视作正例</span></span><br></pre></td></tr></table></figure></li>
<li><p>Interact with Environment and Generate Data (PPO Step 1). 旧策略与环境交互, 生成训练数据:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># code</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="comment"># 等价于强化学习的交互阶段, 用旧策略生成训练数据.</span></span><br><span class="line">    <span class="comment"># (B, L) -&gt; (B, L+H)</span></span><br><span class="line">    query_responses, logitss = policy_model.generate(prompts)</span><br></pre></td></tr></table></figure>
</li>
<li><p>Rating and Calculate Rewards, Values, Advantages (PPO Step 2). 随后, 用训练好的 Reward Model 在初始化一个 Value Model, 并冻结 Reward Model 的参数. Reward 和 Value Model 分别打分, 进一步计算 advantages:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># code</span></span><br><span class="line"><span class="keyword">with</span> torch.n _grad():</span><br><span class="line">    logits = logitss[:, only_consider_responses]                            <span class="comment"># 只考虑回答的部分</span></span><br><span class="line">    _, logits_ref = ref_model(query_responses)[:, only_consider_responses]  <span class="comment"># 用于约束训练后模型分布不会太偏离未训练模型</span></span><br><span class="line">    log_prob = selective_log_softmax(logits)                                <span class="comment"># 注: 此时已做出动作选择, 即 log_prob 的维度是 (B, H)</span></span><br><span class="line">    log_prob_ref = selective_log_softmax(logits_ref)</span><br><span class="line">    <span class="comment"># 起约束作用的 KL散度</span></span><br><span class="line">    kl_divergence = -beta * (log_prob - log_prob_ref)     <span class="comment"># beta 为约束系数</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># (B, L+H) -&gt; (B, 1), 打分, 输出 r _\phi</span></span><br><span class="line">    reward_scores = reward_model.get_reward(query_responses)</span><br><span class="line">    <span class="comment"># 加上KL散度约束的最终 reward</span></span><br><span class="line">    rewards = kl_divergence + reward_scores  <span class="comment"># 需要注意, responses 的每个位置都有 KL 约束, 但是 reward_scores 只加在最后一个位置</span></span><br><span class="line">    <span class="comment"># (B, L+H) -&gt; (B, L+H-1)</span></span><br><span class="line">    values = value_model.get_value(query_responses)[:, :-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据 values 和 rewards 计算 advantages 和 时序差分项: r + V(s+1) = A + V(s), 记作 return</span></span><br><span class="line">    lastdelta = <span class="number">0</span></span><br><span class="line">    advantages_reversed = []</span><br><span class="line">    length = rewards.size()[-<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 计算每个时刻的优势, 同样只考虑 responses 部分</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">reversed</span>(<span class="built_in">range</span>(start, length)):</span><br><span class="line">        nextvalues = values[:, t + <span class="number">1</span>] <span class="keyword">if</span> t &lt; length - <span class="number">1</span> <span class="keyword">else</span> <span class="number">0.0</span></span><br><span class="line">        <span class="comment"># A = delta = r + \gamma V(s+1) - V(s)</span></span><br><span class="line">        delta = (rewards[:, t] + gamma * nextvalues) - values[:, t]</span><br><span class="line">        <span class="comment"># gamma, lam 两个衰减因子</span></span><br><span class="line">        lastdelta = delta + gamma * lam * lastgaelam</span><br><span class="line">        advantages_reversed.append(lastgaelam)</span><br><span class="line">    <span class="comment"># r + V(s+1) = A + V(s)</span></span><br><span class="line">    returns = advantages + values[:, start:]</span><br></pre></td></tr></table></figure></li>
<li><p>Calculate Loss, Update Policy and Value Model (PPO, Step 3). 计算 Loss, 梯度下降:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># code</span></span><br><span class="line"><span class="comment"># 新策略与老策略初始化相同, 此处开始不相同, 因为要更新新策略 num_ppo_epochs 次.</span></span><br><span class="line">logits_old = </span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_ppo_epochs):</span><br><span class="line">    <span class="comment"># policy loss, 策略的损失</span></span><br><span class="line">    new_log_prob = selective_log_softmax(policy_model(query_responses))</span><br><span class="line">    ratio = troch.exp(new_log_prob - log_prob)</span><br><span class="line">    p_loss_1 = - ratio * advantages</span><br><span class="line">    p_loss_2 = -advantages * torch.clamp(ratio, <span class="number">1.0</span> - cliprange,</span><br><span class="line">                                            <span class="number">1.0</span> + cliprange)</span><br><span class="line">    p_loss = torch.<span class="built_in">max</span>(p_loss_1, p_loss_2)  <span class="comment"># 因为带了负号, 所以取 max</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># value loss, 价值的损失: r + V(s+1) - V(s)</span></span><br><span class="line">    new_values = reward_model(query_responses)[:, :-<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 对 value 也裁剪, 使其不至于偏离原模型过多</span></span><br><span class="line">    new_values_clipped = torch.clamp(</span><br><span class="line">                                new_values,</span><br><span class="line">                                values - cliprange_value,</span><br><span class="line">                                values + cliprange_value,</span><br><span class="line">                            )</span><br><span class="line">    v_loss_1 = torch.square(new_values - returns)</span><br><span class="line">    v_loss_2 = torch.square(new_values_clipped - returns)</span><br><span class="line">    v_loss = <span class="number">0.5</span> * torch.<span class="built_in">max</span>(v_loss_1, v_loss_1)</span><br><span class="line"></span><br><span class="line">    loss = p_loss + v_coef * v_loss</span><br><span class="line"><span class="comment"># 极简版, 所有的 Loss 都只对 Responses 的 Token 求.</span></span><br></pre></td></tr></table></figure></li>
</ol>
<p>补充: 加入与参考模型 KL散度 (此处实际就是生成 o_t 的概率比) 后的 reward 计算:</p>
<p>$$<br>r _t &#x3D; r _\phi(q, o _{\le t})-\beta \log\frac{\pi _{\theta _{old}}(o _t|q,o _{&lt;t})}{\pi _{\theta _{ref}}(o _t|,o _{&lt;t})}<br>$$</p>
<blockquote>
<p>即偏离 <code>ref</code> 太远 reward 会下降.</p>
</blockquote>
<h3 id="问答-10"><a href="#问答-10" class="headerlink" title="问答"></a>问答</h3><ol>
<li>PPO 过程中, reward model 会有什么问题?</li>
</ol>
<p><strong>reward model 的准确率会逐渐下降. 因为 reward model 毕竟只是用有限的数据训练出来的, 一般来源于 sft 模型的回答, 因此在 PPO 的过程中, 当模型逐渐偏离 sft 模型, 开始输出一些 reward model 没见过的 OOD 回答时, reward model 的有效性就会下降. 且也可能出现一种情况是, 模型在 PPO 过程中找到了一条捷径, 就是一些对人来说可能没有意义的输出 reward model 会给很高的分数, 最终的结果就是 PPO 训练失败了. 所以 RLHF 的训练步数不能很长.</strong></p>
<ol start="2">
<li>对上面的问题除了少训练点还有什么解法吗?</li>
</ol>
<p><strong>Llama 2 用的方法是训练一段时间后, 从训练后的模型在采样一些 pair, 重新训练 rm, 就是比较麻烦.</strong></p>
<ol start="3">
<li>现阶段能跳过 SFT 直接 RLHF吗?</li>
</ol>
<p><strong>当前来说不行. 直接 RLHF 的搜索空间太大, 成本过高, 且需要有一个天然的、准确的 reward model, 而这在语言中, 除了像数学题、编程题这种有明确评判标准的, 其他的 reward model 构建都需要极大的成本. 所以需要先 SFT 来缩小搜索空间.</strong></p>
<ol start="4">
<li>PPO 为什么在初始阶段会先把 actor model 冻结?</li>
</ol>
<p><strong>为了单独训练 value model. 进而稳定后续 actor model 的学习.</strong></p>
<ol start="5">
<li>PPO 稳定训练 trick.</li>
</ol>
<p><strong>a. 训练 reward model 的时候加入对好样本的 pretrain loss (CE). b. reward 归一化 (mean, std) 与裁剪, advantage 与 reward 相关, 所以不用特别操作 (已融入 trl 中). c. token wise kl-penalty, 也已经融入 reward 的计算中. c. value model 要先训练一段时间再训练 actor, 至于 value model 用 actor 还是 reward 初始化区别不大. d. actor 的训练中也加入 pretrain loss. e. reward model 训练时 L2 正则, 防止 BT 只优化正负例的距离. f. 动量更新 reference model, 大 batch size.</strong></p>
<ol start="6">
<li>PPO 训崩了有什么迹象?</li>
</ol>
<p><strong>PPO 崩溃最主要的情况是 reward hacking, 即模型找到了一个可以让 reward model 打很高分数, 但是对人来说没有任何意义的输出. 具体表现为: reward 骤增, KL 骤增, PPL 骤降, 回答长度骤增.</strong></p>
<ol start="7">
<li>RLHF 的上界?</li>
</ol>
<p><strong>是 reward model 的上界, 即在 rm 下 best-of-N 的上界. 因为 rm 处理的是评判任务, 而 actor 处理的是生成任务, 评判任务本身就比生成简单, 所以 rm 在评判任务上能学得更好, actor 是根据 rm 的反馈学习的, 那么 rm 反馈准确率的上界也就是 rlhf 的上界了.</strong></p>
<ol start="8">
<li>DPO、PPO 解码策略对结果的影响</li>
</ol>
<p><strong>DPO 因为优化整体, 没有 Token-wise 的优化, 所以greedy效果不如 PPO, 因为某些关键的 Token 未必被优化地概率很高, 但相应地 top _p效果还不错; PPO 存在 Token-wise 的优化, 因此有利的 Token 会被增强, 但同样地和有利 Token 很像的也会被增强, 所以 top _p反而没那么强.</strong></p>
<ol start="9">
<li>Llama 多阶段 RLHF 的 reject sampling</li>
</ol>
<p><strong>即在 RLHF 过程中, 会隔段时间让模型生成 k 个答案, 选择 reward model 打分最高的对模型 SFT. 目的是让模型能学到更多样的东西, 一定程度上是在拟合 rm 的 BoN.</strong></p>
<h2 id="DPO"><a href="#DPO" class="headerlink" title="DPO"></a>DPO</h2><p>将 Reward Model 对大模型在相同 prompt 下的两个回答好坏的评判视作 Bradley-Terry Model 的过程, 即将 Reward Model 视作&quot;实力&quot;的参数, 此时 $r(prompt, response)$ 输出的就是大模型这段回答的&quot;实力&quot;. 最终的损失函数为:</p>
<p>$$<br>\mathcal{L} _{dpo}&#x3D;-\mathbb{E} _{(p, o _w, o _l)\sim D}[\log\sigma(\beta\log\frac{\pi _\theta(o _w|p)}{\pi _{ref}(o _w|p)}-\beta\log\frac{\pi _\theta(o _l|p)}{\pi _{ref}(o _l|p)})]<br>$$</p>
<p>其中 $\pi _\theta(o _w|p)$ 为对 Ground Truth 预测概率的和 (整条 Response 序列), 这实际上将 RLHF 转化为了一个类似二分类的监督学习问题.</p>
<blockquote>
<p>Bradley-Terry Model: 一种体育比赛的统计模型, 用参赛队伍两两之间的胜负关系来估计队伍的实力, 进而对未曾交手的两支队伍的胜负进行预测. 该模型认为每只队伍都有一个固定的参赛水平, 对于水平分别为 a 和 b 的两支队伍, 两者交手, a 的胜率为 a &#x2F; (a+b), b 的胜率为 b &#x2F; (a+b). 由此, 根据队伍已有的交手记录, 我们可以得到若干队伍间交手胜率的<strong>观测值</strong>, 然后使用 MLE 就可计算出每支队伍的实力.</p>
</blockquote>
<h3 id="问答-11"><a href="#问答-11" class="headerlink" title="问答"></a>问答</h3><ol>
<li>DPO 第 0 步的 loss 固定吗?</li>
</ol>
<p><strong>固定. 因为此时模型和 ref 一模一样, 故 sigmoid 的值为 0.5, 取 log 是 0.693</strong></p>
<ol start="2">
<li>DPO 在学习过程中 positive 和 negative 的概率同时下降?</li>
</ol>
<p><strong>因为 DPO 优化的是 positive 和 negative 之间的 gap. 所以如果 positive 和 negative sample 的大部分 token 是一致的, 那么 DPO 很难学到 positive sample 中的模式, 因为 BT loss, Bradley-Terry loss 只优化两者间的 gap, 对此, DPO 找到的捷径就是同时降低 negative 和 positive sample 的概率.</strong></p>
<ol start="3">
<li>DPO 训练后模型为什么输出越来越长?</li>
</ol>
<p><strong>这和数据集有关. 正例往往更长, 所以 DPO 就会倾向于输出更长的文本.</strong></p>
<ol start="4">
<li>什么是 Pair RM?</li>
</ol>
<p><strong>原本 RM 看到的是 (prompt, answer), Pair RM 看到的是 (prompt, pos _answer, neg _answer). 这样首先是效率高点, 然后正负例可以看到对方, 模型可以将两者对比着学习, 整体的解释性和泛化性会比一般的好. (原始 RM 容易 overfit)</strong></p>
<ol start="5">
<li>BT model (DPO, RM) 的不足</li>
</ol>
<p><strong>只学习两两间的局部关系, 无法学习全局关系; 容易过拟合字面上差距大的正负例对; 模型只学习到了正例, 对负例输出的概率接近 0, 鲁棒性差.</strong></p>
<ol start="6">
<li>DPO 的变体有哪些?</li>
</ol>
<p><strong>DPO 容易过拟合数据集. IPO: $\argmin (\log\frac{\pi(y _w|x)\pi _{ref}(y _l|x)}{\pi(y _l|x)\pi _{ref}(y _w|x)}-\beta)^2$, 相当于加入 L2 正则, 保证 DPO 不使用 Early stop 的收敛性.</strong></p>
<h2 id="GRPO"><a href="#GRPO" class="headerlink" title="GRPO"></a>GRPO</h2><p><img src="/2025/04/06/LLM/ppo.png" alt="r1"></p>
<center style="font-size:12px; font-weight:bold">Fig. 10. PPO v.s. GRPO</center><br>

<p>PPO 需要知道从当前步开始的最终奖励以估计 $A$, 因而 PPO 会需要一个 Value Model 来估计奖励. GRPO 则通过对同一个 prompt 生成多个回答 $o _i$, 然后用这些回答的总体评价 $r _i$ 作为整个回答的价值 $v _i$, 来省去价值函数, 最后用所有回答的均值和方差对每个回答进行标准化, 来达到类似 GAE 的效果:</p>
<p>$$<br>A _i &#x3D; \frac{r _i-\text{mean}(r)}{\text{std}(r)}<br>$$</p>
<blockquote>
<p>直观地来讲就是: 把 LLM 响应的过程只当作一个 Action, 那么此时的 $r$ 就是实际上的 $v$.</p>
</blockquote>
<p>最终的损失函数为:</p>
<p>$$<br>\mathcal{L} _{grpo}&#x3D;\argmax[\mathbb{E} _{q\sim P(Q), o\sim\pi _{\theta _{old}}(O|q)}\frac{1}{G}\sum _{i&#x3D;1}^{G}\frac{1}{|o _i|}\sum _{t&#x3D;1}^{|o _i|}\min[\frac{\pi _\theta(o _{i,t}|q,o _{i,&lt;t})}{\pi _{\theta _{old}}(o _{i,t}|q,o _{i,&lt;t})}A _i,clip(\frac{\pi _\theta(o _{i,t}|q,o _{i,&lt;t})}{\pi _{\theta _{old}}(o _{i,t}|q,o _{i,&lt;t})},1-\epsilon, 1+\epsilon)A _i]-\beta\mathbb{D} _\text{KL}\left[\pi _\theta|\pi _{\text{ref}}\right]]<br>$$</p>
<blockquote>
<p>或者 $\log\frac{\pi _\theta(o _{i,t}|q,o _{i,&lt;t})}{\pi _{\theta _{old}}(o _{i,t}|q,o _{i,&lt;t})}$, 这样除就变成减了. 上式与 PPO-惩罚基本一致.</p>
</blockquote>
<p>其中 $G$ 是每个 prompt 生成的回答个数 (即 Group 的由来), $|o _i|$ 是每个回答的长度. 最后的 KL 约束项是为了防止最终模型偏离初始模型过远. GRPO 对 KL 约束项的计算也是采用估计的方法, 只不过和 PPO 的不同:</p>
<p>$$\mathbb{D} _{\text{KL}}\left[\pi _\theta|\pi _{\text{ref}}\right] &#x3D; \frac{\pi _{\text{ref}}(o _{i,t} \mid q, o _{i,&lt;t})}{\pi _\theta(o _{i,t} \mid q, o _{i,&lt;t})} - \log \frac{\pi _{\text{ref}}(o _{i,t} \mid q, o _{i,&lt;t})}{\pi _\theta(o _{i,t} \mid q, o _{i,&lt;t})} - 1, $$</p>
<blockquote>
<p>此处计算的 KL 是把当前的 policy $\pi _\theta$ 当作真实分布的, 但由于参数是可更新的, 所以实际上是在限制当前的策略不要偏离 ref 的太远. 上式为凸函数 (开口向上), 所以, 因为最后是梯度下降, $\mathcal{L} _{grpo}$ 还要乘负号, 故对 $D _{KL}$ 的优化是向最小值的.</p>
</blockquote>
<p>计算流程如下:</p>
<p><img src="/2025/04/06/LLM/grpo.png" alt="r3"></p>
<center style="font-size:12px; font-weight:bold">Fig. 11. GRPO</center><br>

<h3 id="问答-12"><a href="#问答-12" class="headerlink" title="问答"></a>问答</h3><ol>
<li>KL 散度的估计方式?</li>
</ol>
<p><strong>设真实分布为 q, 用来近似真实分布的分布为 p. k1 估计: $\log\frac{q(x)}{p(x)}&#x3D;-\log\frac{p(x)}{q(x)}$, 无偏, 但是方差大, 因为有一半可能为负值, 但KL一定为正值; k2 估计: $\frac{1}{2}(\log\frac{q(x)}{p(x)})^2$, 有偏, 但方差小, 因为始终为正; k3 在 k1 的基础上得到, 他在 k1 上加了一个期望为 0 (关于 $q(x)$) 且与 k1 负相关的量, 从而在保证无偏的同时降低方差. 即 $\frac{p(x)}{q(x)}-1$, 这样有一个很美好的性质: $x-1\ge \log(x)$, 所以 $\frac{p(x)}{q(x)}-1-\log\frac{p(x)}{q(x)}$ 恒正!</strong></p>
<ol start="2">
<li>为什么要估计 KL 而不是直接算?</li>
</ol>
<p><strong>太耗资源, 需要很多的 sample. 所以只能用少量的样本估计.</strong></p>
<ol start="3">
<li>Dense Reward</li>
</ol>
<p><strong>类似于 code 和 math 过程分, GRPO dense reward 更好, 鼓励模型探索难题.</strong></p>
<p><img src="/2025/04/06/LLM/r1_pipeline.jpg" alt="r2"></p>
<center style="font-size:12px; font-weight:bold">Fig. 12. DeepSeek-R1 Pipeline</center><br>

<h3 id="随想"><a href="#随想" class="headerlink" title="随想"></a>随想</h3><ul>
<li><p>DPO 和 PPO 的区别与联系. 从目的上的来讲, 大语言模型的强化学习都是要优化模型使得模型能够输出符合人类偏好的输出, 所以最基础的损失函数就是: 给定两个问答 Pair, 一好一坏, 模型在好回答上的 reward 更高. 为了实现这一目标, DPO 是把模型的整个回答视作一次 Action, 因而是一个将该问题转化为了一个特殊的多臂老虎机的问题, 然后又通过转换建立的 Reward 和 Policy 的关系, 从而实现将 LLM 同时视作 reward 和 Policy model, 最大化正例, 最小化负例, DPO 实现的是利用 正例和负例 的对比来优化模型. 他对 reward 是采用样本的观测值来估计的, 或者说是 概率 这一个特殊的 reward 来估计的. 所以就本质上来说, DPO 需要很多的训练样本, 这样统计上估计的 reward 才能够逼近期望; PPO 则是把每次 Next token prediction 视作一次 Action, 因而生成整个回答的过程是个 马尔可夫决策过程, 相应地需要过程的 reward, 然后是直接最大化 reward, 而不是直接将正负例对比地学. 不同于 DPO, PPO 会首先学习一个 reward model 来对 respond 打分, 而不需要很多的样本的平均值做估计, 因此样本效率更高, 但相应地, reward model 毕竟也只是用数据优化的模型, 而不是一个真正地能够完全反映人类偏好的 model, 所以 PPO 在优化的过程中会很容易找到一条捷径, 即 reward hacking, 这时 模型输出的实际是对人没有任何意义的东西, 如一直重复某个词, 但 reward model 却会给其打高分.</p>
</li>
<li><p>DPO 高方差 (无价值函数、离线 (数据无法及时更新, 随着训练进行, 与当前的分布差异越来越大)); PPO 低方差 (引入 value model, 有着更准确的价值估计.)</p>
</li>
<li><p>FA1 和 FA2 的区别: 减少了 Output 的写入次数, FA1 的外层循环 Load K, V, 内层 Load Q, O; FA2 的外层循环 Load Q, 内层 Load K, V. 所以对于 FA2, 一轮内部训练可以把一 block 的 Q 的结果都计算出来, 因而 FA2 在计算的时候可以把 前面的 在 SRAM 里存着; 而 FA1, 由于对于同一个 BLOCK 的 kv, 内部是先计算不同 Block 的 Q, 得到 O, 而此时的 O 是不完整的, 因为只使用了部分的 kv, 所以为了处理下一个 BLOCK 的 Q, 必须要把这个 O 给写入 HBM, 这就增加了 IO. </p>
</li>
<li><p>先切分流水并行，然后张量并行是相邻的卡，剩下的做数据并行。</p>
</li>
</ul>
]]></content>
      <categories>
        <category>Advanced Model</category>
      </categories>
      <tags>
        <tag>Attention Mechanism</tag>
        <tag>Deep Learning</tag>
        <tag>Generative AI</tag>
        <tag>LLM</tag>
      </tags>
  </entry>
  <entry>
    <title>Position Embedding</title>
    <url>/2024/09/19/PositionEmbedding/</url>
    <content><![CDATA[<span id="more"></span>

<p>Unlike RNNs, which recurrently process tokens of a sequence one by one, self-attention ditches sequential operations for parallel computation. To enable self-attention to perceive the sequence order, position embedding is a solution. The position embedding could either be learnable or fixed. Here we only consider the fixed position embedding.</p>
<h1 id="Absolute-Position-Embedding"><a href="#Absolute-Position-Embedding" class="headerlink" title="Absolute Position Embedding"></a>Absolute Position Embedding</h1><p>A typical kind of absolute position embedding is the sinusoidal function used in the vanilla transformer:</p>
<p>$$<br>\begin{cases}<br> p _{pos,2j}&amp;&#x3D;\sin(\frac{pos}{10000^{2j&#x2F;d}}), \\<br> p _{pos,2j+1}&amp;&#x3D;\cos(\frac{pos}{10000^{2j&#x2F;d}}),<br>\end{cases}<br>$$<br>where $pos$ is the position of token while $2j$ and $2j+1$ are embedding indices.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pe = torch.zeros(<span class="number">1</span>, max_len, n_hiddens)</span><br><span class="line">freqs = torch.arange(max_len, dtype=torch.float32).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">freqs /= torch.<span class="built_in">pow</span>(<span class="number">10000</span>, torch.arange(<span class="number">0</span>, n_hiddens, <span class="number">2</span>, dtype=torch.float32) / n_hiddens)</span><br><span class="line">pe[:, :, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(freqs)</span><br><span class="line">pe[:, :, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(freqs)</span><br></pre></td></tr></table></figure>

<ul>
<li>Bert, GPT2: Learnable Absolute PE</li>
<li>Vanilla Transformer, GPT3: Sinusoidal PE</li>
</ul>
<h1 id="Relative-Position-Embedding"><a href="#Relative-Position-Embedding" class="headerlink" title="Relative Position Embedding"></a>Relative Position Embedding</h1><p>Unlike absolute position embedding, relative position embeddings focus on the relative position of <code>query</code> and <code>key</code>.</p>
<p>Generally, a <code>query</code> and a <code>key</code> with position embeddings could be formulated as:</p>
<p>$$<br>\begin{cases}<br>    \mathbf{q} _m&#x3D;\mathbf{W} _q(x _m+p _m), \\<br>    \mathbf{k} _n&#x3D;\mathbf{W} _k(x _n+p _n).<br>\end{cases}<br>$$</p>
<p>The resulting attention is:</p>
<p>$$<br>    \mathbf{q} _m ^T\mathbf{k} _n&#x3D;x _m ^T\mathbf{W} _q ^T\mathbf{W} _kx _n+x _m ^T\mathbf{W} _q ^T\mathbf{W} _k p_n + p _m ^T\mathbf{W} _q ^T\mathbf{W} _k x_n + p _m ^T\mathbf{W} _q ^T\mathbf{W} _k p_n.<br>$$</p>
<p>The relative position embedding only needs to replace $p _m$ and $p _n$ with the relative forms. The simplest one is:</p>
<p>$$<br>    \mathbf{q} _m ^T\mathbf{k} _n&#x3D;x _m ^T\mathbf{W} _q ^T\mathbf{W} _kx _n+x _m ^T\mathbf{W} _q ^T\mathbf{W} _k p _{m-n} + p _{m-n} ^T\mathbf{W} _q ^T\mathbf{W} _k x_n<br>$$</p>
<blockquote>
<p>Relative position embedding are typically encoded as biases and added to <strong>attention scores</strong> (not to <strong>values</strong>). Another difference between relative and absolute position embedding is that relative position embedding is applied to each layer while absolute position embedding is only added in the bottom layer.</p>
</blockquote>
<h2 id="Rotary-Position-Embedding"><a href="#Rotary-Position-Embedding" class="headerlink" title="Rotary Position Embedding"></a>Rotary Position Embedding</h2><p>The intuition of <a href="https://arxiv.org/pdf/2104.09864">RoPE</a> is that the vanilla <code>Add</code> PE not only change the magnitude but also change the radius, which will cause the position embedding to change irregularly, resulting in the model only being able to memorize the data but not actually learn the position. This in turn causes the model to be unable to extend to positions that have not been seen in the <strong>training set</strong>.</p>
<p>RoPE uses the rotation matrix of the two-dimensional vector space so that the same token at different positions only changes in angle. That is:</p>
<p>$$<br>    \mathbf{q} _m ^T\mathbf{k} _n&#x3D;(\mathbf{R} ^d _{\theta, m}\mathbf{W} _qx _m) ^T(\mathbf{R} ^d _{\theta, n}\mathbf{W} _kx _n)&#x3D;x _m ^T\mathbf{W} _q ^T\mathbf{R} _{\theta, n-m} ^d \mathbf{W} _k x _n<br>$$<br>where</p>
<p>$$<br>\mathbf{R} _{\theta, m}&#x3D;\begin{pmatrix}<br> &amp;\cos m\theta _1 &amp;-\sin m\theta _1 &amp;0 &amp;0 &amp;\cdots &amp;0 &amp;0\\<br> &amp;\sin m\theta _1 &amp;\cos m\theta _1 &amp;0 &amp;0 &amp;\cdots &amp;0 &amp;0\\<br> &amp;0 &amp;0 &amp;\cos m\theta _2 &amp;-\sin m\theta _2 &amp;\cdots &amp;0 &amp;0\\<br> &amp;0 &amp;0 &amp;\sin m\theta _2 &amp;\cos m\theta _2 &amp;\cdots &amp;0 &amp;0\\<br> &amp;\vdots &amp;\vdots &amp;\vdots &amp;\vdots &amp;\ddots &amp;\vdots &amp;\vdots\\<br> &amp;0 &amp;0 &amp;0 &amp;0 &amp;\cdots &amp;\cos m\theta _{d&#x2F;2} &amp;-\sin m\theta _{d&#x2F;2}\\<br> &amp;0 &amp;0 &amp;0 &amp;0 &amp;\cdots &amp;\sin m\theta _{d&#x2F;2} &amp;\cos m\theta _{d&#x2F;2}\\<br>\end{pmatrix}<br>$$<br>is the rotary matrix with the same $\theta _i$ as the sinusoidal function. Intuitively, the rotation matrix is ​​realized by <strong>grouping the embedding dimensions in pairs</strong> to expand the two-dimensional rotation matrix to multiple dimensions.</p>
<p><img src="/2024/09/19/PositionEmbedding/1.png" alt="1"></p>
<center style="font-size:12px; font-weight:bold">Fig. 1. Vanilla PE (left) and RoPE (right)</center><br>

<p>Compared with the vanilla position embeddings, RoPE has several advantages:</p>
<ol>
<li>Incorporating relative position information more efficiently.</li>
<li>Long-term decay: the attention weight will decay when the relative position increases.</li>
</ol>
<p>Adopted by:</p>
<ul>
<li><a href="https://github.com/meta-llama/llama3/blob/main/llama/model.py">Llama</a></li>
<li>GPT3.5, GPT4, GPT4o</li>
</ul>
<blockquote>
<p>About <a href="https://mp.weixin.qq.com/s/54YdSdB1uX-i7mt7kasFIQ">Context Window Extension</a></p>
</blockquote>
<h3 id="Fast-Calculation"><a href="#Fast-Calculation" class="headerlink" title="Fast Calculation"></a>Fast Calculation</h3><p>Actually, in the two-dimensional case, $\mathbf{R} _\theta x$ ($x&#x3D;(x _1, x _2)$) can be converted to complex number calculation, that is:</p>
<p>$$<br>\mathbf{R} _\theta x&#x3D;(\cos\theta + i\sin\theta)(x _1+ix _2)&#x3D;(\cos\theta x _1-\sin\theta x _2) + i(\sin\theta x _1 + \cos\theta x _2)<br>$$</p>
<p>whose coeffiencts are the result. The following is a quick start (from <a href="https://github.com/meta-llama/llama3/blob/main/llama/model.py">Llama</a>):</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">precompute_freqs_cis</span>(<span class="params">n_hiddens: <span class="built_in">int</span>, max_len: <span class="built_in">int</span>, base: <span class="built_in">float</span> = <span class="number">10000.0</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Turn R_&#123;\theta,m&#125; into complex form </span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    pos = torch.arange(max_len, dtype=torch.float32)</span><br><span class="line">    freqs = <span class="number">1.0</span> / torch.<span class="built_in">pow</span>(base, torch.arange(<span class="number">0</span>, n_hiddens, <span class="number">2</span>, dtype=torch.float32) / n_hiddens)</span><br><span class="line">    <span class="comment"># outer product, shape [max_len, n_hiddens//2]</span></span><br><span class="line">    freqs = torch.outer(pos, freqs)</span><br><span class="line">    <span class="comment"># complex 64, shape, shape [max_len, n_hiddens//2]</span></span><br><span class="line">    freqs_cis = torch.polar(torch.ones_like(freqs, freqs))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> freqs_cis</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">reshape_for_broadcast</span>(<span class="params">freqs_cis: torch.Tensor, x: torch.Tensor</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    freqs_cis: [max_len, n_hiddens//2] (complex)</span></span><br><span class="line"><span class="string">    x: [batch, max_len, n_hiddens//2] or [..., max_len, n_hiddens//2] (complex)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    ndim = x.ndim</span><br><span class="line">    <span class="keyword">assert</span> <span class="number">0</span> &lt;= <span class="number">1</span> &lt; ndim</span><br><span class="line">    <span class="keyword">assert</span> freqs_cis.shape == (x.shape[<span class="number">1</span>], x.shape[-<span class="number">1</span>])</span><br><span class="line">    shape = [d <span class="keyword">if</span> i == <span class="number">1</span> <span class="keyword">or</span> i == ndim - <span class="number">1</span> <span class="keyword">else</span> <span class="number">1</span> <span class="keyword">for</span> i, d <span class="keyword">in</span> <span class="built_in">enumerate</span>(x.shape)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> freqs_cis.view(*shape)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">apply_rotary_emb</span>(<span class="params"></span></span><br><span class="line"><span class="params">    xq: torch.Tensor,</span></span><br><span class="line"><span class="params">    xk: torch.Tensor,</span></span><br><span class="line"><span class="params">    freqs_cis: torch.Tensor,</span></span><br><span class="line"><span class="params"></span>)</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Assume xq and xk: [batch, max_len, n_hiddens]</span></span><br><span class="line"><span class="string">    freqs_cis: [max_len, n_hiddens//2] (complex)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># [batch, max_len, n_hiddens] -&gt; [batch, max_len, n_hiddens//2] (complex)</span></span><br><span class="line">    xq_ = torch.view_as_complex(xq.<span class="built_in">float</span>().reshape(*xq.shape[:-<span class="number">1</span>], -<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">    xk_ = torch.view_as_complex(xk.<span class="built_in">float</span>().reshape(*xk.shape[:-<span class="number">1</span>], -<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">    <span class="comment"># [max_len, n_hiddens//2] -&gt; [1, max_len, n_hiddens//2] (complex)</span></span><br><span class="line">    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)</span><br><span class="line">    <span class="comment"># [batch, max_len, n_hiddens//2] (complex) -&gt; [batch, max_len, n_hiddens//2, 2] -&gt; [batch, max_len, n_hiddens]</span></span><br><span class="line">    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(<span class="number">3</span>)</span><br><span class="line">    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> xq_out.type_as(xq), xk_out.type_as(xk)</span><br></pre></td></tr></table></figure>

<h2 id="ALiBi"><a href="#ALiBi" class="headerlink" title="ALiBi"></a>ALiBi</h2><p><a href="https://arxiv.org/pdf/2108.12409">ALiBi</a></p>
<h2 id="CoPE"><a href="#CoPE" class="headerlink" title="CoPE"></a>CoPE</h2><p><a href="https://arxiv.org/pdf/2405.18719">CoPE</a></p>
<h1 id="No-Position-Embedding"><a href="#No-Position-Embedding" class="headerlink" title="No Position Embedding"></a>No Position Embedding</h1><p><a href="https://arxiv.org/pdf/2203.16634">NoPE</a> means training transformers without position embedding. It can only apply for <strong>decoder-only</strong> structure (GPT, Llama), but not <strong>encoder-only</strong> structure (Bert). The reason might be that the causal attention in autoregressive transformer language models allows them to predict the number of attendable tokens at each position, i.e. the number of tokens in the sequence that precede the current one .</p>
]]></content>
      <categories>
        <category>Advanced Model</category>
      </categories>
      <tags>
        <tag>Attention Mechanism</tag>
        <tag>Generative AI</tag>
      </tags>
  </entry>
  <entry>
    <title>Diffusion</title>
    <url>/2024/06/17/Diffusion/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="What-is-Diffusion-Model"><a href="#What-is-Diffusion-Model" class="headerlink" title="What is Diffusion Model"></a>What is Diffusion Model</h1><blockquote>
<p>Sculpture is born from stone, I just cut off the part which is unnecessary. --Michelangelo</p>
</blockquote>
<p>In diffusion models, while training, the &#39;stone&#39; is generated from the training data by adding random noise. The sculpture is revealed by iteratively removing the noise added.</p>
<p>Unlike other generative models, the diffusion model does not generate data but generates noise. That is, the generator of diffusion model outputs noise, which is then removed from the noisy data. The denoised data is used as the input for the next denoising process. Therefore, the ground truth of each denoising process is the noise added and the prediction is the noise generated.</p>
<p><img src="/2024/06/17/Diffusion/1.png" alt="1"></p>
<center style="font-size:12px; font-weight:bold">Fig. 1. Pipeline of Diffusion</center><br>

<h1 id="Maths-behind-Diffusion-x2F-Generation"><a href="#Maths-behind-Diffusion-x2F-Generation" class="headerlink" title="Maths behind Diffusion&#x2F;Generation"></a>Maths behind Diffusion&#x2F;Generation</h1><p>The goal of image generation: given $z$ from a normal distribution, output $G(z)&#x3D;x$, where $x$ is similar to the original image distribution.</p>
<p><img src="/2024/06/17/Diffusion/2.png" alt="2"></p>
<center style="font-size:12px; font-weight:bold">Fig. 2. Goal of Image Generation</center><br>

<blockquote>
<p>The same is true for conditional generation, except that we consider the distribution that meets this condition.</p>
</blockquote>
<p>Hence, given training set $\{x^1,...,x^m\}\in P_{data}(x)$, we aim to find $\theta$ that maximizes the probability of these observations. And this is maximum likelihood estimation:</p>
<p>$$<br>\theta ^*&#x3D; arg\max _\theta \prod _{i&#x3D;1} ^m P _{\theta}(x ^i)<br>$$</p>
<p><img src="/2024/06/17/Diffusion/3.png" alt="3"></p>
<center style="font-size:12px; font-weight:bold">Fig. 3. Maximum Likelihood Estimation (MLE)</center><br>

<p>Actually, MLE is equal to minimizing <em>KL Divergence</em> :</p>
<p>$$<br>\begin{align*}<br>    \theta ^*&#x3D;&amp; arg\max _\theta \prod _{i&#x3D;1} ^m P _{\theta}(x ^i)&#x3D;arg\max _\theta \sum _{i&#x3D;1} ^m \log P _{\theta}(x ^i)\\<br>    \approx &amp;arg\max _\theta E _{x\sim P _{data}} \log P _{\theta}(x)\\<br>    &#x3D;&amp;arg\max _\theta \int _x P _{data}(x) \log P _{\theta}(x)\\<br>    &#x3D;&amp;arg\max _\theta \int _x P _{data}(x) \log P _{\theta}(x) - \int _x P _{data}(x)\log P _{data}(x) dx\\<br>    &#x3D;&amp;arg\max _\theta \int _x P _{data}(x) \log \frac{P _{\theta}(x)}{P _{data}(x)}\\<br>    &#x3D;&amp;arg\min _\theta KL(P _{data}||P _{\theta})<br>\end{align*}<br>$$</p>
<h1 id="Diffusion-Models"><a href="#Diffusion-Models" class="headerlink" title="Diffusion Models"></a>Diffusion Models</h1><h2 id="Denoising-Diffusion-Probabilistic-Models-DDPM"><a href="#Denoising-Diffusion-Probabilistic-Models-DDPM" class="headerlink" title="Denoising Diffusion Probabilistic Models (DDPM)"></a>Denoising Diffusion Probabilistic Models (DDPM)</h2><p>In DDPM, </p>
<p>$$<br>P _\theta(x _0)&#x3D;\int _{x _1:x_T}P(x _T)P _{\theta}(x _{T-1}|x _T)...P _\theta(x _{t-1}|x _t)...P _\theta(x _{0}|x _1)dx _1:x _T<br>$$</p>
<p><img src="/2024/06/17/Diffusion/4.png" alt="4"></p>
<center style="font-size:12px; font-weight:bold">Fig. 4. Denoising</center><br>

<p>Similar to <a href="/2024/01/20/VAE/">VAE</a>, we actually only optimize the lower bound of $P _\theta(x)$, that is:</p>
<p>$$<br>arg\max _\theta L _b&#x3D;E _{q(x _1:x _T| x)}[\log (\frac{P(x: x _T)}{q (x _1: x _T|x)})]<br>$$</p>
<p>where $q(x _1:x _T| x)&#x3D;q(x _1|x)q(x _2| x _1)...q(x _T| x _{T-1})$ is the forward process, which is equal to the encoder of VAE.</p>
<h3 id="Forward-Process"><a href="#Forward-Process" class="headerlink" title="Forward Process"></a>Forward Process</h3><p>During forward process, multi-steps could be turned to single-step. As shown in the following figure:</p>
<p><img src="/2024/06/17/Diffusion/5.png" alt="5"></p>
<center style="font-size:12px; font-weight:bold">Fig. 5. One-step Forward</center><br>

<h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><p>Therefore, during training, we actually only add noise once and denoise once:</p>
<p><img src="/2024/06/17/Diffusion/6.png" alt="6"></p>
<center style="font-size:12px; font-weight:bold">Fig. 6. Training</center><br>

<h3 id="Generation"><a href="#Generation" class="headerlink" title="Generation"></a>Generation</h3><p>While generating, we randomly sample data from the normal distribution and generate the image after $T$ times of denoising.</p>
<p><img src="/2024/06/17/Diffusion/7.png" alt="7"></p>
<center style="font-size:12px; font-weight:bold">Fig. 7. Generation</center><br>

<p>where the first term is mean and the second term is variance.</p>
<h2 id="Stable-Diffusion"><a href="#Stable-Diffusion" class="headerlink" title="Stable Diffusion"></a>Stable Diffusion</h2><p>Common architecture of modern (Text2Image) generative networks:</p>
<p><img src="/2024/06/17/Diffusion/8.png" alt="8"></p>
<center style="font-size:12px; font-weight:bold">Fig. 8. Architecture of Modern Generative Networks</center><br>

<ul>
<li><strong>Text Encoder</strong>: Encode text to latent representation, which is essential for the quality of final output.</li>
<li><strong>Generation Model</strong>: Any generation model, e.g., VAE, Diffusion, and GAN.</li>
<li><strong>Decoder</strong>: Decode latent representation to image.</li>
</ul>
<p>These three parts could be trained independently. For the decoder, it can accept two different inputs, depending on how you train it:</p>
<p><img align="left" src="/2024/06/17/Diffusion/9.png" style=" width:380px; padding: 0px 0px; "> <img align="mid" src="/2024/06/17/Diffusion/10.png" style=" width:380px; padding: 0px 0px; "></p>
<ol>
<li>A smaller version of the original image (Imagen): we can train this decoder in a supervised learning manner.</li>
<li>Latent representation (Stable Diffusion, DALL-E): we can train this decoder via AE.</li>
</ol>
<p>The following is the pipeline of stable diffusion, which is similar to the common architecture:</p>
<p><img src="/2024/06/17/Diffusion/11.png" alt="11"></p>
<center style="font-size:12px; font-weight:bold">Fig. 9. Architecture of Stable Diffusion</center><br>

<ul>
<li>In addition to text, stable diffusion also supports other conditional generation.</li>
<li>Stable diffusion train decoder in latent space. Hence, while training, the training image is transformed to latent representation $z$ using the encoder of AE. </li>
<li>The forward and denoising processes are also finished in the latent space.</li>
</ul>
<p><img src="/2024/06/17/Diffusion/12.png" alt="12"></p>
<center style="font-size:12px; font-weight:bold">Fig. 10. Forward in Latent Space</center><br>

<h1 id="Why-Diffusion-Works"><a href="#Why-Diffusion-Works" class="headerlink" title="Why Diffusion Works"></a>Why Diffusion Works</h1><p>The diffusion model can actually be regarded as an autoregressive model with a global perspective. It ensures the integrity of the generated image while achieving self-correction through iterative denoising.</p>
]]></content>
      <categories>
        <category>Advanced Model</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Generative AI</tag>
      </tags>
  </entry>
  <entry>
    <title>VAE</title>
    <url>/2024/01/20/VAE/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="Intuitive-AE和VAE"><a href="#Intuitive-AE和VAE" class="headerlink" title="Intuitive: AE和VAE"></a>Intuitive: AE和VAE</h1><p><img src="/2024/01/20/VAE/1.png" alt="1"></p>
<center style="font-size:12px; font-weight:bold">Fig. 1. AE, VAE</center><br>

<p>训练时，两者都是优化输入图片和输出图片的差距来更新参数。最后部署、生成图片时，只使用Decoder，接受某个向量输入，生成图片。</p>
<p>AE的Encoder本质上是将一个图片映射为高维向量空间的一个点，而VAE的Encoder通过加入随机噪声，使得图片被映射为高维空间的一段区间。这样的好处在于，当输入Decoder的向量介于某两个向量之间时，VAE更有可能输出一个结合了这两个向量对应图片特征的图片，因为训练的误差会诱导网络这么做。</p>
<p><img src="/2024/01/20/VAE/2.png" alt="2"></p>
<center style="font-size:12px; font-weight:bold">Fig. 2. Intuitive. AE, VAE</center>

<h1 id="原理-噪声角度"><a href="#原理-噪声角度" class="headerlink" title="原理: 噪声角度"></a>原理: 噪声角度</h1><p>VAE的Encoder输出两个向量，一个原始编码$m$ (均值)，一个方差$\sigma$。我们期望加了噪声后的分布能尽可能地趋近于一个正态分布，因此引入一个从正态分布中采样出的向量$e$引导噪声。$\sigma$和$e$结合生成最终的噪声。但是，极端情况为学到的$\sigma$趋近于负无穷，使得VAE退化为AE。因此，除了输入图片和输出图片产生的$\mathcal{L} _1$以外，引入$\mathcal{L} _2$:</p>
<p>$$<br>\mathcal{L} _2 &#x3D; \min\sum (\text{exp}(\sigma _i)-(1+\sigma _i) + m _i ^2)<br>$$</p>
<p>其中前两项在$\sigma _i$取0时取最小值，这将迫使$\sigma _i$能够产生有效的噪声。最后一项是L2正则。</p>
<h1 id="原理-高斯混合模型角度"><a href="#原理-高斯混合模型角度" class="headerlink" title="原理: 高斯混合模型角度"></a>原理: 高斯混合模型角度</h1><p>对于一个给定的图片集，其在高维空间服从一个高斯混合分布$P(x)$ ($x$是图的RGB矩阵)，$P(x)$的值表明某个$x$属于这个空间的概率 (如某张图片是宝可梦精灵的概率)。</p>
<p><img src="/2024/01/20/VAE/3.png" alt="3"></p>
<center style="font-size:12px; font-weight:bold">Fig. 3. GMM</center><br>

<p>高斯混合模型由多个高斯分布组成，因而可写成:</p>
<p>$$<br>P(x)&#x3D;\sum _m P(m)P(x | m)<br>$$</p>
<p>其中$P(m)$是$m$高斯分布在混合分布中的权重。$P(x|m)$即为$m$高斯分布，有均值$\mu _m$和方差$\sigma _m$。极限情况下，高斯混合模型由无数个高斯分布组成:</p>
<p>$$<br>P(x) &#x3D; \int _z P(z)P(x |z)dz<br>$$</p>
<p>假设$z$服从一个标准正态分布，要将$z$对应为一个高斯分布，只需要生成均值$\mu(z)$和方差$\sigma (z)$即可:</p>
<p><img src="/2024/01/20/VAE/4.png" alt="4"></p>
<center style="font-size:12px; font-weight:bold">Fig. 4. Decoder</center><br>

<p>生成这样映射的过程显然是一个参数估计的过程，可由最大似然估计 (MLE) 优化:</p>
<p>$$<br>L &#x3D; \max\sum _x \log P(x)<br>$$</p>
<p>这又可以转化为一个EM过程。$z$是一个隐变量，可由已知的$x$估计，对$\log P(x)$做如下修改:</p>
<p>$$<br>\log P(x) &#x3D; \int _z q(z|x)\log P(x)dz<br>$$</p>
<p>$q(z|x)$是给定$x$，$z$的概率密度，显然其积分为1，故上式是一个恒等式。进一步处理:</p>
<p>$$<br>\begin{align*}<br>    \log P(x) &#x3D;&amp;\int _z q(z|x)\log (\frac{P(z,x)}{P(z|x)})dz\\<br>    &#x3D;&amp;\int _z q(z|x)\log (\frac{P(z,x)}{P(z|x)}\frac{q(z|x)}{q(z|x)})dz\\<br>    &#x3D;&amp;\int _z q(z|x)\log (\frac{P(z,x)}{q(z|x)})dz+\int _z q(z|x)\log (\frac{q(z|x)}{P(z|x)})dz<br>\end{align*}<br>$$</p>
<p>第一步用的是条件概率密度，其他都是普通的变换。第二项被称为KL散度 ($\text{KL}(q(z|x)||P(z|x))$)，用于衡量两个分布的相似性，大于0。故</p>
<p>$$<br>\log P(x) \ge L _b&#x3D;\int _z q(z|x)\log (\frac{P(z,x)}{q(z|x)})dz<br>$$</p>
<p>这一步体现了引入$q(z|x)$的作用。$q(z|x)$的值不会影响$L$ (积分恒为1)，但是通过优化$q(z|x)$，使其趋近于$P(z|x)$，我们可以让KL散度项逐渐趋于0，使得:</p>
<p>$$<br>\log P(x) \approx L _b<br>$$</p>
<p>于是MLE的优化对象就变为$L _b$，而$L _b$可进一步转化为:</p>
<p>$$<br>\begin{align*}<br>    L _b&#x3D;&amp;\int _z q(z|x)\log (\frac{P(x|z)P(z)}{q(z|x)})dz\\<br>    &#x3D;&amp;\int _z q(z|x)\log (\frac{P(z)}{q(z|x)})dz +\int _z q(z|x)\log (P(x|z))dz<br>\end{align*}<br>$$</p>
<p>其中前项是$P(z)$和$q(z|x)$的KL散度的相反数。而$q(z|x)$由$x$经过映射生成:</p>
<p><img src="/2024/01/20/VAE/5.png" alt="5"></p>
<center style="font-size:12px; font-weight:bold">Fig. 5. Encoder</center><br>

<p>这项实际上就是VAE的$\mathcal{L} _2$，因为要$\max L _b$，则必须要$\min \text{KL}(P(z),q(z|x))$，也就是要让生成的编码尽可能地趋向正态分布。</p>
<p>而第二项实际上是在求$\log P(x|z)$在分布$q(z|x)$下的均值:</p>
<p>$$<br>\int _z q(z|x)\log (P(x|z))dz&#x3D;E _{q(z|x)}[\log P(x|z)]<br>$$</p>
<p>$P(x|z)$又由$z$经过映射生成。最终便可整合为VAE的过程:</p>
<p><img src="/2024/01/20/VAE/6.png" alt="6"></p>
<center style="font-size:12px; font-weight:bold">Fig. 6. VAE</center><br>

<p>即$x$生成$q(z|x)$高斯分布，然后从中采样出$z$，$z$生成高斯分布$P(x|z)$，由于均值$\mu (x)$处的概率最大，因此$x$越趋近$\mu (x)$，概率就越大，这也就是$\mathcal{L} _1$。</p>
<h1 id="Conditional-VAE"><a href="#Conditional-VAE" class="headerlink" title="Conditional VAE"></a>Conditional VAE</h1><p>在训练时给VAE一些引导，使其Encoder的生成有倾向性。可用于文本生成图片。</p>
<h1 id="Shortcoming-of-VAE"><a href="#Shortcoming-of-VAE" class="headerlink" title="Shortcoming of VAE"></a>Shortcoming of VAE</h1><p>只是对训练集的模仿。</p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p>台湾大学李宏毅课程。</p>
]]></content>
      <categories>
        <category>Advanced Model</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Generative AI</tag>
      </tags>
  </entry>
  <entry>
    <title>Computational Complexity: Complexity of Counting</title>
    <url>/2023/12/02/CountingComplexity/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="Counting-Problem"><a href="#Counting-Problem" class="headerlink" title="Counting Problem"></a>Counting Problem</h1><p>一些计数问题：</p>
<ul>
<li>$\sharp \text{CYCLE}$：计算有向图G中环的个数。</li>
<li>$\sharp \text{SAT}$：使得该公式为真的真值指派的个数。</li>
</ul>
<blockquote>
<p>计数问题要远难于判定问题。</p>
</blockquote>
<p>定理：如果$\sharp \text{CYCLE}$有多项式的算法，则$\textbf{P}&#x3D;\textbf{NP}$。</p>
<h1 id="sharp-textbf-P"><a href="#sharp-textbf-P" class="headerlink" title="$\sharp \textbf{P}$"></a>$\sharp \textbf{P}$</h1><p>对函数$f:\{0,1\} ^* \to \textbf{N}$，如果$\exists p : \textbf{N} \to \textbf{N}$和一个多项式图灵机$\mathbb{M}$使得$\forall x \in \{0,1\}  ^*$，有：</p>
<p>$$<br>f(x)&#x3D;|\{y \in \{0,1\} ^{p(|x|)}| \mathbb{M}(x,y)&#x3D;1\}|<br>$$</p>
<p>其中，右边表示使得括号成立的$y$的个数。这样的$f$是在$\sharp \textbf{P}$中的。</p>
<blockquote>
<p>上述定义可视为用“证据”定义的，因而也可用非确定图灵机定义$\sharp \textbf{P}$，此时$f$的值即为非确定图灵机回答YES的路径数。</p>
<p>$\textbf{PP}$可视为$\sharp \textbf{P}$的决策版本。</p>
</blockquote>
<h2 id="textbf-FP"><a href="#textbf-FP" class="headerlink" title="$\textbf{FP}$"></a>$\textbf{FP}$</h2><p>函数$(f:\{0,1\} ^*\to \textbf{N}) \in \textbf{FP}$，如果该函数可由多项式图灵机计算。</p>
<blockquote>
<p>$\textbf{FP} \subseteq \sharp\textbf{P}$</p>
</blockquote>
<h2 id="sharp-textbf-P-完全"><a href="#sharp-textbf-P-完全" class="headerlink" title="$\sharp \textbf{P}$完全"></a>$\sharp \textbf{P}$完全</h2><p>$f \in \sharp \textbf{P}\text{-complete}$如果$f \in \sharp \textbf{P}$且$\sharp \textbf{P} \in \textbf{FP} ^f$。</p>
<blockquote>
<p>即$\sharp \textbf{P}$中的问题都可以通过调用$f$解决。</p>
</blockquote>
<p>$\sharp \textbf{SAT}$是$\sharp \textbf{P}\text{-complete}$的。还是C-L归约。</p>
<blockquote>
<p>$\sharp \textbf{SAT}$：$&lt;\varphi, i&gt;$，其中$\varphi$是可取范式，$i$是使得$\varphi$为真的真值指派的个数。</p>
</blockquote>
<h1 id="Valiant-Theorem"><a href="#Valiant-Theorem" class="headerlink" title="Valiant Theorem"></a>Valiant Theorem</h1><p>Perm是$\sharp \textbf{P}$完全的。</p>
<p>Perm转换为有向图中包含所有结点的环。</p>
<p>把3CNF的变量和语句表示为图。</p>
<blockquote>
<p>最终Perm&#x3D;$4 ^{3m}\sharp \varphi$。</p>
</blockquote>
<p>还要证明Perm$\le _K$0-1Perm：把边转为$k$条权为1的平行边。</p>
<h1 id="Universal-Hash-Function"><a href="#Universal-Hash-Function" class="headerlink" title="Universal Hash Function"></a>Universal Hash Function</h1><p>$\mathcal{H} \subseteq B ^A$，其中$B ^A$表示从$A$到$B$的函数。$\mathcal{H}$是通用哈希函数族，如果：</p>
<p>$$\text{Pr} _{h \in _R \mathcal{H}}[h(x)&#x3D;h(x&#39;)]\le \frac{1}{|B|}$$</p>
<p>其中$x,x&#39; \in A,x \ne x&#39;$。</p>
<h2 id="m-wise-Independent-Hash-Function-Family"><a href="#m-wise-Independent-Hash-Function-Family" class="headerlink" title="m-wise Independent Hash Function Family"></a>m-wise Independent Hash Function Family</h2><p>$$<br>\text{Pr} _{h \in _R \mathcal{H} _{n,k}}[\land _{i&#x3D;1} ^m h(x _i)&#x3D; y _i]&#x3D;\frac{1}{2 ^{mk}}<br>$$</p>
<p>则称$\mathcal{H} _{n,k}$是m-independent的。其中，$x _i$互不相等。$\mathcal{H} _{n,k}$将长度为$n$的串映射为长度为$k$的串。换句话说，$\mathcal{H} _{n,k}$能保证对$m$个串，其对每一位的映射都是独立的。</p>
<blockquote>
<p>多项式线性方程的解，因为只有唯一解，所以能求出唯一的系数$a _0,...,a _{m-1}$。范德蒙德式。</p>
</blockquote>
<h1 id="Valiant-Vazirani-Theorem"><a href="#Valiant-Vazirani-Theorem" class="headerlink" title="Valiant-Vazirani Theorem"></a>Valiant-Vazirani Theorem</h1><p>$\textbf{UP}$，对其中的类，非确定图灵机的所有路径中，至多有一条路径接受它。$\textbf{P}\subseteq \textbf{UP} \subseteq \textbf{NP}$。</p>
<blockquote>
<p>$\textbf{USAT}$：只有唯一的真值指派使之为真。</p>
</blockquote>
<p>定理：$\forall n$变量的$\phi$，存在多项式概率图灵机$\mathbb{A}$：</p>
<p>$$<br>\begin{align*}<br>    &amp;\phi \in \textbf{SAT} \to \text{Pr}[\mathbb{A}(\phi)\in \textbf{USAT}] \ge 1&#x2F;(8n) \\<br>    &amp;\phi \notin \textbf{SAT} \to \text{Pr}[\mathbb{A}(\phi)\in \textbf{SAT}] &#x3D;0<br>\end{align*}<br>$$</p>
<p>证明：对$\phi(x,y...,z)$，定义归约$\phi \land x&#x3D;c _1 \land y&#x3D; c _2\land...\land z &#x3D; c _n$。由于$\phi$没有多项式算法，所以$c _1,...,c _n$是满足合取范式的猜测。</p>
<blockquote>
<p>详细证明：集合并公式的拆分。<br>不能通过重复实验来提高概率，因为每次随机算法$\mathbb{A}$生成的都是随机的公式。</p>
</blockquote>
<h2 id="oplus-textbf-P"><a href="#oplus-textbf-P" class="headerlink" title="$\oplus \textbf{P}$"></a>$\oplus \textbf{P}$</h2><p>$\oplus\textbf{P}$，对其中的类，非确定图灵机的所有路径中，只有奇数条路径接受它（或者0条则表拒绝）。$\textbf{UP} \subseteq \oplus \textbf{P}$。</p>
<blockquote>
<p>$\oplus \textbf{SAT}$是$\oplus \textbf{P}$完全的。</p>
</blockquote>
<p>将前面的$\textbf{USAT}$换成$\oplus \textbf{SAT}$也是对的，且能重复实验提高精度，这就是Toda Theorem。</p>
<p>Toda Theorem：任何$\textbf{PH}$问题都能通过调用多项式次$\sharp \textbf{P}$完成，即：</p>
<p>$$<br>\textbf{PH} \subseteq \textbf{P} ^{\sharp \textbf{P}}<br>$$</p>
]]></content>
      <categories>
        <category>CS7313: Computational Complexity</category>
      </categories>
      <tags>
        <tag>Math</tag>
        <tag>Theoretical Computer Science</tag>
      </tags>
  </entry>
  <entry>
    <title>MatrixTheory: 特殊矩阵与矩阵分解</title>
    <url>/2023/11/24/MatrixTheory5/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="特殊矩阵"><a href="#特殊矩阵" class="headerlink" title="特殊矩阵"></a>特殊矩阵</h1><p>一些复数域的特殊矩阵。</p>
<h2 id="正规矩阵"><a href="#正规矩阵" class="headerlink" title="正规矩阵"></a>正规矩阵</h2><p>复数域可酉对角化（不一定正交对角化）的矩阵。$Q^* &#x3D; Q ^{-1}$。</p>
<p>$$<br>Q^* A Q &#x3D; \Lambda<br>$$</p>
<p>定义：复数域上的方阵，满足$AA^*&#x3D;A^*A$</p>
<h3 id="性质"><a href="#性质" class="headerlink" title="性质"></a>性质</h3><ol>
<li>与正规矩阵酉相似的矩阵还是正规矩阵，即$B &#x3D; Q^*A Q$，其中$A$是正规矩阵，则$B$也是正规矩阵。</li>
<li>$A$为正规三角矩阵，则$A$一定为对角矩阵。</li>
<li>$A$为正规矩阵$\iff$$A$可酉对角化。</li>
</ol>
<blockquote>
<p>左到右：舒尔定理——复数域的方阵一定可酉相似于三角矩阵。而正规三角矩阵一定为对角矩阵。</p>
</blockquote>
<p>实数域的正规矩阵：$A ^T &#x3D; A$，$A ^T &#x3D; -A$，$A ^T &#x3D; A ^{-1}$<br>复数域的正规矩阵：$A^* &#x3D; A$，$A^* &#x3D; -A$，$A^* &#x3D; A ^{-1}$</p>
<h4 id="Hermite矩阵：-A-x3D-A"><a href="#Hermite矩阵：-A-x3D-A" class="headerlink" title="Hermite矩阵：$A^* &#x3D; A$"></a>Hermite矩阵：$A^* &#x3D; A$</h4><p>实对称矩阵的推广。性质：</p>
<ol>
<li>特征值都是实数，类似地，反Hermite矩阵的特征值都是纯虚数或0。</li>
<li>复二次型：$x^* A x$（一定是实数），相应地，可定义正定、标准化。</li>
</ol>
<blockquote>
<p>等积变换：正交变换、酉变换$x &#x3D; Qy$</p>
</blockquote>
<h4 id="酉矩阵：-AA-x3D-A-A-x3D-E"><a href="#酉矩阵：-AA-x3D-A-A-x3D-E" class="headerlink" title="酉矩阵：$AA^*&#x3D;A^*A&#x3D;E$"></a>酉矩阵：$AA^*&#x3D;A^*A&#x3D;E$</h4><ol>
<li>特征值模为1。</li>
</ol>
<h1 id="矩阵分解（20-30分）"><a href="#矩阵分解（20-30分）" class="headerlink" title="矩阵分解（20~30分）"></a>矩阵分解（20~30分）</h1><h2 id="满秩分解"><a href="#满秩分解" class="headerlink" title="满秩分解"></a>满秩分解</h2><p>化埃尔米特标准型求。四个子空间。</p>
<blockquote>
<p>可能能用于求矩阵的次方。如$A ^2 &#x3D; PQPQ&#x3D;P(QP)Q$。特别是对于$A$的秩为1的时候，$P$和$Q$都是向量，此时$QP$是一个数。</p>
</blockquote>
<h2 id="正交三角分解"><a href="#正交三角分解" class="headerlink" title="正交三角分解"></a>正交三角分解</h2><p>$A _{m\times n}$：必须列满秩才有正交三角分解。</p>
<p>$$<br>A _{m\times n}&#x3D; U _{m \times n}R _{n \times n}<br>$$</p>
<p>其中$U$的列向量单位正交，$R$是主对角元大于0的上三角。</p>
<blockquote>
<p>$U$即为$A$的列向量的斯密特单位正交化。$R _{ij}&#x3D;(\alpha _i, \gamma _j)(i \ne j),R _{ii}&#x3D;||\beta _i||$且上三角。或者$R &#x3D; U^* A$。</p>
<p>正交三角分解是唯一的。</p>
<p>应用：解线性方程组。</p>
</blockquote>
<h2 id="谱分解"><a href="#谱分解" class="headerlink" title="谱分解"></a>谱分解</h2><p>$A _{n \times n}$：可对角化的方阵才有谱分解。</p>
<p>$P&#x3D;(\alpha _1, \alpha _2,..., \alpha _n)$，其中$\alpha _i$为特征向量。</p>
<p>最基本的谱分解：<br>$$<br>A &#x3D; (\alpha _1,...,\alpha _n)eye(\lambda _1,...,\lambda _n)(\beta _1 ^T,...,\beta _n ^T)<br>$$</p>
<p>其中$P ^{-1}&#x3D;(\beta _1 ^T,...,\beta _n ^T)$（竖着的）</p>
<p>于是：</p>
<p>$$<br>A &#x3D; \lambda _1 \alpha _1 \beta _1 ^T +...+ \lambda _n \alpha _n \beta _n ^T<br>$$</p>
<p>记$S _1,..., S _m$为不同的特征值，则：</p>
<p>$$<br>A &#x3D; S _1 G _1+...+ S _m G _m<br>$$</p>
<p>性质：</p>
<ol>
<li>$\sum _{i &#x3D; 1} ^m G _i &#x3D; E$</li>
<li>$\beta _i ^T \alpha _j &#x3D; 1 (i &#x3D; j);0 (i \ne j)$。$(\alpha _i \beta _i ^T) ^2 &#x3D; \alpha _i \beta _i ^T$，$(\alpha _i \beta _i ^T)(\alpha _j \beta _j ^T) &#x3D; 0 (i \ne j)$。故$G _i ^2 &#x3D; G _i$，$G _i G _j &#x3D; 0 (i \ne j)$。</li>
<li>$r (G _i) &#x3D; n _i (\text{特征值的代数重数})$。</li>
<li>谱分解唯一。用性质1、2证明。</li>
</ol>
<p>正规矩阵的谱分解：不需要专门求逆矩阵。</p>
<p>$$<br>A ^2 &#x3D; \sum \lambda _i ^2 G _i<br>$$</p>
<h2 id="三角分解（cholesky分解）"><a href="#三角分解（cholesky分解）" class="headerlink" title="三角分解（cholesky分解）"></a>三角分解（cholesky分解）</h2><p>$A _{n\times n}$：秩为$r$的方阵，$A&#x3D;LU$，其中$L$为单位下三角，$U$是上三角。</p>
<ul>
<li>条件：1~r阶顺序主子式非零。</li>
</ul>
<p>本质是高斯消元：即行变换相当于左乘矩阵，让行变换后的矩阵是个上三角，此时左乘的矩阵一定是一个单位下三角。由于左乘的单位下三角是可逆的，故左乘其逆矩阵即可得到$A&#x3D;LU$。</p>
<ul>
<li>注：此时的行变换只能用上面的行去减下面的行。</li>
</ul>
<h3 id="实正定矩阵"><a href="#实正定矩阵" class="headerlink" title="实正定矩阵"></a>实正定矩阵</h3><p>首先，一定有三角分解，但是更特殊，称Cholesky分解：$A&#x3D;R ^T R$。$R$为主对角元大于零的上三角。</p>
<p>因为正定矩阵一定是对称的，所以可以正交相似对角化：$Q ^TA Q&#x3D;\Lambda$，可得$A &#x3D; Q\Lambda Q ^T&#x3D;Q\sqrt{\Lambda}\sqrt{\Lambda}Q ^T&#x3D;(\sqrt{\Lambda}Q ^T) ^T (\sqrt{\Lambda}Q ^T)&#x3D;B ^T B&#x3D;(\sqrt{\Lambda}Q ^T) ^T Q ^T Q(\sqrt{\Lambda}Q ^T)&#x3D;(Q\sqrt{\Lambda}Q ^T) ^T(Q\sqrt{\Lambda}Q ^T) &#x3D;C ^TC$，显然，$C$是一个正定矩阵。</p>
<p>而$B$是满秩的，所以一定有正交三角分解，即$B &#x3D; UR$，$U$为正交矩阵，于是$B ^TB&#x3D;(UR) ^T(UR)&#x3D;R ^TR$，即为所求。</p>
<blockquote>
<p>这样的分解是唯一的。</p>
</blockquote>
<p>求法：同时行列变换（仍然只能上减下、左减右），将$A$化为对角矩阵，分别左右乘逆矩阵，然后对角矩阵开根号，前两个当一个矩阵，后两个当一个矩阵即可。</p>
<h2 id="奇异值分解（极分解）"><a href="#奇异值分解（极分解）" class="headerlink" title="奇异值分解（极分解）"></a>奇异值分解（极分解）</h2><p>$A _{m\times n}$，$r(A) &#x3D; r$，$A _{m\times n}&#x3D; U _{m\times m} D _{m\times n} V^* _{n\times n}$，其中$U,V$是酉矩阵，$D$的所有元素中只有主对角线上前面的元素$\delta _i$非零，$\delta _i &gt; 0,0\le i\le r$，$\delta _i$称为$A$的奇异值。</p>
<ul>
<li>奇异值：$A A^*$的特征值开根号。</li>
<li>$U$：$A A^*$特征值的单位正交向量组成的矩阵。</li>
<li>$V$：$A^* A$特征值的单位正交向量组成的矩阵。</li>
</ul>
<h3 id="一些性质"><a href="#一些性质" class="headerlink" title="一些性质"></a>一些性质</h3><ul>
<li>$r(A) &#x3D; r (A^*) &#x3D; r (AA^*) &#x3D; r (A^* A)$：即证$N(A)&#x3D;N(AA^*)$。</li>
<li>$AA^*$与$A^* A$的非零特征值完全一样：$A^* A$特征值为$\lambda$，特征向量为$\alpha$，则$AA^*$的特征值为$\lambda$，特征向量为$A \alpha$。</li>
<li>$A^* A$和$A A^*$都是半正定矩阵，特征值开根号即为奇异值。</li>
<li>设$\lambda$为$A^* A$的一个非零特征值，$\alpha _1,...,\alpha _k$为其单位正交特征向量，则$\lambda$为$A A^*$特征向量，$A\alpha _1,...,A\alpha _k$为其正交特征向量，但不是单位的，要除以$\sqrt{\lambda}$（即奇异值）才是单位化的。</li>
</ul>
<h3 id="奇异值分解求法"><a href="#奇异值分解求法" class="headerlink" title="奇异值分解求法"></a>奇异值分解求法</h3><ol>
<li>求$A^*A$<strong>大于0</strong>的特征值$\lambda _i$，及其对应的<strong>单位正交</strong>的特征向量$\alpha _i$；求0特征值对应的<strong>单位正交</strong>的特征向量$\alpha _{i+1}$。</li>
<li>求$AA^*$<strong>大于0</strong>的特征值（和$A^*A$）相同，及其对应的<strong>单位正交</strong>的特征向量$\beta _i$（$\beta _i &#x3D; (A \alpha _i) &#x2F; \sqrt{\lambda _i}$）；求0特征值对应的<strong>单位正交</strong>的特征向量$(\beta _{i+1})$。（无特殊求法）</li>
</ol>
<p>$A(\alpha _1,..., \alpha _i, \alpha _{i+1}, ...,\alpha _n)&#x3D;(\beta _1,...,\beta _i,\beta _{i+1},..,\beta _n)\Lambda(\sqrt{\lambda _1},..,\sqrt{\lambda _i},0,...,0)$，故$A&#x3D;UDV^*$。其中$U&#x3D;(\beta _1,...,\beta _i,\beta _{i+1},..,\beta _n)$，$V&#x3D;(\alpha _1,..., \alpha _i, \alpha _{i+1}, ...,\alpha _n)$。</p>
<blockquote>
<p>先求$AA^*$还是$A^*A$看哪个阶数小。</p>
</blockquote>
<p>$\alpha _1,..,\alpha _i$是$A$的行空间中的向量，$\beta _1,..,\beta _i$是$A$的列空间中的向量；$\alpha _{i+1},...,\alpha _n$是$N(A)$的向量，$\beta _{i+1},..,\beta _n$是$N ^T(A)$中的向量。</p>
<h3 id="极分解"><a href="#极分解" class="headerlink" title="极分解"></a>极分解</h3><p>若$A$是方阵，则$A&#x3D;UDV^*&#x3D;(UDU^*)(UV^*)$，其中，左边是一个半正定矩阵，右边是酉矩阵，这就是极分解。或者$A&#x3D;(UV^*)(VDV^*)$，其中左边是酉矩阵，右边是半正定矩阵。若$A$可逆，则为半正定变为正定。</p>
<h3 id="A是正规矩阵"><a href="#A是正规矩阵" class="headerlink" title="A是正规矩阵"></a>A是正规矩阵</h3><p>原始式很重要。牢记，有些就是对原始式做变形。</p>
<h1 id="范数"><a href="#范数" class="headerlink" title="范数"></a>范数</h1><h2 id="向量范数"><a href="#向量范数" class="headerlink" title="向量范数"></a>向量范数</h2><p>长度的推广。</p>
<p>满足性质：</p>
<ol>
<li>$||\alpha|| &gt; 0$，正定性</li>
<li>$||k \cdot \alpha|| &#x3D; |k| ||\alpha||$，齐次性</li>
<li>$||\alpha + \beta|| \le ||\alpha|| + ||\beta||$，三角不等式</li>
</ol>
<p>非负函数。</p>
<blockquote>
<p>定义了范数的线性空间称<em>赋范线性空间</em>。</p>
</blockquote>
<ul>
<li>$\mathcal{l} _1$：$\sum |x _i|$</li>
<li>$\mathcal{l} _2$：$(\sum |x _i| ^2) ^{1&#x2F;2}$</li>
<li>$\mathcal{l} _p$：$(\sum |x _i| ^p) ^{1&#x2F;p}(p \ge 1)$</li>
<li>$\mathcal{l} _\infty$：$\max\{|x _i|,1\le i\le n\}$</li>
</ul>
<p>以上统称$p$范数。向量范数是等价的。等价：</p>
<p>$$<br>c _1,c _2, c _2||\alpha|| _b \le ||\alpha|| _a\le c _1||\alpha|| _b<br>$$</p>
<h2 id="矩阵范数"><a href="#矩阵范数" class="headerlink" title="矩阵范数"></a>矩阵范数</h2><p>除前面三条性质外，还有：</p>
<ul>
<li>$|||AB||| \le |||A|||\space |||B|||$</li>
</ul>
<p>F范数：</p>
<ul>
<li>$|||A||| _F&#x3D;(\sum |a _{ij}| ^2) ^{1&#x2F;2}&#x3D;\sqrt{\text{tr}(AA^*)}&#x3D;\sqrt{\text{tr}(A^*A)}$</li>
</ul>
<p>Schur不等式，$A &#x3D; (a _{ij}) _{n\times n},\lambda _1,...,\lambda _n$，则：</p>
<p>$$<br>\sum |\lambda _i| ^2 \le \sum |a _{ij}| ^2\iff\text{A是正规矩阵取等号}<br>$$</p>
<p>Schur定理：$Q^* A Q &#x3D; \text{diag}(\lambda _1, ...,\lambda _n)+\text{上三角}$，其中$Q^*&#x3D;Q ^{-1}$。</p>
<h2 id="相容"><a href="#相容" class="headerlink" title="相容"></a>相容</h2><p>若</p>
<p>$$<br>||A \alpha|| \le |||A|||\cdot ||\alpha||<br>$$<br>其中$A$是矩阵，$\alpha$是向量，则称这两个范数是相容的。</p>
<h3 id="算子范数"><a href="#算子范数" class="headerlink" title="算子范数"></a>算子范数</h3><p>由向量范数$||\cdot|| _a$构造相容的矩阵范数：</p>
<p>$$<br>|||A|||&#x3D;\max _{x \ne 0,x \in C ^n}\frac{||Ax|| _a}{||x|| _a}&#x3D;\max _{||x|| _a&#x3D;1}||Ax|| _a<br>$$</p>
<p>如$||\alpha|| _2$导出的矩阵范数：</p>
<p>$$<br>|||A||| _2&#x3D;&#x3D;\max _{||x|| _2&#x3D;1}||Ax|| _2<br>$$</p>
<p>$$<br>||Ax|| _2 ^2&#x3D;(Ax,Ax)&#x3D;x^* A^* Ax<br>$$</p>
<p>设$A^*A$的$n$个特征值为$\lambda _1\ge ...\ge \lambda _n \ge 0$，对应的单位正交特征向量为$\alpha _1,...,\alpha _n$。显然，这是$C ^n$的一组基，故：</p>
<p>$$<br>x &#x3D; x _1\alpha _1+...+x _n \alpha _n<br>$$</p>
<p>$$<br>x^*&#x3D; \overline{x} _1\alpha _1^*+...+\overline{x} _n \alpha _n^*<br>$$</p>
<p>$$<br>x^* A^* A x&#x3D;(\overline{x} _1\alpha _1^*+...+\overline{x} _n \alpha _n^*)A^* A(x _1\alpha _1+...+x _n \alpha _n)<br>$$</p>
<p>即：</p>
<p>$$<br>\begin{align*}<br>x^* A^* A x<br>&amp;&#x3D;(\overline{x} _1\alpha _1^*+...+\overline{x} _n \alpha _n^*)(x _1\lambda _1\alpha _1+...+x _n\lambda _n \alpha _n)\\<br>&amp;&#x3D;\lambda _1 |x _1| ^2+...+\lambda _n |x _n| ^2\\<br>&amp;\le \lambda _1(|x _1| ^2+...+ |x _n| ^2)<br>\end{align*}<br>$$</p>
<p>而可以取等，所以$x^* A^* Ax &#x3D; \lambda _1$，$|||A||| _2 &#x3D;\sqrt{\lambda _1}$。其中$\lambda _1$为最大特征值。<em>称谱范数</em>。</p>
<blockquote>
<p>作业：求$||\alpha|| _1$和$||\alpha|| _\infty$的导出的矩阵算子范数。（提示：矩阵行取模最大、列取模最大）。</p>
</blockquote>
<h2 id="向量、矩阵序列的收敛性"><a href="#向量、矩阵序列的收敛性" class="headerlink" title="向量、矩阵序列的收敛性"></a>向量、矩阵序列的收敛性</h2><p>向量序列$x ^{(k)}&#x3D;(x _1 ^{(k)},...,x _n ^{(k)}) ^T$，收敛：$\lim _{k \to \infty} x ^{(k)}&#x3D;x\iff \lim _{k\to \infty}||x ^{(k)}-x||&#x3D;0$。</p>
<p>同理，可定义矩阵序列及其收敛。</p>
<blockquote>
<p>矩阵或向量的收敛是每个元素都收敛到一个值。</p>
</blockquote>
<p>矩阵的幂序列：$A ^{(k)}&#x3D; A ^k$。化为Jordan标准型，Jordan块的$k$次方，二项展开i即可，不用记公式。</p>
<ul>
<li>$|\lambda| _i &lt; 1$，收敛到0；</li>
<li>$|\lambda| _i&#x3D;1$且$\lambda _i &#x3D; 1$且$m &#x3D; 1$，收敛；</li>
<li>其他发散</li>
</ul>
<p>故，矩阵的谱半径小于$1$时幂矩阵序列幂收敛到0。</p>
<h2 id="谱半径范数是最小的矩阵范数"><a href="#谱半径范数是最小的矩阵范数" class="headerlink" title="谱半径范数是最小的矩阵范数"></a>谱半径范数是最小的矩阵范数</h2><p>$\rho (A) \le |||A|||$</p>
<blockquote>
<p>证明：令$B &#x3D; A &#x2F; (|||A|||+\epsilon)$，则$|||B|||&#x3D;|||A|||&#x2F;(|||A|||+\epsilon) &lt; 1$。</p>
</blockquote>
<h2 id="矩阵级数"><a href="#矩阵级数" class="headerlink" title="矩阵级数"></a>矩阵级数</h2><p>向量&#x2F;矩阵序列的项相加。$\sum _{k&#x3D;0} ^{\infty}A ^{(k)}$。收敛即$\sum a _{ij} ^{(k)}$都收敛。</p>
<h3 id="矩阵幂级数"><a href="#矩阵幂级数" class="headerlink" title="矩阵幂级数"></a>矩阵幂级数</h3><p>$\sum _{k&#x3D;0} ^\infty a _k A ^k$收敛性与$f(t)&#x3D;\sum _{k&#x3D;0} ^\infty a _k t ^k$有关。假设为Jordan块，则每一行的和就是$f(\lambda)$的泰勒级数。最终：</p>
<p>$$<br>\sum a _k A ^k &#x3D; P\text{diag}(f(J _1),...,f(J _s))p ^{-1}<br>$$</p>
<p>其中$\rho(A)&lt; r$，其中$r$为数项幂级数的收敛半径。</p>
<h3 id="矩阵函数"><a href="#矩阵函数" class="headerlink" title="矩阵函数"></a>矩阵函数</h3><p>$e ^A$，$\sin A$,$\cos A$：可泰勒展开化为矩阵多项式求。而矩阵多项式又可用哈密尔顿凯勒定理求：</p>
<ul>
<li>哈密尔顿凯勒定理求：$A ^n$可由$A ^{n-1},...,E$线性表示。即$f(A)&#x3D;\varPhi(\lambda)g+\text{余式}$。其中$g$是某个零化多项式，$\text{余式}$是比它低一次的多项式。只要待定系数求出余式的系数即可，带入的数是零化多项式的根（缺少约束就两边求导）。</li>
</ul>
]]></content>
      <categories>
        <category>MATH6005: Matrix Theory</category>
      </categories>
      <tags>
        <tag>Math</tag>
      </tags>
  </entry>
  <entry>
    <title>Computational Complexity: Randomized Computation</title>
    <url>/2023/11/13/RandomizedComputation/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="Probabilistic-Turing-Machine"><a href="#Probabilistic-Turing-Machine" class="headerlink" title="Probabilistic Turing Machine"></a>Probabilistic Turing Machine</h1><p>$\textbf{PTM}$是$\textbf{NDTM}$的升级，其以确定的概率（一般均为0.5）选择$\delta _0$和$\delta _1$。故概率图灵机的输出$\mathbb{P}(x)$是一个随机分布（变量）。$<br>\text{Pr}[\mathbb{P}(x)&#x3D;y]$表示给定输入$x$，该概率图灵机输出$y$的概率。</p>
<p>一个函数$\phi$是概率可计算的（能被概率图灵机$\mathbb{P}$计算），若：</p>
<p>$$<br>\phi(x)&#x3D;<br>\begin{cases}<br>    y,\text{if}\space \text{Pr}[\mathbb{P}(x)&#x3D;y] &gt; 1&#x2F;2 \\<br>    \uparrow, \text{if no such y exists}<br>\end{cases}<br>$$</p>
<p>一个语言$L$能被$\textbf{PTM}$判定，当且仅当：</p>
<p>$$<br>\text{Pr}[\mathbb{P}(x)&#x3D;L(x)] &gt; 1&#x2F;2<br>$$</p>
<blockquote>
<p>概率图灵机可计算的函数就是可计算函数。</p>
</blockquote>
<h2 id="Average-Case-Time-Complexity"><a href="#Average-Case-Time-Complexity" class="headerlink" title="Average Case Time Complexity"></a>Average Case Time Complexity</h2><p>概率图灵机可以定义平均时间复杂性。</p>
<p>With bounded error:</p>
<p>$$<br>\phi(x)&#x3D;<br>\begin{cases}<br>    y,\text{if}\space \text{Pr}[\mathbb{P}(x)&#x3D;y] \ge 1&#x2F;2 + \epsilon \\<br>    \uparrow, \text{if no such y exists}<br>\end{cases}<br>$$</p>
<blockquote>
<p>带bounded error是为了避免出现任何问题都能平均两步解决的错误结论。</p>
</blockquote>
<h2 id="Polynomial-Identity-Testing"><a href="#Polynomial-Identity-Testing" class="headerlink" title="Polynomial Identity Testing"></a>Polynomial Identity Testing</h2><p>著名随机算法——等价性测试：测试两个电路是否是等价的。</p>
<blockquote>
<p>在有限域中随机取值，直接输入该电路。</p>
</blockquote>
<h1 id="textbf-PP"><a href="#textbf-PP" class="headerlink" title="$\textbf{PP}$"></a>$\textbf{PP}$</h1><p>若$\forall x \in \{0,1\} ^*$，$\text{Pr}[\mathbb{P}(x)&#x3D;L(x)] &gt; 1&#x2F;2$，且对任意的选择，$\mathbb{P}$在$T(|x|)$步内停机，则$\mathbb{P}$在$T(n)$时间内判定$L$。（$T$为最坏时间复杂性）</p>
<p>上面的定义即为$\textbf{PP}$类，其等价定义为：有个多项式时间的普通图灵机$\mathbb{M}$，使得：</p>
<p>$$<br>\text{Pr} _{r \in \{0,1\} ^{p(|x|)}}[\mathbb{M}(x,r)&#x3D;L(x)] &gt; 1&#x2F;2<br>$$</p>
<p>其中$r$为一个随机串。</p>
<p>$$<br>\textbf{NP},\text{co}\textbf{NP} \subseteq \textbf{PP} \subseteq \textbf{PSPACE}<br>$$</p>
<h2 id="PP完全"><a href="#PP完全" class="headerlink" title="PP完全"></a>PP完全</h2><p>都是$\textbf{SAT}$的变形。</p>
<p>$\phi \in \text{MajSAT}$如果有超过一半的真值指派，使得$\phi&#x3D;1$。</p>
<blockquote>
<p>C-L归约，$\delta$的选择转变为一个个真值指派。</p>
</blockquote>
<p>$\textbf{PP}$是封闭的，所以对$A \in \textbf{PP}$，$B \in \textbf{PP}$，有$A\cap B \in \textbf{PP}$，$A \cup B \in \textbf{PP}$。</p>
<h1 id="textbf-BPP"><a href="#textbf-BPP" class="headerlink" title="$\textbf{BPP}$"></a>$\textbf{BPP}$</h1><p>B：Bounded-Error</p>
<p>接受标准：With bounded-error，即要严格大于$1&#x2F;2$。时间复杂度：最坏时间复杂度与平均时间复杂度都行（马尔可夫不等式），两者定义出来的是等价的。$\textbf{BPP}$一般用的标准是$2&#x2F;3$。</p>
<p>$$<br>\textbf{P} \subseteq \textbf{BPP} \subseteq \textbf{PP}<br>$$</p>
<p>$$<br>\textbf{BPP}&#x3D; \textbf{coBPP}<br>$$</p>
<blockquote>
<p>$\textbf{BPP}$是很鲁棒的，出错概率大和出错概率小的$\textbf{BPP}$定义的是同一个类。（出错概率高，则多运行几次，取出现多的为最终值）。</p>
<p>错误压缩定理：可以把错误压缩到指数的倒数。</p>
</blockquote>
<h2 id="带神谕的BPP"><a href="#带神谕的BPP" class="headerlink" title="带神谕的BPP"></a>带神谕的BPP</h2><p>$$<br>\textbf{BPP} ^{\textbf{BPP}} &#x3D; \textbf{BPP}<br>$$</p>
<blockquote>
<p>错误压缩，让子程序的调用几乎不出错。</p>
</blockquote>
<h2 id="textbf-BPP-完全"><a href="#textbf-BPP-完全" class="headerlink" title="$\textbf{BPP}$完全"></a>$\textbf{BPP}$完全</h2><p>不存在。如果$\textbf{BPP}&#x3D;\textbf{P}$，则存在。</p>
<p>Adleman Theorem：</p>
<p>$$<br>\textbf{BPP} \subseteq \textbf{P} _{\text{poly}}<br>$$</p>
<p>定理：</p>
<p>$$<br>\textbf{BPP} \subseteq \Sigma _2 ^p \cap \Pi _2 ^p<br>$$</p>
<blockquote>
<p>用错误压缩定理证明。</p>
</blockquote>
<h1 id="textbf-ZPP"><a href="#textbf-ZPP" class="headerlink" title="$\textbf{ZPP}$"></a>$\textbf{ZPP}$</h1><p>平均运行时间为P且不出错的概率图灵机：</p>
<p>$$<br>\textbf{ZPP}&#x3D;\bigcup _{c \in \text{N}}\textbf{ZTIME}(n ^c)<br>$$</p>
<blockquote>
<p>如一些排序算法</p>
</blockquote>
<p>$L \in \textbf{ZPP}$等价于存在最坏时间为$\textbf{P}$的概率图灵机，其在多项式时间内输出$\{0,1,?\}$，其中$?$表示算不出来，要求算不出来的概率小于等于1&#x2F;3。</p>
<blockquote>
<p>单向出错算法：回答是一定对，回答否可能错。</p>
</blockquote>
<p>多项式时间的单向出错算法：<br>$$<br>\textbf{RP}&#x3D;\bigcup _{c \in \text{N}} \textbf{RTIME}(n ^c)<br>$$</p>
<p>其与$\textbf{ZPP}$存在关系：</p>
<p>$$<br>\textbf{ZPP}&#x3D;\textbf{RP}\cap \textbf{coRP}<br>$$</p>
<blockquote>
<p>左到右：若让$?$是始终回答NO，则ZPP就是RP；若始终回答YES，则ZPP就是coRP；<br>右到左：RP回答YES一定是对的，coRP回答NO一定是对的，因此交叉运行两个算法即可。</p>
</blockquote>
<p>RP也有错误压缩定理。类似地，ZPP也有错误压缩定理。</p>
]]></content>
      <categories>
        <category>CS7313: Computational Complexity</category>
      </categories>
      <tags>
        <tag>Math</tag>
        <tag>Theoretical Computer Science</tag>
      </tags>
  </entry>
  <entry>
    <title>Graphs of  Different Types</title>
    <url>/2023/11/07/DifferentTypesofGraph/</url>
    <content><![CDATA[<span id="more"></span>


<h1 id="同配图-amp-异配图"><a href="#同配图-amp-异配图" class="headerlink" title="同配图 &amp; 异配图"></a>同配图 &amp; 异配图</h1><ul>
<li>同配图（Homophilic&#x2F;Homophilous Graph）：具有相似特征或相同标签的结点趋向于连接在一起，即相连的结点一般属于同一个类别；</li>
<li>异配图（Heterophilic&#x2F;Heterophilous Graph）：相连的结点的特征或标签不具有相似性。</li>
</ul>
<p><img src="/2023/11/07/DifferentTypesofGraph/1.png" alt="1"></p>
<center style="font-size:12px; font-weight:bold">Fig. 1. Homophilic Graph and Heterophilic Graph</center>

<h1 id="同构图-amp-异构图"><a href="#同构图-amp-异构图" class="headerlink" title="同构图 &amp; 异构图"></a>同构图 &amp; 异构图</h1><ul>
<li>同构图（Homogeneous Graph）：结点和边都只有一个类型，即结点只有一类，结点间若存在关系，则该关系是确定的。</li>
<li>异构图（Heterogeneous Graph）：结点或边有多种类型，即结点的类型不定，结点间若存在关系，关系的类型也是不定的。</li>
</ul>
<p><img src="/2023/11/07/DifferentTypesofGraph/2.jpg" alt="2"></p>
<center style="font-size:12px; font-weight:bold">Fig. 2. Homogeneous Graph and Heterogeneous Graph</center>

<h1 id="时序图、时空图"><a href="#时序图、时空图" class="headerlink" title="时序图、时空图"></a>时序图、时空图</h1><ul>
<li>时序图（Temporal&#x2F;Dynamic graph）：结点和边的信息随时间变化的图。</li>
<li>时空图（Spatio-temporal graph）：在时序图的基础上，图的结构信息还反映了空间信息。</li>
</ul>
<p><img src="/2023/11/07/DifferentTypesofGraph/3.jpg" alt="3"></p>
<center style="font-size:12px; font-weight:bold">Fig. 3. Dynamic graph</center><br>

<p><img src="/2023/11/07/DifferentTypesofGraph/4.png" alt="4"></p>
<center style="font-size:12px; font-weight:bold">Fig. 4. Spatio-temporal graph</center>

<h1 id="二部图"><a href="#二部图" class="headerlink" title="二部图"></a>二部图</h1><ul>
<li>二部图（Bipartite graph）：节点可以分为两个子集的图，每一个边都连接属于不同子集的节点。</li>
</ul>
<p><img src="/2023/11/07/DifferentTypesofGraph/5.png" alt="5"></p>
<center style="font-size:12px; font-weight:bold">Fig. 5. Bipartite graph</center>]]></content>
      <categories>
        <category>GNN</category>
      </categories>
      <tags>
        <tag>GNN</tag>
      </tags>
  </entry>
  <entry>
    <title>Computational Complexity: Circuit Complexity</title>
    <url>/2023/10/31/CircuitComplexity/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="Boolean-Circuit-Model"><a href="#Boolean-Circuit-Model" class="headerlink" title="Boolean Circuit Model"></a>Boolean Circuit Model</h1><p>布尔电路（Boolean Circuit）是一种不一致模型，对于同一个算法，不同的输入大小对应不同的布尔电路。（每个结点都有输入，也有对应的布尔运算，与、或、非）</p>
<blockquote>
<p>单调电路：不包含非门的电路。</p>
</blockquote>
<h2 id="电路复杂性"><a href="#电路复杂性" class="headerlink" title="电路复杂性"></a>电路复杂性</h2><p>电路复杂性即电路$C$包含的<em>门</em>的个数$|C|$。</p>
<blockquote>
<p>布尔电路族：解决同一个算法的不同大小的输入的一群电路称布尔电路族（Boolean Circuit Family）$\{C _n\} _{n \in \text{N}}$，其复杂度$S(n)$定义为$\forall n, |C _n| \le S(n)$。</p>
<p>电路能算的图灵机不一定能算。</p>
</blockquote>
<h2 id="Nonuniform-Complexity-Class"><a href="#Nonuniform-Complexity-Class" class="headerlink" title="Nonuniform Complexity Class"></a>Nonuniform Complexity Class</h2><p>电路族判定问题：如果存在一个$S(n)$大小的电路族$\{C _n\} _{n\in \text{N}}$，使得对任意$x \in \{0,1\} ^n$，$x \in L$当且仅当$C _n(x)&#x3D;1$，则称该电路族判定了问题$L$，也即问题$L$是在$\textbf{SIZE}(S(n))$中的。</p>
<blockquote>
<p>对非一致模型，$\textbf{SIZE}(cS(n)) \ne \textbf{SIZE}(S(n)),c &gt; 2$。</p>
</blockquote>
<h2 id="香农定理"><a href="#香农定理" class="headerlink" title="香农定理"></a>香农定理</h2><p>大多数$n$个输入的布尔函数，其电路复杂性一般大于$\frac{2 ^n}{n}-O(\frac{2 ^n}{n})$。证明的说明：</p>
<ol>
<li>$S$为电路门的个数。$3 ^S$表门有与、或、非三种，$2S$表每个门有0或1两种输入，$S+n+2$表不同的输入来源（直接输入、别的门的输出），故大小为$S$的电路个数约为$(S+n+2) ^{2S} 3 ^S$；</li>
<li>$(S-1)!$表不同排列但功能相同的电路；</li>
<li>$2 ^{2 ^n}$表具有$n$个输入的布尔函数的个数。</li>
</ol>
<h2 id="Circuit-Hierachy-Theorem"><a href="#Circuit-Hierachy-Theorem" class="headerlink" title="Circuit Hierachy Theorem"></a>Circuit Hierachy Theorem</h2><p>如果对$\epsilon &gt; 0$，$n &lt; (2+\epsilon)S(n) &lt; S&#39;(n) &lt;&lt; 2 ^n &#x2F;n$，则$\textbf{SIZE}(S(n)) \subsetneq \textbf{SIZE}(S&#39;(n))$</p>
<h1 id="Uniform-Circuit"><a href="#Uniform-Circuit" class="headerlink" title="Uniform Circuit"></a>Uniform Circuit</h1><p>一致电路族：可在对数空间内由$1 ^n$生成$C _n$。</p>
<blockquote>
<p>一致电路族能判定的问题就是$\text{P}$。</p>
</blockquote>
<p>右往左：Cook-Levin归约的用条带表示的格局图可视为一个电路。</p>
<h2 id="Circuit-Satisfability"><a href="#Circuit-Satisfability" class="headerlink" title="Circuit Satisfability"></a>Circuit Satisfability</h2><p>电路可满足问题$\textbf{CKT}-\textbf{SAT}$：一个代表输入大小为$n$的电路$C$的字符串$s$，若$\exists u \in \{0,1\} ^n$，使得$C(u)&#x3D;1$，则该字符串是在$\textbf{CKT}-\textbf{SAT}$里面的。</p>
<blockquote>
<p>$\textbf{CKT}-\textbf{SAT}$是$\text{NP}$完全的。</p>
<p>引申开来，任何NP问题都可以对数空间归约到SAT，因为CKT-SAT可以对数空间归约到SAT。</p>
</blockquote>
<h1 id="P-x2F-poly"><a href="#P-x2F-poly" class="headerlink" title="P &#x2F;poly"></a>P &#x2F;poly</h1><p>带advice的图灵机：</p>
<ul>
<li>$\text{P} _{&#x2F;\text{poly}}$</li>
<li>$\text{NP} _{&#x2F;\text{poly}}$</li>
<li>...</li>
</ul>
<p>其中advice即$\alpha _n \in \{0,1\} ^{a(n)}$。</p>
<ul>
<li>对于$L \in \text{P} _{&#x2F;\text{poly}}$，$x \in L \iff \mathbb{M}(x,\alpha(n))&#x3D;1$，且判定时间为多项式时间，$a(n)$亦为多项式。</li>
<li>对于$L \in \text{NP} _{&#x2F;\text{poly}}$，$x \in L \iff \mathbb{N}(x,\alpha(n))&#x3D;1$，且判定时间为多项式时间，$a(n)$亦为多项式。</li>
</ul>
<p>定理：<br>$$<br>\text{P} _{&#x2F;\text{poly}}&#x3D;\bigcup _c \textbf{SIZE}(cn ^c)<br>$$</p>
<blockquote>
<p>证明：</p>
<p>右到左：用对$C _n$的描述作为advice，由于$C _n$是多项式的，对它的描述也是多项式的；</p>
<p>左到右：将格局图的计算过程翻译成电路图，并把advice烧进电路。</p>
</blockquote>
<h1 id="Karp-Lipton-Theorem"><a href="#Karp-Lipton-Theorem" class="headerlink" title="Karp-Lipton Theorem"></a>Karp-Lipton Theorem</h1><p>显然$\textbf{P} \subseteq \textbf{P} _{&#x2F;\text{poly}}$。</p>
<p>定理：若$\textbf{NP} \subseteq \textbf{P} _{&#x2F;\text{poly}}$，则$\textbf{PH}&#x3D; \Sigma _2 ^p$</p>
<p>只需要证明$\Pi _2 ^p\subseteq \Sigma _2 ^p$，即证明$\Pi _2 ^p-\text{SAT}\subseteq \Sigma _2 ^p-\text{SAT}$。</p>
<h1 id="NC-and-AC"><a href="#NC-and-AC" class="headerlink" title="NC and AC"></a>NC and AC</h1><p>电路模型研究并行运算--同一层的门可以并行运算。</p>
<ul>
<li><p>有效的并行算法（Efficient Parallel Algorithm）：用多项式个处理器可以加速到对数的多项式时间的算法。</p>
</li>
<li><p>$\text{NC} ^d$：可被大小为$p(n)$，深度为$\log ^d (n)$的一致电路族$\{C _n\} _{n \in N}$判定的问题类。</p>
</li>
</ul>
<p>$$<br>\textbf{NC} &#x3D; \bigcup _{d \in \text{N}} \textbf{NC} ^d<br>$$</p>
<blockquote>
<p>$\textbf{NC} \in \textbf{P}$</p>
</blockquote>
<p>$\textbf{AC}$是$\textbf{NC}$的扩展，它可以接受无数个输入，但事实上：</p>
<p>$$<br>\textbf{AC}&#x3D;\textbf{NC}<br>$$</p>
<p>可达性就是布尔矩阵连乘（$A ^n$），故$\text{Reachability} \in \textbf{NC} ^2$。</p>
<h1 id="textbf-P-完全"><a href="#textbf-P-完全" class="headerlink" title="$\textbf{P}$完全"></a>$\textbf{P}$完全</h1><p>对数空间归约保持<em>高效可并行性</em>，即若$L \le _L L&#39; \in \textbf{AC}$，则$L \in \textbf{AC}$。</p>
<ul>
<li>对数空间归约时，每一位的输出都可并行计算；</li>
<li>格局图矩阵（多项式大小）的每一位都可并行计算，可在对数空间输出；</li>
<li>$\text{Reachability} \in \textbf{NC} ^2$。</li>
</ul>
<h2 id="textbf-P-完全-1"><a href="#textbf-P-完全-1" class="headerlink" title="$\textbf{P}$完全"></a>$\textbf{P}$完全</h2><p>$L&#39; \in \textbf{PC}$，如果$\forall L \in \textbf{P}$，$L \le _L L&#39;$。</p>
<blockquote>
<p>$\text{Circuit-Eval}$ 是$\textbf{PC}$的。单调电路也是$\textbf{PC}$的。</p>
</blockquote>
<p>$$<br>\textbf{NC} ^1 \subseteq \textbf{L} \subseteq \textbf{NL} \subseteq \textbf{NC} ^2...\textbf{NC} ^i \subseteq \textbf{P}<br>$$</p>
<ul>
<li>$\textbf{NL} \in \textbf{NC} ^2$，因为$\text{Reachability} \in \textbf{NC} ^2$。</li>
</ul>
<h1 id="Hastad-Switching-Lemma"><a href="#Hastad-Switching-Lemma" class="headerlink" title="Hastad Switching Lemma"></a>Hastad Switching Lemma</h1><p>$$<br>\textbf{NC} ^0 \subsetneq \textbf{AC} ^0 \subsetneq \textbf{NC} ^1<br>$$</p>
<blockquote>
<p>有些函数无法用常数高度的电路计算。即常数高度的电路严格弱于对数高度的电路。</p>
</blockquote>
<p>该引理专门用于研究电路复杂性的下界。</p>
<p>Minterm：$x _1,...,x _n$组成的集合，可取非，可$\land$。</p>
<p>$f$的极小项：$f _\rho (x _1,..., x _n)$恒等于1且不存在$\text{sub}-\rho$。</p>
<blockquote>
<p>极小项就是合取项。</p>
</blockquote>
]]></content>
      <categories>
        <category>CS7313: Computational Complexity</category>
      </categories>
      <tags>
        <tag>Math</tag>
        <tag>Theoretical Computer Science</tag>
      </tags>
  </entry>
  <entry>
    <title>MatrixTheory: 特征值、特征向量</title>
    <url>/2023/10/25/MatrixTheory4/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="不变子空间"><a href="#不变子空间" class="headerlink" title="不变子空间"></a>不变子空间</h1><p>$V: \mathbb{F}上$线性空间，$T \in L(V)$，$U$是$V$的子空间。若$\forall \alpha \in U$，$T(\alpha) \in U$，即$T(U) \subseteq U$，则称$U$是$V$的关于$T$的不变子空间。</p>
<blockquote>
<p>平凡子空间$\{0\}$和$V$是不变子空间。线性变换的零空间和像空间也是不变子空间。</p>
</blockquote>
<h2 id="特征值、特征向量"><a href="#特征值、特征向量" class="headerlink" title="特征值、特征向量"></a>特征值、特征向量</h2><p>若$V &#x3D; U _1 \oplus U _2 \oplus U _3$，其中$U _1...U _3$均为不变子空间，则以不变子空间的基为基向量的线性变换矩阵为分块对角矩阵。进一步地，若存在$\text{dim}V$个不变子空间且这些子空间的直和为$V$，则线性变换矩阵就是对角矩阵，对角线的值为特征值，不变子空间的基为特征向量。</p>
<p>$T(\alpha _i) &#x3D; \lambda _i \alpha _i$，$\lambda _i$为特征值，$\alpha _i$为特征向量，其中$\lambda \in \mathbb{F}$，$\alpha \not &#x3D;{0}$。</p>
<blockquote>
<p>这和$Ax &#x3D; \lambda x$是等价的，只不过矩阵的只能是有限维的。</p>
<p>不同特征值对应的特征向量线性无关。</p>
<p>$T\in L(V)$，$\mathbb{F}&#x3D;C$，$\text{dim}V&#x3D;n$，则$T$在<em>复数域</em>内一定有特征值。</p>
</blockquote>
<p>$T \in L(V)$，$U$是不变子空间</p>
<ol>
<li>$T | _U \in L(U)$，缩小算子$T$的定义域。</li>
<li>$T &#x2F; U: V&#x2F;U \to V&#x2F;U$，商空间到商空间的映射，只有在不变子空间下才合理。如$(T&#x2F;U)(\alpha + U) &#x3D; T(\alpha) + U$</li>
</ol>
<h2 id="简单的基下矩阵"><a href="#简单的基下矩阵" class="headerlink" title="简单的基下矩阵"></a>简单的基下矩阵</h2><p>对$T \in L(V)$，$\mathbb{F}&#x3D;C$，$\text{dim}V &#x3D; n$，一定存在一组基，使得基下的矩阵是上三角。</p>
<blockquote>
<p>证明：即证$T(\alpha _i) \in \text{span}[\alpha _1, ...,\alpha _i]$。要用到商空间和商空间的线性变换。</p>
</blockquote>
<p>等价地，复数域上的任意一个方阵一定相似于一个上三角。</p>
<h2 id="特征子空间"><a href="#特征子空间" class="headerlink" title="特征子空间"></a>特征子空间</h2><p>特征子空间$E(T, \lambda) &#x3D; \text{null}(T - \lambda I)$，内部元素为所有的特征向量+零向量。不同特征值的特征子空间的和是直和。</p>
<p>$$<br>E(T,\lambda _1) \oplus E(T, \lambda _2) \oplus...\oplus E(T, \lambda _m) \le V<br>$$</p>
<blockquote>
<p>取等当且仅当存在一组基使得基下的矩阵为对角矩阵；<br>也当且仅当$V$可以分成$n$个一维不变子空间的直和。</p>
</blockquote>
<h1 id="内积空间"><a href="#内积空间" class="headerlink" title="内积空间"></a>内积空间</h1><p>内积空间：定义了内积的线性空间。其中，内积满足：</p>
<ol>
<li>$(\alpha, \beta)&#x3D;(\beta, \alpha)$</li>
<li>$(\alpha+\beta,\gamma)&#x3D;(\alpha, \gamma)+(\beta,\gamma)$</li>
<li>$(k\cdot\alpha, \beta)&#x3D;k(\beta, \alpha)$</li>
<li>$(\alpha,\alpha)\ge 0,\text{iff}\space \alpha&#x3D;0\space \text{取等}。$</li>
</ol>
<p>对复数域，坐标相乘相加的内积要对第二项取共轭，即：</p>
<p>$$<br>(\alpha, \beta)&#x3D;\alpha \overline{\beta}<br>$$</p>
<blockquote>
<p>实数域的内积空间称<em>欧式空间</em>，复数域上的称<em>酉空间</em>。</p>
</blockquote>
<h2 id="内积的性质"><a href="#内积的性质" class="headerlink" title="内积的性质"></a>内积的性质</h2><ol>
<li>设$\beta \in V$，$\beta$固定。定义$T: V \to \mathbb{F}$，$\forall \alpha \in V$，$T(\alpha) &#x3D; (\alpha, \beta)$，$T \in L(V,\mathbb{F})$</li>
<li>$(0,\beta) &#x3D; (\beta,0)&#x3D;0$</li>
<li>$(\alpha, \beta + \gamma)&#x3D;(\alpha, \beta)+(\alpha,\gamma)$</li>
<li>$(\alpha, k\cdot\beta)&#x3D;\overline{k}(\alpha, \beta)$</li>
<li>$(\sum _{i&#x3D;1} ^m x _i \alpha _i,\sum _{j&#x3D;1} ^n \gamma _j \beta _j )&#x3D;\sum\sum x _i \overline{y _j}(\alpha _i, \beta _j)$</li>
</ol>
<h2 id="向量的长度"><a href="#向量的长度" class="headerlink" title="向量的长度"></a>向量的长度</h2><p>$$<br>||\alpha|| &#x3D; \sqrt{(\alpha, \alpha)}<br>$$</p>
<ol>
<li>$||k\alpha|| &#x3D; |k|\sqrt{(\alpha, \alpha)}$</li>
<li>$|(\alpha, \beta)|\le ||\alpha||||\beta||$（柯西不等式）。当且仅当两者线性相关等号成立。</li>
<li>$||\alpha + \beta||\le ||\alpha||+||\beta||$（三角不等式）。当且仅当两者共线等号成立。</li>
</ol>
<h1 id="正交向量组、标准正交向量组、标准正交基"><a href="#正交向量组、标准正交向量组、标准正交基" class="headerlink" title="正交向量组、标准正交向量组、标准正交基"></a>正交向量组、标准正交向量组、标准正交基</h1><p>正交向量组：向量间两两正交。正交向量组一定线性无关。</p>
<p>校准正交组：单位化的正交向量组。</p>
<p>标准正交基：可由一组线性无关的基经过斯密特正交化得到。</p>
<blockquote>
<p>斯密特正交化前后的基张成的空间是相同的空间。所以$\exists$标准正交基，使得基下的矩阵为上三角。</p>
<p>实数域的转置等价于复数域的共轭转置。</p>
<p>向量$\alpha$在标准正交基下的线性表示系数为$(\alpha, \gamma _i)$。</p>
</blockquote>
<h2 id="正交补空间"><a href="#正交补空间" class="headerlink" title="正交补空间"></a>正交补空间</h2><p>$V$内积空间，$U$子空间，定义：</p>
<p>$$<br>U ^{\perp} &#x3D; \{\alpha \in V: \forall \beta \in U, (\alpha,\beta)&#x3D;0\}<br>$$</p>
<p>$U ^{\perp}$是$V$的子空间，是$U$的正交不空间，且$V&#x3D;U \oplus U ^{\perp}$。</p>
<h3 id="度量矩阵"><a href="#度量矩阵" class="headerlink" title="度量矩阵"></a>度量矩阵</h3><p>$$<br>G &#x3D;<br>\begin{bmatrix}<br>    (\alpha _1, \alpha _1)...(\alpha _n, \alpha _1)\\<br>    ... &amp;\\\<br>    (\alpha _1, \alpha _n)...(\alpha _n, \alpha _n)<br>\end{bmatrix}<br>$$</p>
<p>则任意两个向量在指定基$\alpha _1,...,\alpha _n$下的内积为：</p>
<p>$$<br>(\alpha, \beta)&#x3D;\overline{y}Gx<br>$$</p>
<p>其中$y$是$\beta$在基下的坐标，$x$是$\alpha$在基下的坐标。</p>
<blockquote>
<p>实数域的$G$是个对称的正定矩阵。<br>复数域的$G$是个共轭对称的正定矩阵。（共轭转置：$G ^*&#x3D;G$）<br>正定一定对称。</p>
</blockquote>
<p>一个正定矩阵定义一个内积。不同基下的$G$矩阵合同。</p>
<h3 id="投影变换"><a href="#投影变换" class="headerlink" title="投影变换"></a>投影变换</h3><p>$V &#x3D; U \oplus U ^\perp,\forall \alpha \in V,\alpha &#x3D; \beta + \gamma,\beta \in U, \gamma \in U ^\perp$。定义$P _U: V \to V,\forall \alpha \in V, \alpha &#x3D; \beta + \gamma,P _U(\alpha)&#x3D;\beta$。$P _U$即为$V$中的向量$\alpha$到$U$的投影变换。投影变换有性质：</p>
<ol>
<li>$\text{null}P _U &#x3D; U ^\perp$</li>
<li>$\text{range}P _U &#x3D; U$</li>
<li>$P ^2 _U &#x3D; P _U$</li>
<li>设$\epsilon _1,...,\epsilon _k$为$U$的一组标准正交基，则$P _U (\alpha) &#x3D; (\alpha, \epsilon _1) \epsilon _1+...+(\alpha, \epsilon _k) \epsilon _k$</li>
</ol>
<h3 id="最佳近似向量"><a href="#最佳近似向量" class="headerlink" title="最佳近似向量"></a>最佳近似向量</h3><p>内积空间$V$，子空间$U$，$\beta \in V$，$\beta \notin U$，若有$\alpha \in U$，使得$\forall \gamma \in U$，$||(\beta - \alpha)|| \le ||\beta -\gamma||$，则称$\alpha$为$\beta$在$U$的最佳近似向量。</p>
<blockquote>
<p>即距离最小的。实际上$\alpha$就是$\beta$的正交投影向量。</p>
<p>用途：求矛盾方程的最佳近似解（又称最小二乘解）。如$Ax&#x3D;b$没有解，则$A ^TAx&#x3D;A^Tb$求出来的$x$就是最佳近似解。因为没有解，所以$b$不在列空间$C$中，列空间的正交补空间为$N(A ^T)$，设$A x _0$为最佳近似向量，则b在正交补空间的投影可表示为$b - A x _0$，正交补空间为$N(A ^T)$，所以有$A ^T(b - Ax _0)&#x3D;0$。</p>
</blockquote>
<h2 id="内积空间的线性变换"><a href="#内积空间的线性变换" class="headerlink" title="内积空间的线性变换"></a>内积空间的线性变换</h2><p>内积空间$V$，算子$T \in L(V)$。若$\forall \alpha,\beta \in V,(\alpha,\beta)&#x3D;(T(\alpha),T(\beta))$，则称该变换为等积变换。对应地，有：</p>
<ul>
<li>$||\alpha||&#x3D;||T(\alpha)||$，等长变换；</li>
<li>$||\alpha-\beta||&#x3D;||T(\alpha)-T(\beta)||$，等距变换；</li>
<li>$\epsilon _1,...,\epsilon _k$为$V$的标准正交基，$T(\epsilon _1),...,T(\epsilon _k)$也是标准正交基；</li>
<li>$T(\epsilon _1,..,\epsilon _n)&#x3D;(\epsilon _1,..,\epsilon _n)A _{n\times n},A ^* A &#x3D; A A ^* &#x3D;E$，这种变换称正交变换，即保持内积的变换。</li>
</ul>
<p>这几个都是等价的，也称正交变换。</p>
<blockquote>
<p>伴随变换：$(T(\alpha),\beta)&#x3D;(\alpha,S(\beta))$，其基下矩阵满足$A &#x3D; B ^*$。</p>
<p>自伴随$T&#x3D;S$，此时$A ^*&#x3D;A$</p>
</blockquote>
<h1 id="广义特征向量"><a href="#广义特征向量" class="headerlink" title="广义特征向量"></a>广义特征向量</h1><p>对$T\in L(V)$，$\lambda$为特征值，$(T -\lambda I) ^j (\alpha) &#x3D; 0$，$j$为正整数，$\alpha \ne 0$，则称$\alpha$为特征值$\lambda$的广义特征向量。$G(T,\lambda)$为广义特征子空间。</p>
<ol>
<li>$\{0\}&#x3D;\text{null}T ^0 \subseteq \text{null}T _1\subseteq ... \subseteq \text{null}T ^k$</li>
<li>若$\text{null}T ^{k+1} &#x3D;\text{null} ^k$，则$\text{null} ^{n+k+1}&#x3D;\text{null} ^{n+k}$</li>
<li>$\text{dim}V&#x3D;n$，则$\text{null}T ^n&#x3D;\text{null} T ^{n+1}$</li>
<li>$\text{dim}V&#x3D;n$，则$V &#x3D; \text{null} T ^n \oplus \text{range}T ^n$</li>
</ol>
<ul>
<li>$G(T,\lambda)&#x3D;\text{null}(T-\lambda I) ^{\text{dim}V}$，$T\in L(V)$，$\lambda _1,...,\lambda _m$是不同特征值，其对应的不同广义特征向量$\alpha _1,...,\alpha _m$线性无关。</li>
</ul>
<blockquote>
<p>证明：分别作用线性变换使得只有一项留下。如，定义$k$为使得$(T-\lambda _1 I) ^k (\alpha _1)\ne 0$的最大整数。</p>
</blockquote>
<h2 id="幂零变换"><a href="#幂零变换" class="headerlink" title="幂零变换"></a>幂零变换</h2><p>$N \in L(V)$，若$N ^k &#x3D; 0$，则称$N$为幂零变换，类似于幂零矩阵。对幂零变换，一定有$N ^{\text{dim}V}&#x3D;0$。</p>
<p>存在某个基，使得幂零变换在基下的上三角矩阵的对角线元素全为0，即幂零变换的特征值都是0。（因为上三角矩阵的对角线元素就是特征值）</p>
<p>$T \in L(V)$，$\lambda _1,...,\lambda _m$为不同特征值，则：</p>
<ol>
<li>$V &#x3D; G(T,\lambda _1)\oplus...\oplus G(T,\lambda _m)$;</li>
<li>$G(T,\lambda _i)$是$T$的不变子空间；</li>
<li>$(T - \lambda _i I) | _{G(T,\lambda _i)}$是幂零变换。</li>
</ol>
<blockquote>
<p>对2：$\forall \alpha \in G(T, \lambda _i) &#x3D; \text{null}(T - \lambda _i I) ^k$，有$(T - \lambda _i I) ^k (\alpha) &#x3D; 0$，$(T - \lambda _i I) ^k (T(\alpha)) &#x3D; T((T - \lambda _i I) ^k(\alpha))&#x3D;0$<br>对3：广义特征子空间的定义；</p>
</blockquote>
<blockquote>
<p>不变子空间的基下矩阵为分块对角阵。<br>特征值数、特征子空间维数：几何重数<br>广义特征向量、广义特征子空间维数：代数重数</p>
</blockquote>
<p>$V$存在由广义特征向量构成的基，基下的矩阵为分块对角，块数为特征值数，块的维数该特征值下广义特征子空间的维数。进一步地，该分块对角矩阵可被优化为上三角矩阵。</p>
<p>$$<br>T | _{G(T,\lambda _i)} &#x3D; (T - \lambda _i I) | _{G (T, \lambda _i)} + \lambda _i I | _{G (T, \lambda _i)}<br>$$</p>
<p>即，拆分为一个幂零变换和恒等变换的和。</p>
<h1 id="Jordan标准型"><a href="#Jordan标准型" class="headerlink" title="Jordan标准型"></a>Jordan标准型</h1><p>基下矩阵的进一步简化：只有对角线有值，且次对角线全为1。</p>
<p>$$<br>\begin{bmatrix}<br>    \lambda _1, 1, ...,...\\<br>    0, \lambda _2, 1,...\\<br>    0, 0, \lambda _3, 1<br>\end{bmatrix}<br>$$</p>
<blockquote>
<p>同一个特征值的Jordan块的数目取决于线性无关的特征向量的个数。</p>
</blockquote>
<p>$V$存在由广义特征向量构成的基，基下的矩阵为Jordan标准型。</p>
<blockquote>
<p>先研究幂零变换的Jordan标准型，其他可由幂零变换+恒等变换得到。</p>
</blockquote>
<p>若$N \in L(V)$为幂零变换，则$V$中存在一组向量$\alpha _1, \alpha _2,...,\alpha _m$，及一组非负整数$k _1, k _2,...,$，使得：</p>
<p>$$<br>\begin{align*}<br>    &amp;N ^{k _1}(\alpha _1), ... , N(\alpha _1), \alpha _1 \\<br>    &amp;N ^{k _2}(\alpha _2), ... , N(\alpha _2), \alpha _2 \\<br>    &amp;...\\<br>    &amp;N ^{k _m}(\alpha _m), ... , N(\alpha _m), \alpha _m \\<br>    &amp;N ^{K _i + 1}(\alpha _i)&#x3D;0<br>\end{align*}<br>$$</p>
<p>化为$V$的一组基。其中$m$为Jordan块个数。</p>
<blockquote>
<p>同一个特征值的Jordan块个数等于几何重数。<br>相似于Jordan标准型的特征向量为广义特征向量。</p>
</blockquote>
<h2 id="Hamilton-Cayley-Them"><a href="#Hamilton-Cayley-Them" class="headerlink" title="Hamilton-Cayley Them"></a>Hamilton-Cayley Them</h2><p>线性变换的特征多项式：</p>
<p>$$<br>f _{\tau}(\lambda) &#x3D; (\lambda - \lambda _1) ^{n _1}...(\lambda - \lambda _m) ^{n _m}<br>$$</p>
<p>其中$\sum n _i &#x3D; n$，若将该线性变换带入，即将$\lambda$换为$T$，则$f _\tau (T)$是一个零变换。该多项式称为零化多项式。</p>
<p>由该定理，$A ^n$以及更高次的$A _{n+1}$都可由$A ^{n-1},...,E$线性表示。若$A$可逆，则逆也可以由$A ^{n-1},...,E$线性表示。</p>
<h3 id="最小多项式"><a href="#最小多项式" class="headerlink" title="最小多项式"></a>最小多项式</h3><p>$A$的零化多项式中，次数$n$最低的首一多项式（最高项系数为1），记作$m _A(\lambda)$。</p>
<ol>
<li>最小多项式是唯一的；</li>
<li>设$f(\lambda)$为$A$的任意零化多项式，则$m _A(\lambda) | f(\lambda)$，即$f(\lambda)$一定可以被$m _A (\lambda)$整除。特别的，的任意零化多项式，则$m _A(\lambda) | f _A(\lambda)$，因为特征多项式是零化多项式；</li>
<li>$P ^{-1}A P &#x3D; B$，则$m _A(\lambda)&#x3D;m _B(\lambda)$；</li>
<li>$A$的任一特征值为$m _A (\lambda)$（对任意零化多项式也成立）的根；</li>
<li>若$A$是分块对角，则每个块的最小多项式$m _{A _i}(\lambda) | f(\lambda)$，即$A$的最小多项式是每个块最小多项式的最小公倍式；</li>
<li>$A$可对角化当且仅当$m _A(\lambda)$无重根（复数域上）。</li>
</ol>
<blockquote>
<p>Jordan标准型的最小多项式就是最大次数的几个不同Jordan块的最小多项式的乘积。<br>$\lambda ^n &#x3D; 1$无重根。</p>
</blockquote>
<blockquote>
<p>Jordan型求法：a. 求特征值：若特征值均不同，则对角化；否则求几何重数。对三阶矩阵，一定能求；对四阶，若所有特征值相同，几何重数为2，则可能的Jordan块组合为2+2和3+1，用最小多项式检验哪个是对的即可。（更高阶的不需要掌握）</p>
</blockquote>
<h1 id="圆盘定理"><a href="#圆盘定理" class="headerlink" title="圆盘定理"></a>圆盘定理</h1><p>近似地计算特征值 &amp; 确定特征值的范围：</p>
<p>$$<br>|\lambda - a _{ii}| \le \sum\limits _{i \ne j} ^n |a _{ij}|<br>$$</p>
<h2 id="第一圆盘定理"><a href="#第一圆盘定理" class="headerlink" title="第一圆盘定理"></a>第一圆盘定理</h2><p>$A$的任意一个特征值一定会落在某个圆盘内。</p>
<h2 id="第二圆盘定理"><a href="#第二圆盘定理" class="headerlink" title="第二圆盘定理"></a>第二圆盘定理</h2><p>几个圆盘叠在一起，那么这个叠加后的区域就有几个特征值。由此推知，若圆盘互相分离，则$A$一定有$n$个不同的特征值，$A$一定可以对角化，这是对复数域，若是实数域，则还可以说明特征值都是实数（因为此时圆盘是关于$x$轴对称的，根都是成对出现的，除了$x$轴上的根）。</p>
<h1 id="谱-amp-谱半径"><a href="#谱-amp-谱半径" class="headerlink" title="谱 &amp; 谱半径"></a>谱 &amp; 谱半径</h1><p>谱：所有特征值构成的集合。</p>
<p>谱半径$\rho (A)$：特征值模的最大值。</p>
<p>$$<br>\begin{align*}<br>    \mu&#x3D;&amp;\max \{\sum \limits _{j&#x3D;1} ^n |a _{ij}|\} _{1\le i \le n} \\<br>    \mu &#39;&#x3D;&amp;\max \{\sum \limits _{i&#x3D;1} ^n |a _{ij}|\} _{1\le j \le n} \\<br>    \rho (A) &amp;\le \min \{\mu,\mu &#39;\}<br>\end{align*}<br>$$</p>
<p>事实上：$\rho (A) \le ||A||$。</p>
<blockquote>
<p>$||A||$是矩阵范数。</p>
</blockquote>
]]></content>
      <categories>
        <category>MATH6005: Matrix Theory</category>
      </categories>
      <tags>
        <tag>Math</tag>
      </tags>
  </entry>
  <entry>
    <title>Computational Complexity: Polynomial Hierarchy</title>
    <url>/2023/10/25/PolynomialHierarchy/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="Problem-Beyond-NP"><a href="#Problem-Beyond-NP" class="headerlink" title="Problem Beyond NP"></a>Problem Beyond NP</h1><p>$\textbf{MINIMAL}&#x3D;\{\phi | \phi \text{DNF} \land \forall \text{DNF}\psi. |\psi| &lt; |\phi| \to \exists u. !{(\psi(u) &lt;&#x3D;&gt;\phi(u))}\}$，即$\phi$是所有等价的析取范式中最短的。</p>
<h1 id="Meyer-Stockmeyer-39-s-Definition"><a href="#Meyer-Stockmeyer-39-s-Definition" class="headerlink" title="Meyer-Stockmeyer&#39;s Definition"></a>Meyer-Stockmeyer&#39;s Definition</h1><p>$$<br>\begin{align*}<br>    \Sigma _0 ^p &amp;&#x3D;\textbf{P}\\<br>    \Sigma _{i+1} ^p &amp;&#x3D;\textbf{NP} ^{\Sigma _{i} ^p}\\<br>    \Delta _{i+1} ^{p}&amp;&#x3D;\textbf{P} ^{\Sigma _{i} ^p} \\<br>    \Pi _i ^p &amp;&#x3D; \textbf{co-}\Sigma _{i} ^p<br>\end{align*}<br>$$</p>
<p>有：</p>
<ul>
<li>$\Sigma _{i} ^p \subseteq \Delta _{i+1} ^{p} \subseteq \Sigma _{i + 1} ^p$</li>
<li>$\Pi _i ^p \subseteq \Delta _{i+1} ^{p} \subseteq \Pi _{i + 1} ^p$</li>
</ul>
<p>其中第二项是对第一项的取补。</p>
<p>定义多项式谱系为：$\textbf{PH}&#x3D;\bigcup _{i \ge 0}\Sigma _i ^p&#x3D;\bigcup _{i \ge 0}\Pi _i ^p&#x3D;\bigcup _{i \ge 0}\Delta _{i+1} ^p$。</p>
<p>某个语言$L \in \Sigma _i ^p$或$L \in \Pi _i ^p$的证据：</p>
<ul>
<li>对$\Sigma _i ^p$，证据为$\exists u _1 \in \{0,1\} ^{q(|x|)}.\forall u _2 \in \{0,1\} ^{q (|x|)}...Q _i u _i \in \{0,1\} ^{q (|x|)}$</li>
<li>对$\Pi _i ^p$，证据为$\forall u _1 \in \{0,1\} ^{q(|x|)}.\exists u _2 \in \{0,1\} ^{q (|x|)}...Q _i u _i \in \{0,1\} ^{q (|x|)}$</li>
</ul>
<p>这两个是对偶问题，只需证第一个即可。</p>
<ul>
<li>从右到左：猜测一个$u _1$，对每个猜测，计算$\Pi _{i-1} ^P$，而于是所有的猜测就是$\textbf{NP} ^{\Pi _{i-1} ^P}&#x3D;\textbf{NP} ^{\Sigma _{i-1} ^P}&#x3D;\Sigma _{i} ^P$。</li>
<li>从左到右：Cook-Levin定理，转化为$\textbf{SAT}$，再把$\textbf{SAT}$转化为量词的交替。</li>
</ul>
<blockquote>
<p>$\Sigma _i \textbf{SAT}$是$\Sigma _i ^p$中最难的问题，即它是$\Sigma _i ^p$完全的。证明：将$\mathbb{M}(x,u _1,...,u _{i+1})$Cook-Levin归约到$\Sigma _i \textbf{SAT}$。</p>
</blockquote>
<p>因为$\Sigma _i \textbf{SAT} \subseteq \textbf{PSPACE}$，所以$\textbf{PH} \subseteq \textbf{PSPACE}$</p>
<h2 id="Alternating-Turing-Machine"><a href="#Alternating-Turing-Machine" class="headerlink" title="Alternating Turing Machine"></a>Alternating Turing Machine</h2><p>交替图灵机：非确定图灵机的推广，每个状态都被标注为$\{\exists,\forall,\text{halt}\}$之一。交替图灵机$\text{ATM}$接受$x$当且仅当交替图灵机的执行序列中存在一个<em>接受子树</em>。</p>
<blockquote>
<p>接受子树：所有叶子结点标记$\text{halt}$且均为$1$，若某个非叶子结点被标记为$\exists$则它只有一个孩子在接受子树中，若被标记为$\forall$则有两个孩子接受子树中。</p>
</blockquote>
<p>$\text{TQBF}$可用$\text{ATM}$在平方时间和线性空间判定。</p>
<ul>
<li>$\textbf{NSPACE}(S(n)) \subseteq \textbf{ATIME}(S ^2 (n))$：格局图、路径，$\exists$则猜测，$forall$则并行计算，共猜测$S$次，每次耗时$S$。</li>
<li>$\textbf{ATIME}(T(n)) \subseteq \textbf{SPACE}(T (n))$：DFS。</li>
<li>$\textbf{ASPACE}(S(n)) \subseteq \bigcup _ {c \ge 0}\textbf{TIME}(c ^{S(n)})$：格局图，枚举。</li>
<li>$\textbf{TIME}(T(n)) \subseteq \textbf{ASPACE}(\log T(n))$：Cook-Levin证明一的图从下往上，猜测$\exist$，并行地验证$\forall$。每条路径空间为对数空间。</li>
</ul>
<p><img src="/2023/10/25/PolynomialHierarchy/1.png" alt="1"></p>
<center style="font-size:12px;font-weight:bold">Fig. 1.</center>

<blockquote>
<p>无穷谱系猜测是比$\textbf{NP}&#x3D;\textbf{P}$更强的假设，它假设多项式谱系间存在严格的包含关系。</p>
</blockquote>
]]></content>
      <categories>
        <category>CS7313: Computational Complexity</category>
      </categories>
      <tags>
        <tag>Math</tag>
        <tag>Theoretical Computer Science</tag>
      </tags>
  </entry>
  <entry>
    <title>MatrixTheory: Linear Transformation Matrix</title>
    <url>/2023/10/18/MatrixTheory3/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="线性变换与矩阵"><a href="#线性变换与矩阵" class="headerlink" title="线性变换与矩阵"></a>线性变换与矩阵</h1><p>$T:U \to V$</p>
<p>零空间：$\text{null}\space T&#x3D;\{\alpha \in U:T(\alpha)&#x3D;0\}$</p>
<p>单射$\quad\text{iff}\quad\text{null}\space T &#x3D; {0}$。</p>
<p>像空间：$\text{range}\space T&#x3D;\{\beta \in V:\exists \alpha \in U,T(\alpha)&#x3D;\beta\}$</p>
<p>满射$\quad\text{iff}\quad\text{range}\space T &#x3D; V$。</p>
<p>维数公式：$\text{dim}(\text{range})+\text{dim}(\text{null})&#x3D;\text{dim}(U)$</p>
<blockquote>
<p>前提：有限维、线性变换。</p>
</blockquote>
<h2 id="线性变换矩阵"><a href="#线性变换矩阵" class="headerlink" title="线性变换矩阵"></a>线性变换矩阵</h2><p>对$\text{dim}U&#x3D;n,\text{dim}V&#x3D;m$，$T(\alpha _1,..., \alpha _n)&#x3D;(\beta _1, ...,\beta _m)C _{m\times n}$，其中$\alpha _1,...,\alpha _n$和$\beta _1,...,\beta _m$分别为$U$和$V$的基，称$C _{m\times n}$为线性变换$T$在给定基$\alpha _1,...,\alpha _n:\beta _1,...,\beta _m$下的矩阵。</p>
<blockquote>
<p>线性变换由一组基的变换唯一确定。</p>
<p>自身到自身的线性变换称为“算子”：$T \in L(U)$。此时，线性变换前后的基可以取一样的。</p>
<p>矩阵就是线性变换，线性变换就是矩阵·。</p>
</blockquote>
<h3 id="线性变换矩阵求零、像空间"><a href="#线性变换矩阵求零、像空间" class="headerlink" title="线性变换矩阵求零、像空间"></a>线性变换矩阵求零、像空间</h3><p>$T(\alpha) &#x3D; T((\alpha _1,...,\alpha _n)X)&#x3D; T(\alpha _1,...,\alpha _n)X&#x3D;(\beta _1,...,\beta _m)AX$</p>
<ul>
<li><p>求零空间<br>$T(\alpha)&#x3D;0\to \text{null}T\to N(A)$，此时得出的是能让$T(\alpha) &#x3D; 0$的$X$的基，记为$X&#39;$，再用$(\alpha _1,...,\alpha _n)X&#39;$即可得到零空间的基。</p>
</li>
<li><p>求像空间<br>此时要求的是像空间的基，而$AX$相当于对$A$进行列变换，因而$A$的秩就是像空间的维数，列变换得到列空间$C&#39;$，计算$(\beta _1,...,\beta _m)C&#39;$即可得到像空间的基。故$\text{range}T\to C(A)$</p>
</li>
</ul>
<blockquote>
<p>核心：什么样的基能够使得线性变化矩阵最简单。</p>
</blockquote>
<h3 id="线性变换与相似"><a href="#线性变换与相似" class="headerlink" title="线性变换与相似"></a>线性变换与相似</h3><p>同一个线性变换（算子）在不同基下的矩阵相似：</p>
<blockquote>
<p>对$T: U\to U$，$T (\alpha _1,...,\alpha _n) &#x3D; (\alpha _1,...,\alpha _n)A$；对$T: V \to V$，$T (\beta _1,...,\beta _m) &#x3D; (\beta _1,...,\beta _m)B$；对两个基，有过渡矩阵$P$，使得$(\beta _1,...,\beta _m)&#x3D;(\alpha _1,...,\alpha _n)P$，则：<br>$$<br>\begin{align*}<br>T (\beta _1,...,\beta _m)<br>&amp;&#x3D; (\beta _1,...,\beta _m)B \\<br>&amp;&#x3D; (\alpha _1,...,\alpha _n)PB \\<br>&amp;&#x3D; T((\alpha _1,...,\alpha _n)P) \\<br>&amp;&#x3D; (\alpha _1,...,\alpha _n)AP<br>\end{align*}<br>$$<br>故有：$PB&#x3D;AP$，若$P$可逆，则易知$A$和$B$是相似的。</p>
</blockquote>
<h2 id="逆变换"><a href="#逆变换" class="headerlink" title="逆变换"></a>逆变换</h2><p>若存在线性变换$T: U \to V$和$S: V\to U$，使得$ST &#x3D; I$，$TS&#x3D;I$，则称$S$为$T$的逆变换，其中$ST$和$TS$是之前定义的线性变换的乘法运算，$I$为恒等变换。逆变换若存在则是唯一的。</p>
<p>线性变换$T$可逆的充要条件：$T$单射且满射。</p>
<h2 id="同构"><a href="#同构" class="headerlink" title="同构"></a>同构</h2><p>若存在一个$T \in L(U,V)$，且$T$可逆，则称线性空间$U$和$V$是同构的。</p>
<p>两线性空间$U$、$V$同构的充要条件：$\text{dim}U&#x3D;\text{dim}V$。</p>
<ul>
<li>找同构映射：</li>
</ul>
<ol>
<li>定义一个映射$T$；</li>
<li>证明映射$T$是线性变换；</li>
<li>证明映射$T$单射且满射。</li>
</ol>
<ul>
<li>$V$为$\mathbb{F}$上的$n$维线性空间，则$V \simeq \mathbb{F} ^n$</li>
<li>$L(U,V)\simeq \mathbb{F} ^{n \times n}$</li>
</ul>
<p>线性变换就是矩阵。两个线性空间的线性变换空间的维数是者两个线性空间维数的乘积：</p>
<p>$$<br>\text{dim}L(U,V)&#x3D;\text{dim}U\cdot \text{dim}V<br>$$</p>
<h2 id="乘积空间"><a href="#乘积空间" class="headerlink" title="乘积空间"></a>乘积空间</h2><p>$V _1, V _2,..., V _k$为$\mathbb{F}$上的线性空间，则$V _1\times V _2\times ...\times V _k&#x3D;\{(\alpha _1,\alpha _2,...,\alpha _k):\alpha _i\in V _i \}$就称为$V _1, V _2,..., V _k$的乘积空间。乘积空间还是线性空间。</p>
<blockquote>
<p>$V _1, V _2,..., V _k$可以是元素类型不同的空间。乘积空间的加法、数乘仍是各元素在原空间的加法和数乘。<br>乘积空间的维数为各成员空间的维数和。</p>
</blockquote>
<h2 id="商空间"><a href="#商空间" class="headerlink" title="商空间"></a>商空间</h2><p>$V$是$\mathbb{F}$上的线性空间，$U$是$V$的子空间，$\alpha \in V$，定义$\alpha + U &#x3D; \{\alpha + \beta:\beta \in U\}$，则商空间为$V&#x2F;U&#x3D;{\alpha + U:\alpha \in V}$。</p>
<ul>
<li>商空间的加法：$(\alpha + U) + (\beta + U) &#x3D; (\alpha + \beta) + U$</li>
<li>商空间的数乘：$k\cdot (\beta + U) &#x3D; k\cdot \beta + U$</li>
</ul>
<p>下列三个公式等价：</p>
<ol>
<li>$\alpha - \beta \in U$</li>
<li>$\alpha + U &#x3D; \beta + \ U$</li>
<li>$(\alpha + U) \cap (\beta + U) \ne \emptyset$</li>
</ol>
<blockquote>
<p>平行线</p>
</blockquote>
<p>商空间也是线性空间，零向量是$\{\alpha + U,\alpha \in U\}$。</p>
<p>$\text{dim}(V&#x2F;U)&#x3D;n-k$，其中$\text{dim}V &#x3D; N$，$\text{dim}U&#x3D;k$</p>
<blockquote>
<p>证明：定义$T: V \to V &#x2F;U$，则对$\alpha \in V$，$T(\alpha) &#x3D; \alpha + U$（证明线性变换略）。</p>
<p>易知这个线性变换是满射的，即$\text{dim}\text{range}T&#x3D;\text{dim}(V&#x2F;U)$。而零空间，$T(\alpha) &#x3D; 0$，满足该条件的$\alpha$即$U$中的所有元素，故$\text{dim}\text{null}T &#x3D; \text{dim}U$。又$\text{dim}V &#x3D; \text{dim}\text{range}T + \text{dim}\text{null}T$，故$\text{dim}(V&#x2F;U) &#x3D; \text{dim}V - \text{dim}U$。</p>
</blockquote>
]]></content>
      <categories>
        <category>MATH6005: Matrix Theory</category>
      </categories>
      <tags>
        <tag>Math</tag>
      </tags>
  </entry>
  <entry>
    <title>Computational Complexity NP-Completeness</title>
    <url>/2023/10/10/ComputationalComplexityNPComp/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="textbf-NP-Completeness"><a href="#textbf-NP-Completeness" class="headerlink" title="$\textbf{NP}-$Completeness"></a>$\textbf{NP}-$Completeness</h1><p>判定一个问题是否是NP问题，需要证据与验证，即：</p>
<blockquote>
<p>当存在一个多项式$p: \textbf{N}\to \textbf{N}$和一个$\text{P}-\text{time}$图灵机$\mathbb{M}$，使得对于语言$L$以及$x \in \{0,1\} ^*$：<br>$$<br>x \in L \space\text{iff}\space \exists u\in \{0,1\} ^{p(|x|)},\mathbb{M}(x,u)&#x3D;1<br>$$<br>则称$L$是一个$\textbf{NP}$问题，并称这个图灵机$\mathbb{M}$为一个验证器。</p>
</blockquote>
<p>上面的定理中，$u$就称为能够证明$L$是一个$\textbf{NP}$问题的“证据”。该定理说明，若某个问题是$\textbf{NP}$问题，则一定存在一个确定图灵机$\mathbb{M}$，它能通过不超过多项式长度的证据判定该问题在$\textbf{NP}$中。</p>
<blockquote>
<p>事实上，$u$就是非确定图灵机$\mathbb{N}$正确判断$x$的一条路径。因为要验证的是$\textbf{NP}$，所以要求路径长度（即$u$的长度）为多项式。</p>
</blockquote>
<h2 id="Karp-Reduction"><a href="#Karp-Reduction" class="headerlink" title="Karp Reduction"></a>Karp Reduction</h2><p>Karp Reduction即为多项式时间归约，其定义为：</p>
<blockquote>
<p>若存在一个多项式时间的可计算函数$r:\{0,1\} ^* \to \{0,1\} ^*$，使得对所有的$x \in \{0,1\} ^*$，<br>$$<br>x \in L \space \text{iff} \space r(x) \in L &#39;<br>$$<br>则称$L$可多项式归约到$L&#39;$，记为$L \le _K L&#39;$。</p>
</blockquote>
<p>多项式归约也具有传递性。</p>
<ul>
<li>如果$L \in \text{NP}$且$L \le _K L&#39;$，则称$L&#39;$是$\textbf{NP}$难的；</li>
<li>若$L&#39; \in \textbf{NP}$，则称$L &#39;$是$\textbf{NP}$完全的。</li>
</ul>
<p>若P &#x3D; NP，则EXP &#x3D; NEXP。底层相等，上层也相等。</p>
<h1 id="Propositional-Logic：命题逻辑"><a href="#Propositional-Logic：命题逻辑" class="headerlink" title="Propositional Logic：命题逻辑"></a>Propositional Logic：命题逻辑</h1><p>命题公式，即一个逻辑公式，是由真假值（0&#x2F;1）、命题变量和逻辑操作子通过归纳构造的。一个命题变量$x$是一个布尔量，也称作“字（Literal）”，其否定形式为$\overline{x}$。逻辑运算符包括$\lor$（或、析取），$\land$（与、合取）。若干个“字”的析取称<em>语句</em>，若干个“字”的合取称<em>单项式</em>。</p>
<ul>
<li>合取范式（Conjunctive Normal Form，CNF）：若干语句的合取，记作$\lor _{i}(\land _j v _{ij})$。</li>
<li>析取范式（Disjunctive Normal Form，DNF）：若干单向式的析取，记作$\land _{i}(\lor _j v _{ij})$。</li>
</ul>
<p>对于某一公式（合取或析取，记作$\phi$），若存在$v _{ij}$的某种真值指派，使得$\phi$为真，则称该公式是可满足的（Satisfiable）。对于所有可满足的合取范式组成的集合$\textbf{SAT}$，存在如下定理：</p>
<p>$$<br>\textbf{SAT} \in \textbf{NP}<br>$$</p>
<p>$k\textbf{SAT}$是指每个语句只含有$k$个字的合取范式。对于$k\textbf{SAT}$，有如下结论：</p>
<ol>
<li>$2\textbf{SAT}\in \textbf{P}$</li>
<li>$\textbf{SAT}\le _K 3\textbf{SAT}$</li>
</ol>
<p>结论2表明，$k$（$k \ge 3$）$\textbf{SAT}$可由$3\textbf{SAT}$表示。</p>
<p>布尔函数$f:\{0,1\} ^\ell \to \{0,1\}$将$\ell$长的$01$串映射为一个布尔值，它可以被表示为一个合取范式，且该合取范式的大小不会超过$\ell 2 ^\ell$。</p>
<h2 id="Cook-Levin-Theorem"><a href="#Cook-Levin-Theorem" class="headerlink" title="Cook-Levin Theorem"></a>Cook-Levin Theorem</h2><p>定理：$\textbf{SAT}$是$\textbf{NP}$完全的。该定理证明如下：</p>
<blockquote>
<p>由于$\textbf{SAT}\in \textbf{NP}$，故要证上述定理成立，只需要证明$\forall L \in \textbf{NP}$，$L \le _K \textbf{SAT}$，即证明$x \in L$当且仅当$\phi(x) \in \textbf{SAT}$，又$x \in L$等价于$\exists u \in \{0,1\} ^{ ^{p (|x|)}},\mathbb{M}(x,u)&#x3D;1$，所以即证明：<br>$$<br>\phi(x) \in \textbf{SAT}\space \text{iff}\space \exists u \in \{0,1\} ^{ ^{p (|x|)}},\mathbb{M}(x,u)&#x3D;1<br>$$<br>要证明上式，即证明<strong>图灵机的计算过程可以一个合取范式表示</strong>。</p>
<p>由于任何多带的图灵机都可以单带图灵机在多项式的代价下模拟，所以此处只考虑一条带子。则对于图灵机的整个计算过程，其产生的所有格局中的每一个格子、带头是否在该格子、当前的状态可用$1+\log \Gamma +\log Q$个比特编码，记其长度为$\ell$。对于格局$C _{i+1}$和其上一个格局$C _{i}$，$C _{i + 1}$的每一个格子的值只会受到其正上方、左上方、右上方三个$C _{i}$格子的影响。所以，对于$C _i \to C _{i+1}$每一个格子的转移函数可以表示为$f:\{0,1\} ^{3\ell}\to \{0,1\} ^\ell$，其等价于$f:\{0,1\} ^{4\ell} \to \{0,1\}$，转换为合取范式，其大小为$4\ell 2 ^{4 \ell}$，由于$\ell$为常量，所以前式为常数。前后格局所有格子的转换都可用上式表示，记该转移函数为$C$。终止格局数为多项式，格局的大小也是多项式，所以$x \to \phi(x)$的转换是多项式的。证毕。</p>
</blockquote>
<p>此外，计算$C _{ij}$的空间是对数的，只需要记录$i$和$j$即可。</p>
<h3 id="性质"><a href="#性质" class="headerlink" title="性质"></a>性质</h3><ol>
<li>库克-莱文归约是个<em>莱文归约</em>（Levin Reduction），即使得$\phi(x)$成立的一组真值指派$u$就是使得$\mathbb{M}(x,u)&#x3D;1$的证据$u$。</li>
<li>归约是对数空间的。</li>
</ol>
<p>判定问题有多项式时间算法，则相应的计算问题也有。</p>
<blockquote>
<p>判定（Decision）和计算（Search）问题在是否有高效算法上是等价的，即若我们能在多项式时间内判定某个问题，我们就能在多项式时间内计算其对应的计算问题。</p>
</blockquote>
<h1 id="Karp-Theorem"><a href="#Karp-Theorem" class="headerlink" title="Karp Theorem"></a>Karp Theorem</h1><p>Karp证明了21个NP完全问题，如$\textbf{THEOREM}$——能否在多项式的公式长度的行内证明某公式是NP完全的。</p>
<h2 id="Berman-Hartmanis-Conjecture"><a href="#Berman-Hartmanis-Conjecture" class="headerlink" title="Berman-Hartmanis Conjecture"></a>Berman-Hartmanis Conjecture</h2><p>猜测（Conjecture）：任何的NP完全问题都是多项式同构的，即只有一个NP完全问题。若该猜测为真，则$\textbf{P} \neq \textbf{NP}$。</p>
<blockquote>
<p>多项式同构：$A \le _K ^1 B$表存在从$A$到$B$的单射Karp归约，若$A \le _K ^1 B \le _K ^1 A$，则称$A$和$B$是多项式同构的，即$A \simeq _P B$</p>
</blockquote>
<p>对于集合$S \subseteq \{0,1\} ^* $，若$S$内部长度不超过$n$的串有指数个，即$|S ^{\le n}|&#x3D; 2 ^{n ^{O(1)}}$，则称该集合是稠密的；反之若只有多项式个，即$|S ^{\le n}|&#x3D; {n ^{O(1)}}$，则称该集合是稀疏的。$\textbf{SAT}$是稠密的，而$\textbf{P}$类问题是稀疏的，稠密集合不可能和稀疏集合同构，因为大的映射小的或小的映射大的不可能是单射的，所以$\textbf{SAT} \ne \textbf{P}$。</p>
<h1 id="Ladner-Theorem"><a href="#Ladner-Theorem" class="headerlink" title="Ladner Theorem"></a>Ladner Theorem</h1><p>定理：若$\textbf{NP} \neq \textbf{P}$，则存在介于$\textbf{P}$和$\textbf{NP}$之间的语言。</p>
<p>证明的基本思路：从$\textbf{SAT}$中排除一些元素，但不至于退化为$2\textbf{SAT}$，则新的语言就介于$\textbf{NP}$和$\textbf{P}$之间。记该新语言为$\textbf{SAT} _H$。</p>
<p>$$<br>\textbf{SAT} _H(x)&#x3D;<br>\begin{cases}<br>    1, &amp;x \in \textbf{SAT} _H\\<br>    0, &amp;x \notin \textbf{SAT} _H<br>\end{cases}<br>$$</p>
<p>$$<br>H(n)&#x3D;<br>\begin{cases}<br>    i, &amp;\text{对}|x|\le \log n\text{可在}i |x| ^i\text{时间内判定}\textbf{SAT} _H(x)\text{的最小图灵机下标}\\<br>    \log \log n, &amp;\text{otherwise}<br>\end{cases}<br>$$</p>
<p>$H(n)$是单调不减的，其时间复杂度为：</p>
<p>$$<br>T(n)&#x3D;(\log\log n)2 ^{\log n}(c\cdot C\log C+ 2 ^{\log n}+ T(\log n)+...)<br>$$</p>
<ul>
<li>$\log\log n$是因为我们要枚举所有的$i &lt; \log \log n$以找到下标最小的图灵机；</li>
<li>$2 ^{\log n}$是因为对于每一次枚举，我们都要验证所有长度小于$\log n$的序列，共有指数个；</li>
<li>$c\cdot C\log C$是用通用图灵机模拟$\mathbb{M} _i(x)$计算$i |x| ^i$步的时间；</li>
<li>$2 ^{\log n}$是暴力计算$\textbf{SAT} _H(x)$的时间；</li>
<li>后面这两项需要是因为我们要将模拟结果与$textbf{SAT} _H(x)$的真实值进行比较；</li>
<li>$T(\log n)$是因为在计算$\textbf{SAT} _H(x)$时还要计算$H(|\psi|)$，而$\psi$的长度为$\log n$。</li>
<li>可以算得$T(n) &#x3D; o(n ^3)$。</li>
</ul>
<h2 id="H-n-的性质"><a href="#H-n-的性质" class="headerlink" title="$H(n)$的性质"></a>$H(n)$的性质</h2><ol>
<li>如果$H(n)$是有限的，由于$H(n)$是非递减的，$H(n)$最终会收敛到$i$，即$\exists n \ge N$，使$H(n)&#x3D;i$恒成立。于是$\exists$图灵机$\mathbb{M}_i$，它在多项式时间$i|x| ^i$内判定$\textbf{SAT} _H$。</li>
<li>$\textbf{SAT} _H \in \textbf{P}\space \space \text{iff} \space \space H(n)$是有限的。右到左即性质1，左到右：</li>
</ol>
<blockquote>
<p>设图灵机$\mathbb{M} _i$在$c n ^c$内判定$\textbf{SAT}_H$。由于图灵机编码的任意性，我们可以取$i \ge c$，而$H(n)$输出的是图灵机的最小编码，所以一定有$H(n) \le i$。</p>
</blockquote>
<h2 id="两个方向的证明"><a href="#两个方向的证明" class="headerlink" title="两个方向的证明"></a>两个方向的证明</h2><p>推论1：如果$\textbf{P} \ne \textbf{NP}$，则$\textbf{SAT} _H \notin \textbf{P}$。</p>
<blockquote>
<p>因为若$\textbf{SAT} _H \in \textbf{P}$，则由$\textbf{SAT} _H$的定义，$\textbf{SAT}\le _K \textbf{SAT} _H$。</p>
</blockquote>
<p>推论2：如果$\textbf{P} \ne \textbf{NP}$，则$\textbf{SAT} _H$不是NP完全的。</p>
<blockquote>
<p>不懂</p>
</blockquote>
<h1 id="Baker-Gill-Solovay-Theorem"><a href="#Baker-Gill-Solovay-Theorem" class="headerlink" title="Baker-Gill-Solovay Theorem"></a>Baker-Gill-Solovay Theorem</h1><p>定理：存在$A$使得$\textbf{P} ^A &#x3D; \textbf{NP} ^A$，也存在$B$$\textbf{P} ^B \ne \textbf{NP} ^B$</p>
<blockquote>
<p>对前项的证明：<br>$$<br>\textbf{PSPACE}&#x3D;\textbf{P} ^{\textbf{PSPACE}} \subset \textbf{NP} ^{\textbf{PSPACE}} &#x3D; \textbf{PSPACE}<br>$$<br>于是$\textbf{P} ^{\textbf{PSPACE}} &#x3D; \textbf{NP} ^{\textbf{PSPACE}}$。该项说明，若$A$的表达能力足够强，则$\textbf{P} ^A$会与$\textbf{NP} ^A$的表达能力相当。</p>
<p>对后项的证明：<br>令$B _0 &#x3D; \{\}$，$n _0 &#x3D; 0$，并使得$B _{i + 1}$和$B _i$存在如下递推关系：</p>
<ul>
<li>让图灵机$\mathbb{M} _i ^{B _i}$在问题$1 ^{n _i + 1}$上运行$2 ^{n _{i+1}-1}$步；</li>
<li>若不停机，则令$B _{i+1} &#x3D; B _i$；</li>
<li>若接受，则令$B _{i+1} &#x3D; B _i$；</li>
<li>若拒绝，则令$B _{i+1} &#x3D; B _i \cup \{s\}$，其中$s$是长度为$n _{i+1}$的字符串，且在图灵机运行的过程中没有被用于问子程序$B _i$（因为图灵机只运行$2 ^{n _{i+1}-1}$步，不能枚举完所有的长度为$n _{i+1}$的串）。</li>
</ul>
<p>此外，还规定$n _{i+1}$是严格递增的，且必须大于所有的会被作为图灵机$\mathbb{M} _0 ^{B _0}(x _1)...\mathbb{M} _{i-1} ^{b _{i-1}}(x _i)$的输入串$x$。<br>最终，$B$就定义为$B &#x3D; \bigcup _{i \in \textbf{N}} B _i$。</p>
<p>再定义$U _B &#x3D; \{1 ^n | B \text{包含长度为}n\text{的串}\}$，即，若$B$包含长度为$n$的串，则$U _B$包含长度为$n$的全1串。对于任意的全1串，我们可证$U _B \in \textbf{NP} ^B,U _B \notin \textbf{P} ^B$。左边易证，我们只需枚举所有长度为$n$的字符串并询问$B$即可，对于右边：</p>
<p>假设存在$\mathbb{M} _i ^B$在多项式时间判定$U _B$，则，如果：</p>
<ul>
<li>$\mathbb{M} _i ^B(1 ^{n+1})&#x3D;0$，那么$B$中不包含长度为$n + 1$的字符串，而长度为$n + 1$的字符串只能在$B _{i+1} &#x3D; B _i \cup \{s\}$被加入到$B _{i+1}$中，所以$B _{i+1}$中不包含长度为$n+1$的串。</li>
</ul>
<p>由此我们可以推出矛盾：</p>
<ul>
<li>$\mathbb{M} _i ^{B _i}(1 ^{n+1})&#x3D;1\to B _{i+1} &#x3D; B _i \to\mathbb{M} _i ^{B _{i+1}}(1 ^{n+1})&#x3D;1 \to \mathbb{M} _i ^B(1 ^{n+1})&#x3D;1$</li>
</ul>
<p>证毕。</p>
</blockquote>
<h2 id="Oracle-Turing-Machine：神谕图灵机"><a href="#Oracle-Turing-Machine：神谕图灵机" class="headerlink" title="Oracle Turing Machine：神谕图灵机"></a>Oracle Turing Machine：神谕图灵机</h2><p>神谕图灵机$\mathbb{M} ^?$是一种特殊的图灵机，它有一条额外的读写带和新的三种状态$q _{text{query}}, q _{\text{yes}}, q _{\text{no}}$。</p>
<blockquote>
<p>这条额外的带子实际上就是“子程序”。“$?$”可被替换为一个实际的问题&#x2F;语言，如$B$。</p>
</blockquote>
<p>神谕图灵机通过进入$q _{\text{query}}$状态来询问某个字符串$b$是否属于$B$，这相当于一次“子程序”调用。如果$b \in B$，则进入$q _\text{yes}$状态；否则，进入$q _{\text{no}}$状态</p>
<blockquote>
<p>子程序调用只算作一次计算！</p>
</blockquote>
<p>对神谕图灵机的编码是对机器的编码，与神谕无关。</p>
<h2 id="Cook-Reduction"><a href="#Cook-Reduction" class="headerlink" title="Cook Reduction"></a>Cook Reduction</h2><p>如果存在一个多项式时间复杂度的神谕图灵机$\mathbb{M} ^?$，使得语言$L$可以被图灵机$\mathbb{M} ^{L&#39;}$判定，则称语言$L$可以Cook归约到$L&#39;$，记作$L \le _C L&#39;$。</p>
<h2 id="Lowness"><a href="#Lowness" class="headerlink" title="Lowness"></a>Lowness</h2><p>如果$\textbf{A} ^{\textbf{B}}&#x3D;\textbf{A}$，则称复杂性类$\textbf{B}$相对于复杂性类$\textbf{A}$是低的。</p>
<blockquote>
<p>换句话说，$\textbf{A}$调用$\textbf{B}$并不能增强自身判定问题的能力，这说明相对于$\textbf{A}$，$\textbf{B}$解决的问题是更简单的问题。</p>
</blockquote>
]]></content>
      <categories>
        <category>CS7313: Computational Complexity</category>
      </categories>
      <tags>
        <tag>Math</tag>
        <tag>Theoretical Computer Science</tag>
      </tags>
  </entry>
  <entry>
    <title>Computational Complexity Space Complexity 1</title>
    <url>/2023/09/28/ComputationalComplexitySpaceComp1/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="Space-Bounded-Computation：空间边界的计算"><a href="#Space-Bounded-Computation：空间边界的计算" class="headerlink" title="Space Bounded Computation：空间边界的计算"></a>Space Bounded Computation：空间边界的计算</h1><p>空间是计算资源，是可重用的。研究空间复杂度，只考虑只有一条带子的图灵机即可。</p>
<p>空间复杂性（函数）的定义：对函数$S: \mathbf{N}\to \mathbf{N}$和语言$L \in \{0,1\} ^ *$，如果存在常数$c$和图灵机$\mathbb{M}$能使用不超过$cS(n)$的非空工作带格子判定输入长度为$n$的$L$，就称$L \in \textbf{SPACE}(S(n))$。</p>
<blockquote>
<p>不计算输入所占用的格子。空间可构造和完全空间可构造完全类比时间可构造和完全时间可构造。</p>
</blockquote>
<p>对非确定图灵机，$L \in \textbf{NSPACE}((S(n)))$当且仅当每条路径上使用的非空工作带格子都不超过$cS(n)$。</p>
<h2 id="Configuration-Graph：格局图"><a href="#Configuration-Graph：格局图" class="headerlink" title="Configuration Graph：格局图"></a>Configuration Graph：格局图</h2><ul>
<li>$C _{\text{start}}$：初始格局；</li>
<li>$C _{\text{accept}}$：接受格局，图灵机中止且带子上的结果为1。</li>
</ul>
<p>格局图：一个图灵机$\mathbb{M}$的格局图$G _{\mathbb{M},x}$是一个有向图，它的结点是某个格局$C$，边是一次计算。若使用格局图，则$\mathbb{M}$接受$x$当且仅当在格局图$G _{\mathbb{M},x}$上存在从$C _{\text{start}}$到$C _{\text{accept}}$的路径。</p>
<h3 id="Size-of-Configuration-Graph：格局图的大小"><a href="#Size-of-Configuration-Graph：格局图的大小" class="headerlink" title="Size of Configuration Graph：格局图的大小"></a>Size of Configuration Graph：格局图的大小</h3><p>对于一个时间复杂度为$S(n)$的图灵机$\mathbb{M}$，由于只考虑一条工作带，因此，某个长度为$n$输入$x$，其某一步的格局可用$O(S(n))个$比特编码。因此，整个格局图的结点个数就是$2 ^{O(S(n))}$。</p>
<p>当我们想要判断某两个结点间是否存在边时，我们只需要判断某两个格局$C _1$和$C _2$是否存在一步到达的关系，即看能否通过仅修改某个格局$C _1$编码的$1$个bit和表示读写头的几个bit，就让$C _1&#x3D;C _2$。不难看出，该算法的时间复杂度是$O(S(n))$，空间复杂度是$O(\log S(n))$，因为我们只需要按位比较，并记录位置的编码即可，而$O(S(n))$个位置可用$O(\log S(n))$的二进制串表示。</p>
<blockquote>
<p>但要求$C _1$和$C _2$交叉输入，否则带头的来回移动会让时间复杂度为平方。</p>
</blockquote>
<h2 id="Space-vs-Time"><a href="#Space-vs-Time" class="headerlink" title="Space vs Time"></a>Space vs Time</h2><p>设空间函数$S(n)$是空间可构造的，则：</p>
<p>$$<br>\textbf{TIME}(S(n))\subseteq \textbf{SPACE}(S(n))\subseteq \textbf{NSPACE}(S(n))\subseteq \textbf{TIME}(2 ^{O(S(n))})<br>$$</p>
<blockquote>
<p>第一个包含关系：一步最多多用一个格子；<br>第二个包含关系：确定是非确定的特例；<br>第三个包含关系：判定一个非确定图灵机能否在$O(S(n))$的空间内判定每个问题，等价于验证格局图中是否存在由$C _\text{start}$到$C _{\text{accept}}$的路径，而这可通过BFS在多项式时间复杂度内完成。格局图中总结点数为$2 ^{O(S(n))}$，因此第三个包含关系成立。</p>
</blockquote>
<p>注意：以上相互包含的都是<strong>问题集合</strong>，如第一个包含关系意为：时间复杂度为$S(n)$的问题是空间复杂度为$S(n)$问题的子集。</p>
<blockquote>
<p>对所有的空间可构造的$S(n)$，$\textbf{TIME}(S(n))\subseteq \textbf{SPACE}(S(n)&#x2F; \log S(n))$</p>
</blockquote>
<h2 id="Space-Complexity-Class"><a href="#Space-Complexity-Class" class="headerlink" title="Space Complexity Class"></a>Space Complexity Class</h2><ul>
<li>$\textbf{L}$类：$\textbf{SPACE}(\log (n))$</li>
<li>$\textbf{NL}$类：$\textbf{NSPACE}(\log (n))$</li>
<li>$\textbf{PSPACE}$类：$\bigcup _{c&gt;0} \textbf{SPACE}(n ^c)$</li>
<li>$\textbf{NPSPACE}$类：$\bigcup _{c&gt;0} \textbf{NSPACE}(n ^c)$</li>
</ul>
<p>$$<br>\textbf{NP}\subseteq \textbf{PSPACE}<br>$$</p>
<p>上式成立是因为空间是可以重复利用的，故我们可以用<strong>相同</strong>的空间在<em>多项式时间内</em>（<em>空间</em>）验证<strong>不同</strong>的路径是否成立。</p>
<h3 id="PATH-is-in-NL"><a href="#PATH-is-in-NL" class="headerlink" title="PATH is in NL"></a>PATH is in NL</h3><p>$\text{PATH}&#x3D;\{&lt;G,s,t&gt;\}$定义为判断在图$G$中是否存在从$s$到$t$的路径。</p>
<p>$$\text{PATH}\in \textbf{NL}$$</p>
<p>这是因为：在有$n$个结点的图中，不考虑重复边，$s$到$t$的最长路径长度为$n-1$。因此，我们总可以不断地从$s$做$n-1$次猜测，每次猜测选择一个邻居，走到邻居，并继续猜测。总会有那么一个猜测路径，使得我们可以到达$t$（若$s$和$t$之间确实存在路径）。这一过程中，我们需要记录当前结点，而当前结点可用$\log n$的空间编码，还要记录步数，而步数可用$\log (n-1)$的空间编码，又因为空间是可以重用的，所以总的空间复杂度为$O(\log n)$。</p>
<blockquote>
<p>对空间复杂度为$S(n)$的图灵机，通用图灵机模拟它的空间复杂度为$O(S(n))$。</p>
</blockquote>
<h2 id="Space-Hierarchy-Theorem：空间谱系定理"><a href="#Space-Hierarchy-Theorem：空间谱系定理" class="headerlink" title="Space Hierarchy Theorem：空间谱系定理"></a>Space Hierarchy Theorem：空间谱系定理</h2><p>若$f$和$g$都是空间可构造的，且$f(n)&#x3D;o(g(n))$，则：</p>
<p>$$<br>\textbf{SPACE}(f(n)) \subsetneq \textbf{SPACE}(g(n))<br>$$</p>
<blockquote>
<p>证明类似时间谱系定理。</p>
</blockquote>
<h1 id="Logspace-Reduction：对数空间归约"><a href="#Logspace-Reduction：对数空间归约" class="headerlink" title="Logspace Reduction：对数空间归约"></a>Logspace Reduction：对数空间归约</h1><p>隐式对数空间可计算（Implicitly Logspacec Computable）函数$f:\{0,1\} ^ *\to \{0,1\} ^ *$满足如下三个性质：</p>
<ol>
<li>输出长度有限：$\exists c,\forall x,|f(x)|\le c |x| ^c$</li>
<li>输出长度可在对数空间内计算：$\{&lt;x,i&gt;|i \le |f(x)|\}\in L$</li>
<li>输出结果可确定：$\{&lt;x,i&gt;|f(x) _i &#x3D; 1\}\in L$</li>
</ol>
<p>以上三个性质的推断都依赖于前一个性质。如果存在这么一个函数$f$，使得对于问题$B$和问题$C$，$x\in B$当且仅当$f(x) \in C$，那么我们称$B$是可在对数空间归约于$C$的，即$B \le _L C$。</p>
<blockquote>
<p>$f(x)$作为$C$的输入，是不能被直接写到$C$的输入带上的，因为$f(x)$的长度是多项式的。不过，$f(x)$的三个性质使得图灵机可以在对数空间内计算出$f(x)$任意位置的值，由于空间的可重用性，输入$f(x)$实际只用了对数空间（每次一位一位地输入）。</p>
<p>$B$对数空间归约到$C$，说明在<strong>不消耗更多空间资源</strong>的情况下，$C$是更复杂的问题。</p>
<p>对数空间归约具有传递性，即，若$B \le _L C$，$C \le _L D$，则$B \le _L D$。证明：</p>
<ul>
<li>$B \le _L C$，所以对$x \in B$，可在对数空间$O(\log |x|)$内计算$f(x)$。</li>
<li>$C \le _L D$，所以对$f(x) \in C$，可在对数空间$O(\log |f(x)|)$内计算$g(f(x))$。</li>
<li>又由于空间的可重复利用性，用于计算$f(x)$的对数空间可被重复用于计算$g(f(x))$，因此$O(\log |f(x)|)&#x3D;O(\log |x|)$。</li>
</ul>
</blockquote>
<h1 id="text-PSPACE-Completeness"><a href="#text-PSPACE-Completeness" class="headerlink" title="$\text{PSPACE}$ Completeness"></a>$\text{PSPACE}$ Completeness</h1><p>空间完备性（Space Completeness）：如果$\forall L \in \textbf{PSPACE}$，$L \le _L L&#39;$，则称语言$L&#39;$是$\textbf{PSPACE}-\text{hard}$的。如果进一步地，$L &#39; \in \textbf{PSPACE}$，则称语言$L&#39;$是$\textbf{PSPACE}-\text{complete}$的。某个完备的（Complete）语言是这个空间复杂性类中最难的语言&#x2F;问题。</p>
<h2 id="Quantified-Boolen-Formula"><a href="#Quantified-Boolen-Formula" class="headerlink" title="Quantified Boolen Formula"></a>Quantified Boolen Formula</h2><p>量化布尔公式（Quantified Boolen Formula，QBF）被定义为这样的式子：</p>
<p>$$<br>Q _1 x _1 Q _2 x _2...Q _n x _n.\space \phi(x _1,...,x _n)<br>$$</p>
<p>其中，$Q _i$是量词（Quantifier），要么是$\forall$，要么是$\exists$；$x _i$是布尔值，要么是$0$，要么是$1$；$\phi$是不包含$x _1,...,x _n$以外的任何自由变量的布尔公式，该公式要么为$\text{True}$，要么为$\text{False}$，如：</p>
<p>$$<br>\forall x,\exists y,x&#x3D;y<br>$$</p>
<p>显然，上述公式值为$True$。</p>
<p>空间复杂度两个原则：格局图、深度优先</p>
<h2 id="Stockmeyer-Meyer-Theorem"><a href="#Stockmeyer-Meyer-Theorem" class="headerlink" title="Stockmeyer-Meyer Theorem"></a>Stockmeyer-Meyer Theorem</h2><p>定理：TQBF是$\textbf{PSPACE}-\text{complete}$的。定理中，$\text{TQBF}$是所有值为$\text{True}$的QBF集合。定理的证明如下：</p>
<blockquote>
<p>要证明上述定理，即要证明$\forall L \in \textbf{PSPACE}$，$L \le _L $TQBF，且TQBF$\in \textbf{PSPACE}$。</p>
<p>1.TQBF$\in \textbf{PSPACE}$</p>
<p>假定某个QBF为$\psi &#x3D; Q _1 x _1 Q _2 x _2...Q _n x _n.\space \phi(x _1,...,x _n)$，由于$x _i$只有两种取值可能，因此我们可以将$\psi$所有可能的形式表示为一棵二叉树：<br><img src="/2023/09/28/ComputationalComplexitySpaceComp1/1.png" alt="1"></p>
<center style="font-size:12px;font-weight:bold">Fig. 1. QBF</center><br>

<p>对该二叉树使用一次深度优先遍历（实际上应该使用<strong>后序遍历</strong>）即可判定该问题。DFS过程中，我们可以自下而上求出所有非叶子结点的值，最终根结点的值就是QBF的判定结果，整个过程中，我们最多需要记录$n$个祖先，因而该算法的空间复杂度实际上为$O(n)$。因此TQBF$\in \textbf{PSPACE}$。证毕。</p>
<p>2.$\forall L \in \textbf{PSPACE}$，$L \le _L $TQBF</p>
<p>$L \in \textbf{PSPACE}$说明存在图灵机$\mathbb{M}$，对输入$x \in L$，它能在多项式空间内输出$\mathbb{M}(x)&#x3D;1$以及格局图中存在$C _{\text{start}}$到$C _{\text{accept}}$的路径。</p>
<p>要证明条件2，我们只需要证明对$x \in L$，存在$\phi(x)\in $TQBF。不妨令$\phi(x)&#x3D;\psi _i (C,C&#39;)$，其中，当格局$C$和$C&#39;$之间存在长度不超过$2 ^i$的路径时$\psi _i (C,C&#39;)&#x3D;1$。</p>
<p>对于$\psi _i (C,C&#39;)$，若其值为$1$，则在$C$和$C&#39;$之间一定存在一个格局$C&#39;&#39;$，它到这两个格局的路径长度不超过$2 ^{i - 1}$，因此$\psi _i (C,C&#39;)$就可以被转换为$\psi _{i-1} (C,C&#39;&#39;) \lor  \psi _{i-1} (C&#39;,C&#39;&#39;)$。该问题可被表示为：<br>$$<br>\exists C&#39;&#39; \forall D ^1 \forall D ^2.((D ^1&#x3D;C \land D ^2 &#x3D;C&#39;))\lor ((D ^1&#x3D;C&#39;&#39; \land D ^2 &#x3D;C)) \rightarrow \psi _{i-1}(D ^1, D ^2)<br>$$<br>只要我们能够判断上面前蕴含后的关系成立，则$\psi _i (C,C&#39;)&#x3D;1$成立。判别上式的空间复杂度为$|\psi _i|&#x3D;|\psi _{i-1}(D ^1, D ^2)|+cS(|x|)$，其中，后一项是用于记录箭头前的格局的空间（每个格局空间复杂度$S(|x|)$）。那么对于空间复杂度为$S(|x|)$的$L$，它的格局图中最多有$2 ^{S(|x|)}$个结点，于是判断$L$，就可以转为判断$\psi _{S(|x|)}$，即判断QBF$\phi (x)$。$|\phi (x)|&#x3D;|\psi _{S(|x|)}|&#x3D;O(S(|x|) ^2)$，多项式的平方仍然是多项式，所以$x\in L$，$\phi (x) \in $TQBF，证毕。前面的式子能成立是因为最终$\psi _{S(|x|)}$会被转化为$\psi _{1}$，期间会多出$S(|x|)$个$c S(|x|)$。</p>
</blockquote>
<h1 id="Savitch-Theorem"><a href="#Savitch-Theorem" class="headerlink" title="Savitch Theorem"></a>Savitch Theorem</h1><p>定理：如果$S$是空间可构造的，那么$\textbf{NSPACE}(S(n)) \subseteq \textbf{SPACE}(S(n) ^2)$。证明如下：</p>
<blockquote>
<p>该定理的证明与上面的条件2证明几乎一致。对于空间复杂度为$S(n)$的非确定图灵机$\mathbb{N}$，它可以在$S(n)$的空间内判定$L$，其格局图的大小为$2 ^{S(n)}$。很自然地，判定$L$可以被转换为判定格局图中是否存在$C _{\text{start}}$到$C _{\text{accept}}$的路径。该问题可以被$\phi (x)$在$c S(n) ^2$空间内解决。所以$\textbf{NSPACE}(S(n)) \subseteq \textbf{SPACE}(S(n) ^2)$。证毕。</p>
</blockquote>
<p>由于多项式的平方还是多项式，所以由上述定理，我们可以得到：</p>
<p>$$<br>\textbf{NPSPACE} \subseteq\textbf{PSPACE}<br>$$</p>
<p>又因为确定图灵机是非确定图灵机的特例：</p>
<p>$$<br>\textbf{PSPACE} \subseteq \textbf{NPSPACE}<br>$$</p>
<p>所以：</p>
<p>$$<br>\textbf{PSPACE} &#x3D; \textbf{NPSPACE}<br>$$</p>
<h1 id="NL-Completeness"><a href="#NL-Completeness" class="headerlink" title="NL Completeness"></a>NL Completeness</h1><p>如果$C \in \textbf{NL}$，且对$\forall B \in \textbf{NL}$，$B \le _L C$，则$C$是$\textbf{NL}-$ complete。</p>
<p>定理：$\text{PATH}$是$\textbf{NL}$完全的。证明如下：</p>
<blockquote>
<p>由于前面已经证明$\text{PATH} \in \textbf{NL}$，所以要证明上述结论，即要证明对$\forall L \in \textbf{NL}$，$L \le _L \text{PATH}$，即要证明$\forall x \in L$，$\exists f$，$f(x) \in \text{PATH}$。</p>
<p>与前面的证明类似，对在$\log(n)$空间内判定$L$的非确定图灵机$\mathbb{N}$，设其输入为$x$，则该问题可以被转换为在格局图$G _{\mathbb{N},x}$中判断$C _\text{start}$和$C _\text{accept}$间是否存在路径，即：<br>$$<br>x \to &lt; G _{\mathbb{N},x},C _\text{start},C _\text{accept} &gt;<br>$$<br>由于$\mathbb{N}$的空间复杂度是$\log (n)$，所以格局图中的每一个格局都可以用$\log (n)$的bit编码，且共有多项式个格局（顶点）。用邻接矩阵$A$表示格局图$G _{\mathbb{N},x}$的结构关系，由于各个顶点的编码长度为$\log (n)$，所以任意两个顶点的一步可达性可在对数空间内判定，又由于空间的可重复利用性，整个邻接矩阵可以在对数空间内被构造出来，而起始格局和终止格局都是固定的$01$串，所以$&lt; G _{\mathbb{N},x},C _\text{start},C _\text{accept} &gt;$可以在对数空间内被构造出来，而$&lt; G _{\mathbb{N},x},C _\text{start},C _\text{accept} &gt; \in \text{PATH}$，所以$L \le _L \text{PATH}$。证毕。</p>
</blockquote>
<p>有向无环图的$\text{PATH}$问题也是$\textbf{NL}$完全的。</p>
<h1 id="Immerman-Szelepcsenyi-Theorem"><a href="#Immerman-Szelepcsenyi-Theorem" class="headerlink" title="Immerman-Szelepcsenyi Theorem"></a>Immerman-Szelepcsenyi Theorem</h1><p>补类（补问题）：一个类的补类指在所有类中刨去改类后剩下的类，如，对$\textbf{T}$类，其补类记为$\text{co}\textbf{T}$或$\overline{\textbf{T}}$，定义为：</p>
<p>$$<br>\text{co}\textbf{T}&#x3D;\overline{\textbf{T}}&#x3D;\{\{0,1 ^*\}-L \in \textbf{T}\}<br>$$</p>
<p>一个有趣的结论：</p>
<p>$$<br>\begin{align*}<br>    \text{co}\textbf{NPSPACE}<br>    &amp;&#x3D;\overline{\textbf{NPSPACE}} \\<br>    &amp;&#x3D;\overline{\textbf{PSPACE}}\\<br>    &amp;&#x3D;\textbf{PSPACE}\\<br>    &amp;&#x3D;\textbf{NPSPACE}<br>\end{align*}<br>$$</p>
<p>定理：$\overline{\text{PATH}}\in \textbf{NL}$。$\overline{\text{PATH}}$的含义为：从$A$出发到不了$B$。的证明如下：</p>
<blockquote>
<p>???????</p>
</blockquote>
<p>由上述定理，我们可以得到$\overline{\textbf{NL}}&#x3D; \textbf{NL}$：</p>
<blockquote>
<p>$A \le _L B$，则$\overline{A} \le _L \overline{B}$</p>
<p>因为$L \le _L \text{PATH}$，所以$\overline{L} \le _L \overline{\text{PATH}}$，所以$\overline{\text{PATH}}$是$\overline{\textbf{NL}}$完全的，而$\overline{\text{PATH}}\in \textbf{NL}$，所以$\overline{\textbf{NL}}&#x3D; \textbf{NL}$。</p>
</blockquote>
<p>事实上，对$\forall S(n) \ge \log (n)$，若$S(n)$时间可构造，则：</p>
<p>$$<br>\overline{\textbf{NSPACE}(S(n))}&#x3D;\textbf{NSPACE}(S(n))<br>$$</p>
<p>$$<br>\textbf{L}\subseteq \textbf{NL}\subseteq \textbf{P}\subseteq \textbf{NP}\subseteq \textbf{PSPACE}\subseteq \textbf{EXP}<br>$$</p>
]]></content>
      <categories>
        <category>CS7313: Computational Complexity</category>
      </categories>
      <tags>
        <tag>Math</tag>
        <tag>Theoretical Computer Science</tag>
      </tags>
  </entry>
  <entry>
    <title>Contrastive Learning</title>
    <url>/2023/09/27/ContrastiveLearning/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="CV领域的对比学习发展路径"><a href="#CV领域的对比学习发展路径" class="headerlink" title="CV领域的对比学习发展路径"></a>CV领域的对比学习发展路径</h1><p>什么是对比学习？对比学习是属于无监督&#x2F;自监督学习范式的。在监督学习的分类问题中，我们希望模型能够精确地预测输入属于的类别，而在对比学习中，模型不需要知道数据的真实标签，只要最终的输出能够把不同的类别区分开就好。因而，对比学习的模型就是一个特征提取器，其模型将输入的特征提取出来作为输出，使得在输出的特征空间中，相似的数据尽可能地相邻，而不相似的数据尽可能地远离，如下图所示</p>
<p><img src="/2023/09/27/ContrastiveLearning/1.png" alt="1"></p>
<center style="font-size:12px; font-weight:bold">Fig. 1. Contrastive Learning</center><br>

<p>对比学习的典型范式是<strong>代理任务</strong>+<strong>目标函数</strong>。</p>
<ul>
<li>代理任务：代理任务是一些不像分类、目标检测一样具有实际意义的应用场景，但我们假定该模型是为了解决这个代理问题而训练的，而实际上它只是用于生成自监督信号以更新特征提取器，从而能够让我们获得更好的预训练模型。在NLP中，BERT预训练中用到的填词等任务就可以被视为代理任务。在CV中，如下面会提到的九宫格图像相对位置预测、图片着色等都属于代理任务。不过，在CV的对比学习中，更常用的代理任务是个体判别（Instance discrimination），即将同类的个体与其他个体区分开来。&gt; 更通俗地来说，代理任务是为了生成类似监督学习的“标签”，使得无监督学习也有比较的对象（像监督学习中的Ground Truth和prediction一样），有了比较对象，我们才能用合适的metric构建目标函数。</li>
<li>目标函数：产生梯度。<ol>
<li>生成式网络：用生成的图片与原图片做对比，可以是$L1$或$L2$ losses。</li>
<li>判别式网络：对图片本身做划分，如作九宫格划分，用一个格子预测另一个格子在其哪个方位，实际上转化为了一个交叉熵损失。</li>
<li>对比式：衡量被提取的数据特征间在特征空间的相似性，不同于前两种的是（特别是生成式），由于编码器是在不断更新的，被提取的数据特征也是在不断被更新的，因而对比的对象不像前两者是固定的。</li>
<li>对抗性：衡量概率分布的差异。（不太懂）</li>
</ol>
</li>
</ul>
<h2 id="百花齐放"><a href="#百花齐放" class="headerlink" title="百花齐放"></a>百花齐放</h2><ol>
<li>InstDisc: Memory bank。字典内容一致性不好。</li>
<li>InvaSpread: 端到端，两个编码器都梯度下降。字典大小受限。</li>
<li>CPC: InfoNCE。</li>
<li>CMC: 多视角。</li>
</ol>
<h2 id="CV双雄"><a href="#CV双雄" class="headerlink" title="CV双雄"></a>CV双雄</h2><ol>
<li>MoCoV1: 动态编码器、Memory bank变队列。</li>
<li>SimCLRV1: 端到端。</li>
<li>MoCoV2</li>
<li>SimCLRV2</li>
<li>SWaV</li>
</ol>
<h2 id="不用负样本"><a href="#不用负样本" class="headerlink" title="不用负样本"></a>不用负样本</h2><ol>
<li>BYOL: MSE LOSS，一个编码器预测另一个编码器。</li>
<li>SimSiam</li>
</ol>
<h2 id="基于Transformer"><a href="#基于Transformer" class="headerlink" title="基于Transformer"></a>基于Transformer</h2><ol>
<li>MoCoV3</li>
<li>DINO</li>
</ol>
<blockquote>
<p>骨干网络由ResNet换为ViT。</p>
</blockquote>
<p><img src="/2023/09/27/ContrastiveLearning/1.png" alt="1"></p>
<center style="font-size:12px; font-weight:bold">Fig. 1. </center>

<h1 id="MoCo"><a href="#MoCo" class="headerlink" title="MoCo"></a>MoCo</h1><p>对比学习是一次字典查询的过程。</p>
<ol>
<li><p>字典要大；</p>
</li>
<li><p>字典的内容连续性要好。</p>
</li>
<li><p>队列作为字典的数据结构：每个mini batch，老key出去，更新后的key作为new key进来。</p>
</li>
<li><p>动量编码器：$\theta _k &#x3D; m*\theta _k+(1-m)*\theta _q$，保证字典中key的一致性。</p>
</li>
</ol>
<blockquote>
<p>$m$很大，文中取$0.99$或$0.999$。</p>
</blockquote>
<h2 id="NCE-Noise-Contrastive-Estimation"><a href="#NCE-Noise-Contrastive-Estimation" class="headerlink" title="NCE(Noise Contrastive Estimation)"></a>NCE(Noise Contrastive Estimation)</h2><p>当分类任务的类别很多时，交叉熵的计算时间是难以承受的，因为交叉熵的分母必须对样本在<strong>所有类别</strong>上出现的可能进行求和。对于Instance discrimination，每个样本就是一个类，在这种情况下，用交叉熵是不现实的。<br>NCE(Noise Contrastive Estimation)将多分类问题转化为了多个<strong>二分类</strong>问题，所有的样本都只有两类：来自data samples的<strong>正类</strong>和来自noise samples的负类。</p>
<h2 id="InfoNCE"><a href="#InfoNCE" class="headerlink" title="InfoNCE"></a>InfoNCE</h2><p>InfoNCE是对NCE的改进，它比NCE更加接近交叉熵。InfoNCE将正例视作一类，将单个的负例也视作一类。因此，对于$1$正例$K$负例的采样，总类别数是$K+1$。InfoNCE实际上就是类别数为$K+1$且带温度参数$\tau$的交叉熵。还有一点特殊的是，由于我们想匹配的只是$q$和正例$k _+$，所以$\mathcal{L} _q$的分子永远都只会是$\exp(q\cdot k _+ &#x2F; \tau)$。</p>
<p>$$<br>\mathcal{L} _q&#x3D;-\log \frac{\exp(q\cdot k _+ &#x2F; \tau)}{\sum _ {i&#x3D;0} ^K\exp(q\cdot k _i &#x2F; \tau)}<br>$$</p>
<p>上式中，$k _0$即为$k _+$。$\mathcal{L} _q$很好地体现了我们的优化目标：**$q$和正例的相似性出现在分子，所以越大越好，相应地，分母上$q$与负例的相似性越小越好**。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># f_q, f_k: encoder networks for query and key</span></span><br><span class="line"><span class="comment"># queue: dictionary as a queue of K keys (CxK)</span></span><br><span class="line"><span class="comment"># m: momentum</span></span><br><span class="line"><span class="comment"># t: temperature</span></span><br><span class="line"></span><br><span class="line">f_k.params = f_q.params <span class="comment"># initialize</span></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> loader: <span class="comment"># load a minibatch x with N samples</span></span><br><span class="line">   x_q = aug(x) <span class="comment"># a randomly augmented version</span></span><br><span class="line">   x_k = aug(x) <span class="comment"># another randomly augmented version</span></span><br><span class="line"></span><br><span class="line">   q = f_q.forward(x_q) <span class="comment"># queries: NxC  256x128</span></span><br><span class="line">   k = f_k.forward(x_k) <span class="comment"># keys: NxC  256x128</span></span><br><span class="line">   k = k.detach() <span class="comment"># no gradient to keys</span></span><br><span class="line"></span><br><span class="line">   <span class="comment"># positive logits: Nx1  256x1</span></span><br><span class="line">   l_pos = bmm(q.view(N,<span class="number">1</span>,C), k.view(N,C,<span class="number">1</span>))</span><br><span class="line">   <span class="comment"># negative logits: NxK  256x65536</span></span><br><span class="line">   l_neg = mm(q.view(N,C), queue.view(C,K))</span><br><span class="line"></span><br><span class="line">   <span class="comment"># logits: Nx(1+K)  256x65537</span></span><br><span class="line">   logits = cat([l_pos, l_neg], dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">   <span class="comment"># contrastive loss, Eqn.(1)</span></span><br><span class="line">   labels = zeros(N) <span class="comment"># positives are the 0-th</span></span><br><span class="line">   loss = CrossEntropyLoss(logits/t, labels)</span><br><span class="line"></span><br><span class="line">   <span class="comment"># SGD update: query network</span></span><br><span class="line">   loss.backward()</span><br><span class="line">   update(f_q.params)</span><br><span class="line"></span><br><span class="line">   <span class="comment"># momentum update: key network</span></span><br><span class="line">   f_k.params = m*f_k.params+(<span class="number">1</span>-m)*f_q.params</span><br><span class="line"></span><br><span class="line">   <span class="comment"># update dictionary</span></span><br><span class="line">   enqueue(queue, k) <span class="comment"># enqueue the current minibatch</span></span><br><span class="line">   dequeue(queue) <span class="comment"># dequeue the earliest minibatch</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>Paper</tag>
        <tag>Unsupervised Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>MatrixTheory: Finite Dimensional Linear Space</title>
    <url>/2023/09/27/MatrixTheory2/</url>
    <content><![CDATA[<span id="more"></span>


<h1 id="有限维线性空间"><a href="#有限维线性空间" class="headerlink" title="有限维线性空间"></a>有限维线性空间</h1><ul>
<li>线性组合：对数域$\mathbb{F}$上的线性空间$V$，对$\alpha _1,...,\alpha _k \in V$，$x _1,...,x _k \in \mathbb{F}$，称：<br>$$<br>x _1 \cdot\alpha _1+x _2\cdot \alpha + ...+x _k\cdot\alpha _k<br>$$为其线性组合。</li>
</ul>
<blockquote>
<p>注意，“$\cdot$”为该线性空间所定义的数乘运算。</p>
</blockquote>
<ul>
<li>定义$\text{span}[\alpha _1,...,\alpha _k]&#x3D;\{x _1 \cdot\alpha _1+x _2\cdot \alpha + ...+x _k\cdot\alpha _k\}$为向量组$\alpha _1,...,\alpha _k$的张成子空间。该子空间为包含了$\alpha _1,...,\alpha _k$的最小子空间。</li>
</ul>
<p>对某个线性空间，若其可以由有限个向量张成，则称该线性空间为<em>有限维线性空间</em>，反之为<em>无限维线性空间</em>。</p>
<h2 id="线性无关-amp-线性相关"><a href="#线性无关-amp-线性相关" class="headerlink" title="线性无关 &amp; 线性相关"></a>线性无关 &amp; 线性相关</h2><ul>
<li>对某个张成$\text{span}[\alpha _1,...,\alpha _k]$，$\forall \alpha \in \text{span}[\alpha _1,...,\alpha _k]$，若$\alpha &#x3D; x _1 \cdot\alpha _1+x _2\cdot \alpha + ...+x _k\cdot\alpha _k$的系数$x _1,x _2,...,x _k$唯一，则称$\alpha _1,...,\alpha _k$线性无关。</li>
</ul>
<p><strong>判断的充要条件：$x _1 \cdot\alpha _1+x _2\cdot \alpha + ...+x _k\cdot\alpha _k&#x3D;0$当且仅当$x _1&#x3D;x _2&#x3D;...&#x3D;x _k&#x3D;0$。</strong> 反之若$x _i$不全为0，则称线性相关。</p>
<h3 id="线性相关定理"><a href="#线性相关定理" class="headerlink" title="线性相关定理"></a>线性相关定理</h3><p>若$\alpha _1,...,\alpha _m$线性相关：</p>
<ol>
<li>$\exists j \in \{1,...,m\}$，使得$\alpha _j \in \text{span}[\alpha _1,...,\alpha _{j-1}]$；<blockquote>
<p>证明：<br>$x _1 \cdot\alpha _1+x _2\cdot \alpha _2 + ...+x _m\cdot\alpha _m&#x3D;0$<br>假设$x _i$不为零的最大下标为$j$，则：<br>$x _1 \cdot\alpha _1+x _2\cdot \alpha _2 + ...+x _j\cdot\alpha _j&#x3D;0$<br>则：<br>$x _j\cdot\alpha _j&#x3D;-x _1 \cdot\alpha _1-x _2\cdot \alpha _2 - ...-x _{j-1}\cdot\alpha _{j-1}$<br>则：<br>$\alpha _j&#x3D;-x _1&#x2F;x _j \cdot\alpha _1-x _2 &#x2F;x _j \cdot \alpha _2 - ...-x _{j-1} &#x2F;x _j \cdot\alpha _{j-1}$</p>
</blockquote>
</li>
<li>如果将$\alpha _j$从向量组中删除，则剩余的向量组成的张成与原张成一样。</li>
</ol>
<blockquote>
<p>$j&#x3D;1$时说明$\alpha _1$是零向量。</p>
</blockquote>
<h2 id="基"><a href="#基" class="headerlink" title="基"></a>基</h2><ul>
<li>基：一组线性无关的向量$\alpha _1,...,\alpha _k$张成的空间$V$。这组向量称为该张成的<em>基</em>。</li>
</ul>
<p>有限维线性空间一定存在一组基。</p>
<ul>
<li>易由线性相关定理推出，即删除法；</li>
<li>也可由基的扩充定理推出，即添加法：$\alpha _1,...,\alpha _k$，但$\text{span}[\alpha _1,...,\alpha _k]&lt; V$，则可在其后添加向量，使之成为$V$的一组基。</li>
</ul>
<p>基向量的数目$k$则称为该空间的维数$\text{dim}$。$\{0\}$的维数为0。</p>
<ul>
<li>补空间：$U\oplus W&#x3D;V$，则称$U$为$W$补空间。补空间可以有无穷多个，除非$U&#x3D;W$，则其补空间只能为$\{0\}$。</li>
</ul>
<h2 id="维数公式"><a href="#维数公式" class="headerlink" title="维数公式"></a>维数公式</h2><p>空间$V$，子空间$U$、$W$，则：</p>
<p>$$<br>\text{dim}(U+W)&#x3D;\text{dim}(U)+\text{dim}(W)-\text{dim}(U\cap W)<br>$$</p>
<p>$$<br>\text{dim}(U _1\oplus U _2 \oplus...\oplus U _n)&#x3D;\text{dim}(U _1)+\text{dim}(U _2)+...+\text{dim}(U _n)<br>$$</p>
<h2 id="与矩阵相关的空间"><a href="#与矩阵相关的空间" class="headerlink" title="与矩阵相关的空间"></a>与矩阵相关的空间</h2><p>$A _{m\times n}$：</p>
<ul>
<li>$N(A)$：零空间，$Ax&#x3D;0$，$R ^n$的子空间，可用行变换求解。</li>
<li>$C(A)$：列空间，列向量张成的空间，$R ^m$的子空间，也可用行变换求解。因为行变换不会改变列向量的相关性。</li>
<li>$R(A)$：行空间，行向量张成的空间，$R ^n$的子空间，可用行变换求解。</li>
<li>$N(A ^T)$：左零空间，$A ^Tx&#x3D;0$，$R ^m$的子空间，也可用行变换。</li>
</ul>
<p>Hermite标准型：阶梯阵的基础上令行第一个非零为$1$，且其所在列的其他位置都是$0$。Hermite标准化后，行空间即为Hermite的线性无关行，但列空间由原列向量组成。求左零空间时，可求$(A|E)$的行变换，得到$(\frac{H}{0}|P)$，则$PA&#x3D;\frac{H}{0}$。</p>
<h2 id="满秩分解"><a href="#满秩分解" class="headerlink" title="满秩分解"></a>满秩分解</h2><p>对$A _{m\times n},r(A)&#x3D;r$，$A _{m\times n}&#x3D;P _{m\times r} \times Q _{r\times n}$，其中$P$列满秩，$Q$行满秩，这样的分解称<em>满秩分解</em>。</p>
<h3 id="存在性"><a href="#存在性" class="headerlink" title="存在性"></a>存在性</h3><p>$$<br>P _1 A Q _1 &#x3D;<br>\begin{pmatrix}<br>    E _r&amp; 0 \\<br>    0&amp; 0<br>\end{pmatrix}<br>$$</p>
<p>则：</p>
<p>$$<br>\begin{align*}<br>    A<br>    &amp;&#x3D;<br>P _1 ^{-1}<br>\begin{pmatrix}<br>    E _r&amp; 0 \\<br>    0&amp; 0<br>\end{pmatrix}<br>Q _1 ^{-1}\\<br>    &amp;&#x3D;<br>P _1 ^{-1}<br>\begin{pmatrix}<br>    E _r\\<br>    0<br>\end{pmatrix}<br>\begin{pmatrix}<br>    E _r\space 0<br>\end{pmatrix}<br>Q _1 ^{-1}<br>\end{align*}<br>$$</p>
<p>则：</p>
<p>$$<br>\begin{align*}<br>    P &#x3D; P _1 ^{-1}<br>\begin{pmatrix}<br>    E _r\\<br>    0<br>\end{pmatrix}\\<br>    Q &#x3D;\begin{pmatrix}<br>    E _r\space 0<br>\end{pmatrix}<br>Q _1 ^{-1}<br>\end{align*}<br>$$</p>
<h3 id="唯一性"><a href="#唯一性" class="headerlink" title="唯一性"></a>唯一性</h3><p>$$<br>A &#x3D; P \times Q&#x3D;P \times E _r \times Q&#x3D;P\times U ^{-1}\times U \times Q<br>$$</p>
<p>所以一定不唯一。</p>
<h3 id="求法"><a href="#求法" class="headerlink" title="求法"></a>求法</h3><p>求Hermite标准型$H _A$，则非零行所组成矩阵即为$Q$，线性无关列所对应的$A$的列向量组成的矩阵即为$P$</p>
<h1 id="线性变换"><a href="#线性变换" class="headerlink" title="线性变换"></a>线性变换</h1><p>对$\mathbb{F}$上的线性空间$U$和$V$，若：</p>
<ol>
<li>$\forall \alpha,\beta \in U$，$T(\alpha + \beta)&#x3D;T(\alpha)+T(\beta)$</li>
<li>$\forall k \in \mathbb{F}$，$\alpha \in U$，$T(k \cdot \alpha)&#x3D;k\cdot T(\alpha)$</li>
</ol>
<p>则称$T$是线性空间$U$到$V$的线性变换。</p>
<h2 id="线性变换的性质"><a href="#线性变换的性质" class="headerlink" title="线性变换的性质"></a>线性变换的性质</h2><ol>
<li>$T(0)\to 0$。证明：$T(0)&#x3D;T(0+0)&#x3D;T(0)+T(0)$</li>
<li>$T(-\alpha) &#x3D; 1T(\alpha)$</li>
<li>设$\alpha _1,\alpha _2,...,\alpha _k \in U$线性相关，则$T(\alpha _1),T(\alpha _2),...,T(\alpha _k)$线性相关</li>
<li>线性变换由一组基的像<strong>唯一</strong>确定</li>
</ol>
<h2 id="线性变换的运算"><a href="#线性变换的运算" class="headerlink" title="线性变换的运算"></a>线性变换的运算</h2><p>对线性空间$U$和$V$的线性变换集合$L(U,V)$，$T _1,T _2 \in L(U,V)$，定义：</p>
<ul>
<li>$(T _1 + T _2)(\alpha)&#x3D;T _1(\alpha)+T _2 (\alpha)$</li>
<li>$k \in \mathbb{F}$，$(k\cdot T _1)(\alpha)&#x3D;k\cdot T _1(\alpha)$</li>
</ul>
<blockquote>
<p>$(T _1 + T _2)$和$(k\cdot T _1)$也是$U\to V$的线性变换。换句话说，集合$L(U,V)$的加法和数乘运算是封闭的。<br>零变换是零向量。<br>负变换是负函数。</p>
</blockquote>
<p>因此，集合$L(U,V)$是一个线性空间。</p>
<p>$T \in L(U,V),S\in L (V,W)$，定义：</p>
<p>$$<br>(S\cdot T)(\alpha)&#x3D;S(T(\alpha))<br>$$</p>
<p>类似于复合函数的定义。易证$S\cdot T$是$U\to W$的线性变换。</p>
<blockquote>
<p>上述复合运算的结合律成立，即$(T _1\cdot T _2)\cdot T _3&#x3D; T _1\cdot (T _2\cdot T _3)$。<br>分配律亦成立。但交换律不一定成立。</p>
</blockquote>
]]></content>
      <categories>
        <category>MATH6005: Matrix Theory</category>
      </categories>
      <tags>
        <tag>Math</tag>
      </tags>
  </entry>
  <entry>
    <title>MatrixTheory: Linear Space and Subspace</title>
    <url>/2023/09/24/MatrixTheory1/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="Linear-Space：线性空间"><a href="#Linear-Space：线性空间" class="headerlink" title="Linear Space：线性空间"></a>Linear Space：线性空间</h1><ul>
<li>定义：设$V$为一个非空集合，$\mathbb{F}$为一个数域，$ V$中定义了一个封闭的加法运算“$+$”，对$\forall \alpha,\beta,\gamma\in V$,满足：</li>
</ul>
<ol>
<li>结合律：$(\alpha+\beta)+\gamma&#x3D;\alpha+(\beta+\gamma)$</li>
<li>交换律：$\alpha+\beta&#x3D;\beta+\alpha$</li>
<li>存在<em>零向量</em>：$\alpha+0&#x3D;\alpha$</li>
<li>存在<em>负向量</em>：$\alpha+(-\alpha)&#x3D;0$</li>
</ol>
<blockquote>
<p>满足2、3、4的集合称<em>群</em>，满足1的<em>群</em>称<em>交换群</em>或<em>阿贝尔群</em>。</p>
</blockquote>
<p>定义了另一个封闭的数乘运算“$\cdot$”，对$\forall \alpha,\beta\in V,k _1,k _2\in\mathbb{F}$,满足：</p>
<ol>
<li>数乘的结合律：$k _1\cdot(k _2\cdot\alpha)&#x3D;(k _1k _2)\cdot\alpha$</li>
<li>数乘关于向量加法的分配律：$k _1\cdot(\alpha+\beta)&#x3D;k _1\cdot\alpha+k _1\cdot\beta$</li>
<li>数乘关于数的加法的分配律：$(k _1+k _2)\cdot\alpha&#x3D;k _1\cdot\alpha+k _2\cdot\alpha$</li>
<li>数乘的初始条件：$1\cdot\alpha&#x3D;\alpha$</li>
</ol>
<p>则称$ V$为数域$\mathbb{F}$上的线性空间。</p>
<blockquote>
<p>若某个运算的结果仍处于运算子所在的集合，则该运算是封闭的。对不同的线性空间，其内部的基本元素可以是任意的，如标量、向量、矩阵、函数等。但是对于一个确定的线性空间，其内部的元素类型必须是一致的。</p>
<p>加法和数乘只是一个符号或者说法，它们不一定是真的“$+$”和“$\cdot$”。</p>
</blockquote>
<h2 id="Proof-of-Linear-Space：线性空间的证明（重要）"><a href="#Proof-of-Linear-Space：线性空间的证明（重要）" class="headerlink" title="Proof of Linear Space：线性空间的证明（重要）"></a>Proof of Linear Space：线性空间的证明（重要）</h2><p>要想证明某个集合$V$是数域$\mathbb{F}$上的线性空间，只需要干两件事：</p>
<ol>
<li>证明加法和数乘运算的封闭性；</li>
<li>证明上面的<strong>8</strong>项规则成立。</li>
</ol>
<h2 id="Properties-of-Linear-Space：线性空间的性质"><a href="#Properties-of-Linear-Space：线性空间的性质" class="headerlink" title="Properties of Linear Space：线性空间的性质"></a>Properties of Linear Space：线性空间的性质</h2><ol>
<li><em>零向量</em>是唯一的。</li>
<li><em>负向量</em>是唯一的。</li>
<li>$\forall k\in\mathbb{F},\alpha\in V$，$k\cdot\alpha&#x3D;0$当且仅当$k&#x3D;0$或$\alpha&#x3D;0$。</li>
</ol>
<blockquote>
<p>以上三个性质都可用反证法证明。</p>
</blockquote>
<h1 id="Subspace：子空间"><a href="#Subspace：子空间" class="headerlink" title="Subspace：子空间"></a>Subspace：子空间</h1><ul>
<li>定义：$ V$是数域$\mathbb{F}$上的线性空间，$U\subset V$，若$U$对$V$上定义的两个运算仍构成$\mathbb{F}$上的线性空间，则称$U$为$V$的子空间。</li>
</ul>
<p>要判别一个集合$V$的子集$U$是否是原集合所构成线性空间的子空间，我们不再需要逐一地证明<strong>8</strong>项规则，只需要证明：</p>
<ol>
<li>加法和数乘运算在集合$U$中封闭；</li>
<li>集合$U$不是空集。</li>
</ol>
<p>只要证明了上面这两项成立，则<strong>8</strong>项规则自动成立。这是因为：$U\subset V$，所以只要$U$中有元素，那么它们就自动地满足结合、交换律，<em>负向量</em>也相应地存在；若$U$的加法、数乘运算还是封闭的，那么由向量与其<em>负向量</em>的加法又可得出<em>零向量</em>存在，由此，<strong>8</strong>项规则成立。</p>
<h1 id="Intersection-Space"><a href="#Intersection-Space" class="headerlink" title="Intersection Space"></a>Intersection Space</h1><ul>
<li>对线性空间$V$的两个子空间$U$和$W$，若它们的交集非空，则其交空间$U\cap W$也是$V$的子空间。</li>
</ul>
<blockquote>
<p>上述结论可以推广至$n$个子空间的交集。</p>
</blockquote>
<h1 id="Sum-Space"><a href="#Sum-Space" class="headerlink" title="Sum Space"></a>Sum Space</h1><ul>
<li>对线性空间$V$的两个子空间$U$和$W$，它们的和空间定义为：</li>
</ul>
<p>$$<br>U+W&#x3D;\{\alpha+\beta;\alpha\in U,\beta\in W\}<br>$$</p>
<p>和空间是$V$的子空间。</p>
<blockquote>
<p>上述结论可以推广至$n$个子空间的和。</p>
<p>要注意把和空间和并空间$U\cup W$区分开来。并空间并不一定是$V$的子空间，只有两个空间之间存在包含关系时其并空间才是$V$的子空间。</p>
</blockquote>
<h1 id="Direct-Sum"><a href="#Direct-Sum" class="headerlink" title="Direct Sum"></a>Direct Sum</h1><ul>
<li>定义：对线性空间$V$的两个子空间$U$和$W$，$\forall\alpha\in U+W$，即$\alpha&#x3D;\alpha _1+\alpha _2$，其中$\alpha _1\in U$，$\alpha _2 \in W$，若$\alpha _1$和$\alpha _2$唯一，则称这种和为<em>直和</em>，记作$U\oplus W$。</li>
</ul>
<p>直和也可以推广至$n$个子空间的和。我们有通用的方法来判定某个和是否是直和，但对于$n&#x3D;2$的情况，还有专门的特殊方法。</p>
<h2 id="Judgment-of-Direct-Sum：-n-ge-2"><a href="#Judgment-of-Direct-Sum：-n-ge-2" class="headerlink" title="Judgment of Direct Sum：$n\ge 2$"></a>Judgment of Direct Sum：$n\ge 2$</h2><p>对线性空间$V$的$n$个子空间$U _1,...,U _n$，它们的和$(U _1+...+U _n)$是直和当且仅当其和空间的<em>零向量</em>只能表示为$0&#x3D;0+...+0$。</p>
<blockquote>
<ul>
<li>必要性：<br>若$U _1+...+U _n$是直和$U _1\oplus ...\oplus U _n$，则其零<em>向量</em>的表示唯一。又$0&#x3D;0+...+0$，所以必要性显然成立。</li>
<li>充分性：<br>假设$\alpha\in U _1+...+U _n$，且$\alpha$存在两个表示$\alpha&#x3D;\alpha _1 + ... + \alpha _n$与$\alpha&#x3D;\beta _1 + .. + \beta _n$。令第二个表示取<em>逆向量</em>并与第一个表示相加，则有：<br>$$0&#x3D;(\alpha _1-\beta _1)+...+(\alpha _n - \beta _n)$$<br>因为$0$的表示唯一，所以：$\alpha _1 &#x3D;\beta _1,...,\alpha _n &#x3D;\beta _n$。证毕。</li>
</ul>
</blockquote>
<h2 id="Judgment-of-Direct-Sum：-n-x3D-2"><a href="#Judgment-of-Direct-Sum：-n-x3D-2" class="headerlink" title="Judgment of Direct Sum：$n&#x3D; 2$"></a>Judgment of Direct Sum：$n&#x3D; 2$</h2><ul>
<li>若$n&#x3D;2$，则有特殊的判别法：</li>
</ul>
<p>对线性空间$V$的$2$个子空间$U _1,U _2$，它们的和$(U _1 + U _2)$是直和当且仅当其它们的交空间只有<em>零向量</em>，即$U \cap W &#x3D; \{0\}$。</p>
<blockquote>
<ul>
<li>必要性：<br>若$(U _1 + U _2)$是直和$U _1 \oplus U _2$，假设存在$\alpha \in U\cap W$，因为$U \cap W$是子空间，所以存在<em>负向量</em>使得：<br>$$0 &#x3D; \alpha + (-\alpha)$$<br>而$0&#x3D;0+0$，矛盾，所以必要性成立。</li>
<li>充分性：<br>若$U \cap W &#x3D; \{0\}$，假设$\alpha \in U\cap W$存在两个不同的表示$\alpha &#x3D; \alpha _1 + \beta _1$和$\alpha &#x3D; \alpha _2 + \beta _2$，则：<br>$$\alpha _1 - \alpha _2 &#x3D; \beta _2 - \beta _1$$<br>易知$\alpha _1 - \alpha _2\in U$，$\beta _2 - \beta _1\in W$，所以它们是$U$和$W$的公共部分，而$U \cap W&#x3D;0$，所以只能$\alpha _1&#x3D;\alpha _2$，$\beta _1 &#x3D; \beta _2$。</li>
</ul>
</blockquote>
<p>对$n&gt;2$，仅两两相交为<em>零向量</em>不行。</p>
]]></content>
      <categories>
        <category>MATH6005: Matrix Theory</category>
      </categories>
      <tags>
        <tag>Math</tag>
      </tags>
  </entry>
  <entry>
    <title>Ridge Regression</title>
    <url>/2023/09/22/RidgeRegression/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="向量求导"><a href="#向量求导" class="headerlink" title="向量求导"></a>向量求导</h1><p>更确切地说是标量对向量求导，即，对：</p>
<p>$$<br>x\in\mathbb{R}^n,f:\mathbb{R}^{n}\to \mathbb{R}<br>$$</p>
<p>求标量$f(x)$对向量$x$的导数。与其一个个地展开，下列两个性质可以帮助我们快速地求解标量对向量的导数以及二阶导的海森矩阵：</p>
<ol>
<li>$\frac{\partial f(x)}{\partial v}&#x3D;v^T\nabla f(x)$</li>
</ol>
<blockquote>
<p>其中$v$是方向向量，即$f(x)$对$v$的方向导数是$v^T\nabla f(x)$，这是很显然的，因为在二维的方向导数我们就有$\frac{\partial f}{\partial l}&#x3D;f _x\cos \alpha+f _y \cos \beta$。推广到$n$维，$\cos\alpha...\cos\gamma$就是$v$的各个元素。</p>
<ul>
<li>例1：求$f(x)&#x3D;x^T\mathbf{A}x$的梯度（$\mathbf{A}$是对称矩阵）：<br>$$<br>\begin{align*}<br>  \frac{\partial f(x)}{\partial v}&#x3D;v^T\nabla f(x)<br>  &amp;&#x3D;\lim_{t\to 0}\frac{f(x+tv)-f(x)}{t} \\<br>  &amp;&#x3D;\lim_{t\to 0}\frac{(x+tv)^T\mathbf{A}(x+tv)-x^T\mathbf{A}x}{t}\\<br>  &amp;&#x3D;\lim_{t\to 0}\frac{tv^t\mathbf{A}x+x^T\mathbf{A}tv}{t}\\<br>  &amp;&#x3D;v^t\mathbf{A}x+x^T\mathbf{A}v\\<br>  &amp;&#x3D;2v^t\mathbf{A}x<br>\end{align*}$$<br>由于$v$的任意性，$\nabla f(x)&#x3D;2\mathbf{A}x$</li>
<li>例2：$f(\mathbf{\beta})&#x3D;||\mathbf{X}\beta-y||^2$，求其对$\mathbf{\beta}$的梯度。<br>$$<br>\begin{align*}<br>  \frac{\partial f(\beta)}{\partial v}&#x3D;v^T\nabla f(\beta)<br>  &amp;&#x3D;\lim_{t\to 0}\frac{f(\beta+tv)-f(\beta)}{t} \\<br>  &amp;&#x3D;\lim_{t\to 0}\frac{||\mathbf{X}(\beta+tv)-y||^2-||\mathbf{X}\beta-y||^2}{t}\\<br>  &amp;&#x3D;\lim_{t\to 0}\frac{||\mathbf{X}\beta-y+\mathbf{X}tv||^2-||\mathbf{X}\beta-y||^2}{t}\\<br>  &amp;&#x3D;\lim_{t\to 0}\frac{||\mathbf{X}tv||^2+2\sum((\mathbf{X}\beta-y)\odot (\mathbf{X}tv))}{t}\\<br>  &amp;&#x3D;2\sum((\mathbf{X}\beta-y)\odot (\mathbf{X}v))\\<br>  &amp;&#x3D;2(\mathbf{X}v)^T(\mathbf{X}\beta-y)\\<br>  &amp;&#x3D;2v^T\mathbf{X}^T(\mathbf{X}\beta-y)<br>\end{align*}$$<br>故$\nabla f(\beta)&#x3D;2\mathbf{X}^T(\mathbf{X}\beta-y)$</li>
</ul>
</blockquote>
<ol start="2">
<li>$\nabla ^2f(x)\cdot v&#x3D;\frac{\partial \nabla f(x)}{\partial v}$</li>
</ol>
<blockquote>
<p>其中$\nabla ^2f(x)$是海森矩阵$\mathbf{H}$，$\mathbb{H} _{[i,j]}&#x3D;\frac{\partial ^2 f}{\partial x _i \partial x_j}$。不难看出，$\nabla ^2f(x)\cdot v$是一个向量，其第$i$个元素为：$(\frac{\partial ^2 f}{\partial x _i \partial x_1},\frac{\partial ^2 f}{\partial x _i \partial x_2},...,\frac{\partial ^2 f}{\partial x _i \partial x_n})\cdot v$。</p>
<p>若令$g(x _i)&#x3D;\frac{\partial f}{\partial x _i}$，则第$i$个元素实际上就是$\frac{\partial g(x _i)}{\partial v}$。</p>
<p>继续推广，令$g(x)&#x3D;(\frac{\partial f}{\partial x _1},\frac{\partial f}{\partial x _2},...,\frac{\partial f}{\partial x _n})&#x3D;\nabla f(x)$，则有$\nabla ^2f(x)\cdot v&#x3D;\frac{\partial g(x)}{\partial v}&#x3D;\frac{\partial \nabla f(x)}{\partial v}$。</p>
<ul>
<li>例3：求$f(x)&#x3D;x^T\mathbf{A}x$的海森矩阵（$\mathbf{A}$是对称矩阵）：<br>$$<br>\begin{align*}<br>  \nabla ^2f(x)\cdot v&#x3D;\frac{\partial \nabla f(x)}{\partial v}<br>  &amp;&#x3D;\lim_{t\to 0}\frac{\nabla f(x+tv)-\nabla f(x)}{t} \\<br>  &amp;&#x3D;\lim_{t\to 0}\frac{2\mathbf{A}(x+tv)-2\mathbf{A}x}{t}\\<br>  &amp;&#x3D;\lim_{t\to 0}\frac{2\mathbf{A}tv}{t}\\<br>  &amp;&#x3D;2\mathbf{A}v<br>\end{align*}<br>$$<br>由于$v$的任意性，$\nabla  ^2 f(x)&#x3D;2\mathbf{A}$</li>
</ul>
</blockquote>
<h1 id="矩阵求导"><a href="#矩阵求导" class="headerlink" title="矩阵求导"></a>矩阵求导</h1><p>前面提到的向量求导的性质可以很容易地推广到矩阵求导（标量对矩阵求导），对：</p>
<p>$$<br>A\in\mathbb{R}^{n\times m},f:\mathbb{R}^{n\times m}\to \mathbb{R}<br>$$</p>
<p>有：</p>
<ol>
<li>$\frac{\partial f(\mathbf{A})}{\partial D}&#x3D;&lt; D,\nabla f(\mathbf{A}) &gt;$</li>
<li>$\nabla ^2f(\mathbf{A})\cdot D&#x3D;\frac{\partial \nabla f(\mathbf{A})}{\partial D}$</li>
</ol>
<p>其中，$D$是一个矩阵，$D\in \mathbb{R} ^{n\times m}$。由于内积是各元素相乘求和，故实际上$v^T\nabla f(x)$也可以表示为$&lt; v,  \nabla f(x)&gt;$。性质$1$是显然的，只要把$D$拉伸为一个长$n\times m$的方向向量即可。</p>
<blockquote>
<ul>
<li>例4：$f(\mathbf{A})&#x3D;\text{tr}(\mathbf{AB})$，求其对$\mathbf{A}$的梯度。<br>$$<br>\begin{align*}<br>  \frac{\partial f(\mathbf{A})}{\partial D}&#x3D;&lt; D,\nabla f(\mathbf{A}) &gt;<br>  &amp;&#x3D;\lim_{t\to 0}\frac{f(\mathbf{A}+tD)-f(\mathbf{A})}{t} \\<br>  &amp;&#x3D;\lim_{t\to 0}\frac{\text{tr}[(\mathbf{A}+tD)\mathbf{B}]-\text{tr}(\mathbf{AB})}{t}\\<br>  &amp;&#x3D;\lim_{t\to 0}\frac{\text{tr}(tD\mathbf{B})}{t}\\<br>  &amp;&#x3D;\text{tr}(D\mathbf{B})\\<br>  &amp;&#x3D;&lt; D,\mathbf{B}^T &gt;<br>\end{align*}<br>$$<br>故$\nabla f(\mathbf{A})&#x3D;\mathbf{B}^T$。最后的等式成立因为$\text{tr}(D\mathbf{B})&#x3D;&lt;D_1,{\mathbf{B} ^T} _1&gt;+...+&lt;D_m,{\mathbf{B} ^T} _m&gt; &#x3D;&lt; D,\mathbf{B}^T &gt;$，其中$D_i$表$D$的第$i$个行向量。</li>
</ul>
</blockquote>
<h1 id="Ridge-Regression：岭回归"><a href="#Ridge-Regression：岭回归" class="headerlink" title="Ridge Regression：岭回归"></a>Ridge Regression：岭回归</h1><p>岭回归（英文名：ridge regression, Tikhonov regularization）是一种专用于共线性数据分析的有偏估计回归方法，实质上是一种改良的最小二乘估计法，通过放弃最小二乘法的无偏性，以损失部分信息、降低精度为代价获得回归系数更为符合实际、更可靠的回归方法，对病态数据的拟合要强于最小二乘法。</p>
<blockquote>
<p>共线性数据是指一些相互间存在比例关系的数据，这些数据的存在会导致输入参数$\mathbf{X}^T\mathbf{X}$的行列式为0，进而导致其不可逆。</p>
</blockquote>
<p>例2中$f(\mathbf{\beta})&#x3D;||\mathbf{X}\beta-y||^2$实际上就是对分类问题的最小二乘估计函数。令其对$\beta$的梯度为0，我们就可以得到参数$\beta$的精确值：</p>
<p>$$<br>\begin{align*}<br>\nabla f(\beta)&#x3D;2&amp;\mathbf{X}^T(\mathbf{X}\beta-y)&#x3D;0\\\<br>&amp;\downarrow\\<br>\hat{\beta}&#x3D;&amp;(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^Ty<br>\end{align*}<br>$$</p>
<p>但是，由于上面提到的问题，$(\mathbf{X}^T\mathbf{X})^{-1}$不一定存在，因而上述式子也就不一定成立，且仅仅一阶导为0我们不能判定此时的$\beta$就是极小值点。岭回归就是为了解决这一问题而提出的。对岭回归，有两种合理的解释，其中第一种从特征值的角度出发，第二种从惩罚项（正则）的角度出发。两种解释的结果是一样的，但是第二种解释与神经网络中的正则项联系更加紧密。</p>
<h2 id="特征值角度"><a href="#特征值角度" class="headerlink" title="特征值角度"></a>特征值角度</h2><p>由：</p>
<p>$$<br>y^T\mathbf{X}^T\mathbf{X}y&#x3D;(\mathbf{X}y)^T(\mathbf{X}y)\ge 0<br>$$</p>
<p>可知，$\mathbf{X}^T\mathbf{X}$是半正定矩阵，也就是说，其特征值$\lambda _i \ge 0$。要让$\mathbf{X}^T\mathbf{X}$可逆，我们只要使得其特征值$\lambda _i &gt; 0$即可。故，我们将$\hat{\beta}$更改为：</p>
<p>$$<br>\hat{\beta}(k)&#x3D;(\mathbf{X}^T\mathbf{X}+k\mathbf{I})^{-1}\mathbf{X}^Ty,\quad k&gt;0\tag{1}<br>$$</p>
<p>由此，我们可以确保$\mathbf{X}^T\mathbf{X}$是正定矩阵，也就可以保证其可逆。</p>
<h2 id="惩罚项角度"><a href="#惩罚项角度" class="headerlink" title="惩罚项角度"></a>惩罚项角度</h2><p>回到最小二乘函数$f(\mathbf{\beta})&#x3D;||\mathbf{X}\beta-y||^2$，我们想从中确定一个$\hat{\beta}$，使得$f(\beta)$取最小值，即：</p>
<p>$$<br>\hat{\beta}&#x3D;\text{arg}\min _{\beta}||\mathbf{X}\beta-y||^2<br>$$</p>
<p>使用原始式子我们可以得到：</p>
<p>$$<br>\hat{\beta}&#x3D;(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^Ty<br>$$</p>
<p>而有些极端情况下$\mathbf{X}^T\mathbf{X}$的行列式即便不为0也是一个接近0的极小值。这将导致$(\mathbf{X}^T\mathbf{X})^{-1}$的行列式极大、内部元素的差异大，而这将间接地导致$\beta$数值的不稳定（这里面的关系不太懂）。为了限制$\beta$的波动，使其更稳定，我们需要对$\beta$的取值进行限制，即对其过大的取值进行一定程度的惩罚：</p>
<p>$$<br>\hat{\beta}&#x3D;\text{arg}\min _{\beta}||\mathbf{X}\beta-y||^2+\lambda ||\beta||^2\tag{2}<br>$$</p>
<p>式$(2)$中新加入的项就是惩罚项，实际上就是深度学习中的L2正则项，对上式求导并令梯度为0可得：</p>
<p>$$<br>\begin{align*}<br>2\mathbf{X}^T(\mathbf{X}\beta-y)&amp;+2\lambda \beta&#x3D;0\\<br>\downarrow&amp;\\<br>\hat{\beta}&#x3D;(\mathbf{X}^T\mathbf{X}+&amp;\lambda\mathbf{I}) ^{-1}\mathbf{X}^Ty\tag{3}<br>\end{align*}<br>$$</p>
<p>式$(3)$与式$(1)$完全一致。</p>
<h2 id="极小值验证"><a href="#极小值验证" class="headerlink" title="极小值验证"></a>极小值验证</h2><p>进一步地，我们要验证$\hat{\beta}$就是极小值点，即我们要证明：</p>
<p>$$<br>\nabla ^2f(\beta)&#x3D;\nabla ^2 (||\mathbf{X}\beta-y||^2+\lambda ||\beta||^2)<br>$$</p>
<p>是正定矩阵。这可以由向量求导的性质2得出：</p>
<p>$$<br>\begin{align*}<br>    \nabla ^2f(\beta)\cdot v&#x3D;\frac{\partial \nabla f(x)}{\partial v}<br>    &amp;&#x3D;\frac{\partial 2\mathbf{X}^T(\mathbf{X}\beta-y)+2\lambda \beta}{\partial v} \\<br>    &amp;&#x3D;\lim_{t\to 0}\frac{2\mathbf{X}^T(\mathbf{X}(\beta+tv)-y)+2\lambda (\beta+tv)-2\mathbf{X}^T(\mathbf{X}\beta-y)-2\lambda \beta}{t}\\<br>    &amp;&#x3D;\lim_{t\to 0}\frac{2\mathbf{X}^T\mathbf{X}tv+2\lambda tv}{t}\\<br>    &amp;&#x3D;2\mathbf{X}^T\mathbf{X}v+2\lambda v\\<br>    &amp;&#x3D;(2\mathbf{X}^T\mathbf{X}+2\lambda\mathbf{I})v<br>\end{align*}<br>$$</p>
<p>即$\nabla ^2f(\beta)&#x3D;2\mathbf{X}^T\mathbf{X}+2\lambda\mathbf{I}$，易知这是正定矩阵。所以$\hat{\beta}$就是我们要求的极小值点。</p>
<h2 id="岭回归的收缩性"><a href="#岭回归的收缩性" class="headerlink" title="岭回归的收缩性"></a>岭回归的收缩性</h2><p>有定理能够证明：</p>
<p>$$<br>||\hat{\beta}(k)||\le ||\hat{\beta}||<br>$$</p>
<p>是严格成立的。事实上，岭回归的作用在于：保留蕴含信息量多的维度（参数），而剔除包含信息量较少的维度（参数）。这表现为某些不重要的参数$\beta _i$的数值接近0。</p>
<h1 id="LASSO"><a href="#LASSO" class="headerlink" title="LASSO"></a>LASSO</h1><p>LASSO采用的惩罚项是L1正则，即$\lambda \sum _{i&#x3D;1} ^{p}|\beta _i|$。惩罚项的不同使得LASSO表现出与岭回归不同的性质，如：</p>
<ul>
<li>LASSO会使得一些参数项变为0，让原参数向量变<strong>稀疏</strong>，而岭回归只会让参数向量收缩，并让一些接近0。</li>
</ul>
<p>它们之间的区别可以用下面的图表示：</p>
<p><img src="/2023/09/22/RidgeRegression/1.png" alt="1"></p>
<center style="font-size:12px; font-weight:bold">Fig. 1. Ridge Regression</center>

<p><img src="/2023/09/22/RidgeRegression/2.png" alt="2"></p>
<center style="font-size:12px; font-weight:bold">Fig. 2. LASSO</center><br>

<p>其中，上图为岭回归中的参数随$\lambda$的变化，下图为LASSO。横坐标均为$\lambda$，负轴方向为$\lambda$增大的方向。之所以会有这些的区别，是因为：若把岭回归和LASSO的最小二乘函数都视作拉格朗日函数，并将其惩罚项视作对$\beta$的约束条件，则其求解可转化为在约束条件下求$||\mathbf{X}\beta-y||^2$的最小值。最终的取值点表现为约束边界与$||\mathbf{X}\beta-y||^2$所确定等高线的切点：</p>
<p><img src="/2023/09/22/RidgeRegression/3.png" alt="3"></p>
<center style="font-size:12px; font-weight:bold">Fig. 3. Lagrange</center><br>

<p>而由于LASSO约束边界的不光滑性，切点更容易落在让某个参数分量为0的尖点上。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://www.bilibili.com/video/BV1Bg4y1i76R">手把手教会机器学习与数据挖掘理论：岭回归（Ridge Regression）与LASSO</a></li>
</ul>
]]></content>
      <tags>
        <tag>Math</tag>
      </tags>
  </entry>
  <entry>
    <title>Computational Complexity: Time Complexity 3</title>
    <url>/2023/09/22/ComputationalComplexityTimecom3/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="Time-Hierarchy-Theorem：时间谱系理论"><a href="#Time-Hierarchy-Theorem：时间谱系理论" class="headerlink" title="Time Hierarchy Theorem：时间谱系理论"></a>Time Hierarchy Theorem：时间谱系理论</h1><p>理论：如果$f$和$g$都是时间可构造的，且$f(n)\log f(n)&#x3D;o(g(n))$，则$\text{TIME}(f(n))\subsetneq\text{TIME}(g(n))$。</p>
<blockquote>
<p>小o表示算法运行时间的<strong>严格上限</strong>，$f&#x3D;o(g)$表示$f$运行的上限是$g$，换句话说，$f$的运行时间比$g$增长得慢。</p>
</blockquote>
<p>该理论表明若$g$的运行时间增长快于$f\log f$运行时间的增长，则$f(n)$时间内能接受的语言是$g(n)$时间内能接受语言的子集。该理论的证明如下：</p>
<blockquote>
<p>要证明$\text{TIME}(f(n))\subsetneq\text{TIME}(g(n))$，只要反证$\text{TIME}(g(n))\subsetneq\text{TIME}(f(n))$不成立即可。也就是说只要找到一个语言L，$L\in\text{TIME}(g(n))$，但是$L\notin\text{TIME}(f(n))$即可。</p>
<p>假设存在TM $\mathbb{D}$，它接受语言L，并且：</p>
<ul>
<li>对于输入$x$，$\mathbb{D}$模拟$\mathbb{M} _x(x)$运行$g(|x|)$步（这是可行的，因为$g$是时间可构造的）；</li>
<li>如果模拟可以在$g(|x|)$步内完成（即停机），则$\mathbb{D}(x)$输出$\overline{\mathbb{M} _x(x)}$，否则输出$0$（$1$也行，不影响）。<br>按照这样定义，$\mathbb{D}$会在$g(|x|)$停机，因此$L\in\text{TIME}(g(n))$。</li>
</ul>
<p>此时再假设$L\in\text{TIME}(f(n))$，且图灵机$\mathbb{M} _z$在至多$2f(n)$内接受L。由于图灵机编码的任意性，取足够长的$z$，使得$f(|z|)\log f(|z|)&lt; g(|z|)$严格成立，则会出现：</p>
<ul>
<li>$\mathbb{D}(z)&#x3D;\mathbb{M} _z(z)$，因为$\mathbb{D}$和$\mathbb{M} _z$都接受L；</li>
<li>$\mathbb{D}(z)&#x3D;\overline{\mathbb{M} _z(z)}$，因为$\mathbb{M} _z(z)$可被通用图灵机在$O(f(|z|)\log f(|z|))$的时间内模拟，而$\mathbb{D}(x)$会模拟$\mathbb{M} _z (z)$进行$g(|x|)$步，又$f(|z|)\log f(|z|)&lt; g(|z|)$严格成立，所以在$\mathbb{D}(x)$模拟$\mathbb{M} _z (z)$进行的$g(|x|)$步内，$\mathbb{D}(x)$已经停机，即$\mathbb{D}(z)&#x3D;\overline{\mathbb{M} _z(z)}$。</li>
</ul>
<p>上述结论相互矛盾，原假设不成立，$L\notin\text{TIME}(f(n))$。证毕。</p>
</blockquote>
<p>上面的证明用到了对角线方法。</p>
<p>由时间谱系定理：</p>
<p>$$<br>\begin{align*}<br>    \text{EXP}&#x3D;&amp;\bigcup _{c\ge 1}\text{TIME}(2^{n^c}) \\<br>    \text{2EXP}&#x3D;&amp;\bigcup _{c\ge 1}\text{TIME}(2 ^{2^{n^c}}) \\<br>    \vdots<br>\end{align*}<br>$$</p>
<p>存在严格的包含关系，即$\text{EXP}\subset \text{2EXP}...$</p>
<h1 id="Nondeterministic-Time-Hierarchy-Theorem：非确定时间谱系理论"><a href="#Nondeterministic-Time-Hierarchy-Theorem：非确定时间谱系理论" class="headerlink" title="Nondeterministic Time Hierarchy Theorem：非确定时间谱系理论"></a>Nondeterministic Time Hierarchy Theorem：非确定时间谱系理论</h1><p>理论：如果$f$和$g$都是时间可构造的，且$f(n+1)&#x3D;o(g(n))$，则$\text{NTIME}(f(n))\subsetneq\text{NTIME}(g(n))$。</p>
<p>设有非确定图灵机$\mathbb{Z}$，只要证明$\mathbb{Z}$判定的语言L在$\text{NTIME}(g(n))$但不在$\text{NTIME}(f(n))$中即可。$\mathbb{Z}$的定义如下：</p>
<ol>
<li><strong>只</strong>处理长度大于1的全1串$1 ^n$；</li>
<li>输入带和<strong>第一条工作带</strong>同步地移动，第一条工作带在第一个工作区写入1，然后在后续的工作区中持续写入0，期间偶尔写入1；</li>
<li>用$h _0, h _1...$记录下所有写有1的工作区的下标；</li>
<li>在<strong>第二条工作带</strong>上，枚举所有的和时钟$2f(n)$硬连接的非确定图灵机NDTMs（的编码），记这些图灵机为$\mathbb{L} _1,\mathbb{L} _2...$；</li>
<li>在生成非确定图灵机$\mathbb{L} _i$后，$\mathbb{Z}$模拟计算$\mathbb{L} _{i-1}(1 ^{h _{i -1}+1})$，并将结果（0或1）写入第二条工作带上，<strong>同时</strong>，在<strong>第一条工作带</strong>的$h _i$位置写上1。<blockquote>
<p>注意，因为$h _0&#x3D;1$，所以$\mathbb{L} _0(1 ^{h _0+1})$是一开始就可以确定的，不需要等第一条工作带运行。<br>$\mathbb{L} _{i-1}(1 ^{h _{i -1}+1})$中的$\mathbb{L} _{i-1}$和$1 ^{h _{i-1}+1}$都通过回溯之前的历史得到，回溯的时间复杂度为$O(n)$，因为$h _{i -1}$是单调递增的，故只需要局部回溯。而$\mathbb{L} _{i-1}(1 ^{h _{i -1}+1})$则用新的工作带（写入$\mathbb{L} _{i-1}$与$1 ^{h _{i-1}+1}$）、草稿带来计算（在$2f(n)$必内停机）。</p>
</blockquote>
</li>
</ol>
<p>$\mathbb{Z}$的输出规定为：</p>
<ol>
<li>如果在输入串$1 ^n$扫描结束时，最新的$h _i &#x3D; n$且$\mathbb{L} _{i-1}(1 ^{h _{i -1}+1})&#x3D;0$，则$\mathbb{Z}$接受$1 ^n$；</li>
<li>反之如果$h _{i-1} &lt; n &lt; h _i$，则$\mathbb{Z}$模拟$\mathbb{L} _{i-1}$以非确定图灵机的方式计算$1 ^{n+1}$$g(n)$步，并以其结果作为最后的输出。</li>
</ol>
<p>矛盾性的证明：</p>
<blockquote>
<p>假设语言L被$\mathbb{Z}$接受：</p>
<ul>
<li>则L$\in \text{NTIME}(g(n))$，因为$\mathbb{Z}$运行的时间复杂度只由完全扫描完输入后的计算时间决定，而输出规定2可在$g(n)$时间内完成（输出规定1为常量时间）。</li>
</ul>
<p>再假定$\exists \mathbb{L} _i$在$f(n)$时间内接受L。于是，因为$f(n+1)&#x3D;o(g(n))$，所以输出规定2中$\mathbb{Z}$对$\mathbb{L} _i$（此处的$i$指输入扫描结束后计算的最后一个$h$的前一个）的模拟可以被完成。故，由于$\mathbb{L} _i$和$\mathbb{Z}$都接受L：<br>$$\mathbb{L} _i(1 ^{h _i + 1})&#x3D;\mathbb{Z}(1 ^{h _i + 1})$$<br>又由输出规定2：对输入串的长度$m&#x3D; h_i+1$，$\mathbb{Z}$的输出结果与$\mathbb{L} _i(1 ^{m+1})&#x3D;\mathbb{L} _i(1 ^{h _i + 1+1})$的结果相同：<br>$$\mathbb{Z}(1 ^{h _i + 1})&#x3D;\mathbb{L} _i(1 ^{h _i + 2})$$<br>又由两者都接受L：<br>$$\mathbb{L} _i(1 ^{h _i + 2})&#x3D;\mathbb{Z}(1 ^{h _i + 2})$$<br>若输入长度一直这样增加下去，则总会有一个时刻，$h _i +k&#x3D;h _{i+1}$，此时：<br>$$\mathbb{L} _i(1 ^{h _{i+1}})&#x3D;\mathbb{Z}(1 ^{h _{i+1}})$$<br>若$n&#x3D;h _{i+1}$，则由输出规定1：<br>$$\mathbb{Z}(1 ^{n})&#x3D;\mathbb{Z}(1 ^{h _{i+1}})\ne \mathbb{L} _i (1 ^{h _i+1})$$<br>出现矛盾，因此不存在这样的$\mathbb{L} _i$在$f(n)$时间内接受L。证毕。</p>
</blockquote>
<h1 id="Gap-Theorem：间隙定理"><a href="#Gap-Theorem：间隙定理" class="headerlink" title="Gap Theorem：间隙定理"></a>Gap Theorem：间隙定理</h1><p>描述：对任何一个可计算函数$r(x)\ge x$，一定存在一个可计算函数$b(x)$，使得$\text{TIME}(b(x))&#x3D;\text{TIME}(r(b(x)))$。</p>
<p>时间谱系定理告诉我们$\text{TIME}(n ^c)\subsetneq \text{TIME}(2 ^{n ^c})$。而间隙定理说明，并不是对所有的$b(n)$，$\text{TIME}(b(n))\subsetneq \text{TIME}(2 ^{b(n)})$都成立，即有可能$\text{TIME}(b(n))&#x3D;\text{TIME}(2 ^{b(n)})$。证明如下：</p>
<blockquote>
<ul>
<li>定义一个$r(x)\ge x$，以及$[0,r(k _x)]$上的$x+1$个不相交区间$[k _0, r(k _0)],...,[k _x, r(k _x)]$，其中：<br>$$<br>\begin{align*}<br>  k _0&#x3D;&amp;0 \\<br>  k _{i+1}&#x3D;&amp; r(k _i)+1,i &lt; x<br>\end{align*}$$</li>
<li>再定义$P(i,k)$为：对所有的$j \le i$及其编码的图灵机$\mathbb{M} _j$，对长度为$i$的字符串输入$z$，若$\mathbb{M} _j(z)$在$k$步内停机或者在$r(k)$步仍未停机，则$P(i,k)&#x3D;\text{True}$；否则$P(i,k)&#x3D;\text{False}$。</li>
</ul>
<p>对这些图灵机$\mathbb{M} _0,...,\mathbb{M} _i$，它们能够接受总输入个数为：<br>$$n _i&#x3D;\sum ^i _{j&#x3D;0}|\Gamma _j| ^i$$<br>因此它们的运行时间有 $n _i$种可能。而$[0,r(k _{n _i})]$上有$n _i+1$个不相交区间$[k _0, r(k _0)],...,[k _{n _i}, r(k _{n _i})]$。也就是说，总会有那么几个区间，在这些区间，没有任何的$\mathbb{M} _j(z)$运行步数在它们的范围内。换句话说，在这些区间的左端点$k _m$上，$P(i,k _m)$是$\text{True}$的。<br>由此，我们可以定义一个函数$b(i)$，对任何一个$i$，它的值是这些区间中的某一个的左端点，即$b(i)&#x3D;k _m$。于是我们有$P(i,b(i))$对于所有的$i$都成立。</p>
<ul>
<li>进一步地，假设$\mathbb{M} _j$在$r(b(n))$的时间内接受L。</li>
</ul>
<p>则对于任何的输入$x$，若其长度$n \ge j$，由$P(n,b(n))&#x3D;\text{True}$，我们有：<br>$\mathbb{M} _j(x)$要么在$b(n)$步内停机，要么$r(b(n))$步还不停机。<br>然而，$\mathbb{M} _j$在$r(b(n))$的时间内接受L，所以$\mathbb{M} _j(x)$在$r(b(n))$步内一定已经停机，所以$\mathbb{M} _j(x)$在$b(n)$步内停机，所以$\mathbb{M} _j$在$b(n)$的时间内接受L，所以：<br>$$\text{TIME}(b(n))&#x3D;\text{TIME}(r(b(n)))$$</p>
</blockquote>
<p>我们可以确保$b(n)$和$r(b(n))$满足$f(n)$和$g(n)$的大小关系，因为$r(x)$可以是任意的，只要$r(x)\ge x$即可。但$b(n)$却不满足时间谱系定理，这说明这样的$b(n)$是时间不可构造的。</p>
]]></content>
      <categories>
        <category>CS7313: Computational Complexity</category>
      </categories>
      <tags>
        <tag>Math</tag>
        <tag>Theoretical Computer Science</tag>
      </tags>
  </entry>
  <entry>
    <title>Computational Complexity: Time Complexity 2</title>
    <url>/2023/09/21/ComputationalComplexityTimecom2/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="Diagonal-Method：对角线方法"><a href="#Diagonal-Method：对角线方法" class="headerlink" title="Diagonal Method：对角线方法"></a>Diagonal Method：对角线方法</h1><p>通用图灵机$\mathbb{U}(\alpha, x)$能够模拟$\mathbb{M} _\alpha (x)$的计算，因此，我们可以枚举每一个$\alpha$和$x$的编码，并制定如下所示的表：</p>
<p><img src="/2023/09/21/ComputationalComplexityTimecom2/1.jpg" alt="1"></p>
<center style="font-size:12px; font-weight:bold">Fig. 1. UTM</center><br>

<p>在上表中，每列从上到下枚举TM的编码 ，每行从左到右枚举输入$x$，使用UTM可以计算出每一格内的值。右表是一个假想的结果，其中$\uparrow$表示死循环。而这个表中，对角线上的值是一个特殊值，因为在对角线上，$\alpha&#x3D;x$，即UTM计算的是$\mathbb{M} _\alpha(\alpha)$。利用对角线做反证法，可以证明很多有趣的结论，这就是所谓的对角线方法（Diagonal Method）。</p>
<blockquote>
<p>通俗地来说，对角线方法就是用自己来推翻自己。</p>
</blockquote>
<p>下面给出了两个应用对角线方法的案例。</p>
<h2 id="UC：不可计算函数"><a href="#UC：不可计算函数" class="headerlink" title="UC：不可计算函数"></a>UC：不可计算函数</h2><p>定义函数：</p>
<p>$$<br>\text{UC}(\alpha) &#x3D;<br>\begin{cases}<br>    0,&amp; \text{if}\space \mathbb{M} _{\alpha} (\alpha)&#x3D;1\\<br>    1,&amp; \text{otherwise}<br>\end{cases}<br>$$</p>
<p>利用对角线方法可以证明上述函数是不可解的，也就是说没有图灵机能够求解该函数：</p>
<blockquote>
<p>假设存在图灵机$\mathbb{W}$能够计算UC，那么我们有：<br>$$<br>UC(\alpha) &#x3D; \mathbb{M} _{\llcorner \mathbb{W} \lrcorner}(\alpha)<br>$$<br>若：<br>$$<br>\mathbb{M} _{\llcorner \mathbb{W} \lrcorner}(\llcorner \mathbb{W} \lrcorner)&#x3D;1<br>$$<br>则：<br>$$<br>UC(\llcorner \mathbb{W} \lrcorner)&#x3D;\mathbb{M} _{\llcorner \mathbb{W} \lrcorner}(\llcorner \mathbb{W} \lrcorner)&#x3D;0<br>$$<br>同理，若$\mathbb{M} _{\llcorner \mathbb{W} \lrcorner}(\llcorner \mathbb{W} \lrcorner)&#x3D;0$，我们也会推出$\mathbb{M} _{\llcorner \mathbb{W} \lrcorner}(\llcorner \mathbb{W} \lrcorner)&#x3D;1$。两者互相矛盾。<br>因此，不存在这么一个图灵机，也就是说UC是不可计算的。</p>
</blockquote>
<h2 id="HALT：停机问题"><a href="#HALT：停机问题" class="headerlink" title="HALT：停机问题"></a>HALT：停机问题</h2><p>HALT，停机问题是这样定义的：</p>
<p>$$<br>\text{HALT}(\alpha,x) &#x3D;<br>\begin{cases}<br>    1,&amp; \text{if}\space \mathbb{M} _{\alpha} (x)\space \text{terminates}\\<br>    0,&amp; \text{otherwise}<br>\end{cases}<br>$$</p>
<p>停机问题可以用UC函数来证明：</p>
<blockquote>
<p>假设存在$\mathbb{M} _H$计算了停机问题，则对于任意的$\alpha$都有：<br>$$<br>\mathbb{M} _H(\alpha,\alpha)&#x3D;\text{HALT}(\alpha,\alpha) &#x3D;<br>\begin{cases}<br>    1,&amp; \text{if}\space \mathbb{M} _{\alpha} (\alpha)\space \text{terminates}\\<br>    0,&amp; \text{otherwise}<br>\end{cases}<br>$$<br>于是我们可以定义一个新的函数$\text{MU}(\alpha)$：<br>$$<br>\text{MU}(\alpha) &#x3D;<br>\begin{cases}<br>    0,&amp; \text{if}\space \mathbb{M} _{H} (\alpha,\alpha)&#x3D;1\space\&amp;\space\mathbb{M} _{\alpha} (\alpha)&#x3D;1\\<br>    1,&amp; \text{otherwise}<br>\end{cases}<br>$$<br>在上述函数的定义下，如果$\mathbb{M} _{\alpha} (\alpha)$停机了，那么其值要么是$1$，要么是$0$，其中$1$会使得$\text{MU}(\alpha)$输出$0$；如不停机，则$\text{MU}(\alpha)$会和$\mathbb{M} _{\alpha} (\alpha)$值为$1$一样输出$1$。换句话说，条件$ \mathbb{M} _{H} (\alpha,\alpha)&#x3D;1$在$\text{MU}$中并不发挥作用，只要$\mathbb{M} _{\alpha} (\alpha)&#x3D;1$，$\text{MU}(\alpha)$就只会输出$1$。于是：<br>$$<br>\text{MU}(\alpha) &#x3D;\text{UC}(\alpha)<br>$$<br>而UC是不可计算的，所以上面的所有证明都不成立，所以不存在这么一个图灵机计算了停机问题。</p>
</blockquote>
<p>停机问题不可计算说明，对于任意的图灵机$\mathbb{M} _\alpha$和输入$x$，在实际运行之前我们无法预先知道$\mathbb{M} _\alpha(x)$是否能停机。</p>
<blockquote>
<p>实际上上面的证明还用了归约法（Reduction Method），即通过放宽判定条件，使得某一问题$q$成为另一个问题$Q$的子问题。此时我们若证明$Q$不可计算，那么$q$也就相应地无法计算了。</p>
</blockquote>
<h1 id="Speedup-Theorem：加速理论"><a href="#Speedup-Theorem：加速理论" class="headerlink" title="Speedup Theorem：加速理论"></a>Speedup Theorem：加速理论</h1><p>加速理论解决的是对于某个问题，是否存在最快的算法的问题。直觉告诉我们，这个答案是否定的，因为只要我们有足够的空间，我们就可以空间换时间，通过“打表”把每一种输入的答案都列举出来，那么对于所有的问题我们都可以用<strong>常量的时间</strong>解决。而实际上也正是如此，没有最快的算法，只有更快的算法（理论上）。</p>
<blockquote>
<p><em>Blum’s Speedup Theorem</em>是关于此的很强的一个结论，但它的具体内容和证明都比较复杂，课程中没有涉及，本文也只记录关于线性加速理论的内容。</p>
</blockquote>
<h2 id="Linear-Speedup-Theorem：线性加速理论"><a href="#Linear-Speedup-Theorem：线性加速理论" class="headerlink" title="Linear Speedup Theorem：线性加速理论"></a>Linear Speedup Theorem：线性加速理论</h2><p>Hartmanis和Stearns于1965年证明了线性加速理论：</p>
<p><em>Linear Speedup Theorem：设图灵机$\mathbb{M}$在$T(n)$步内计算了$f$，任取$\epsilon &gt; 0$，总存在图灵机$\mathbb{M}&#39;$，它能在$\epsilon T(n)+n+2$步内计算$f$。</em></p>
<p>这里用到的方法是“压缩带子”，即通过编码，使得$\mathbb{M}&#39;$上的一格能够记录$\mathbb{M}$上$m$格的内容，这样$\mathbb{M}&#39;$的一步就相当于$\mathbb{M}$上的$m$步，其证明如下：</p>
<blockquote>
<p>对于$\mathbb{M}(\Gamma,Q,\delta)$，用一个新的符号集$\Gamma &#39;$为$\Gamma$中的$m$个字符用$1$个字符编码。不难看出，这样的符号集$\Gamma &#39;$是巨大的，其字符种类至少为$|\Gamma &#39;| &gt; |\Gamma|^m$。我们用$\Gamma &#39;$构建一个新的图灵机$\mathbb{M}&#39;(\Gamma &#39;,Q&#39;,\delta &#39;)$，对该图灵机：</p>
<ul>
<li>使用与$\mathbb{M}$相同的带子数；</li>
<li>对于$\mathbb{M}$纸带上长度为$n$的编码，$\mathbb{M}&#39;$可以在$n+2$步内将其转换为$n&#x2F;m$长的编码；（多出的2步是起始符和终止）</li>
<li>$\mathbb{M}&#39;$在$n&#x2F;m$步可以将所有的带头回归到原点；</li>
<li>$\mathbb{M}&#39;$纸带上的任何一个格子对应着$\mathbb{M}$上的$m$个格子。某一时刻$\mathbb{M}$在$m$个格子内<strong>任意一个格子</strong>上的$m$步在$\mathbb{M}&#39;$上至多会涉及到<strong>3</strong>个格子，因此$\mathbb{M}&#39;$只需要记录下这三个格子的值、做一步写入操作、移动一次带头，至多<strong>5</strong>步就可以模拟$\mathbb{M}$上的$m$步。<br>因此，总时间：<br>$$<br>n+2+\frac{n}{m}+5\frac{T(n)}{m}\le n+2 + \frac{6}{m}T(n)<br>$$<br>我们可以取$\epsilon&#x3D;6&#x2F;m$，$m&#x3D;6&#x2F;\epsilon$，证毕。</li>
</ul>
</blockquote>
<p><img src="/2023/09/21/ComputationalComplexityTimecom2/2.jpg" alt="2"></p>
<center style="font-size:12px; font-weight:bold">Fig. 2. 1 -> m</center>

<h1 id="Time-Complexity-Class：时间复杂性类"><a href="#Time-Complexity-Class：时间复杂性类" class="headerlink" title="Time Complexity Class：时间复杂性类"></a>Time Complexity Class：时间复杂性类</h1><p>对于语言L（即一个判定问题集合），如果存在图灵机$\mathbb{M}$和常数$c&gt;0$，使得$\mathbb{M}$能在$cT(n)$的时间内判定L，那么我们说$L\subseteq \text{TIME}(T(n))$。换句话说，$\text{TIME}(T(n))$中的语言都具有$O(T(n))$的算法。</p>
<p>复杂性类（Complexity Class）是一些具有类似复杂度的问题的集合，或者说是一些<strong>具有类似复杂度解</strong>的问题的集合。因为对于大多数的问题，我们都能用简单或复杂的方法将其解出来，换句话说，我们不能定义问题的复杂性，我们只能定义<strong>解的复杂性</strong>。显然，上面提到的$\text{TIME}(T(n))$不是一个复杂性类，因为它是和模型相关的。比如，对于回文问题，我们可以在$O(n)$的时间内解决，此时回文问题在$\text{TIME}(n)$中，但我们也可以用$O(n^2)$的方法解决，此时回文问题在$\text{TIME}(n^2)$中。</p>
<p>下面给出一些常见的时间复杂性类，这里面就有之前提到的P、NP问题。</p>
<h2 id="Complexity-Class-P"><a href="#Complexity-Class-P" class="headerlink" title="Complexity Class P"></a>Complexity Class P</h2><p>$\text{P}$类，即多项式类，所有具有多项式时间算法的问题都属于这一类：</p>
<p>$$<br>\text{P}&#x3D;\bigcup _{c\ge 1}\text{TIME}(n^c)<br>$$</p>
<p>在承认强CT论题的情况下，$\text{P}$类是与模型无关的，也就是说它是一个复杂性类。</p>
<blockquote>
<p>强CT论题（Strong Church-Turing Thesis）：所有物理上可实现的机器都能在多项式的时间复杂度内被图灵机模拟。</p>
</blockquote>
<h2 id="Complexity-Class-EXP"><a href="#Complexity-Class-EXP" class="headerlink" title="Complexity Class EXP"></a>Complexity Class EXP</h2><p>$\text{EXP}$类，即指数时间复杂性类：</p>
<p>$$<br>\text{EXP}&#x3D;\bigcup _{c\ge 1}\text{TIME}(2^{n^c})<br>$$</p>
<p>底数是无关紧要的，既可以是2，也可以是任何数字。出于规定，$\text{P}\subseteq\text{EXP}$，所以$n^c$不可以写成$cn$，否则集合内部的封闭性复合运算将不被满足。如：$n^3\in\text{P}$，所以$n^3\in\text{EXP}$，$2^n\in\text{EXP}$，那么将$n^3$和$2^n$复合得到的$2 ^{n^3}$理应也在$\text{EXP}$中。若将$n^c$写成$cn$则$2 ^{n^3}$将不被包含在$\text{EXP}$中，这显然是错误的。</p>
<h1 id="Nondeterministic-Turing-Machine：非确定图灵机"><a href="#Nondeterministic-Turing-Machine：非确定图灵机" class="headerlink" title="Nondeterministic Turing Machine：非确定图灵机"></a>Nondeterministic Turing Machine：非确定图灵机</h1><p>非确定性图灵机（Nondeterministic Turing Machine）具有两个迁移函数$\delta _1,\delta _2$，它在运行过程中会<strong>随机</strong>地选择一个迁移函数来执行，并产生相应的结果。不难看出，NDTM的计算路径可以被视为一棵二叉树，叶子结点表示某一条会停机并产生结果的执行路径，有些路径可能无限长：</p>
<p><img src="/2023/09/21/ComputationalComplexityTimecom2/3.jpg" alt="3"></p>
<center style="font-size:12px; font-weight:bold">Fig. 3. DTM and NDTM</center><br>

<p>上述的非确定图灵机在物理上是无法实现的，而之前所提到的物理上可实现的图灵机都称为确定图灵机（Deterministic Turing Machine，DTM）。</p>
<h2 id="Guessing：猜测"><a href="#Guessing：猜测" class="headerlink" title="Guessing：猜测"></a>Guessing：猜测</h2><p>NDTM通常被用来解决存在性问题。事实上，对于输入$x$，NDTM的每一条计算路径都在试图构造或者<strong>猜测</strong>一个对$x$存在性的证明，若构造&#x2F;猜测成功，则NDTM接受$x$，否则拒绝$x$。对于某一个决策问题或者语言，在NDTM中有如下规定：</p>
<ul>
<li>对于一台NDTM$\mathbb{N}$，当输入为$x$时，若$\mathbb{N}(x)$存在<strong>一条</strong>计算路径中止于接受格局（即$\mathbb{N}(x)&#x3D;1$），则称$\mathbb{N}$接受$x$；否则若所有的计算路径都失败，则称$\mathbb{N}$拒绝$x$。</li>
<li>对于一语言L，如果$x\in L$当且仅当$\mathbb{N}$接受$x$，则称$\mathbb{N}$接受L。</li>
</ul>
<blockquote>
<p>这实际上就是<strong>枚举</strong>，其中，有些解路径可能很短，能达到<strong>多项式的时间复杂度</strong>，而有些解路径可能很长，需要指数的时间复杂度。</p>
</blockquote>
<h2 id="Complexity-Class-NP-amp-NEXP"><a href="#Complexity-Class-NP-amp-NEXP" class="headerlink" title="Complexity Class NP &amp; NEXP"></a>Complexity Class NP &amp; NEXP</h2><p>类似于DTM，对于NDTM，我们也可以定义时间复杂性类：对于语言L（即一个判定问题集合），如果存在非确定图灵机$\mathbb{N}$和常数$c&gt;0$，使得$\mathbb{N}$能在$cT(n)$的时间内判定L，那么我们说$L\subseteq \text{NTIME}(T(n))$。换句话说，$\text{NTIME}(T(n))$中的语言都具有$O(T(n))$的算法。</p>
<p>同样地，定义非确定多项式（Non-deterministic Polynomial）复杂性类$\text{NP}$：</p>
<p>$$<br>\text{NP}&#x3D;\bigcup _{c\ge 1}\text{NTIME}(n^c)<br>$$</p>
<p>非确定指数复杂性类$\text{NEXP}$：</p>
<p>$$<br>\text{NEXP}&#x3D;\bigcup _{c\ge 1}\text{NTIME}(2^{n^c})<br>$$</p>
<p>它们与DTM的关系为：</p>
<p>$$<br>\text{P}\subseteq\text{NP}\subseteq\text{EXP}\subseteq\text{NEXP}<br>$$</p>
<ul>
<li>第一、三层包含是因为DTM是NDTM的特例，即$\delta_1&#x3D;\delta_2$；</li>
<li>第二层包含是因为DTM可以用指数的时间将NDTM的所有路径走一遍。</li>
</ul>
<blockquote>
<p>$\text{NP}$和$\text{P}$的区别在于，$\text{P}$的解的复杂度都是多项式的，而$\text{NP}$只是存在多项式复杂度的特解。能否证明$\text{P}&#x3D;\text{NP}$，是理论计算机科学界中的一大难题。</p>
</blockquote>
<h2 id="39-Universal-39-Nondeterministic-Turing-Machine：“通用”非确定图灵机"><a href="#39-Universal-39-Nondeterministic-Turing-Machine：“通用”非确定图灵机" class="headerlink" title="&#39;Universal&#39; Nondeterministic Turing Machine：“通用”非确定图灵机"></a>&#39;Universal&#39; Nondeterministic Turing Machine：“通用”非确定图灵机</h2><p>一个“通用”的非确定图灵机$\mathbb{V}$在模拟某个时间复杂度为$T(n)$的图灵机$\mathbb{M}$时需要四条带子：</p>
<ol>
<li>输入带；</li>
<li>存放snapshot的工作带，该工作带存放$\mathbb{V}$对$\mathbb{M}$执行时产生的所有snapshot的<strong>猜测</strong>，快照数为$O(T(n))$；</li>
<li>存放转移函数序列（即$01$串）的工作带，该工作带$01$串的长度也是$O(T(n))$，实际上就是$\delta_1$和$\delta_2$序列；</li>
<li>存放对被模拟工作带验证结果的工作带，它会记录一些snapshot，以验证对某个被模拟带子的模拟是否正确，若正确则清空，验证下一个带子。</li>
</ol>
<p>快照个数$O(T(n))$，$01$串的个数也是$O(T(n))$，故模拟的时间复杂度也是$T(n)$。这是在正确猜测的情况下，但实际上猜测序列、快照不一定是正确的，$T(n)$也不一定时间可构造，所以这台“通用”的非确定图灵机甚至有可能不停机，故“通用”是带引号的。若$T(n)$时间可构造则可掐表，以确保$\mathbb{V}$会停机。</p>
<blockquote>
<p>注意，以上的$T(n)$只对非确定图灵机的一条路径，即对非确定图灵机所“猜测”的任意一条路径，模拟它的时间复杂度可以到$T(n)$。</p>
</blockquote>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://zhuanlan.zhihu.com/p/348250098">计算复杂性（6）——世界难题：非确定型图灵机，NP类</a></li>
</ul>
]]></content>
      <categories>
        <category>CS7313: Computational Complexity</category>
      </categories>
      <tags>
        <tag>Math</tag>
        <tag>Theoretical Computer Science</tag>
      </tags>
  </entry>
  <entry>
    <title>Dataset Distillation: A Summary</title>
    <url>/2023/09/20/DDSummary/</url>
    <content><![CDATA[<span id="more"></span>

<p>文中大多数内容都基于Yu et al.的论文<a href="https://arxiv.org/pdf/2301.07014.pdf">Dataset Distillation: A Comprehensive Review</a>。</p>
<h1 id="Performance-Matching"><a href="#Performance-Matching" class="headerlink" title="Performance Matching"></a>Performance Matching</h1><p>Performance Matching，表现&#x2F;效果匹配，是最先被提出的一种基于机器学习的数据蒸馏方法。这种方法核心思想在于将<em>Synthetic Data</em>视作神经网络中的一个“超参数”，如学习率一般，但不同的是，它是可学习的超参数。其优化过程分两步：</p>
<ol>
<li>用<em>Synthetic Data</em>训练神经网络；</li>
<li>将原训练集$\mathcal{T}$输入用<em>Synthetic Data</em>训练的神经网络，计算得到预测的<em>Loss</em>，以该<em>Loss</em>作为损失函数更新<em>Synthetic Data</em>。</li>
</ol>
<p>最初的基于Performance Matching的方法的精度和训练速度都不怎么样，但后来有学者将第一步中的神经网络替换为了核函数，反而使得Performance Matching超过了多数后来的方法。</p>
<h2 id="Meta-Learning-Based-Methods"><a href="#Meta-Learning-Based-Methods" class="headerlink" title="Meta Learning Based Methods"></a>Meta Learning Based Methods</h2><p>该方法最先在Wang et al.的<a href="https://arxiv.org/pdf/1811.10959.pdf">DATASET DISTILLATION</a>中被提出，也是数据蒸馏开山立宗的方法，一般简称<em>DD</em>。<em>Meta Learning</em>也就是把<em>Synthetic Data</em>视作一个可学习的超参数。它获得<em>Synthetic Data</em>的思路就是上面所提到的，基本公式与流程如下：</p>
<p>$$<br>\begin{align*}<br>    \theta ^{(t)}&amp;&#x3D;\theta ^{(t-1)}-\eta\nabla l(\mathcal{S};\theta ^{(t-1)})\tag{1}\\<br>    \mathcal{S}^{(\tau)}&amp;&#x3D;\mathcal{S}^{(\tau-1)}-\eta&#39;\nabla L(\theta^{(T)},\mathcal{T})\tag{2}<br>\end{align*}<br>$$</p>
<p>其中，式$(1)$是内层循环，用于更新神经网络，式$(2)$是外层循环，用于更新<em>Synthetic Data</em>。</p>
<p><img src="/2023/09/20/DDSummary/1.png" alt="1"></p>
<center style="font-size:12px; font-weight:bold">Fig. 1. Meta Learning Based Methods</center><br>

<p>显而易见，这是一种<em>Bi-level Optimization Algorithm</em>：内层循环更新神经网络，外层循环更新<em>Synthetic Data</em>。而内层循环又需要用到<em>Synthetic Data</em>来产生梯度以训练网络，因而<em>Synthetic Data</em>的更新内在地会包含一个二阶导，这是极耗内存和时间的。而实际上蒸馏出来的数据效果也不是特别好。</p>
<h2 id="Kernel-Ridge-Regression-Based-Methods"><a href="#Kernel-Ridge-Regression-Based-Methods" class="headerlink" title="Kernel Ridge Regression Based Methods"></a>Kernel Ridge Regression Based Methods</h2><p>该方法最初在Nguyen et al.的<a href="https://arxiv.org/pdf/2011.00050.pdf">DATASET META-LEARNING FROM KERNEL RIDGEREGRESSION</a>中被提出，一般简称<em>KIP</em>。这种方法是建立在<strong>无限宽的全连接层可以用特殊的核函数来拟合</strong>这一研究成果的。<em>KIP</em>用到的核函数被称为<em>Neural Tangent Kernel</em>（NTK）。它本质上也是<em>Meta Learning</em>，只不过使用核函数来替代了内层循环，并用<em>Kernel Ridge Regression</em>计算损失函数，使得原本的双层优化变为了一层：</p>
<p>$$<br>\begin{align*}<br>    L(\mathcal{S},\mathcal{T})&amp;&#x3D;||Y _\mathcal{T}-K _{X _\mathcal{T}X _\mathcal{S}}(K _{X _\mathcal{S}X _\mathcal{S}}+\lambda I)^{-1}Y _\mathcal{S}||^2\\<br>    \mathcal{S}^{(\tau)}&amp;&#x3D;\mathcal{S}^{(\tau-1)}-\eta&#39;\nabla L(\mathcal{S},\mathcal{T})<br>\end{align*}<br>$$</p>
<p>当然，还有使用其他核函数的版本，但一般用的都是<em>NTK</em>，此处就不再赘述了。</p>
<blockquote>
<p>文中还提到了另一个脱胎于<em>Kernel Ridge Regression</em>的方法<a href="https://arxiv.org/pdf/2206.00719.pdf"><em>FRePo</em></a>，该方法在目前来说取得了最好的蒸馏效果。</p>
</blockquote>
<h1 id="Parameter-Matching"><a href="#Parameter-Matching" class="headerlink" title="Parameter Matching"></a>Parameter Matching</h1><p>Parameter Matching，参数匹配，这种方法的想法很直观：要想<em>Synthetic Data</em>取得和训练集相近的泛化精度，只要<em>Synthetic Data</em>训练出来的网路的参数（Parameter）和训练集训练出来的网络的参数相近即可。不过实际上并没有人会直接匹配参数，而是转而去匹配训练过程中的梯度，即让<em>Synthetic Data</em>产生和训练集相近的梯度，因而这类方法也可以叫<em>Gradient Matching</em>。</p>
<h2 id="Single-Step-Parameter-Matching"><a href="#Single-Step-Parameter-Matching" class="headerlink" title="Single-Step Parameter Matching"></a>Single-Step Parameter Matching</h2><p>Single-Step Parameter Matching在<a href="https://arxiv.org/pdf/2006.05929.pdf">DATASET CONDENSATION WITH GRADIENT MATCHING</a>中被Zhao et al.提出，是最先被提出的基于参数匹配的数据蒸馏方法，一般称<em>DC</em>。其核心思想很简单：</p>
<ol>
<li>训练集$\mathcal{T}$和<em>Synthetic Data</em> $\mathcal{S}$分别过一次神经网络；</li>
<li>分别计算梯度；</li>
<li>计算梯度差异，作为$Loss$；</li>
<li>更新<em>Synthetic Data</em>，用更新后的<em>Synthetic Data</em>更新神经网络。</li>
</ol>
<p>文中用到的作为$Loss$的梯度差异函数为$\cos$函数，即更关心梯度的方向：</p>
<p>$$<br>Loss(\mathcal{S},\mathcal{T})&#x3D;1-\frac{grad(\mathcal{S})\cdot grad(\mathcal{T})}{||grad(\mathcal{S})||\space||grad(\mathcal{T})||}<br>$$</p>
<p>上面的$grad$是指输入数据在神经网络上所产生的梯度。基本流程如下：</p>
<p><img src="/2023/09/20/DDSummary/2.png" alt="2"></p>
<center style="font-size:12px; font-weight:bold">Fig. 2. Meta Learning Based Methods</center><br>

<blockquote>
<p>后续有不少学者在损失函数$Loss$上做文章，如不仅关心方向，还关心大小（欧式距离）等，此处就不再赘述了。</p>
</blockquote>
<h2 id="Multi-Step-Parameter-Matching"><a href="#Multi-Step-Parameter-Matching" class="headerlink" title="Multi-Step Parameter Matching"></a>Multi-Step Parameter Matching</h2><p>有一步梯度匹配，就会有多步梯度匹配。Multi-Step Parameter Matching最先在<a href="https://arxiv.org/pdf/2203.11932.pdf">Dataset Distillation by Matching Training Trajectories</a>中被Cazenavette et al.提出，一般简称<em>MTT</em>。在Kernel Ridge Regression Based Methods出现之前，这是表现最好的方法。其本质上是对Single-Step的一种优化，因为单步的匹配本质上是一种<em>贪心</em>的算法，它不一定能够得到全局的最优，而用训练集的多步梯度下降来匹配<em>Synthetic Data</em>的一步或两步梯度下降能够为更新<em>Synthetic Data</em>带来更加全局的、稳定的优化路径。方法示意图如下：</p>
<p><img src="/2023/09/20/DDSummary/3.png" alt="3"></p>
<center style="font-size:12px; font-weight:bold">Fig. 3. Multi-Step Parameter Matching</center><br>

<p>此外，文中用到的损失函数也不同，作者使用的损失函数是规范化了的欧式距离（的平方）：</p>
<p>$$<br>Loss(\mathcal{S},\mathcal{T})&#x3D;\frac{||\theta ^{\mathcal{S}} _{t+N}-\theta ^{\mathcal{T}} _{t+M}||^2 _2}{||\theta ^{\mathcal{T}} _{t}-\theta ^{\mathcal{T}} _{t+M}||^2 _2}<br>$$</p>
<p><em>MTT</em>的另一大优点是蒸馏速度很快，因为其训练集训练的网络都是预先训练好的，这些预训练好的网络被称为<em>buffer</em>或者<em>teacher net</em>。后续也有不少的学者对<em>MTT</em>进行了优化，如：</p>
<ul>
<li><a href="https://arxiv.org/pdf/2211.11004.pdf"><em>FTD</em></a>：优化了<em>buffer</em>，使得生成的<em>buffer</em>能够提供更接近于<em>Synthetic Data</em>训练网络的优化路径；</li>
<li><a href="https://arxiv.org/pdf/2209.14609.pdf"><em>DDPP</em></a>：优化了更新<em>Synthetic Data</em>的时机，即预先评估本次更新<em>Synthetic Data</em>能够带来的收益，根据收益大小来决定是否要执行本次更新。评估的方式是为$\theta ^{\mathcal{T}} _{t+M}&#x2F;\theta ^{\mathcal{S}} _{t+N}$设置一个阈值$\epsilon$，当两者的比例小于该阈值时就不执行本次<em>Synthetic Data</em>的更新。</li>
<li>...</li>
</ul>
<h1 id="Distribution-Matching"><a href="#Distribution-Matching" class="headerlink" title="Distribution Matching"></a>Distribution Matching</h1><p>Distribution Matching，分布匹配，核心思想是让<em>Synthetic Data</em>的统计分布尽可能地接近训练集。接近程度的衡量指标一般用的是<em>Maximum Mean Discrepancy</em>（MMD），即用<em>高阶矩</em>来衡量两个分布的接近程度，而高级矩的计算则常以通过神经网络将输入特征映射到高维空间后，计算高维空间向量距离的方式进行。这种方法最大的优点就是蒸馏速度很快。</p>
<h2 id="Single-Layer-Distribution-Matching"><a href="#Single-Layer-Distribution-Matching" class="headerlink" title="Single Layer Distribution Matching"></a>Single Layer Distribution Matching</h2><p>该方法是分布匹配最初的方法，由Zhao et al.在<a href="https://arxiv.org/pdf/2110.04181.pdf">Dataset Condensation with Distribution Matching</a>提出，一般简称<em>DM</em>。思路与前文提到的一致。其计算高阶矩所采用的神经网络是原来用于分类任务的神经网络，不过去掉了softmax操作。基本流程为：</p>
<p><img src="/2023/09/20/DDSummary/4.png" alt="4"></p>
<center style="font-size:12px; font-weight:bold">Fig. 4. Single Layer Distribution Matching</center>

<h2 id="Multi-Layer-Distribution-Matching"><a href="#Multi-Layer-Distribution-Matching" class="headerlink" title="Multi Layer Distribution Matching"></a>Multi Layer Distribution Matching</h2><p>该方法由Wang et al.在<a href="https://arxiv.org/pdf/2203.01531.pdf">Cafe: Learning to condense dataset by aligning features</a>上提出，一般简称<em>CAFE</em>。实际上是对<em>DM</em>的优化，主要的不同点在于：<em>DM</em>只对整个神经网络的输出向量进行匹配，而<em>CAFE</em>则是对每一个隐藏层的输出向量进行匹配并相加。本质上差不多，没有跳出Distribution Matching的框架。</p>
]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>Paper</tag>
        <tag>Dataset Distillation</tag>
      </tags>
  </entry>
  <entry>
    <title>Computational Complexity: Time Complexity 1</title>
    <url>/2023/09/12/ComputationalComplexityTimeComp/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="Notation：记号约定"><a href="#Notation：记号约定" class="headerlink" title="Notation：记号约定"></a>Notation：记号约定</h1><ul>
<li>$\log x$默认为$\log _2x$。</li>
<li>若$S$是有限的符号集，如$\left\{0,1\right\}$，则$S ^n$表示所有长度为$n$的由$S$中字符组成的字符串集合，$S ^*$表示任意长度的字符串集合。对于单个符号$x$，$x ^n$则表示$n$个$x$组成的字符串。</li>
<li>字符串$x$的长度记为$|x|$或$n$，这一节及后续的相关章节都会用$n$。</li>
<li>记$\llcorner x\lrcorner$为$x$的二进制编码。</li>
<li>记$1^n$为长度为$n$的任意$01$串。</li>
</ul>
<h1 id="Turing-Machine：图灵机模型"><a href="#Turing-Machine：图灵机模型" class="headerlink" title="Turing Machine：图灵机模型"></a>Turing Machine：图灵机模型</h1><p>图灵机（Turing Machine），又称图灵计算机，指一个抽象的机器，是英国数学家艾伦・麦席森・图灵于1936年提出的一种抽象的计算模型，即将人们使用纸笔进行数学运算的过程进行抽象，由一个虚拟的机器替代人类进行数学运算。它有多条无限长的纸带，纸带分成了一个一个的小方格，每个方格有不同的内容。有一个机器头在纸带上移来移去。机器头有一组内部状态，还有一些固定的程序。在每个时刻，机器头都要从当前纸带上读入一个方格信息，然后结合自己的内部状态查找程序表，根据程序输出信息到纸带方格上，并转换自己的内部状态，然后进行移动。</p>
<h2 id="K-Tap-Turing-Machine"><a href="#K-Tap-Turing-Machine" class="headerlink" title="$K$-Tap Turing Machine"></a>$K$-Tap Turing Machine</h2><p>$k$带图灵机$\mathbb{M}$有$k$条纸带，其中：</p>
<ul>
<li>第一条纸带是<strong>只读纸带</strong>（Input Tape），它包含了输入图灵机的问题或数据规模；</li>
<li>其他纸带是<strong>可读写</strong>的工作纸带（Work Tape），其中最后一条纸带又同时作为记录最后输出的纸带（Output Tape）；</li>
<li>各纸带的<strong>带头</strong>（Tape Head）指明了正在读&#x2F;写的纸带信息。</li>
</ul>
<p><img src="/2023/09/12/ComputationalComplexityTimeComp/1.png" alt="1"></p>
<center style="font-size:12px;font-weight:bold">Fig. 1. K-tape turing machine</center><br>

<blockquote>
<p>不难看出，图灵机模拟了人类解题时<strong>读题（Input）</strong>、<strong>打草稿（Work）</strong>和<strong>写答案（Output）</strong>的过程。</p>
</blockquote>
<p>一个$k$带的图灵机$\mathbb{M}$可由一个三元组$(\Gamma,Q,\delta)$唯一描述：</p>
<ul>
<li>$\Gamma$是一个有限的符号集，称<em>Alphabet</em>。我们规定，符号集必须只由$\left\{0,1,\square,\triangleright\right\}$组成，其中$\left\{0,1\right\}$是有效字符，而$\left\{\square,\triangleright\right\}$则是分别表示<em>空白字符</em>和<em>纸带开始</em>的功能字符；</li>
<li>$Q$是一个有限状态集，包含了该图灵机的所有状态，包括起始状态$q _{start}$和停机状态$q _{halt}$；</li>
<li>$\delta$是一个转移函数（Transition Function），实际上相当于烧至在图灵机中的一个<em>程序</em>。</li>
</ul>
<p>图灵机当前的状态（State）会被记录在一个状态寄存器$q$中。在任何时刻$t$，图灵机都能够获取图灵机在这个时刻的快照（Snapshot）：每个纸带的符号$\Gamma ^k$以及图灵机当前的状态$q _t$，记该快照为$Q\times\Gamma ^k$，那么转移函数$\delta$便可指挥图灵机进入新的快照$Q\times\Gamma ^{k-1}\times\left\{L,S,R\right\} ^k$，即：</p>
<p>$$<br>Q\times\Gamma ^k\to Q\times\Gamma ^{k-1}\times\left\{L,S,R\right\} ^k<br>$$</p>
<p>其中，$Q\times\Gamma ^{k-1}\times\left\{L,S,R\right\} ^k$分别表示“图灵机进入什么状态，$k-1$个读写纸带写入什么内容，$k$个带头分别怎么移动”。我们规定，带头一次只能向左或向右移动一个位置或者不移动。图灵机初始化时，状态为$q _{start}$，每个带头都在左边，带子上都写着$\triangleright$，除输入带以外，其他带子的其他位置都写着$\square$。</p>
<p>图灵机在每个时刻的信息除了快照，还有格局（Configuration），格局拥有比快照更加丰富的内容：</p>
<ul>
<li>图灵机当前的状态；</li>
<li>所有纸带的内容；</li>
<li>当前带头的位置。</li>
</ul>
<p>不难看出，格局包含了一次转移函数计算所需要的全部内容（带头位置结合纸带内容唯一确定了当前纸带的符号），因此，一次转移函数计算就是由<strong>一个格局到另一个格局</strong>的过程，这也称一次计算（Computation）。</p>
<blockquote>
<p>格局不仅与$\mathbb{M}$有关，还与输入串的长度有关，而快照则只和$\mathbb{M}$有关。</p>
</blockquote>
<h2 id="Function、Problem、Language：函数、问题、语言"><a href="#Function、Problem、Language：函数、问题、语言" class="headerlink" title="Function、Problem、Language：函数、问题、语言"></a>Function、Problem、Language：函数、问题、语言</h2><p><strong>一个问题就是一个函数</strong>。对于任何一个问题，它都有解决的前提条件以及根据这些条件得到的问题答案，而函数接受输入，得到输出，两者在本质上是具有同一性的。在图灵机中，任何问题的前提条件（规模、数据结构、数据等）都可以用有限个$01$串表示，记为$\left\{0,1\right\}^*$。</p>
<p>故而，<strong>问题</strong>（Problem）就是一个<strong>函数</strong>（Function）：</p>
<p>$$<br>f:\left\{0,1\right\}^* \to<br> \left\{0,1\right\}^*<br>$$</p>
<p>这个函数实现了对问题的回答，即由问题的$01$串表示得到对这个问题答案的$01$串表示。</p>
<p>我们称图灵机$\mathbb{M}$计算&#x2F;解决（Compute&#x2F;Solve）了问题$f$，当且仅当对于任意的$x\in\left\{0,1\right\}^*$均有$\mathbb{M}(x)&#x3D;f(x)$。其中$\mathbb{M}(x)$的含义是：如果$x$在$f$的定义域内，当给图灵机$\mathbb{M}$的输入带上写上$x$并进行计算时，它最终会停机，且输出带上写着$\mathbb{M}(x)$；如果$x$不在$f$的定义域内，则图灵机永不停机（死循环）。图灵机会停机记为$\mathbb{M}(x)\downarrow$，永不停机记为$\mathbb{M}(x)\uparrow$。</p>
<h3 id="Decision-Problem：判定问题"><a href="#Decision-Problem：判定问题" class="headerlink" title="Decision Problem：判定问题"></a>Decision Problem：判定问题</h3><p>判定问题是一个特殊的问题，它的输出是个布尔值，即只有$0$或$1$：</p>
<p>$$<br>d:\left\{0,1\right\}^* \to<br> \left\{0,1\right\}<br>$$</p>
<p>对于判定问题，我们不再用解决问题，而称图灵机$\mathbb{M}$判定（Decide）了问题$d$。</p>
<h3 id="Language：语言"><a href="#Language：语言" class="headerlink" title="Language：语言"></a>Language：语言</h3><p>语言（Language）$L$是一个字符串集合$L\subseteq \left\{0,1\right\}^*$。若：</p>
<p>$$<br>\begin{align*}<br>    &amp;\forall x\in L,\mathbb{M}(x)&#x3D;1\\<br>    &amp;\forall x\notin L,\mathbb{M}(x)&#x3D;0<br>\end{align*}<br>$$<br>我们则称图灵机$\mathbb{M}$接受（Accept）了语言$L$。换句话说，图灵机$\mathbb{M}$能够识别出$L$及其子集。</p>
<h3 id="语言与判定问题"><a href="#语言与判定问题" class="headerlink" title="语言与判定问题"></a>语言与判定问题</h3><p>不难看出，一个语言就是一类判定问题：</p>
<p>$$<br>d(x)&#x3D;<br>\begin{cases}<br>    1,&amp;x\in L\\<br>    0,&amp;x\notin L<br>\end{cases}<br>$$</p>
<h2 id="Time-Function：时间函数"><a href="#Time-Function：时间函数" class="headerlink" title="Time Function：时间函数"></a>Time Function：时间函数</h2><p>时间函数与时间复杂度有着近乎相同的含义，它们都不是现实世界中连续的时间，而是图灵&#x2F;计算机世界中离散的时间。对于时间函数，其单位是一步计算，即一次转移函数；对时间复杂度，其单位是计算机的一次基本操作，如加、乘等。由于我们并不能知道图灵机$\mathbb{M}$在解决某个问题时转移函数的执行次数，我们就需要一个计时器，这个计时器也是一个图灵机$\mathbb{T}$，它与$\mathbb{M}$同启同停，并在结果输出$\mathbb{M}$转移函数执行的次数。这样的计时器定义的函数就称为时间函数：</p>
<p>$$<br>T: N \to N<br>$$</p>
<p>它实现问题的规模（输入串的<strong>长度</strong>$n$）到转移函数执行步数$T(n)$的映射。时间函数定义了图灵机解决某一问题时间的上限，即：若图灵机$\mathbb{M}$解决输入长度为$n$的$f$需要<strong>至多</strong>$T(n)$步，那么我们称$\mathbb{M}$<strong>在</strong>$T(n)$<strong>的时间内解决了</strong>$f$。</p>
<h3 id="Time-Constructible-Function：时间可构造函数"><a href="#Time-Constructible-Function：时间可构造函数" class="headerlink" title="Time Constructible Function：时间可构造函数"></a>Time Constructible Function：时间可构造函数</h3><p>用图灵机$\mathbb{T}$实现时间函数要求：</p>
<ol>
<li>图灵机$\mathbb{T}$的执行步数为$T(n)$；</li>
<li>图灵机$\mathbb{T}$的输出结果$\mathbb{T}(n)$为$T(n)$。</li>
</ol>
<p>如，若定义$T(n)&#x3D;n^2$，则该图灵机必须在$n^2$步由初始状态$\llcorner n\lrcorner$转变为停止状态$\llcorner n^2 \lrcorner$，然而这样的图灵机是不一定存在的。</p>
<p>时间可构造函数是为了尽量避免上述的问题而采用的退而求其次的方法。它不同于时间函数的地方在于：</p>
<ol>
<li>时间可构造函数$T$不要求输入串为$\llcorner n \lrcorner$，任意的$01$串$1^n$均可；</li>
<li>时间可构造函数$T$不要求步数严格为$T(n)$，只要是同一个数量级即可，即$O(T(n))$即可。</li>
</ol>
<p>只要存在这么一个$1 ^n$，使得图灵机$\mathbb{T}$能在$O(T(n))$步下转移到$\llcorner T(n) \lrcorner$状态并且停机，那么我们就称$T$是可构造时间函数。若将条件2升级为时间函数的条件，则称$T$为完全时间可构造的（Fully Time Constructible Function），即$T$转移到$\llcorner T(n) \lrcorner$状态的步数严格为$T(n)$。</p>
<h2 id="计算复杂性与模型无关"><a href="#计算复杂性与模型无关" class="headerlink" title="计算复杂性与模型无关"></a>计算复杂性与模型无关</h2><p>在不同的模型、不同的图灵机上，计算同一个函数$f$所花的时间可能不同。但是直观上来看，问题&#x2F;函数的复杂性是其客观性质，应该与模型无关。</p>
<ul>
<li><em>强邱奇-图灵论题（Church-Turing Thesis）</em>：任何物理上可以实现的计算装置A，都可以被图灵机以多项式的代价模拟。即，A的$t$个步骤可以用图灵机的$t^c$个步骤模拟，其中$c$是常数，与A有关（即取决于将A编码的最简单方式）。直观地理解，只要正确地编码，简单的图灵机可以以时间为代价模拟复杂的图灵机。<em>另一种说法：物理上能实现的机器，其最大表达能力不会超过图灵机</em>。</li>
</ul>
<blockquote>
<p>Oblivious Turing Machine（遗忘图灵机）：这种图灵机的读写头的位置只与输入字符串的长度$n$、当前步数$i$有关。换句话说，无论输入的字符串是什么，只要长度一致，那么它们在第$i$步时，读写头的位置就是固定的。</p>
</blockquote>
<h1 id="Turing-Machine-as-String：图灵机的编码"><a href="#Turing-Machine-as-String：图灵机的编码" class="headerlink" title="Turing Machine as String：图灵机的编码"></a>Turing Machine as String：图灵机的编码</h1><p>计算理论的核心：二进制位串可以编码任何有限的语法对象，如程序、图灵机、算法。一台图灵机是一个有限对象，因此也可以用$01$串对它进行编码。由于字母表$\Gamma$和状态集$Q$可以直接从转移函数$\delta$中获取，要编码一台图灵机，只要对它的所有转移函数进行编码即可。</p>
<blockquote>
<p>转移函数实际上就是计算机指令。</p>
</blockquote>
<p>如，对于一台TM：$|\Gamma|\le 32$，$|Q|\le 4$，设其中的一条指令$\delta$为：</p>
<p>$$<br>&lt;q_1, a, b, c&gt;\to &lt;q_2, e, d, L, S, R&gt;<br>$$</p>
<p>则，我们可以用5位$01$串对字母表进行编码，用4位$01$串对状态码进行编码（不用2位是为了更好区分？），用2位$01$串对带头的移动指令进行编码，再引入分隔符号$-$，可得到该指令的编码：</p>
<p>$$<br>0001- 00001- 00010- 00011-- 0010- 00100 - 00101 - 10 - 00 - 01<br>$$</p>
<p>记上面的指令为$\delta _i$。对于一台图灵机的全部指令，再用一个新的分隔符$&#x3D;$隔开，在将指令串起，就得到整台图灵机的编码：</p>
<p>$$<br>\delta _1&#x3D;\delta _2&#x3D; ...&#x3D;\delta _n<br>$$</p>
<p>上述编码形式中，我们一共使用了4种符号$\{0,1,-,&#x3D;\}$。此时，我们在将这些符号映射为$01$串：</p>
<p>$$<br>\{0,1,-,&#x3D; \}\to\{01,10,00,11\}<br>$$</p>
<p>我们便得到了一台图灵机$\mathbb{M}$的$01$串形式的编码。</p>
<blockquote>
<p>由于此处打不出十字<code>\dag</code>和双十字<code>\ddag</code>，故用$-$和$&#x3D;$代替。</p>
<p>任何一台图灵机，只要我们知道它的转移函数，都可以用这种方式进行编码。</p>
</blockquote>
<h2 id="图灵机编码的性质"><a href="#图灵机编码的性质" class="headerlink" title="图灵机编码的性质"></a>图灵机编码的性质</h2><ol>
<li>每一台图灵机都有无数个$01$串编码。</li>
</ol>
<p>这点很好理解。一台图灵机代表一种解决某个问题的方式。对于一台解决特定问题的图灵机，我们完全可以为其加上许多<strong>多余的、无用的</strong>状态或符号，此时图灵机的编码变长了，但是它还是在用相同的方法解决完全一样的问题，因而我们将前后两台图灵机视作<strong>同一台图灵机</strong>。</p>
<ol start="2">
<li>每一个$01$串编码都对应这一台图灵机。</li>
</ol>
<p>由于$01$串编码的任意性，某个$01$串对应的图灵机可能完全没有实际用处，如对于任意输入都直接停机的空图灵机。</p>
<p>上述的两条规定有一定的方便性。它们使得我们可以将所有的图灵机及其对应的函数列举出来：</p>
<ul>
<li>记$\llcorner \mathbb{M}\lrcorner$为某台图灵机的编码；</li>
<li>记$\mathbb{M} _\alpha$为编码$\alpha$所对应的图灵机；</li>
<li>记一个特殊的双射函数将任意的$01$串映射为一个自然数：$\{ 0,1 \}^* \to N$。</li>
</ul>
<p>由此，我们可以得到所有图灵机的排列：</p>
<p>$$<br>\mathbb{M}_0,\mathbb{M}_1,...,\mathbb{M}_i,...<br>$$</p>
<p>以及其对应的所有可计算函数的排列：</p>
<p>$$<br>\phi_0,\phi_1,...,\phi _i,...<br>$$</p>
<blockquote>
<p>由于一台图灵机有无数的$01$串编码，在这些排列中，每台图灵机都会出现无数次。</p>
<p>一个函数是可计算的当且仅当有一台图灵机能实现这个函数。</p>
</blockquote>
<h1 id="Universal-Turing-Machine：通用图灵机（套娃开始）"><a href="#Universal-Turing-Machine：通用图灵机（套娃开始）" class="headerlink" title="Universal Turing Machine：通用图灵机（套娃开始）"></a>Universal Turing Machine：通用图灵机（套娃开始）</h1><p>如上所述，任何一台图灵机都可以用$01$串编码，而输入图灵机的数据的形式也是$01$串，那么，我们就可以把图灵机的$01$串形式<strong>输入到另一台图灵机中，这台图灵机是一台特殊的图灵机，它可以模拟输入图灵机的运行</strong>。这样的图灵机就称<strong>通用图灵机</strong>（UTM）。事实上，计算机就是这样一台通用图灵机。因为我们在计算机上所编写的每一个程序都可以被视为一台图灵机，而任何一个正确的程序（只要有相应的编码器）都可以在现代的计算机上运行。换句话说，现代的计算机能够模拟任何一台图灵机的运行。</p>
<ul>
<li><p>高效通用图灵机定理：存在通用图灵机$\mathbb{U}$，使得下面两个式子定理成立：</p>
<ol>
<li>对于任意的$x,\alpha \in \{0,1\} ^*$，均有$\mathbb{U}(\alpha,x)\simeq\mathbb{M}_\alpha(x)$；</li>
<li>若$\mathbb{M}_\alpha(x)$的时间函数是$T(n)$，则$\mathbb{U}(\alpha,x)$的时间函数是$cT(n)\log T(n)$，其中$c$是$\alpha$的多项式$|\alpha| ^c$，但是与$\alpha,x$均无关。</li>
</ol>
<p>通俗地来讲，对于任意一台图灵机$\mathbb{M}_\alpha$，通用图灵机$\mathbin{U}$都可以模拟它的计算过程，且至多多耗费对数倍的时间。</p>
</li>
</ul>
<p>下面两小节是对于定理$2$的简单证明。</p>
<h2 id="Proof-of-Hartmanis-and-Stearns"><a href="#Proof-of-Hartmanis-and-Stearns" class="headerlink" title="Proof of Hartmanis and Stearns"></a>Proof of Hartmanis and Stearns</h2><p>Hartmanis和Stearns在1965年给出了通用图灵机$O(cT^2(n))$的证明。</p>
<p>用通用图灵机$\mathbb{U}$模拟一台$k$带的图灵机$\mathbb{M} _{\alpha}$，通用图灵机的输入带上给出图灵机的编码$\alpha$以及图灵机的输入$x$，我们想要得到的效果是通用图灵机的输出带上输出结果$\mathbb{M} _{\alpha}(x)$或者如$\mathbb{M} _\alpha$一样永不停机。构造这样的通用图灵机需要<strong>5</strong>条带子：</p>
<ol>
<li>输入带：输入为被模拟图灵机的编码$\alpha$和被模拟图灵机的输入$x$；</li>
<li>输出带：输出为$\mathbb{M}_{\alpha}(x)$；</li>
<li>模型工作带：记录$\mathbb{M} _{\alpha}$的转移函数；</li>
<li>状态工作带：记录$\mathbb{M} _{\alpha}$当前的工作状态；</li>
<li>主工作带：通用图灵机的带子数目必须是确定的，我们不能用不能确定数目的$k-2$条带子去模拟被模拟图灵机的工作带，因此，主工作带上记录了被模拟图灵机工作带及其带头的内容。</li>
</ol>
<p><img src="/2023/09/12/ComputationalComplexityTimeComp/2.jpg" alt="2"></p>
<center style="font-size:12px;font-weight:bold">Fig. 2. 5-tape universal turing machine</center><br>

<p>这样的通用图灵机工作过程是很明确的：</p>
<ol>
<li>从输入带、输出带、主工作带、状态工作带获取$\mathbb{M}_{\alpha}$当前的快照：$k$条带子的值以及当前状态$q$；</li>
<li>扫描模型工作带，获取转移函数$\delta$；</li>
<li>模拟$\mathbb{M} _{\alpha}$的转移函数，向主工作带、输出带写入$k-1$个值，更改状态工作带上的状态；</li>
<li>移动输入带、主工作带、输出带，以模拟$\mathbb{M} _{\alpha}$中$k$个带头的移动。</li>
</ol>
<h3 id="主工作带的构造"><a href="#主工作带的构造" class="headerlink" title="主工作带的构造"></a>主工作带的构造</h3><p>要使主工作带上能存放被模拟图灵机的$k-2$条工作带及其带头的信息，较好的方法是将主工作带横向划分为$k-2$层，每层存放一条带子的信息，并让带子双向无限延伸（线性代价）、读写图对齐与0号位，如下图所示：</p>
<p><img src="/2023/09/12/ComputationalComplexityTimeComp/3.jpg" alt="3"></p>
<center style="font-size:12px;font-weight:bold">Fig. 3. Main working tape</center><br>

<p>此时，主工作带的读写头为虚拟读写头，始终指向0号位，读写头的移动“虚拟地”转变为了<strong>各层带子的移动</strong>。但是，<strong>实际上主工作带仍然只有一个带头，每次要获取被模拟图灵机的快照时，带头便一层一层地扫描，每扫描完一层，便回归到0号位，再扫描下一层，并记录本层的数据。</strong>最终，$k-2$层纸带的数据组成的$k-2$元组被特殊编码记录在主工作带的特定区域上。</p>
<blockquote>
<p>实际上的主工作带上只有对齐后的$k-2$元组的编码。</p>
</blockquote>
<h3 id="时间分析"><a href="#时间分析" class="headerlink" title="时间分析"></a>时间分析</h3><p>模拟$\mathbb{M}_\alpha$的一步计算时，主要的开销在主工作带的移动上，因此只用考虑主工作带移动的开销即可：$\mathbb{M}_\alpha$的时间函数是$T(n)$，因此$\mathbb{M}_\alpha$的工作带上可能出现的最长字符串量级为$O(T(n))$（即每一步都在记录新的数据），字符串、$k-2$元组在主工作带上的编码不会超过$c$（$c$为$|\alpha|$的多项式），$\mathbb{M} _\alpha$每移动一次带头，可能引起主工作带的带头在某一层上移动一整层，时间复杂度为$O(cT(n))$。</p>
<p>因此，$\mathbb{U}(\alpha,x)$模拟$\mathbb{M} _\alpha$的时间复杂度为$O(cT(n))*O(T(n))&#x3D;O(c T^2 (n))$</p>
<h2 id="Proof-of-Hennie-and-Stearns"><a href="#Proof-of-Hennie-and-Stearns" class="headerlink" title="Proof of Hennie and Stearns"></a>Proof of Hennie and Stearns</h2><p>Hennie和Stearns在1966年给出了$O(cT(n)\log T(n))$的证明。在他们的证明中，通用图灵机的构造基本不变，只不过增加了一条草稿工作带，并在主工作带中的有效字符间插入了新的缓冲符号$\boxtimes$，这些缓冲符号可以被其他有效符号覆盖，如下图所示：</p>
<p><img src="/2023/09/12/ComputationalComplexityTimeComp/4.png" alt="4"></p>
<center style="font-size:12px;font-weight:bold">Fig. 4. Main working tape</center><br>

<p>此外，还进一步地将无限长的双向带作了左右分区，每个分区$R _i$的范围为$[2 ^{i+1}-1,2 ^{i+2}-2]$，$L _i$的范围为$[-2 ^{i+2}+2, -2^{i+1}+1]$，长度均为$2 ^{i+1}$，$R_0$和$L_0$以0号为分界。并且规定：</p>
<ol>
<li>每个分区要么全是满（全是有效符号）的，要么全是空（全是缓冲符号）的，要么是半满的（有一般是有效符号）；</li>
<li>$R_i$和$L_i$的有效符号之和始终为$2^{i+1}$；</li>
<li>0号位上的符号只能是有效符号。</li>
</ol>
<p>假设初始时某层右半区的有效符号全部在$R _i$分区，$R_i$是全满的，那么自然地$L_0\sim L _{i-1}$也是全满的。若此时$\mathbb{M} _{\alpha}$进行了一步右移操作，则该层的纸带应“相对地左移”。此时，主工作带上的操作是：带头移动到$R_i$范围，在草稿带上记录下$2^i$个字符内容，分别覆盖掉$R _0,R _1 \sim R _{i-1}$上$1,1\sim 2 ^{i-1}$个缓冲字符，使得右半区全部变成半满。对于左半区，相似的操作，不过是将$L _0 \sim L _{i-1}$中的一半有效字符都放到$L _i$上，使得左半区变成全半满。这样便完成了整层左移（对应$\mathbb{M} _\alpha$右移）的操作。</p>
<h3 id="时间分析-1"><a href="#时间分析-1" class="headerlink" title="时间分析"></a>时间分析</h3><p>同样地，只用考虑主工作带上的移动情况。在主工作带上，一次移动，最远要跑到$R_i$或者$L_i$。而当跑过一次$R_i$或者$L_i$后，其他的分区都变成了半满，此时考虑最坏的情况，即$\mathbb{M}_\alpha$一直往一个方向移动，则在某一层上要把左或右分区$0\sim i-1$的有效符号搬空，至少需要$1+2+...+2 ^{i-1}&#x3D;2^i-1$步，再下一次移动才会涉及到$R_i$或者$L_i$。换句话说，两次最远距离的移动间隔至少为$2^i$步。</p>
<p>$\mathbb{M}_\alpha$至多有$T(n)$步，$\mathbb{U}(\alpha,x)$对最长区间的操作不超过$T&#x2F;2^i$次，操作最长区间时耗时$O(c2^i)$，而区间的总个数$i$不会超过$\log T(n)$，故时间复杂度为：</p>
<p>$$<br>\sum\limits _{i&#x3D;1} ^{\log T(n)}\frac{T(n)}{2^i}O(c2^i)&#x3D;cT(n)\sum\limits _{i&#x3D;1} ^{\log T(n)}O(1)&#x3D;cT(n)\log T(n)<br>$$</p>
<blockquote>
<p>缓冲字符的作用在于，它使得每层纸带上的内容可以通过移动而聚集在带头，即0号位附近，使得单步下的局部扫描成为可能，不像之前那样每次可能都要扫描一整层。</p>
</blockquote>
<h2 id="Oblivious-Turing-Machine-Theorem：遗忘图灵机理论"><a href="#Oblivious-Turing-Machine-Theorem：遗忘图灵机理论" class="headerlink" title="Oblivious Turing Machine Theorem：遗忘图灵机理论"></a>Oblivious Turing Machine Theorem：遗忘图灵机理论</h2><p>该理论的内容为：<em>假设语言L由一个在$O(T(n))$时间内运行的$T(n)$时间可构造的TM $\mathbb{M}$计算，那么存在一个遗忘图灵机$\mathbb{U}$，在$O(T(n) \log T(n))$时间内判定L</em>。</p>
<p>上述的遗忘图灵机可以采用通用图灵机来构造。通用图灵机在划分区间$L_i$和$R_i$时，是按需划分的，也就是说它扫描到哪划分到哪，因为它并不知道要计算的函数的时间复杂度是多少，也就无法预先分配好足够的区间来完成函数的计算。而对于上述理论，由于$T(n)$是时间可构造的，所以：</p>
<ul>
<li>该通用图灵机可以先在$O(\log T(n))$的时间内计算出$logT(n)$（$T(n)$时间可构造则$\log T(n)$时间可构造），完成区间的划分。</li>
</ul>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://zhuanlan.zhihu.com/p/260217775">计算复杂性（2）——运行时间与效率</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/260512272">计算复杂性（3）——计算机的套娃：通用图灵机</a></li>
</ul>
]]></content>
      <categories>
        <category>CS7313: Computational Complexity</category>
      </categories>
      <tags>
        <tag>Math</tag>
        <tag>Theoretical Computer Science</tag>
      </tags>
  </entry>
  <entry>
    <title>Computational Complexity Introduction</title>
    <url>/2023/09/12/ComputationalComplexityIntro/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h1><p>计算复杂性（Computational Complexity），即<strong>使用数学方法对计算中所需的各种资源的耗费（主要是时间和空间）作定量的分析，并研究各类问题之间在计算复杂程度上的相互关系和基本性质</strong>。课程中涉及到的数学方法主要有：</p>
<ul>
<li>Algebraic Method，代数方法；</li>
<li>Probalilistic Method，概率统计方法；</li>
<li>Combinatorial Method，组合方法；</li>
<li>...</li>
</ul>
<h1 id="四类问题"><a href="#四类问题" class="headerlink" title="四类问题"></a>四类问题</h1><p>计算复杂性中的四类问题，即求解的时间复杂度逐级递增的四类问题：</p>
<ol>
<li>P（Polynomial）问题：有多项式时间的算法，时间复杂度O(P(n))；</li>
<li>NP（Nondeterminism Polynomial）问题：能在多项式的时间内得到个别特殊的正确解，但是不知道是否也能在多项式时间内得到所有解；</li>
<li>NPC（Nondeterminism Polynomial Complete）问题：一类特殊的NP问题，所有的NP问题都能约化（复杂化，如用二元一次方程的方法来解一元一次方程问题）成它。那么，只要解决了这个问题，所有能约化成它的NP问题也就解决了；</li>
<li>NP-Hard问题：所有的NP问题都能约化为NP-Hard问题，但是NP-Hard问题本身不一定是一个NP问题。换句话说，NPC问题是NP-Hard问题的子集。</li>
</ol>
]]></content>
      <categories>
        <category>CS7313: Computational Complexity</category>
      </categories>
      <tags>
        <tag>Math</tag>
        <tag>Theoretical Computer Science</tag>
      </tags>
  </entry>
  <entry>
    <title>Transductive Learning and Inductive Learning</title>
    <url>/2023/08/24/TransductiveInductiveLearning/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="Semi-supervised-Learning"><a href="#Semi-supervised-Learning" class="headerlink" title="Semi-supervised Learning"></a>Semi-supervised Learning</h1><p>Transductive Learning（直推式学习）是Semi-supervised Learning（半监督学习）的一种。一般来说，半监督学习分为纯半监督学习（Pure&#x2F;Inductive Semi-supervised Learning）和直推式学习（Transductive Learning）：</p>
<p><img src="/2023/08/24/TransductiveInductiveLearning/1.png" alt="1"></p>
<center style="font-size:12px; font-weight:bold">Fig. 1. Semi-supervised learning</center>


<h2 id="Pure-x2F-Inductive-Semi-supervised-Learning"><a href="#Pure-x2F-Inductive-Semi-supervised-Learning" class="headerlink" title="Pure&#x2F;Inductive Semi-supervised Learning"></a>Pure&#x2F;Inductive Semi-supervised Learning</h2><p>半监督学习出现的一大诱因是现实世界中带标签的数据往往很少，大多数的数据都是无标签的。充分利用这些无标签数据来提高神经网络的性能就是半监督学习的目的。</p>
<p>纯半监督学习是最原始的半监督学习。在纯半监督学习中，训练集为$\mathcal{D}&#x3D;\left\{\mathbf{X} _\text{tr},\mathbf{Y} _\text{tr}, \mathbf{X} _\text{unlabelled} \right\}$，测试集为$\mathcal{T}&#x3D;\left\{\mathbf{X} _\text{te},\mathbf{Y} _\text{te}\right\}$。在训练的过程中，半监督学习的神经网络能够利用无标签样本$\mathbf{X} _\text{unlabelled}$的信息，但是计算损失$\mathcal{L}$时利用的只是$\mathbf{X} _\text{tr}$和$\mathbf{T} _\text{tr}$，且测试集在训练过程中仍是不可见的。因为目前很多的神经网络都是基于消息传递的，如Attention Mechanism、GCN等，这样的设置使得神经网络在训练过程中能够获得$\mathbf{X} _\text{unlabelled}$传来的信息，这能在一定程度上提高网络的性能。</p>
<h2 id="Transductive-Learning"><a href="#Transductive-Learning" class="headerlink" title="Transductive Learning"></a>Transductive Learning</h2><p>直推式学习是另外一种形式的半监督学习，但是它学习的目的与纯半监督学习完全不同。纯半监督学习的目的是利用无标签的数据来提高模型性能、<strong>泛化能力</strong>，它从训练样本中归纳出数据间的<strong>潜在规律</strong>，以期能预测未知的样本。</p>
<p>不同于纯半监督学习，直推式在训练过程中就已经知道将来要预测什么，因为它并不要求在整个样本分布上都达到很好的性能，而只是期望在<strong>给定的预测样本</strong>上达到最好的性能。</p>
<p>在直推式学习中，训练集为$\mathcal{D}&#x3D;\left\{\mathbf{X} _\text{tr},\mathbf{Y} _\text{tr}, \mathbf{X} _\text{te} \right\}$，测试集为$\mathcal{T}&#x3D;\left\{\mathbf{X} _\text{te},\mathbf{Y} _\text{te}\right\}$。它在训练的过程中已经看过的测试样本的特征，只是没有用其标签来计算损失函数$\mathcal{L}$。在GNN中，直推式学习是很普遍的，常见的GCN、SGC等都是基于直推式学习的。</p>
<h1 id="Inductive-Learning"><a href="#Inductive-Learning" class="headerlink" title="Inductive Learning"></a>Inductive Learning</h1><p>一般的深度学习都属于Inductive Learning（归纳式学习），即训练集是$\mathcal{D}&#x3D;\left\{\mathbf{X} _\text{tr},\mathbf{Y} _\text{tr} \right\}$，测试集是$\mathcal{T}&#x3D;\left\{\mathbf{X} _\text{te},\mathbf{Y} _\text{te}\right\}$。测试样本在训练的过程中完全不可知。</p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul>
<li><a href="https://zhuanlan.zhihu.com/p/349107869">半监督学习</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/336560612">transductive、inductive理解</a></li>
<li><a href="https://blog.csdn.net/weixin_44465396/article/details/129480093">Inductive &amp; Transductive</a></li>
</ul>
]]></content>
      <categories>
        <category>GNN</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Dataset Distillation</tag>
        <tag>GNN</tag>
      </tags>
  </entry>
  <entry>
    <title>Graph Neural Tangent Kernel</title>
    <url>/2023/08/14/GraphNeuralTangentKernel/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="Neural-Tangent-Kernel"><a href="#Neural-Tangent-Kernel" class="headerlink" title="Neural Tangent Kernel"></a>Neural Tangent Kernel</h1><p>对于一个参数$\theta$随机初始化的神经网络$f(x;\theta)$，在小批量随机梯度下降SGD的过程中，训练样本$(x&#39;,y&#39;)$（此处假设批量大小为1）会使得$f(x;\theta)$向着减小$f(x&#39;;\theta)$和$y&#39;$之间误差的方向移动：</p>
<p><img src="/2023/08/14/GraphNeuralTangentKernel/1.png" alt="1"></p>
<center style="font-size:12px; font-weight:bold">Fig. 1. SGD</center><br>

<p>而Neural Tangent Kernel就是一个用于衡量曲线$f(x;\theta)$变化的函数，其定义为：</p>
<p>$$<br>\eta \mathcal{K}(x,x&#39;)&#x3D;f(x;\theta+\eta \frac{df(x&#39;;\theta)}{d\theta})-f(x;\theta)<br>$$</p>
<p>式中，$\eta$是一个更新步长，即学习率。当学习率无限小时，我们便得到$f(x;\theta)$变化曲线：</p>
<p>$$<br>\mathcal{K}(x,x&#39;)&#x3D;\text{lim} _{\eta \to0} \frac{f(x;\theta+\eta \frac{df(x&#39;;\theta)}{d\theta})-f(x;\theta)}{\eta}<br>$$</p>
<p>使用一阶泰勒展开，我们可以得到：</p>
<p>$$<br>\begin{align*}<br>    \mathcal{K}(x,x&#39;)<br>    &#x3D;&amp;\text{lim} _{\eta \to0} \frac{f(x;\theta)+f&#39;(x;\theta)\eta \frac{df(x&#39;;\theta)}{d\theta}-f(x;\theta)}{\eta}\\<br>    &#x3D;&amp;f&#39;(x;\theta)\frac{df(x&#39;;\theta)}{d\theta}\\<br>    &#x3D;&amp;&lt;\frac{df(x;\theta)}{d\theta},\frac{df(x&#39;;\theta)}{d\theta}&gt;<br>\end{align*}<br>$$</p>
<p>对于Fig. 1，其变化曲线为：</p>
<p><img src="/2023/08/14/GraphNeuralTangentKernel/2.png" alt="2"></p>
<center style="font-size:12px; font-weight:bold">Fig. 2. Changing curve</center><br>

<blockquote>
<p>此处假设$(x&#39;,y&#39;)&#x3D;(10,50)$。除$\mathcal{K(10,10)}$只是为了标准化。</p>
</blockquote>
<p>如果不断地进行SGD，那么最终，$\mathcal{K}(x,x&#39;)$和$f(x;\theta)$均会收敛到一个稳定的曲线：</p>
<p><img src="/2023/08/14/GraphNeuralTangentKernel/3.png" alt="3"></p>
<center style="font-size:12px; font-weight:bold">Fig. 3. SGD</center><br>

<p>另一方面，若神经网络的宽度足够大，则在SGD的过程中，参数$\theta$的变化将会十分小，$\mathcal{K}$的变化也会非常小，整个神经网络实际上可以用$f(x;\theta)$的一阶泰勒函数拟合：</p>
<p>$$<br>f(x;\theta)\approx f(x;\theta _0)+\frac{df(x;\theta _0)}{d\theta}(\theta - \theta _0)<br>$$</p>
<p>上式中，对于确定的初始化$\theta _0$，$f(x;\theta _0)$是常数，而$\frac{df(x;\theta _0)}{d\theta}$是$x$的函数，也就是说，若将$\frac{df(x;\theta _0)}{d\theta}$视作一个整体，则$f(x;\theta)$实际上成为了一个线性函数。而这个将非线性空间的回归转化为线性空间回归的$\frac{df(x;\theta _0)}{d\theta}$就是核方法中的映射函数$\phi(x)$：</p>
<p>$$<br>\begin{align*}<br>    \phi(x)&#x3D;&amp;\frac{df(x;\theta _0)}{d\theta}\\<br>    \mathcal{K(x,z)}&#x3D;\phi(x) ^T\phi(z)&#x3D;&amp;&lt;\frac{df(x;\theta)}{d\theta},\frac{df(z;\theta)}{d\theta}&gt;<br>\end{align*}<br>$$</p>
<p>Neural Tangent Kernel的作用在于，对于无限宽的网络（能拟合任何函数的神经网络），只要参数$\theta$以某种合适的方式初始化，$\mathcal{K}(x,x&#39;)$一开始就是收敛值；而对于一般的网络，$\mathcal{K}(x,x&#39;)$在训练过程中也会最终收敛，此时，神经网络的训练实际上就是一个Kernel Gradient Descent的过程。即，用SGD训练神经网络等同于拟合核函数。换句话说，此时的神经网络（的隐藏层）本质上是一个映射函数$\phi(x)$。这也解释了为什么复杂的神经网络不仅没有过拟合，反而得到了很强的泛化精度，因为其最终的收敛值无限地接近核方法得到的解析解。</p>
<h1 id="Graph-Neural-Tangent-Kernel"><a href="#Graph-Neural-Tangent-Kernel" class="headerlink" title="Graph Neural Tangent Kernel"></a>Graph Neural Tangent Kernel</h1><p>Graph Neural Tangent Kernel是Neural Tangent Kernel在GNN上的应用。它定义：</p>
<p>$$<br>\mathcal{K}(G,G&#39;)&#x3D;&lt;\frac{df(G;\theta)}{d\theta},\frac{df(G&#39;;\theta)}{d\theta}&gt;<br>$$</p>
<p>其中$G$和$G&#39;$是两张不同的图，$G&#x3D;(V,E)$，$G&#39;&#x3D;(V&#39;,E&#39;)$。最终$\mathcal{K}(G,G&#39;)$是个核矩阵，维度为$|V|\times|V&#39;|$。此时，若把$G$当作测试集，而$G&#39;$当作训练集，根据SVM中的预测算法：</p>
<p>$$<br>\mathcal{K}(G,G&#39;)Y&#39;<br>$$</p>
<p>就是对测试集的预测。</p>
<p>考虑正则则为：</p>
<p>$$<br>\mathcal{K}(G,G&#39;)[\mathcal{K}(G&#39;,G&#39;)+\lambda I] ^{-1}Y&#39;<br>$$</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://zhuanlan.zhihu.com/p/339971642">直观理解Neural Tangent Kernel</a></li>
<li><a href="https://rajatvd.github.io/NTK/">Understanding the Neural Tangent Kernel</a></li>
<li><a href="https://www.cnblogs.com/manuscript-of-nomad/p/17243296.html">Neural Tangent Kernel（NTK）基础推导</a></li>
<li><a href="https://arxiv.org/abs/1905.13192">Graph Neural Tangent Kernel: Fusing Graph Neural Networks with Graph Kernels</a></li>
</ul>
]]></content>
      <categories>
        <category>Kernel Method</category>
      </categories>
      <tags>
        <tag>Paper</tag>
        <tag>Dataset Distillation</tag>
        <tag>GNN</tag>
        <tag>Machine Learning</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title>Introduction of Generative AI</title>
    <url>/2023/08/14/GenerativeAI/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="生成方式"><a href="#生成方式" class="headerlink" title="生成方式"></a>生成方式</h1><p>各个击破 Autoregressive (AR) model：一次生成一个元素，每一个字（像素）都要等前面的字（像素）生成，常用于文字。生成质量好。<br>一次到位 Non-autoregressive (NAR) model：一次生成所有元素，只要有足够的并行运算能力，可以很快，因而在影像生成中更常用。生成质量差。</p>
<p>复合方法：<br>先各个击破再一次到位</p>
<p>一次到位-&gt;N次到位</p>
<h1 id="常见的图像生成模型"><a href="#常见的图像生成模型" class="headerlink" title="常见的图像生成模型"></a>常见的图像生成模型</h1><p>Raster order：一个pixel一个pixel地生成图片。如OpenAI的影像版GPT，它将图片拉成序列，用GPT的模型来完成图像生成。</p>
<p>输入不仅是文字，还有一个从特定的分布中随机取出的向量。</p>
<h2 id="Variational-Auto-encoder-VAE"><a href="#Variational-Auto-encoder-VAE" class="headerlink" title="Variational Auto-encoder (VAE)"></a>Variational Auto-encoder (VAE)</h2><p>Encoder-Decoder，Encoder接受图片输入，生成指定分布的向量编码，Encoder接受向量输入，还原出图片。</p>
<h2 id="Flow-based-Generative-Model"><a href="#Flow-based-Generative-Model" class="headerlink" title="Flow-based Generative Model"></a>Flow-based Generative Model</h2><p>Encoder接受图片输入，生成指定分布的向量编码。该模型特殊的点在于，Encoder被设计成一个具有“反函数”的架构，因此，将Encoder“反过来”就是Decoder，要达到这个目的，首先要保证输出向量与输入向量的维度一致。</p>
<h2 id="Diffusion-Model"><a href="#Diffusion-Model" class="headerlink" title="Diffusion Model"></a>Diffusion Model</h2><p>图片一直加噪音，然后一直降噪，降噪后的图片就是最终结果。</p>
<p>Denoise：接受带噪音的图片和标志除噪阶段数值，产生噪音，再把生成的噪音从输入图片中减去。Noise Predictor的训练。</p>
<p>Text-to-Image：将文字信息加入Denoise模块。</p>
<p>训练：在之前的基础上加入文字信息。</p>
<p><img src="/2023/08/14/GenerativeAI/1.png" alt="1"></p>
<center style="font-size:12px; font-weight:bold">Fig. 1. VAE，Flow-based，Diffusion</center><br>

<h2 id="Generative-Adversarial-Network-GAN"><a href="#Generative-Adversarial-Network-GAN" class="headerlink" title="Generative Adversarial Network (GAN)"></a>Generative Adversarial Network (GAN)</h2><p>只有Decoder（Generator），Decoder接受原图片输入生成新图片，再用Discriminator来比较新图片和原图片。</p>
<blockquote>
<p>把正态分布的向量和图片对应起来。 </p>
</blockquote>
<h1 id="Text-to-Image"><a href="#Text-to-Image" class="headerlink" title="Text-to-Image"></a>Text-to-Image</h1><p>Text Encoder + Generation Model + Decoder，分开训练再组合。</p>
<p>e.g. Stable Diffusion，DALL-E series，Imagen...</p>
<p>Text Encoder对结果的影响很大，一般越大效果越好，而Generation Model的影响则没那么大。</p>
]]></content>
      <categories>
        <category>Advanced Model</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Generative AI</tag>
      </tags>
  </entry>
  <entry>
    <title>BERT</title>
    <url>/2023/08/14/BERT/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="BERT自监督学习"><a href="#BERT自监督学习" class="headerlink" title="BERT自监督学习"></a>BERT自监督学习</h1><p>类似于GPT，BERT也是一个预训练（自监督学习）的模型，它使用的训练样本是没有标签的文本，这些文本的一部分作为Key，另一部分作为Value。</p>
<p>BERT自监督学习的方式是“填字游戏练习”。由于BERT只是Transformer的编码器，它的输出序列与输入序列的长度是一模一样的，故在自监督学习的过程中，BERT会通过掩码或者替换遮盖住输入序列的一些信息，再通过最后的输出重新找出这些被遮盖的信息。换句话说，BERT以挖去了一些单词的序列作为Key，而以原序列作为Value。</p>
<p><img src="/2023/08/14/BERT/1.png" alt="1"></p>
<center style="font-size:12px; font-weight:bold">Fig. 1. BERT pre-train</center> <br>

<blockquote>
<p>还有其他预训练BERT的方法，如Next Sentence Prediction（被证明没什么用）、Sentence Order Prediction等。</p>
</blockquote>
<h1 id="BERT微调"><a href="#BERT微调" class="headerlink" title="BERT微调"></a>BERT微调</h1><p>预训练后的BERT嫁接上其他模块并微调后，可以被用于完成各种各样的下游任务（Downstream Tasks）。下游任务的训练属于监督学习。</p>
<h2 id="文本情感分类（Text-Sentiment-Analysis）"><a href="#文本情感分类（Text-Sentiment-Analysis）" class="headerlink" title="文本情感分类（Text Sentiment Analysis）"></a>文本情感分类（Text Sentiment Analysis）</h2><p>文本情感分类是一个用于判断语句感情色彩的分类问题。使用BERT+softmax分类器能够很轻易地完成这个任务。具体来说，输入模型的是待分析文本，它以特殊的字符<code>[CLS]</code>开头，最后的分类也只对<code>[CLS]</code>经过BERT后的编码结果进行：</p>
<p><img src="/2023/08/14/BERT/2.png" alt="2"></p>
<center style="font-size:12px; font-weight:bold">Fig. 2. Text sentiment analysis</center>

<h2 id="词性标注（Pos-Tagging）"><a href="#词性标注（Pos-Tagging）" class="headerlink" title="词性标注（Pos Tagging）"></a>词性标注（Pos Tagging）</h2><p>词性标注，即给句子中的每个单词标注词性，也是一个能用BERT完成的下游任务。该任务下，模型的输入是待标注文本，输出是所有待标注文本单词编码结果的softmax：</p>
<p><img src="/2023/08/14/BERT/3.png" alt="3"></p>
<center style="font-size:12px; font-weight:bold">Fig. 3. Pos tagging</center>

<h2 id="自然语言推理（Natural-Language-Inference）"><a href="#自然语言推理（Natural-Language-Inference）" class="headerlink" title="自然语言推理（Natural Language Inference）"></a>自然语言推理（Natural Language Inference）</h2><p>顾名思义，自然语言推理就是让机器推理两个句子间的关系。该任务下，模型的输入是两个待分析文本，文本间用特殊字符<code>[SEP]</code>隔开，最后的分类只对<code>[CLS]</code>经过BERT后的编码结果进行：</p>
<p><img src="/2023/08/14/BERT/4.png" alt="4"></p>
<center style="font-size:12px; font-weight:bold">Fig. 4. Natural language inference</center>

<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://speech.ee.ntu.edu.tw/~hylee/ml/2023-spring.php">李宏毅机器学习</a></li>
</ul>
]]></content>
      <categories>
        <category>Advanced Model</category>
      </categories>
      <tags>
        <tag>Attention Mechanism</tag>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>GPT</title>
    <url>/2023/08/11/GPT/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="ChatGPT"><a href="#ChatGPT" class="headerlink" title="ChatGPT"></a>ChatGPT</h1><p>ChatGPT，全称Chat Generative Pre-trained Transformer，是一个文本生成模型，即用于续写用户的提问、要求的模型。ChatGPT会拥有记忆性是因为最终输入到模型中的文本还包含了过去的对话信息。ChatGPT在部署后不联网，它生成的回答都是基于用户提问的续写，而不是对网络资料的复制粘贴。</p>
<blockquote>
<p>文本生成模型实际上就是语言模型。虽然ChatGPT的名字带“Transformer”，但它实际上只用了Transformer的解码器，因为它的任务是文本生成，而不需要理解文本的意思。</p>
</blockquote>
<h1 id="ChatGPT预训练与监督学习"><a href="#ChatGPT预训练与监督学习" class="headerlink" title="ChatGPT预训练与监督学习"></a>ChatGPT预训练与监督学习</h1><p>预训练（Pre-train），又称自监督学习（Self-supervised Learning）。通过预训练得到的模型一般被称为基石模型（Foundation Model）。自监督学习区别于监督学习的地方在于，自监督学习的Key-value Pair是通过程序自生成的，它可以没有逻辑，可以不符合实际情况。</p>
<p>网络上的所有资料都可以作为自监督学习的训练集。通过程序自动划分文本、生成键值对（Key-value Pair）后，这些键值对就可以作为训练样本来训练语言模型。GPT1-GPT3都用这种方式训练，但这样训练的结果是GPT给出的答案不可控。</p>
<p>若将用网络资料训练的GPT3作为一个预训练模型，再用人类提供的资料做监督学习、微调（Finetune），便得到了GPT3.5，也就是ChatGPT。</p>
<p><img src="/2023/08/11/GPT/1.png" alt="1"></p>
<center style="font-size:12px; font-weight:bold">Fig. 1. Pre-train and supervised learning</center><br>

<p>GPT具有很强的泛化能力。只要在预训练时采用不同语言的文本，GPT就能具有处理这些不同语言文本的能力。</p>
<p><img src="/2023/08/11/GPT/2.png" alt="2"></p>
<center style="font-size:12px; font-weight:bold">Fig. 2. Multilingual pre-train</center>

<h1 id="ChatGPT强化学习"><a href="#ChatGPT强化学习" class="headerlink" title="ChatGPT强化学习"></a>ChatGPT强化学习</h1><p>经过预训练、监督学习后的GPT又会经过一定的强化学习训练。</p>
<p><img src="/2023/08/11/GPT/3.png" alt="3"></p>
<center style="font-size:12px; font-weight:bold">Fig. 3. Reinforcement learning</center><br>

<blockquote>
<p>鉴于ChatGPT本质上是一个文本生成（续写）模型，用户向ChatGPT所提的问题对ChatGPT的回答质量有很大的影响。因此，要学会催眠ChatGPT，即使用正确的Prompting。</p>
</blockquote>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://speech.ee.ntu.edu.tw/~hylee/ml/2023-spring.php">李宏毅机器学习</a></li>
</ul>
]]></content>
      <categories>
        <category>Advanced Model</category>
      </categories>
      <tags>
        <tag>Attention Mechanism</tag>
        <tag>Deep Learning</tag>
        <tag>Reinforcement Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Support Vector Machine</title>
    <url>/2023/08/09/SupportVectorMachine/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="方法论"><a href="#方法论" class="headerlink" title="方法论"></a>方法论</h1><p>对于一个普通的线性二分类问题，我们期望找到这么一个决策边界$\omega ^Tx+b&#x3D;0$（式中$\omega$与$x$均为向量），使得该决策边界能够完全地把数据集中正类和负类隔开：</p>
<p><img src="/2023/08/09/SupportVectorMachine/1.png" alt="1"></p>
<center style="font-size:12px; font-weight:bold">Fig. 1. Linear binary classification</center><br>

<p>在逻辑回归（Logistic Regression）中，我们用的方法本质上是通过凸优化逐步拟合该边界的参数，使得正负类分别位于直线两侧：</p>
<p>$$<br>\begin{align*}<br>    \hat{y}^{(i)}&#x3D;&amp;\frac{1}{1+\exp(-\omega ^Tx-b)}\\<br>    L&#x3D;\frac{1}{m}\sum _{i&#x3D;1} ^{m}[-y ^{(i)}\log&amp;(\hat{y}^{(i)})-(1-y ^{(i)})log(1-\hat{y}^{(i)})]<br>\end{align*}<br>$$</p>
<p>式中，$m$为训练集样本数。由于所有的样本点都将参与到损失函数$L$中，任何一个样本点的偏移都会产生新的梯度而导致决策边界发生一定的偏移。然而，样本终究只是数据的观测值，总会存在一定的噪声使得观测值与真实值存在偏差，假设该噪声表现为最一般的高斯噪声，则一个样本的真实数据值实际上在该样本值的附近：</p>
<p><img src="/2023/08/09/SupportVectorMachine/2.png" alt="2"></p>
<center style="font-size:12px; font-weight:bold">Fig. 2. Gaussian noise</center><br>

<p>由于逻辑回归未考虑这些噪声，近似得到的决策边界可能不是最佳的边界。而SVM则不同，它只考虑距离决策边界近的点，并期望这些离决策边界近的点距离决策边界越远越好。这样做的好处是远离决策边界的点无论怎么偏移都不会对决策边界造成影响：</p>
<p><img src="/2023/08/09/SupportVectorMachine/3.png" alt="3"></p>
<center style="font-size:12px; font-weight:bold">Fig. 3. Decision boundary</center><br>

<p>上图便是采用SVM得出的决策边界，该决策边界实际上只由个别几个距离决策边界近的点决定，而这几个样本点就是支持决策边界的<strong>支持向量</strong>。</p>
<h1 id="线性分类"><a href="#线性分类" class="headerlink" title="线性分类"></a>线性分类</h1><p>对于如何获得线性分类问题的决策边界，逻辑回归和SVM采用了两种不同的思路。逻辑回归通过凸优化和梯度下降来迭代地获得近似解；SVM则通过最大化距离决策边界最近点到决策边界的距离来获得解析解。</p>
<p><img src="/2023/08/09/SupportVectorMachine/1.png" alt="4"></p>
<center style="font-size:12px; font-weight:bold">Fig. 4. Linear binary classification</center><br>

<p>具体来说，对于任何一个二维平面（或超平面）内的决策边界$\omega ^Tx+b&#x3D;0$，样本$(x ^{(i)},y ^{(i)})$到决策边界的函数间隔（Functional Margin）可以定义为：</p>
<p>$$<br>\hat{\gamma} ^{(i)}&#x3D;y ^{(i)}(\omega ^Tx ^{(i)}+b)<br>$$</p>
<p>式中，$y ^{(i)}$正类取1，负类取-1，括号内的项实际上就是未用$||\omega||$归一化的$(x ^{(i)},y ^{(i)})$到直线$\omega ^Tx+b&#x3D;0$的距离（带正负），而$y ^{(i)}$正好可以使得$\hat{\gamma} ^{(i)}$始终为正，因而$\hat{\gamma} ^{(i)}$就是广义的“距离”，称函数间隔。若将$\hat{\gamma} ^{(i)}$归一化，我们可以得到几何间隔（Geometric Margin）：</p>
<p>$$<br>\gamma ^{(i)}&#x3D;\frac{\hat{\gamma} ^{(i)}}{||\omega||}&#x3D;y ^{(i)}\frac{(\omega ^Tx ^{(i)}+b)}{||\omega||}<br>$$</p>
<p>要得到决策边界，我们只需要让距离决策边界最近的点距离决策边界最远即可，即：</p>
<p>$$<br>\max _{\omega,b}(\min _{i&#x3D;1,...,m}\gamma ^{(i)})<br>$$</p>
<p>式中，我们使用了几何间距。几何间距实际上是限制向量$\omega$为单位向量后的函数间距。这种限制的缺点在于，$\omega$的值只能在一个单位圆域的边界上取得：</p>
<p>$$<br>||\omega||&#x3D;1<br>$$</p>
<p>进而导致$\omega$的取值域不是一个凸集，我们也就很难以凸优化的方法来求解决策边界。</p>
<blockquote>
<p>在欧氏空间中，凸集是对于集合内的每一对点，连接该对点的直线段上的每个点也在该集合内。显然，只包括边界的圆不是凸集。</p>
</blockquote>
<p><img src="/2023/08/09/SupportVectorMachine/4.png" alt="5"></p>
<center style="font-size:12px; font-weight:bold">Fig. 5. Circle field </center><br>

<p>所以，采用$\gamma ^{(i)}$的原始形式更好：</p>
<p>$$<br>\max _{\omega,b}(\min _{i&#x3D;1,...,m}\frac{\hat{\gamma} ^{(i)}}{||\omega||}) \tag{1}<br>$$</p>
<p>此时，$\omega$的取值将不再受到限制。更进一步地：<br>$$<br>\min _{i&#x3D;1,...,m}\frac{\hat{\gamma} ^{(i)}}{||\omega||}<br>$$</p>
<p>可以直接用$\hat{\gamma}&#x2F;{||\omega||}$表示，其中：</p>
<p>$$<br>y ^{(i)}(\omega ^Tx ^{(i)}+b)\ge\hat{\gamma},\space\space i&#x3D;1,...,m<br>$$</p>
<p>上式可被视作一个限定条件，限定$\omega$与$b$必须在一个指定的集合中：</p>
<p><img src="/2023/08/09/SupportVectorMachine/5.png" alt="6"></p>
<center style="font-size:12px; font-weight:bold">Fig. 6. Inequality field</center><br>

<blockquote>
<p>线性不等式切出的域一定是个凸集。</p>
</blockquote>
<p>则我们实际上可以简单地令$\hat{\gamma}&#x3D;1$，于是式$(1)$便简化为：</p>
<p>$$<br>\max _{\omega,b}\frac{1}{||\omega||} \tag{2}<br>$$</p>
<p>进一步地将$\omega$放在分母，我们可得：</p>
<p>$$<br>\min _{\omega,b} \frac{||\omega|| ^2}{2}\space\space\text{s.t} \space\space y ^{(i)}(\omega ^Tx ^{(i)}+b)\ge 1,\space\space i&#x3D;1,...,m \tag{3}<br>$$</p>
<blockquote>
<p>平方与分母中的$2$是为了方便求导。</p>
</blockquote>
<p>采用拉格朗日乘数法分别求解限定条件下边界和内部的极值便可得到决策边界的解析解。得到解析解后，对任意点$x$计算$\text{sign}(\omega ^Tx+b)$便可得到它的分类结果。这就是SVM，它实际上是一种最大间隔分类（Maximum Margin Classification）。</p>
<blockquote>
<p>实际求解要更加复杂，常用的方法有内点法（Interior Point Method, IPM）和序列最小优化（Sequential Minimal Optimization, SMO）。</p>
</blockquote>
<h1 id="软边距"><a href="#软边距" class="headerlink" title="软边距"></a>软边距</h1><p>式$(3)$对于Fig. 1（Fig. 4）所示的样本分布实际上是无解的，因为负例中有一个点混入了正例中，导致无法得到一个能够完全划分正例和负例的决策边界。为了解决这个问题，可以在式$(3)$的限定条件中加入松弛变量$\xi$（Slack Variable），使得某些样本点到决策边界的距离不一定严格地大于$1$，甚至可以小于$0$：</p>
<p>$$<br>\begin{align*}<br>    y ^{(i)}(\omega ^Tx ^{(i)}&amp;+b)\ge 1-\xi _i,\space\space i&#x3D;1,...,m\\<br>    &amp;\xi _i \ge0\space\space i&#x3D;1,...,m<br>\end{align*}<br>$$</p>
<p>引入松弛变量后，式$(3)$变为：</p>
<p>$$<br>\begin{align*}<br>    \min _{\omega,b, \xi} (\frac{||\omega|| ^2}{2}&amp;+C\sum _1 ^m \xi _i) \tag{4}  \\<br>    \text{s.t} \space\space y ^{(i)}(\omega ^Tx ^{(i)}+b)\ge 1-&amp;\xi _i,\space\space  \xi _i \ge 0,\space\space i&#x3D;1,...,m<br>\end{align*}<br>$$</p>
<p>式中$C$是一个超参数，新加入的项$\sum _1 ^m \xi _i$是一个正则项，称L1 regularization。该正则项是一类关于模型“先验知识”，可以提高SVM分类的鲁棒性和效率。</p>
<p>这样的SVM称软边距SVM，而之前的则称硬边距SVM。</p>
<h1 id="非线性分类"><a href="#非线性分类" class="headerlink" title="非线性分类"></a>非线性分类</h1><p>非线性分类问题，即仅用一条直线无法将正负类区分开的问题，如下图所示，我们必须用两条直线才能将这四个交叉分布的样本点区分开（1、3象限为负，2、4象限为正）：</p>
<p><img src="/2023/08/09/SupportVectorMachine/6.png" alt="7"></p>
<center style="font-size:12px; font-weight:bold">Fig. 7. Non-linear classification</center><br>

<p>对于这样的问题，常规的线性SVM是无法解的。一个很自然的想法是用函数$\phi(x)$将$x$映射到新空间，使得$x$在新空间线性可分，这时，式$(4)$便变为：</p>
<p>$$<br>\begin{align*}<br>    \min _{\omega,b, \xi} &amp;(\frac{||\omega|| ^2}{2}+C\sum _1 ^m \xi _i) \tag{5} \\<br>    \text{s.t} \space\space y ^{(i)}[\omega ^T\phi(x ^{(i)})+b]&amp; \ge 1-\xi _i,\space\space \xi _i \ge 0,\space\space i&#x3D;1,...,m<br>\end{align*}<br>$$</p>
<p>但是，实际上我们并不会直接定义函数$\phi(x)$，而是定义核函数（Kernel Function）：</p>
<p>$$<br>K(x ^{(i)},x ^{(j)})&#x3D;\phi(x ^{(i)}) ^T\phi(x ^{(j)})<br>$$</p>
<p>使用核函数的原因是，对决策边界的求解可以转为对其对偶问题的求解：</p>
<p>$$<br>\begin{align*}<br>    \max _\alpha \sum _{i&#x3D;1} ^m\alpha _i&amp;-\frac{1}{2}\sum _{i&#x3D;1} ^m \sum _{j&#x3D;1} ^m[\alpha _i \alpha _j y ^{(i)}\phi(x ^{(i)}) ^T\phi(x ^{(j)})y ^{(j)}]\\<br>    \text{s.t}\space\sum _{i&#x3D;1} ^m\alpha _i &amp;y ^{(i)}&#x3D;0,\space\space 0\le\alpha _i\le C,\space\space i&#x3D;1,...,m<br>\end{align*}<br>$$</p>
<blockquote>
<p>对偶问题我也不太懂。</p>
</blockquote>
<p>而后续的预测实际上用到的也只是$\phi(x ^{(i)}) ^T\phi(x)$：</p>
<p>$$<br>\begin{align*}<br>    \omega^Tx+b&#x3D;&amp;[\sum _{i&#x3D;1} ^m \alpha _i y ^{(i)}\phi(x ^{(i)})] ^T\phi(x)+b\\<br>    &#x3D;&amp;\sum _{i&#x3D;1} ^m\alpha _iy ^{(i)}\phi(x ^{(i)})^T\phi(x)+b<br>\end{align*}<br>$$</p>
<p>因此我们完全可以只定义核函数，而无需理会$\phi(x)$，只要保证核函数对各个$x ^{(i)}$和$x ^{(j)}$求值后得到的核矩阵$K$是半正定矩阵以保证$\phi(x)$的存在即可：</p>
<p>$$<br>K _{ij} &#x3D;\phi(x ^{(i)}) ^T\phi(x ^{(j)})<br>$$</p>
<blockquote>
<p>核矩阵是半正定矩阵的核函数的$\phi(x)$存在的证明略。</p>
</blockquote>
<p>只定义核函数的另一个好处在于，一个很简单的核函数的$\phi(x)$可能十分复杂，如对于最常用的高斯核函数RBF Kernel：</p>
<p>$$<br>K(x,z)&#x3D;\exp(-\frac{||x-z|| ^2}{2\sigma ^2})<br>$$</p>
<p>其$\phi(x)$是无穷维的。</p>
<blockquote>
<p>不难看出$\phi(x)$与神经网络的编码器存在一定的等价性。</p>
</blockquote>
]]></content>
      <categories>
        <category>Kernel Method</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>SVM</tag>
        <tag>Supervised Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Cartesian Product in Python</title>
    <url>/2023/07/25/CartesianProduct/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h1><p>笛卡尔乘积（Cartesian product）定义，两个集合$X$和$Y$的笛卡尔积$X\times Y$为第一个对象是$X$的成员而第二个对象是$Y$的成员的所有有序对所组成的集合，如：</p>
<p>$$<br>\begin{align*}<br>    A\space&#x3D;&amp;\space\space\left\{a,b\right\}\\<br>    B\space&#x3D;&amp;\space\space\left\{0,1\right\}\\<br>    A\times B\space&#x3D;&amp;\left\{(a,0),(a,1),(b,0),(b,1)\right\}<br>\end{align*}<br>$$</p>
<blockquote>
<p>笛卡尔积也可以扩展为$n$个集合。</p>
</blockquote>
<h1 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h1><p>Python中有很多能产生笛卡尔积的库函数，它们在与有序对相关的运算中很有用，如GNN中的邻接矩阵等，一些常见的有：</p>
<h2 id="product"><a href="#product" class="headerlink" title="product"></a>product</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> product</span><br><span class="line">product(*iterables, repeat=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p><code>product</code>接受$n$个可迭代对象，并生成它们的笛卡尔积，生成的笛卡尔积也是个可迭代对象，其每个元素是一个元组，代表一个有序对，可以用<code>list</code>将其转化为列表，如：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = [<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(<span class="built_in">list</span>(product(a, b)))</span><br><span class="line">[(<span class="number">1</span>, <span class="string">&#x27;a&#x27;</span>), (<span class="number">1</span>, <span class="string">&#x27;b&#x27;</span>), (<span class="number">2</span>, <span class="string">&#x27;a&#x27;</span>), (<span class="number">2</span>, <span class="string">&#x27;b&#x27;</span>), (<span class="number">3</span>, <span class="string">&#x27;a&#x27;</span>), (<span class="number">3</span>, <span class="string">&#x27;b&#x27;</span>)]</span><br></pre></td></tr></table></figure>

<p><code>repeat</code>代表将前面的可迭代对象重复的次数。如，<code>product(a, b, a, b)</code>等价于<code>product(a, b, repeat=2)</code>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = [<span class="number">1</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = [<span class="string">&#x27;a&#x27;</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(<span class="built_in">list</span>(product(a, b, a, b)))</span><br><span class="line">[(<span class="number">1</span>, <span class="string">&#x27;a&#x27;</span>, <span class="number">1</span>, <span class="string">&#x27;a&#x27;</span>)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(<span class="built_in">list</span>(product(a, b, repeat=<span class="number">2</span>)))</span><br><span class="line">[(<span class="number">1</span>, <span class="string">&#x27;a&#x27;</span>, <span class="number">1</span>, <span class="string">&#x27;a&#x27;</span>)]</span><br></pre></td></tr></table></figure>

<h2 id="np-ix"><a href="#np-ix" class="headerlink" title="np.ix_"></a>np.ix_</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">np.ix_(*args)</span><br></pre></td></tr></table></figure>

<p><code>*args</code>是两个及以上的一维数组，这些数组的数据类型要么是整型<code>int</code>，要么是布尔型<code>bool</code>。<code>np.ix_</code>会返回一个元组，元组的每个元素是一个$n$维的矩阵（取决于输入数组的个数，如果输入两个数组，那么就是$2$维的矩阵），这些元素通过广播机制（broadcasting）生成笛卡尔积。因为这个元组实际上没什么用，所以<code>np.ix_</code>总是被用来进行切片操作：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = [<span class="number">1</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">2</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = [<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(np.ix_(a, b))</span><br><span class="line">(array([[<span class="number">1</span>],</span><br><span class="line">       [<span class="number">5</span>],</span><br><span class="line">       [<span class="number">7</span>],</span><br><span class="line">       [<span class="number">2</span>]]), array([[<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>]]))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x=np.arange(<span class="number">32</span>).reshape((<span class="number">8</span>,<span class="number">4</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(x)</span><br><span class="line">[[ <span class="number">0</span>  <span class="number">1</span>  <span class="number">2</span>  <span class="number">3</span>]</span><br><span class="line"> [ <span class="number">4</span>  <span class="number">5</span>  <span class="number">6</span>  <span class="number">7</span>]</span><br><span class="line"> [ <span class="number">8</span>  <span class="number">9</span> <span class="number">10</span> <span class="number">11</span>]</span><br><span class="line"> [<span class="number">12</span> <span class="number">13</span> <span class="number">14</span> <span class="number">15</span>]</span><br><span class="line"> [<span class="number">16</span> <span class="number">17</span> <span class="number">18</span> <span class="number">19</span>]</span><br><span class="line"> [<span class="number">20</span> <span class="number">21</span> <span class="number">22</span> <span class="number">23</span>]</span><br><span class="line"> [<span class="number">24</span> <span class="number">25</span> <span class="number">26</span> <span class="number">27</span>]</span><br><span class="line"> [<span class="number">28</span> <span class="number">29</span> <span class="number">30</span> <span class="number">31</span>]]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span> (x[np.ix_([<span class="number">1</span>,<span class="number">5</span>,<span class="number">7</span>,<span class="number">2</span>],[<span class="number">0</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">2</span>])])</span><br><span class="line">[[ <span class="number">4</span>  <span class="number">7</span>  <span class="number">5</span>  <span class="number">6</span>]</span><br><span class="line"> [<span class="number">20</span> <span class="number">23</span> <span class="number">21</span> <span class="number">22</span>]</span><br><span class="line"> [<span class="number">28</span> <span class="number">31</span> <span class="number">29</span> <span class="number">30</span>]</span><br><span class="line"> [ <span class="number">8</span> <span class="number">11</span>  <span class="number">9</span> <span class="number">10</span>]]</span><br></pre></td></tr></table></figure>

<p>其中，最后的<code>x[np.ix_([1,5,7,2],[0,3,1,2])]</code>使得<code>x</code>被切片出对应下标的元素，如<code>[1, 0]</code>切出<code>4</code>，<code>[1, 3]</code>切出<code>7</code>，最后的<code>[2, 2]</code>切出<code>10</code>。当输入<code>np.ix_</code>的数组的元素类型为<code>bool</code>时，这些数组会被先转化为其<code>True</code>元素的下标所形成的数组，即先进行一次<code>np.nonzero(boolean_sequence)</code>，所以下面这两种切片是等价的：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(x[np.ix_([<span class="literal">False</span>, <span class="literal">True</span>, <span class="literal">True</span>],[<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>])])</span><br><span class="line">[[ <span class="number">4</span>  <span class="number">5</span>  <span class="number">6</span>  <span class="number">7</span>]</span><br><span class="line"> [ <span class="number">8</span>  <span class="number">9</span> <span class="number">10</span> <span class="number">11</span>]]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(x[np.ix_([<span class="number">1</span>, <span class="number">2</span>],[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])])</span><br><span class="line">[[ <span class="number">4</span>  <span class="number">5</span>  <span class="number">6</span>  <span class="number">7</span>]</span><br><span class="line"> [ <span class="number">8</span>  <span class="number">9</span> <span class="number">10</span> <span class="number">11</span>]]</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Programming Language</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Math</tag>
      </tags>
  </entry>
  <entry>
    <title>Link Prediction</title>
    <url>/2023/07/19/LinkPrediction/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="链接预测的基本方法"><a href="#链接预测的基本方法" class="headerlink" title="链接预测的基本方法"></a>链接预测的基本方法</h1><p>当前，在图机器学习领域，链接预测的主流方法有三种：</p>
<ol>
<li><strong>启发式方法（Heuristic methods）</strong><br> 这种方法认为存在链接关系的结点的特征存在某种共同的特性，这种特性使得它们间有更大的相似度。启发式方法通过定义一种映射来衡量这样的相似度，如结点的共同邻居（Common neighor）、Katz Index等，但是这样的映射不一定对所有的图都有效。</li>
<li><strong>结点嵌入（Node embedding）</strong><br> 这种方法与结点分类（Node classification）中的方法一致，即基于游走的方法学习结点的embedding，常见的方法有：DeepWalk、Node2vec等。这样的方法，没有直接将链接预测任务嵌入到有监督学习的流程中，并且无法较好的利用用户的节点属性，无法达到较好的预测精度。</li>
<li><strong>图神经网络（GNN）</strong><br> 基于基本的图神经网络架构，如GCN、GraphSAGE、GAT等，通过对邻居结点的聚合得到融合了图结构信息的结点表示，再以类似于启发式方法的方式求任意两结点的“相似度”，进而判断两结点间边的有无。最后的判断本质上是个逻辑回归，即二分类问题（有边为1，无边为0）。</li>
</ol>
<h1 id="基于PyG的GNN链接预测"><a href="#基于PyG的GNN链接预测" class="headerlink" title="基于PyG的GNN链接预测"></a>基于PyG的GNN链接预测</h1><p>PyG（PyTorch Geometric）是个建立在PyTorch基础上的图神经网络库，它为训练不同任务（结点分类、链接预测、图分类等）、不同架构（GCN、GraphSAGE、GAT等）的图神经网络提供了方便。使用PyG来完成链接预测，可以省去很多复杂的操作，特别是在数据预处理阶段。事实上，结点分类和链接预测最大的区别就在数据预处理阶段。</p>
<h2 id="数据集的划分"><a href="#数据集的划分" class="headerlink" title="数据集的划分"></a>数据集的划分</h2><p>前面提到过，链接预测本质上是一个二分类问题，只不过它二分类的单位不是一个结点，而是<strong>一对结点</strong>，分类的结果是这对结点间存在（1）或不存在（0）边。这样的不同决定了我们需要对数据集进行额外的处理，以获得训练样本，包括：正采样和负采样，前者采样存在边的结点对，后者采样不存在边的结点对。在结点分类用到的数据集的基础上，使用PyG提供的<code>torch_geometric.transforms.RandomLinkSplit</code>函数可以很方便地完成对训练集、验证集和测试集的采样。</p>
<p><code>torch_geometric.transforms.RandomLinkSplit(num_val, num_test,...)</code>，以下为几个常用参数的说明：</p>
<ol>
<li><code>num_val</code>：验证集中边占所有边比例，默认为0.1。</li>
<li><code>num_test</code>：测试集中边占所有边的比例，默认为0.1。</li>
<li><code>is_undirected</code>：<code>True</code>则假定图是无向图，反之为有向图。</li>
<li><code>add_negative_train_samples</code>：是否为训练集添加负训练样本。一般设置为<code>False</code>（默认也是<code>False</code>），也就是使得训练集中不包含负样本，这样每一轮训练时在训练集中可以重新采样负样本进行训练，由此可以保证每一轮训练中采样得到的负样本都是不一样的，可以有效提高模型泛化能力。验证集和测试集则默认会自动完成负样本的采样。</li>
<li><code>neg_sampling_ratio</code>：采样中正负样本的比例，默认为1。即正负样本个数一致（对验证集、测试集和<code>add_negative_train_samples</code>设置为<code>True</code>后的训练集）。</li>
</ol>
<blockquote>
<p>更多资料见<a href="https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.transforms.RandomLinkSplit.html#torch-geometric-transforms-randomlinksplit">RandomLinkSplit</a>。</p>
</blockquote>
<p><code>RandomLinkSplit</code>的使用方法很简单，因为它被包含在<code>transforms</code>中，所以可以在读数据的时候作为一个参数传入读数据的函数中，以读取<code>Cora</code>数据集为例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch_geometric.transforms <span class="keyword">as</span> T</span><br><span class="line"><span class="keyword">from</span> torch_geometric.datasets <span class="keyword">import</span> Planetoid</span><br><span class="line">train_data, val_data, test_data = Planetoid(root=<span class="string">&#x27;./data/Cora&#x27;</span>, name=<span class="string">&quot;Cora&quot;</span>, </span><br><span class="line">    transform=T.RandomLinkSplit(num_val=<span class="number">0.1</span>, num_test=<span class="number">0.1</span>, is_undirected=<span class="literal">True</span>, add_negative_train_samples=<span class="literal">False</span>))[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>

<p>而一般结点分类任务用到的数据集形式为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = Planetoid(root=<span class="string">&#x27;./data/Cora&#x27;</span>, name=<span class="string">&quot;Cora&quot;</span>)[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>

<blockquote>
<p>注意<code>Planetoid</code>是PyG定义的<code>InMemoryDataset</code>的子类，<code>InMemoryDataset</code>所返回的是数据集中所有的图结构，因此要用下标来读取指定的图。<code>Planetoid</code>能加载的数据集都只有一张图，所以用<code>Planetoid()[0]</code>即可。</p>
</blockquote>
<p>我们不妨来看看这四种data的区别：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(train_data, <span class="string">&quot;\n&quot;</span>, val_data, <span class="string">&quot;\n&quot;</span>, test_data, <span class="string">&quot;\n&quot;</span>, data)</span><br><span class="line">Data(x=[<span class="number">2708</span>, <span class="number">1433</span>], edge_index=[<span class="number">2</span>, <span class="number">8448</span>], y=[<span class="number">2708</span>], train_mask=[<span class="number">2708</span>], val_mask=[<span class="number">2708</span>], test_mask=[<span class="number">2708</span>], edge_label=[<span class="number">4224</span>], edge_label_index=[<span class="number">2</span>, <span class="number">4224</span>])</span><br><span class="line">Data(x=[<span class="number">2708</span>, <span class="number">1433</span>], edge_index=[<span class="number">2</span>, <span class="number">8448</span>], y=[<span class="number">2708</span>], train_mask=[<span class="number">2708</span>], val_mask=[<span class="number">2708</span>], test_mask=[<span class="number">2708</span>], edge_label=[<span class="number">1054</span>], edge_label_index=[<span class="number">2</span>, <span class="number">1054</span>])</span><br><span class="line">Data(x=[<span class="number">2708</span>, <span class="number">1433</span>], edge_index=[<span class="number">2</span>, <span class="number">9502</span>], y=[<span class="number">2708</span>], train_mask=[<span class="number">2708</span>], val_mask=[<span class="number">2708</span>], test_mask=[<span class="number">2708</span>], edge_label=[<span class="number">1054</span>], edge_label_index=[<span class="number">2</span>, <span class="number">1054</span>])</span><br><span class="line">Data(x=[<span class="number">2708</span>, <span class="number">1433</span>], edge_index=[<span class="number">2</span>, <span class="number">10556</span>], y=[<span class="number">2708</span>], train_mask=[<span class="number">2708</span>], val_mask=[<span class="number">2708</span>], test_mask=[<span class="number">2708</span>])</span><br></pre></td></tr></table></figure>

<p>不难看出，图都是同一张图，只不过链接预测用到的数据集中多了<code>edge_label</code>和<code>edge_label_index</code>这两个量：</p>
<ul>
<li><code>edge_label</code>：<code>Tensor</code>，数据集的标签，即<code>y</code>，值为1或0，对于<code>val_data</code>和<code>test_data</code>（<code>train_data</code>未进行负采样），既包含了正样本的标签，又包含了负样本的标签，因此对于上文的数据集，正样本数为527，负样本数也为527。</li>
<li><code>edge_label_index</code>：<code>Tensor</code>，数据集的样本，即<code>X</code>，但是，此处<code>X</code>的内容并不是边的特征，而是边的两个结点的编号，因而也不是用来训练的，其中<code>edge_label_index[0]</code>为起始结点的编号，<code>edge_label_index[1]</code>为终点结点的编号。</li>
</ul>
<blockquote>
<p><code>train_data</code>的负采样可以使用PyG提供的<code>torch_geometric.utils.negative_sampling</code>函数，见<a href="https://pytorch-geometric.readthedocs.io/en/latest/modules/utils.html#torch-geometric-utils">torch_geometric.utils</a>，此处不再赘述。要特别注意把负样本加入<code>edge_label</code>和<code>edge_label_index</code>中。</p>
</blockquote>
<h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><p>链接预测图神经网络的训练与结点分类图神经网络的训练并没有本质的不同。若将<em>Node embedding</em>的过程称为编码过程，最后的链接预测&#x2F;结点分类称为解码过程，那么两者的编码过程是<strong>一致的</strong>，即卷积和消息传递的过程是一致的。两者的不同在解码过程。对于结点分类任务，解码是对得到<em>Node embedding</em>做一次softmax，因此最后的结点的特征维度也被限定为了结点类别的数量；对于链接预测任务，解码是对<code>edge_label_index</code>所选定的边两端的结点计算相似度后求sigmoid，即：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">decoder</span>(<span class="params">self, X, edge_label_index</span>):  <span class="comment"># X为编码后的结点特征</span></span><br><span class="line">    src_node = X[edge_label_index[<span class="number">0</span>]]  <span class="comment"># 取出边起始结点的特征，shape (N x F)</span></span><br><span class="line">    end_node = X[edge_label_index[<span class="number">1</span>]]  <span class="comment"># 取出边终点结点的特征，shape (N x F)</span></span><br><span class="line">    <span class="keyword">return</span> (src_node * end_node).<span class="built_in">sum</span>(dim=<span class="number">1</span>)  <span class="comment"># 向量内积求相似度，shape (N)，此处没有直接sigmoid，放在网络外面做也是一样的</span></span><br></pre></td></tr></table></figure>

<h2 id="精度分析"><a href="#精度分析" class="headerlink" title="精度分析"></a>精度分析</h2><p>由于链接预测是一个二分类问题，因此精度分析采用AUC。</p>
<blockquote>
<p>AUC的含义和计算方法见<a href="/2023/04/13/MachineLearningDiagnostics/#Skewed-datasets">F1 score and ROC &amp; AUC</a>。</p>
</blockquote>
<h1 id="结点分类和链接预测的测试集泄露问题"><a href="#结点分类和链接预测的测试集泄露问题" class="headerlink" title="结点分类和链接预测的测试集泄露问题"></a>结点分类和链接预测的测试集泄露问题</h1><p>测试集泄露，即测试集中的样本特征或者标签在训练网络的过程中被使用了。对于GNN来说，测试集泄露是一个很普遍的问题，因为GNN的消息传递过程不可避免地要用到其他结点的信息，而这些结点就有可能包括测试集的结点。</p>
<h2 id="结点分类"><a href="#结点分类" class="headerlink" title="结点分类"></a>结点分类</h2><p>对于最初的<a href="https://arxiv.org/pdf/1609.02907.pdf">GCN</a>，其在训练模型参数时用到的是整个图的结点特征和邻接矩阵，因而不可避免地传递了测试集结点的特征信息。虽然可以把用来计算损失的训练集结点及其边、用于测试效果的测试集结点及其边从原图中单独拎出来形成两个互不相交的子图来从源头上避免数据泄露（即在训练过程中，只让消息在训练集结点间传递），但是这样做会导致精度下降，因为原图的整体结构被破坏了。</p>
<p><a href="https://arxiv.org/pdf/1706.02216.pdf">GraphSAGE</a>的出现使得大家对GNN的消息传递有了全新的理解，即：只要把<em>待预测结点</em>周围结点的信息传递给<em>待预测结点</em>即可。在这样的视角下，原本耦合于全图的各个结点实际上变成了一个个独立的<strong>子图</strong>，子图的中心结点是待预测结点，而其他结点是要将信息传递给中心结点的辅助结点。于是，单一结点的结点分类成为了可能（原先必须要将整个图喂进模型中），只要为该结点随机取样一个子图即可。相应地，数据泄露的影响也降到最低（此时网络学习的是一种利用邻居结点信息获得中心结点<em>Embedding</em>的方法&#x2F;模式，而不是学习怎样为每个结点生成一个<em>Embedding</em>）。这样的子图&#x2F;采样的方法对于GCN、SGC、GAT等也是成立的，因为它们本质上都是消息传递型的GNN。</p>
<h2 id="链接预测"><a href="#链接预测" class="headerlink" title="链接预测"></a>链接预测</h2><p>类似地，对于链接预测，其在消息传递的过程中会用到结点间的连接信息，也就是结点间的边。但是，不同于结点分类的是，链接预测的对象是边，而边的泄露在消息传递的过程中是可以被规避掉的，因为被传递的消息是结点信息而不是边的信息。</p>
<p>具体来说，如不对前面提到的<code>Cora</code>数据集进行负采样，则其链接预测的训练集、验证集、测试集（比例8：1：1）和全集依次为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(train_data, <span class="string">&quot;\n&quot;</span>, val_data, <span class="string">&quot;\n&quot;</span>, test_data, <span class="string">&quot;\n&quot;</span>, data)</span><br><span class="line">Data(x=[<span class="number">2708</span>, <span class="number">1433</span>], edge_index=[<span class="number">2</span>, <span class="number">8448</span>], y=[<span class="number">2708</span>], train_mask=[<span class="number">2708</span>], val_mask=[<span class="number">2708</span>], test_mask=[<span class="number">2708</span>], edge_label=[<span class="number">4224</span>], edge_label_index=[<span class="number">2</span>, <span class="number">4224</span>])</span><br><span class="line">Data(x=[<span class="number">2708</span>, <span class="number">1433</span>], edge_index=[<span class="number">2</span>, <span class="number">8448</span>], y=[<span class="number">2708</span>], train_mask=[<span class="number">2708</span>], val_mask=[<span class="number">2708</span>], test_mask=[<span class="number">2708</span>], edge_label=[<span class="number">527</span>], edge_label_index=[<span class="number">2</span>, <span class="number">527</span>])</span><br><span class="line">Data(x=[<span class="number">2708</span>, <span class="number">1433</span>], edge_index=[<span class="number">2</span>, <span class="number">9502</span>], y=[<span class="number">2708</span>], train_mask=[<span class="number">2708</span>], val_mask=[<span class="number">2708</span>], test_mask=[<span class="number">2708</span>], edge_label=[<span class="number">527</span>], edge_label_index=[<span class="number">2</span>, <span class="number">527</span>])</span><br><span class="line">Data(x=[<span class="number">2708</span>, <span class="number">1433</span>], edge_index=[<span class="number">2</span>, <span class="number">10556</span>], y=[<span class="number">2708</span>], train_mask=[<span class="number">2708</span>], val_mask=[<span class="number">2708</span>], test_mask=[<span class="number">2708</span>])</span><br></pre></td></tr></table></figure>

<p>事实上，上面四个数据集都代表着四张不同的图，其中前面三张图是最后一张（原图）的<em>生成子图</em>，即前面三张图包含原图的所有结点和部分边。这些信息体现在<code>edge_index</code>上。</p>
<ul>
<li>对于训练集，其不包含负样本的<code>edge_label_index</code>是<code>edge_index</code>的无向图形式（即在<code>edge_index</code>中，无向图的一条边被视为有向图的两条边，而<code>edge_label_index</code>只将其视为一条边，所以<code>edge_label_index</code>的边数是<code>edge_index</code>的一半）。在训练模型过程中，训练集的消息传递会用到所有的训练集边（<code>edge_index</code>），最终的预测也会用到所有的训练集边（<code>edge_label_index</code> &amp; <code>edge_label</code>）+负采样边；</li>
<li>对于验证集，其消息传递的过程中不能使用验证集中特有的边，因此消息传递用到的是训练集的边，这也是为什么验证集的<code>edge_index</code>和训练集的<code>edge_index</code>是一样的。最终的分类过程只会用到验证集特有的边（<code>edge_label_index</code> &amp; <code>edge_label</code>）+负采样边；</li>
<li>对于测试集，其消息传递的过程中也不能使用测试集中特有的边，但是可以使用验证集中特有的边，因此消息传递用到的是训练集和边+验证集特有的边（<code>8448+527*2=9502</code>）。最终的分类过程同样只会用到测试集特有的边+负采样边。</li>
</ul>
<blockquote>
<p>需要特别注意的是，虽然负采样的边不会参与消息传递的过程，但是训练集、验证集和测试集在分类阶段的负采样边同样不能重叠。这很容易实现，只要让负采样在正采样的边所特有的结点间进行即可（e.g. <code>train_data</code>的在8848条边的结点间进行，<code>val_data</code>的在527条边的结点间进行）。</p>
</blockquote>
<p>综合来说，链接预测是可以保证数据完全不泄露的，只要保证训练集、验证集和测试集拥有<code>label</code>的边不同以及除训练集外的消息传递不用有<code>label</code>的边即可。</p>
<blockquote>
<p>在更严格的算法中（e.g. <a href="https://arxiv.org/pdf/1802.09691.pdf">SEAL</a>），训练集的边还被进一步划分为<code>training_supervision_edges</code>和<code>training_message_edges</code>，其中前者是训练集特有的带<code>label</code>的边，而后者是用于消息传递的边。而一般的算法（e.g. <a href="https://arxiv.org/abs/1611.07308">GAE</a>），对这两者不做区分。</p>
</blockquote>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://blog.csdn.net/Cyril_KI/article/details/125956540">PyG搭建GCN实现链接预测</a></li>
<li><a href="https://blog.csdn.net/weixin_41601540/article/details/121500949">Link Prediction Note</a></li>
</ul>
]]></content>
      <categories>
        <category>GNN</category>
      </categories>
      <tags>
        <tag>GNN</tag>
      </tags>
  </entry>
  <entry>
    <title>PageRank</title>
    <url>/2023/07/16/PageRank/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="PageRank"><a href="#PageRank" class="headerlink" title="PageRank"></a>PageRank</h1><p>PageRank将互联网视作由HTML网页组成的有向图（现在不行了，因为有许多不能被外部访问的内部网页，如朋友圈等），并为所有网页的重要性进行打分。当用户搜索相关网页时，PageRank便将得分高的网页放在搜索结果的前面。PageRank将任意互联网结点$i$的重要性定义为$r_i$，并将$i$的一条入度边视作另一个网页对$i$结点的引用，而将出度边视作$i$结点对其他网页的引用。PageRank的评分逻辑可以从三个角度进行理解。</p>
<h2 id="解线性方程组"><a href="#解线性方程组" class="headerlink" title="解线性方程组"></a>解线性方程组</h2><p>这是理解PageRank的第一个角度，也是最原理性的数学角度。一个很符合生活常识的逻辑是：某篇论文的重要性，不取决于这篇论文引用了什么论文，而是取决于哪些论文引用了这篇论文和这些引用了它的论文的重要性。对于互联网网页组成的图，也是如此。假设网页结点$j$的重要性为$r_j$，出度为$d_j$，其出度边平均地将$j$的重要性分给了它引用的网页，而$j$的重要性又是引用了$j$的网页分给$j$的重要性之和，则很自然地，$r_j$可以表示为：</p>
<p>$$<br>r_j&#x3D;\frac{r_i}{3}+\frac{r_k}{4}<br>$$</p>
<p><img src="/2023/07/16/PageRank/1.png" alt="1"></p>
<center style="font-size:12px; font-weight:bold">Fig. 1. Rank of j</center><br>

<p>其中，3和4分别是$i$和$k$的出度。若将所有结点的重要性都用这种方式表示，我们便可以得到一个关于所有结点重要性的方程组：</p>
<p>$$<br>\begin{cases}<br>    \vdots \\<br>    r_j&#x3D;\sum _{i\to j} \frac{r _i}{d _i}\tag{1}\\<br>    \vdots<br>\end{cases}<br>$$</p>
<p>假设图的总结点数为$n$，那么这就是一个$n$元的线性方程组。我们有$n$个方程，然而实际有效的只有$n-1$个方程（很好理解，若$n$个方程都有效，我们将得到唯一解，但实际上将所有的$r$放大同样的倍数得到的值也是合理的），因此需要补充方程</p>
<p>$$<br>\sum _{i\in G}r_i&#x3D;1<br>$$</p>
<p>以限制重要性的取值，其中$i\in G$指结点$i$是图中的结点。</p>
<p>以上便是PageRank的数学逻辑。若图的结点数量较少，我们可以用上面的联立方程组很快地得到解析解。但实际上$n$的值是十分大的，直接求解会很麻烦，因此只能采用近似的方法：<strong>假设每个结点的初始重要性都是$\frac{1}{n}$，将初始重要性代入式$(1)$中，可以得到更新后的重要性，这视作一次迭代。重复上述过程足够多次，所有结点的重要性都将收敛为一个稳定值。</strong> 这样的迭代是很合理的，因为所有网页诞生之初，其重要性都是一致的，只不过在后续网页间互相引用的过程中，其他网页为其引用的网页贡献了重要性，使得被引用的网页变得更加重要，而上述的迭代操作便是对这一过程的模拟。</p>
<h3 id="矩阵形式的方程组"><a href="#矩阵形式的方程组" class="headerlink" title="矩阵形式的方程组"></a>矩阵形式的方程组</h3><p>式$(1)$的方程组可以很容易地被转变为一个矩阵$\mathbf{M}$，其中：</p>
<p>$$<br>\mathbf{M} _{ji}&#x3D;\frac{1}{d _i}<br>$$</p>
<p>表示结点$i$因为引用结点$j$而流向$j$的重要性占$i$自身重要性的比例。再令所有结点的初始重要性为向量$\mathbf{r}$：</p>
<p>$$<br>\mathbf{r}&#x3D;[\frac{1}{n},...,\frac{1}{n}] _{1\times n} ^T<br>$$</p>
<p>于是，一次迭代便可视为$\mathbf{r}$左乘一次$\mathbf{M}$：</p>
<p>$$<br>\mathbf{r} ^{(k+1)}&#x3D;\mathbf{M}\mathbf{r} ^{(k)}<br>$$</p>
<p>其中，$\mathbf{r} ^{(k)}$表示$\mathbf{r}$第$k$次迭代的结果。</p>
<blockquote>
<p>矩阵$\mathbf{M}$又称<em>Stochastic adjacency matrix</em>（随机邻接矩阵），而这种迭代的方法称<em>Power iteration</em>（幂迭代）。从矩阵$\mathbf{M}$的名称不难看出其与邻接矩阵$A$具有不可分的关系，而这也是PageRank能被用于GNN的消息传递的原因之一（我猜的）。</p>
</blockquote>
<p>当$k\to+\infty$时，$\mathbf{r}$将收敛为一个稳定的结果（事实上，$k$为50时就已经基本收敛）。计算机很擅长矩阵运算，因此这也是最常被用来计算PageRank的方法。</p>
<h2 id="随机游走"><a href="#随机游走" class="headerlink" title="随机游走"></a>随机游走</h2><p>另外一种理解角度是随机游走（Random walk）。假设图中有一个随机游走的surfer，在$t$时刻，它位于网页$i$；在$t+1$时刻，它将随机点击网页$i$中的超链接并到达网页$j$。初始时刻，surfer点击任何一个网页的概率相同：</p>
<p>$$<br>\mathbf{p}(0)&#x3D;[\frac{1}{n},...,\frac{1}{n}] _{1\times n} ^T<br>$$</p>
<p>此后，每经过一次随机游走，surfer到达任意一个网页的概率都将发生变化：</p>
<p>$$<br>\mathbf{p}(t+1)&#x3D;\mathbf{M}\mathbf{p}(t)<br>$$</p>
<p>此时，$\mathbf{M} _{ji}$可以视为从网页$i$跳到网页$j$的概率，$\mathbf{p}(t+1) _j$则是$t+1$次随机游走后，仍然在网页$j$的概率，其在数值上等于$t$次随机游走到达任意网页后又跳回网页$j$的数学期望。当$t\to +\infty$时，$\mathbf{p}(t)$便可被视为一开始surfer点击各个网页的概率向量（类似后验概率）。显然，概率越大的网页就越重要。</p>
<blockquote>
<p>这是一种更加合乎人们上网习惯的模拟，也是更好理解的一种角度。</p>
</blockquote>
<h2 id="马尔科夫链"><a href="#马尔科夫链" class="headerlink" title="马尔科夫链"></a>马尔科夫链</h2><p>第三种角度是马尔可夫链，其定义的状态转移实际上可以看作一种随机游走，因此与第二种角度基本上是互通的。</p>
<h2 id="Google-Matrix"><a href="#Google-Matrix" class="headerlink" title="Google Matrix"></a>Google Matrix</h2><p>针对实际的网络，常规的$\mathbf{M}$并不是特别地好用，因为当图中包含以下这两类结点时，用$\mathbf{M}$计算得到的$\mathbf{r}$将失真：</p>
<ol>
<li><p>Dead ends</p>
<p> 此类结点只有入度，没有出度：</p>
<p> <img src="/2023/07/16/PageRank/2.jpg" alt="2"></p>
 <center style="font-size:12px; font-weight:bold">Fig. 2. Dead ends</center><br>

<p> surfer一旦走入这类结点，将被永远地困在里面，导致重要性消失。</p>
</li>
<li><p>Spider traps</p>
<p> 此类结点的唯一出度边是个自旋边：</p>
<p> <img src="/2023/07/16/PageRank/3.jpg" alt="3"></p>
 <center style="font-size:12px; font-weight:bold">Fig. 3. Spider traps</center><br>

<p> Spider traps会导致所有的重要性被这一类结点吸收。</p>
</li>
</ol>
<p>对于这两类结点，PageRank给出的解决方案是：1）Dead ends，人为地为其添加$n$条出度边，使其重要性能够流向所有的结点（包括自己）；2）Spider traps，在每一次迭代任意结点时，令其有$\beta$的概率按照原来的出度边流动，有$1-\beta$的概率随机流向所有结点（包括自己）的任意一个。于是，在处理完Dead ends后，一次迭代后$r_j$变为：</p>
<p>$$<br>\begin{align*}<br>    r_j<br>    &amp;&#x3D;\sum\limits _{i&#x3D;0} ^{n-1} [\beta \mathbf{M} _{ji}+(1-\beta)\frac{1}{n}]r _i\\<br>    &amp;&#x3D;\sum\limits _{i&#x3D;0} ^{n-1} \beta \mathbf{M} _{ji}r _i+(1-\beta)\frac{1}{n}\tag{2}<br>\end{align*}<br>$$</p>
<p>上式成立因为$\sum\limits _{i&#x3D;0} ^{n-1}r_i&#x3D;1$。为了方便计算，PageRank定义了Google Matrix $\mathbf{G}$：</p>
<p>$$<br>    \mathbf{G}&#x3D;\beta\mathbf{M}+(1-\beta)[\frac{1}{n}] _{n\times n}<br>$$<br>$$<br>\begin{align*}<br>        \mathbf{r} ^{(k+1)}<br>        &amp;&#x3D;\mathbf{G}\mathbf{r} ^{(k)}\\<br>        &amp;&#x3D;\beta\mathbf{M}\mathbf{r} ^{(k)}+(1-\beta)[\frac{1}{n}] _{n\times 1}\\<br>        &amp;&#x3D;\beta\mathbf{M}\mathbf{r} ^{(k)}+(1-\beta)\mathbf{S}\tag{3}<br>\end{align*}<br>$$</p>
<blockquote>
<p>从随机游走的角度，式$(2)$可视为人为地提高到达Non-spider traps结点的概率，使得surfer能够跳出Spider traps，但是仍保持总概率和为1。</p>
</blockquote>
<h1 id="Personalized-PageRank"><a href="#Personalized-PageRank" class="headerlink" title="Personalized PageRank"></a>Personalized PageRank</h1><p>与PageRank是要计算图中所有结点的重要性不同，Personalized PageRank的目标是要计算所有结点与结点$j$的关联度。从随机游走的角度出发，我们就必须要让随机游走到达$j$的可能性最大，因为任何一个结点肯定与自身的关联性最大，由此，从$j$到达的结点分到的可能性也会增大，进而与$j$关联性大的结点就会被筛选出来。</p>
<p>要想达到这个目标，我们只能人为地引导surfer向$j$结点跳转，即，将式$(3)$中的$\mathbf{S}$更改为：</p>
<p>$$<br>\mathbf{S}&#x3D;<br>\begin{cases}<br>    \mathbf{S}_j&#x3D;1\\<br>    \mathbf{S}_i&#x3D;0,i\ne j<br>\end{cases}<br>$$</p>
<p>使得任意结点不再是随机跳转到任意一个结点，而是随机跳转到指定结点$j$。</p>
<blockquote>
<p>Personalized PageRank是PPNP（原始版的APPNP）的理论基础。</p>
</blockquote>
<h1 id="Topic-Sensitive-PageRank"><a href="#Topic-Sensitive-PageRank" class="headerlink" title="Topic-Sensitive PageRank"></a>Topic-Sensitive PageRank</h1><p>Topic-Sensitive PageRank可视为拓展版的Personalized PageRank。Topic-Sensitive PageRank的目标是基于用户的个人信息，为其提供更加个性化的搜索结果，如：当一个平时喜欢搜索篮球领域相关问题的人搜索“科比”时，搜索引擎便会为其提供更多有关篮球领域的“科比”而不是其他领域的“科比”的信息。</p>
<p>为达到这个目的，Topic-Sensitive PageRank也是在向量$\mathbf{S}$上做文章，不过它既不像PageRank那样广撒网，也不像Personalized PageRank“钟情于一人”，而是给予已知的拥有目标主题的网页更大的跳转机会，既：</p>
<p>$$<br>\mathbf{S}&#x3D;\frac{[1,0,0,...,0,1,0] ^T}{\sum\limits _{i&#x3D;0} ^{n-1}\mathbf{S} _i}<br>$$</p>
<blockquote>
<p>Topic-Sensitive PageRank是APPNP的理论基础。</p>
</blockquote>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://www.bilibili.com/video/BV1uP411K7yN">改变世界的谷歌PageRank算法</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/450267809">图表示学习系列4——PageRank</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/509849231">【CS224W】04链接分析算法</a></li>
<li><a href="https://blog.csdn.net/likeyou1314918273/article/details/106895794">个性化PageRank算法</a></li>
<li><a href="https://blog.csdn.net/insistGoGo/article/details/9885037">Topic-Sensitive PageRank(主题敏感的PageRank )</a></li>
</ul>
]]></content>
      <categories>
        <category>GNN</category>
      </categories>
      <tags>
        <tag>GNN</tag>
        <tag>Recommender System</tag>
      </tags>
  </entry>
  <entry>
    <title>Expressive Power of GNNs</title>
    <url>/2023/07/15/ExpressivePowerofGNNs/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="消息传递型图神经网络"><a href="#消息传递型图神经网络" class="headerlink" title="消息传递型图神经网络"></a>消息传递型图神经网络</h1><p>图神经网络（GNNs），究其本质，是一种特殊的映射$f$，它将顶点的特征及其图的结构映射为一个包含了图连接信息的向量，这一个过程称为<em>Node Embedding</em>，事实上是一个编码的过程。将图的连接信息和顶点特征一起映射为一个向量，这内在地存在一个消息传递（Message Passing）的过程，因为对于某个中心顶点来说，图的连接信息主要体现为其邻接顶点，要想在中心顶点中包含图的连接信息，就要将其邻接顶点的信息传递到中心顶点去：</p>
<p>$$<br>\begin{align*}<br>    a _{v} ^{(k)}&amp;&#x3D;\text{AGGREGATE} ^{(k)}(\left\{h _u ^{k-1}:u\in\mathcal{N}(v)\right\})\tag{1}\\<br>    h _{v} ^{(k)}&amp;&#x3D;\text{COMBINE} ^{(k)}(h _v ^{(k-1)},a _n ^{(k)})\tag{2}<br>\end{align*}<br>$$</p>
<p>式$(1)$表示中心顶点$v$的邻接顶点集$\mathcal{N}(v)$对中心顶点的第$k$次消息传递，式$(2)$则是中心顶点$v$由自己之前的特征$h _v ^{(k-1)}$和邻接顶点的新消息$a _v ^{(k)}$产生感受野更大的新特征。</p>
<h1 id="图神经网络的表达能力"><a href="#图神经网络的表达能力" class="headerlink" title="图神经网络的表达能力"></a>图神经网络的表达能力</h1><p>对于某个中心顶点$v$，$k$次的消息传递可以生成一个距离中心顶点最远距离为$k$-hop的子图，这个子图也可以被表示成一个高度为$k+1$的根子树（或称计算图）：</p>
<p><img src="/2023/07/15/ExpressivePowerofGNNs/1.png" alt="1"></p>
<center style="font-size:12px; font-weight:bold">Fig. 1. Rooted subtree (k=2)</center><br>

<p>要想使得图神经网络的<em>Node Embedding</em>能够充分体现出中心顶点与全图的关系，并且与其他顶点相互区别，根子树到<em>Embedding</em>的映射就应该是唯一的，因此可以把图神经网络的表达能力定义为：</p>
<p>$$<br>\begin{align*}<br>    \text{GNN的表达能力}&amp;&#x3D;\text{不同计算图学到不同Embedding的能力}\\<br>    &amp;&#x3D;\text{区分根结点Embedding的能力}\\<br>    &amp;&#x3D;\text{区分不同图结构的能力}<br>\end{align*}<br>$$</p>
<p>最理想的情况是对于不同的计算图，其<em>Embedding</em>也不相同。要想达到这样的效果：</p>
<ol>
<li><p>$\text{AGGREGATE}$是单射函数；</p>
</li>
<li><p>$\text{COMBINE}$是单射函数；</p>
</li>
<li><p>若要对整个图进行分析，还需要额外的$\text{READOUT}$操作生成图的全局信息：</p>
<p>$$<br>h _G&#x3D;\text{READOUT}(\left\{h _v ^{(K)}|v\in G\right\})<br>$$<br>此时，$\text{READOUT}$也必须是单射函数。</p>
</li>
</ol>
<h1 id="Weisfeiler-Lehman-test"><a href="#Weisfeiler-Lehman-test" class="headerlink" title="Weisfeiler-Lehman test"></a>Weisfeiler-Lehman test</h1><p>Weisfeiler-Lehman test（WL test）是所有消息传递型图神经网络的“祖宗”，其能力也是消息传递型图神经网络能力的上限，它的消息传递满足上面的三点要求。</p>
<p>WL test假设所有顶点都是无特征的，它初始时为所有顶点都分配了相同的颜色1：</p>
<p><img src="/2023/07/15/ExpressivePowerofGNNs/2.png" alt="2"></p>
<center style="font-size:12px; font-weight:bold">Fig. 2. Assign initial colors</center><br>

<p>之后，通过消息传递，邻接顶点的颜色汇聚到中心顶点：</p>
<p><img src="/2023/07/15/ExpressivePowerofGNNs/3.png" alt="3"></p>
<center style="font-size:12px; font-weight:bold">Fig. 3. Aggregate neighboring colors</center><br>

<p>再经由哈希映射（所有图共用一个哈希表），将顶点映射为新的颜色：</p>
<p><img src="/2023/07/15/ExpressivePowerofGNNs/4.png" alt="4"></p>
<center style="font-size:12px; font-weight:bold">Fig. 4. Combine and hash aggregated colors</center><br>

<p>不断重复Fig 3和Fig 4的操作$k$次：</p>
<p><img src="/2023/07/15/ExpressivePowerofGNNs/5.png" alt="5"></p>
<center style="font-size:12px; font-weight:bold"></center>

<p><img src="/2023/07/15/ExpressivePowerofGNNs/6.png" alt="6"></p>
<center style="font-size:12px; font-weight:bold">Fig. 5. Next round</center><br>

<p>最后，执行$\text{READOUT}$操作。WL test定义$\text{READOUT}$操作为：将整个过程中，每张图所出现的所有颜色的次数作为该图的<em>Embedding</em>（没出现的记为0）：</p>
<p><img src="/2023/07/15/ExpressivePowerofGNNs/7.png" alt="7"></p>
<center style="font-size:12px; font-weight:bold">Fig. 6. Readout</center><br>

<p>并定义：</p>
<p>$$<br>\phi(G_1) ^T\phi(G_2)<br>$$</p>
<p>为两张图的相似度（实际计算时两个向量都要归一化后再点乘）。</p>
<h1 id="GIN"><a href="#GIN" class="headerlink" title="GIN"></a>GIN</h1><p>作者在文章的后半部分提出了全新的消息传递型图神经网络Graph Isomorphism Network（GIN）。作者认为，GIN是对WL test的最优近似，因为WL test可以写为：</p>
<p>$$<br>c _{v} ^{(k)}&#x3D;\text{HASH} ^{(k)}(c _v ^{(k-1)},\left\{c _u ^{k-1}:u\in\mathcal{N}(v)\right\})<br>$$</p>
<p>其中，$c _v ^{(k)}$表示顶点$v$在第$k$次消息传递后的<em>color</em>，而作者将GIN定义为：</p>
<p>$$<br>h _{v} ^{(k)}&#x3D;\text{MLP} ^{(k)}((1+\epsilon ^{(k)})h _v ^{(k-1)}+\sum _{u\in \mathcal{N}(v)}h _u ^{k-1})<br>$$</p>
<p>其中$\text{MLP}$和$\epsilon$均是可学习的参数。作者如此定义GIN是基于两个方面的考虑：</p>
<ol>
<li><p><strong>Sum-pooling具有很强的单射能力</strong><br>  相比于GCN用的mean-pooling和GraphSAGE用的max-pooling，sum-pooling具有更强的区分不同图结构的能力。如下图Fig 7所示的两张图，在这两张图中，所有顶点的特征都相同。虽然它们很明显是两张完全不同的图，但是如果用mean-pooling或者max-pooling，中心顶点$v$和$v&#39;$最终的<em>Embedding</em>将完全相同，因而无法区分出这两张图，而对于sum-pooling，则不会出现这种情况。<br>  <img src="/2023/07/15/ExpressivePowerofGNNs/8.png" alt="8"></p>
  <center style="font-size:12px; font-weight:bold">Fig. 7. Mean and Max both fail</center>
</li>
<li><p><strong>Universal approximation theorem</strong><br>  根据Hornik等提出的<em>Universal approximation theorem</em>，只要参数足够多，一层MLP可以拟合任何的函数。因此，GIN中的MLP是作者用于拟合ML test中的哈希函数的。</p>
<blockquote>
<p>GIN将$\text{READOUT}$也定义为sum，但不同的是，作者将$0-K$的$\text{READOUT}$都拼接了起来以求保留可能在中间过程中出现的更能体现图整体特点的信息：<br>$$<br>  h _G&#x3D;\text{CONCAT}(\text{READOUT}(\left\{h _v ^{(k)}|v\in G\right\})|k&#x3D;0,1,...,K)<br>$$</p>
</blockquote>
</li>
</ol>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://arxiv.org/pdf/1810.00826.pdf">How Powerful are Graph Neural Networks?</a></li>
<li><a href="https://www.bilibili.com/video/BV14W4y1V7gg">传统图机器学习的特征工程-全图【斯坦福CS224W】</a></li>
<li><a href="https://www.bilibili.com/video/BV1MT411Y7da">图神经网络GNN的表达能力</a></li>
</ul>
]]></content>
      <categories>
        <category>GNN</category>
      </categories>
      <tags>
        <tag>Paper</tag>
        <tag>GNN</tag>
      </tags>
  </entry>
  <entry>
    <title>Bug and Usage of Torch</title>
    <url>/2023/07/08/BugAndUsageOfTorch/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="Bugs"><a href="#Bugs" class="headerlink" title="Bugs"></a>Bugs</h1><h2 id="PyG的Planetoid的下载问题"><a href="#PyG的Planetoid的下载问题" class="headerlink" title="PyG的Planetoid的下载问题"></a>PyG的Planetoid的下载问题</h2><p><a href="https://pytorch-geometric.readthedocs.io/en/latest/index.html">PyG</a>（PyTorch Geometric）是图神经网络（GNN）常用的库函数的简称。它里面包含了训练图神经网络时常用的数据集以及网络架构等。PyG是基于PyTorch的，因此它和PyTorch完全兼容。</p>
<p>Planetoid是PyG中包含的图数据集（<code>torch_geometric.datasets.Planetoid</code>）。它里面有<code>Cora</code>、<code>CiteSeer</code>和<code>PubMed</code>这三个常用的引文网络。但是，原数据集是寄存在github上面的，因此在国内下载可能会遇到问题。解决方法为：</p>
<ol>
<li>找到PyG库中的<code>planetoid.py</code>文件；</li>
<li>将：<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">url = <span class="string">&#x27;https://github.com/kimiyoung/planetoid/raw/master/data&#x27;</span></span><br></pre></td></tr></table></figure>
修改为：<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">url=<span class="string">&#x27;https://gitee.com/jiajiewu/planetoid/raw/master/data&#x27;</span></span><br></pre></td></tr></table></figure>
即，在gitee上下载而不用github。</li>
</ol>
<blockquote>
<p>若无图形界面，如简易的Ubuntu，可用<code>sudo find / -name planetoid.py</code>快速找到文件的位置。</p>
</blockquote>
<p>完成上述操作后<code>PubMed</code>数据集的下载仍会出问题，具体报错为<code>_pickle.UnpicklingError: invalid load key, &#39;&lt;&#39;.</code>。原因未知，但可以确定是从服务器上下载文件时出现的问题。解决方法：</p>
<ul>
<li>在<a href="https://gitee.com/jiajiewu/planetoid/tree/master/data">Planetoid</a>中把与<code>PubMed</code>有关的<strong>八</strong>个文件下载下来，手动放到<code>raw</code>文件夹里即可。</li>
</ul>
<blockquote>
<p>需要注意的是，在PyG的<code>Planetoid</code>数据集中，<code>num_classes</code>字段已经被删除，所以要想获得图中顶点的类别数，应该用<code>dataset.y.max().item() + 1</code>（其中<code>dataset</code>是对应的图数据集，如<code>PubMed</code>、<code>Cora</code>等）。</p>
</blockquote>
<h1 id="Usage-of-Torch"><a href="#Usage-of-Torch" class="headerlink" title="Usage of Torch"></a>Usage of Torch</h1><p>这一节包含了一些有用的PyTorch函数用法或语法。本节中，假设<code>a</code>为一个已经定义好的张量<code>torch.tensor</code>。</p>
<h2 id="torch-max"><a href="#torch-max" class="headerlink" title="torch.max"></a>torch.max</h2><p><code>torch.max(input, dim, keepdim=False, *, out=None)</code>或者<code>a.(dim, keepdim=False, *, out=None)</code></p>
<p>该函数将返回一个元组<code>(values, indices)</code>，其中<code>values</code>是<code>input</code>的给定<code>dim</code>的同一行的元素的最大值张量，而<code>indices</code>是该最大值在<code>dim</code>的坐标张量，如：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.randn(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">tensor([[-<span class="number">1.2360</span>, -<span class="number">0.2942</span>, -<span class="number">0.1222</span>,  <span class="number">0.8475</span>],</span><br><span class="line">        [ <span class="number">1.1949</span>, -<span class="number">1.1127</span>, -<span class="number">2.2379</span>, -<span class="number">0.6702</span>],</span><br><span class="line">        [ <span class="number">1.5717</span>, -<span class="number">0.9207</span>,  <span class="number">0.1297</span>, -<span class="number">1.8768</span>],</span><br><span class="line">        [-<span class="number">0.6172</span>,  <span class="number">1.0036</span>, -<span class="number">0.6060</span>, -<span class="number">0.2432</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.<span class="built_in">max</span>(a, <span class="number">1</span>)</span><br><span class="line">torch.return_types.<span class="built_in">max</span>(values=tensor([<span class="number">0.8475</span>, <span class="number">1.1949</span>, <span class="number">1.5717</span>, <span class="number">1.0036</span>]), indices=tensor([<span class="number">3</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]))</span><br></pre></td></tr></table></figure>

<p>得到的是行最大值。该函数可以很容易地得到softmax的最终结果，也可用于分析分类的准确性。</p>
<blockquote>
<p>此类操作属于PyTorch中的张量降维操作，更多类似的操作见<a href="https://pytorch.org/docs/stable/torch.html#reduction-ops">reduction-ops</a></p>
</blockquote>
<h2 id="torch-eq"><a href="#torch-eq" class="headerlink" title="torch.eq"></a>torch.eq</h2><p><code>torch.eq(input, other, *, out=None)</code>或者<code>a.eq(other, *, out=None)</code></p>
<p>该函数将两个张量（<code>input</code>和<code>other</code>）进行逐元素比较，若相同位置的两个元素相同，则返回<code>True</code>；否则，返回<code>False</code>。最终结果是个<code>bool</code>张量：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; torch.eq(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))</span><br><span class="line">tensor([[ True, False],</span><br><span class="line">        [False, True]])</span><br></pre></td></tr></table></figure>

<p>上述操作结合<code>torch.max</code>可以用于分析softmax预测正确的个数，如：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">correctnesss = y_hat.max(dim=1)[1].eq(y).float().sum()</span><br></pre></td></tr></table></figure>

<p>以上<code>y_hat</code>为softmax结果，而<code>y</code>为实际的标签。上面的代码首先求出softmax的预测结果（假设标签为0，1，……，n-1，那么<code>dim</code>的下标就是预测值），然后再比较预测值与实际值是否相同，将正确的个数相加就得到了最终预测正确的个数。</p>
<blockquote>
<p>此类操作属于PyTorch中的数值比较操作，更多类似的操作见<a href="https://pytorch.org/docs/stable/torch.html#comparison-ops">comparison-ops</a></p>
</blockquote>
<h2 id="torch-dtype"><a href="#torch-dtype" class="headerlink" title="torch.dtype"></a>torch.dtype</h2><p><code>torch.dtype</code>指的是张量元素的数据类型。PyTorch支持张量元素类型的随意转换：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.normal(<span class="number">0</span>, <span class="number">1</span>, size=(<span class="number">1</span>,<span class="number">3</span>))  <span class="comment"># PyTorch默认使用float32</span></span><br><span class="line">x = a.<span class="built_in">type</span>(torch.<span class="built_in">int</span>)  <span class="comment"># float32 -&gt; int</span></span><br><span class="line">y = a.<span class="built_in">type</span>(torch.<span class="built_in">bool</span>)  <span class="comment"># float32 -&gt; bool</span></span><br></pre></td></tr></table></figure>

<p>上述操作等价于：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = a.<span class="built_in">int</span>()  <span class="comment"># float32 -&gt; int</span></span><br><span class="line">y = a.<span class="built_in">bool</span>()  <span class="comment"># float32 -&gt; bool</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>更多Tensor数据类型见<a href="https://pytorch.org/docs/stable/tensor_attributes.html#torch-dtype">torch-dtype</a></p>
</blockquote>
<h2 id="torch-nn-NLLLoss-amp-torch-nn-CrossEntropyLoss"><a href="#torch-nn-NLLLoss-amp-torch-nn-CrossEntropyLoss" class="headerlink" title="torch.nn.NLLLoss &amp; torch.nn.CrossEntropyLoss"></a>torch.nn.NLLLoss &amp; torch.nn.CrossEntropyLoss</h2><p>两者本质上都是交叉熵损失函数，但是覆盖的范围不同：</p>
<ol>
<li><code>CrossEntropyLoss</code>会一次性完成softmax、取对数log和交叉熵操作，即：<br>$$-\sum\limits _{n&#x3D;1} ^N Y _{nm}\left\{\log[\text{softmax}(a)]\right\} _{nm}$$<br>式中，$Y$是标签的one-hot编码，其下标$nm$表示第$n$个样本的标签为$m$。当使用<code>CrossEntropyLoss</code>为损失函数时，神经网络的最后一层无需再做softmax和log操作，但这也导致实际预测时还要对神经网络的输出做一次softmax（因为此时只在乎值之间的相对关系，故不用再取log）。</li>
<li><code>NLLLoss</code>只会完成最后一步的交叉熵操作，故在神经网络的最后一层要添加softmax和log操作，不过最后预测时就不同再做额外的softmax了。</li>
<li>即：<br>$$\text{CrossEntropyLoss}&#x3D;\text{softmax}+\text{log}+\text{NLLLoss}$$</li>
</ol>
<h2 id="torch-nn-parameter-Parameter"><a href="#torch-nn-parameter-Parameter" class="headerlink" title="torch.nn.parameter.Parameter"></a>torch.nn.parameter.Parameter</h2><p><code>torch.nn.parameter.Parameter(Tensor: data=None, requires_grad=True)</code></p>
<p>它属于<code>Tensor</code>的子类，但是，不同于<code>Tensor</code>的是，<code>Parameter</code>默认有梯度，且当其与<code>nn.Module</code>类一起使用时，会被自动添加进参数列表，并出现在<code>parameters()</code>迭代器中。当我们自定义的网络需要额外的可训练参数时，可以使用<code>Parameter</code>，但是要记得单独对其进行初始化且<code>data</code>应该以<code>torch.empty(shape)</code>的形式传入（<code>troch.empty</code>将生成未被初始化的张量）。</p>
<blockquote>
<p>更多信息详见<a href="https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter">PARAMETER</a></p>
</blockquote>
<h2 id="torch-bmm"><a href="#torch-bmm" class="headerlink" title="torch.bmm"></a>torch.bmm</h2><p><code>torch.bmm(input, mat2, *, out=None)</code></p>
<p>其中<code>input</code>的形状为$b\times n\times m$，<code>mat2</code>的形状为$b\times m\times k$，最后输出的形状为$b\times n\times k$。换句话说，<code>torch.bmm</code>是对每个<code>batch</code>单独做了矩阵乘法：</p>
<p>$$<br>\text{output}[i] &#x3D; \text{input}[i] \space @\space \text{mat2}[i]<br>$$</p>
<blockquote>
<p>此类操作属于PyTorch中的与线性代数有关的运算，更多信息见<a href="https://pytorch.org/docs/stable/torch.html#blas-and-lapack-operations">BLAS and LAPACK Operations</a></p>
</blockquote>
<h2 id="torch-permute"><a href="#torch-permute" class="headerlink" title="torch.permute"></a>torch.permute</h2><p><code>torch.permute(input, dim)</code>或者<code>a.permute(dim)</code></p>
<p><code>torch.permute</code>，如其字面意思，表维度大小的交换。<code>dim</code>是一个表示新维度次序的列表，如对形状为<code>(2, 3, 4)</code>的张量<code>a</code>，应用<code>a.permute(2, 1, 0)</code>后，将返回一个形状为<code>(4, 3, 2)</code>的张量。对于二维张量或者只交换张量的最后两维的次序，<code>permute</code>相当于对二维张量进行了一次<strong>转置</strong>，其他情况则可视为一次<strong>广义转置</strong>：原来维度$i$位置的值变成了新的维度上$i$位置的值（比如转置就是让原来第$i$列的元素变成第$i$行的元素）。</p>
<blockquote>
<p>需要特别注意<code>permute</code>与<code>reshape</code>和<code>view</code>的区别。<code>reshape</code>和<code>view</code>的运行机理是将原张量从低维到高维拉成一个向量，然后再以新维度分布从高维到低维切割、堆叠，所以<code>reshape</code>和<code>view</code>前后，张量拉成的向量都是一样的，但是<code>permute</code>前后则不是。</p>
</blockquote>
<h2 id="torch-nonzero"><a href="#torch-nonzero" class="headerlink" title="torch.nonzero"></a>torch.nonzero</h2><p><code>torch.nonzero(input, *, out=None, as_tuple=False)</code></p>
<p>其中，<code>input</code>是任意维度的张量。当<code>as_tuple=False</code>时，上述操作会以二维张量的形式返回<code>input</code>中所有值不为0的元素的下标，在该二维张量中，每一行是一个非0元素的完整下标，如：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.nonzero(torch.tensor([[<span class="number">0.6</span>, <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>],</span><br><span class="line"><span class="meta">... </span>                            [<span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.4</span>, <span class="number">0.0</span>],</span><br><span class="line"><span class="meta">... </span>                            [<span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">1.2</span>, <span class="number">0.0</span>],</span><br><span class="line"><span class="meta">... </span>                            [<span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>,-<span class="number">0.4</span>]]))</span><br><span class="line">tensor([[ <span class="number">0</span>,  <span class="number">0</span>],</span><br><span class="line">        [ <span class="number">1</span>,  <span class="number">2</span>],</span><br><span class="line">        [ <span class="number">2</span>,  <span class="number">2</span>],</span><br><span class="line">        [ <span class="number">3</span>,  <span class="number">3</span>]])</span><br></pre></td></tr></table></figure>

<p>当<code>as_tuple=True</code>时，上述操作会返回一个长度为<code>len(input.shape)</code>的元组，元组的第<code>i</code>个元素表示所有非0原则在第<code>i</code>维的下标，如：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.nonzero(torch.tensor([[<span class="number">0.6</span>, <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>],</span><br><span class="line"><span class="meta">... </span>                            [<span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.4</span>, <span class="number">0.0</span>],</span><br><span class="line"><span class="meta">... </span>                            [<span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">1.2</span>, <span class="number">0.0</span>],</span><br><span class="line"><span class="meta">... </span>                            [<span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>,-<span class="number">0.4</span>]]), as_tuple=<span class="literal">True</span>)</span><br><span class="line">(tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]), tensor([<span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>]))</span><br></pre></td></tr></table></figure>

<p><code>torch.nonzero</code>操作在GNN中特别有用。通过使用<code>torch.nonzero</code>，我们可以快速地从邻接矩阵<code>A</code>中获得所有边的顶点。</p>
<blockquote>
<p><code>torch.nonzero</code>是PyTorch中丰富的切片操作中的一种，更多详细的切片操作见<a href="https://pytorch.org/docs/stable/torch.html#indexing-slicing-joining-mutating-ops">Indexing, Slicing, Joining, Mutating Ops</a></p>
</blockquote>
<h2 id="torch-randperm"><a href="#torch-randperm" class="headerlink" title="torch.randperm"></a>torch.randperm</h2><p><code>torch.randperm(n, *, generator=None, out=None, dtype=torch.int64, layout=torch.strided, device=None, requires_grad=False, pin_memory=False) → Tensor</code></p>
<p>一般来说，有用的参数只有<code>n</code>，该参数表明对<code>0</code>到<code>n-1</code>的所有下标随机排序。整个函数会返回随机排序后的下标张量。由于<code>random</code>库中的<code>random.shuffle</code>无法用于<code>Tensor</code>，我们只能使用<code>torch.randperm</code>来生成随机排序的下标，再通过切片来达到对原张量随机排序的目的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.randperm(<span class="number">4</span>)</span><br><span class="line">tensor([<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure>

<blockquote>
<p><code>torch.randperm</code>是PyTorch中丰富的随机采样操作中的一种，更多的随机操作见<a href="https://pytorch.org/docs/stable/torch.html#random-sampling">Random sampling</a></p>
</blockquote>
<h2 id="torch-triu-indices-amp-torch-tril-indices"><a href="#torch-triu-indices-amp-torch-tril-indices" class="headerlink" title="torch.triu_indices &amp; torch.tril_indices"></a>torch.triu_indices &amp; torch.tril_indices</h2><p><code>torch.triu_indices(row, col, offset=0, *, dtype=torch.long, device=&#39;cpu&#39;, layout=torch.strided) → Tensor</code></p>
<p>此处只列出<code>torch.triu_indices</code>的用法，因为两个函数用法是一致的，只不过前者取上三角下标而后者取下三角下标。</p>
<ul>
<li>参数<code>row</code>和<code>col</code>表示矩阵的行数和列数；</li>
<li><code>offset</code>是待取三角相对于主对角线的偏移，上为<code>+</code>，下为<code>-</code>，当取<code>0</code>时表示得到的下标包括主对角线。对于上三角，若想去掉主对角线，则需令<code>offset=1</code>；对于下三角，则需令<code>offset=-1</code>。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.triu_indices(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">tensor([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.triu_indices(<span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">tensor([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.tril_indices(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">tensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]])</span><br></pre></td></tr></table></figure>

<p>需要注意的是，上三角下标的排列顺序是：</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">|---&gt;</span><br><span class="line">| --&gt;</span><br><span class="line">V  -&gt;</span><br></pre></td></tr></table></figure>

<p>而下三角下标是：</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">--&gt;</span><br><span class="line">|</span><br><span class="line">||</span><br><span class="line">|||</span><br><span class="line">VVV</span><br></pre></td></tr></table></figure>

<h2 id="torch-clone-amp-torch-Tensor-detach"><a href="#torch-clone-amp-torch-Tensor-detach" class="headerlink" title="torch.clone() &amp; torch.Tensor.detach()"></a>torch.clone() &amp; torch.Tensor.detach()</h2><p>对<code>torch.clone()</code>，用法为：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">torch.clone(input, *, memory_format=torch.preserve_format)`</span><br><span class="line">或者</span><br><span class="line">a.clone()</span><br></pre></td></tr></table></figure>
<p>对<code>torch.Tensor.detach()</code>，用法为：<code>a.detach()</code></p>
<p>两者看似很像，但其实是作用完全不同的两个函数：</p>
<ul>
<li><code>torch.clone()</code>是对张量的深拷贝，它将产生一个全新的张量，这个张量拥有原张量的所有属性，但是存储空间不重叠，新、旧张量互不影响；</li>
<li><code>torch.Tensor.detach()</code>的作用是产生一个不属于原张量计算图、<code>grad_fn=None</code>且<code>requires_grad=False</code>但是<strong>与原张量共享内存</strong>的张量。也就是说，原张量的变化会影响detach后的张量；</li>
<li>可以这样理解，张量就是一个结构体，它有<code>requires_grad</code>、<code>grad_fn</code>以及数据等属性。<code>torch.clone()</code>创建了一个全新的结构体，<code>torch.Tensor.detach()</code>也创建了一个结构体，但是数据是指向原张量的指针。</li>
</ul>
]]></content>
      <categories>
        <category>Tool</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Deep Learning</tag>
        <tag>Tool</tag>
      </tags>
  </entry>
  <entry>
    <title>Spectral Approaches</title>
    <url>/2023/07/06/SpectralApproach/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="从卷积到傅里叶变换"><a href="#从卷积到傅里叶变换" class="headerlink" title="从卷积到傅里叶变换"></a>从卷积到傅里叶变换</h1><p>图的顶点$i$可表示为信号$f_i$，其中$f _i\in\mathbf{R} ^F$，假设图有$N$个顶点，则整张图可以表示为：</p>
<p>$$<br>f&#x3D;[f_1,f_2,...,f_N]^T<br>$$</p>
<p>基于卷积的理论及其与傅里叶变换的关系（<strong>卷积的傅里叶变换等于傅里叶变换的乘积</strong>），卷积操作可以定义为：</p>
<p>$$<br>\begin{align*}<br>    g\star f<br>    &amp;&#x3D;\mathcal{F} ^{-1}[\mathcal{F}(g\star f)]\\<br>    &amp;&#x3D;\mathcal{F} ^{-1}(\mathcal{F}(g)\odot\mathcal{F}(f))\\<br>    &amp;&#x3D;U(U ^{T}g\odot U ^{T}f) \tag{1}<br>\end{align*}<br>$$</p>
<p>式中，$g$是卷积核，用于提取、汇聚图的空间信息；$U$是图的拉普拉斯矩阵的特征向量矩阵，它被用作离散形式的傅里叶变换的傅里叶基。具体细节见<a href="/2023/07/04/GraphFourierTransform/">Graph Fourier Transform</a>。式子中$U ^{T}g$称为滤波器（filter）。由于$U ^Tg$实际上是对$g$的傅里叶变换，因此：</p>
<p>$$<br>U ^{T}g&#x3D;\hat{g}&#x3D;[\hat{g}(\lambda _1),...,\hat{g}(\lambda _N)]^T<br>$$</p>
<p>即，它是由$g$所包含的各个特征值$\lambda _i$所对应的正交基$u _i$（相当于连续形式的傅里叶变换的不同频率的正弦波形）在$g$中所占的比重所组成的向量。同样地，$U ^Tf$可转化为：</p>
<p>$$<br>U ^{T}f&#x3D;\hat{f}&#x3D;[\hat{f}(\lambda _1),...,\hat{f}(\lambda _N)]^T<br>$$</p>
<p>于是它们的哈达玛积（$\odot$）就为：</p>
<p>$$<br>\begin{align*}<br>    U ^{T}g\odot U ^{T}f<br>    &amp;&#x3D;\hat{g}\odot\hat{f}\\<br>    &amp;&#x3D;<br>    \begin{bmatrix}<br>        \hat{g}({\lambda _1})\times\hat{f}({\lambda _1})\\<br>        \vdots\\<br>        \hat{g}({\lambda _N})\times\hat{f}({\lambda _N})<br>    \end{bmatrix}<br>\end{align*}<br>$$</p>
<p>而：</p>
<p>$$<br>\begin{bmatrix}<br>    \hat{g}(\lambda _1) &amp;\\<br>    &amp; \ddots \\<br>    &amp; &amp; \hat{g}(\lambda _N)<br>\end{bmatrix} \cdot<br>\begin{bmatrix}<br>    \hat{f}({\lambda _1})\\<br>    \vdots\\<br>    \hat{f}({\lambda _N})<br>\end{bmatrix} &#x3D;<br>\begin{bmatrix}<br>    \hat{g}({\lambda _1})\times\hat{f}({\lambda _1})\\<br>    \vdots\\<br>    \hat{g}({\lambda _N})\times\hat{f}({\lambda _N})<br>\end{bmatrix}<br>$$</p>
<p>因此，式$(1)$可以变为：</p>
<p>$$<br>\begin{align*}<br>    g\star f<br>    &amp;&#x3D;U(U ^{T}g\odot U ^{T}f)\\<br>    &amp;&#x3D;U(\text{diag}[\hat{g}(\lambda _1),...,\hat{g}(\lambda _N)]U ^{T}f)\\<br>    &amp;&#x3D;U\text{diag}[\hat{g}(\lambda _1),...,\hat{g}(\lambda _N)] U ^{T}f\tag{2}<br>\end{align*}<br>$$</p>
<blockquote>
<p>矩阵结合律是成立的</p>
</blockquote>
<p>另一种角度，若从右向左看式$(2)$的最后一个等式，上述卷积操作其实就是一次对图信号$f$的<strong>滤波</strong>：</p>
<ol>
<li>$U^T f$将时域的图信号转换为频域的图信号；</li>
<li>$\text{diag}[\hat{g}(\lambda _1),...,\hat{g}(\lambda _N)](U^T f)$对频域的图信号进行滤波，提取特征；</li>
<li>$U(\text{diag}[\hat{g}(\lambda _1),...,\hat{g}(\lambda _N)]U ^{T}f)$将经过滤波的频域图信号转换回时域。</li>
</ol>
<p>基于谱方法（Spectral approaches）的图神经网络的发展实际上就是一个设计更好的滤波器$\text{diag}[\hat{g}(\lambda _1),...,\hat{g}(\lambda _N)]$的过程。</p>
<h1 id="第一代GCN"><a href="#第一代GCN" class="headerlink" title="第一代GCN"></a>第一代GCN</h1><p>为了使得滤波器能够被学习，第一代GCN直接将式$(2)$中的滤波器的对角元素替换为了可学习的参数$w$，于是第一代GCN的卷积操作就变为：</p>
<p>$$<br>Z&#x3D;\sigma(Ug_wU ^{T}f)\tag{3}<br>$$</p>
<p>其中激活函数用于赋予非线性性。上述操作的思路很简单：输入$f$经过第一个卷积层后得到$Z$，$Z$再作为下一个卷积层的输入，逐层地学习、提取信息。然而，缺点也显而易见：需要对拉普拉斯矩阵进行特征分解以得到$U$，而当图顶点数很多时，这样的特征分解是十分困难的。</p>
<h1 id="第二代GCN"><a href="#第二代GCN" class="headerlink" title="第二代GCN"></a>第二代GCN</h1><p>鉴于$\text{diag}[\hat{g}(\lambda _1),...,\hat{g}(\lambda _N)]$是特征值对角矩阵$\Lambda$的函数，第二代GCN采用k阶多项式来近似的拟合这个函数，即：</p>
<p>$$<br>g _w(\Lambda)\approx\sum\limits _{k&#x3D;0} ^Kw _k\Lambda ^k<br>$$</p>
<p>将其带入式$(3)$，则第二代GCN为：</p>
<p>$$<br>\begin{align*}<br>    Z<br>    &amp;&#x3D;\sigma(Ug_w(\Lambda)U ^{T}f)\\<br>    &amp;&#x3D;\sigma(U\sum\limits _{k&#x3D;0} ^Kw _k\Lambda ^kU ^{T}f)\\<br>    &amp;&#x3D;\sigma(\sum\limits _{k&#x3D;0} ^Kw _kL ^kf)<br>\end{align*}<br>$$</p>
<blockquote>
<p>$U\Lambda ^k U ^T&#x3D;L^k$，其中，$L$为拉普拉斯矩阵。</p>
</blockquote>
<p>不难看出，第二代GCN不对拉普拉斯矩阵进行特征分解，而是直接用拉普拉斯矩阵对$f$进行变换。同时，可学习的参数个数也有原来的$N$个下降为$K+1$个。与第一代相比，第二代GCN的计算复杂度明显更低。虽然如此，仍然需要计算$L ^k$，时间复杂度为$O(N ^2)$。</p>
<p>事实上，由于：</p>
<p>$$<br>L&#x3D;D-A\tag{4}<br>$$</p>
<p>其中$A$为邻接矩阵，式$(4)$的证明见<a href="/2023/07/03/Laplacian/#图拉普拉斯矩阵">图拉普拉斯矩阵</a>。因此:</p>
<p>$$<br>L ^k&#x3D;C_k ^0D ^k-C_k ^1D ^{k-1}A+...+(-1) ^kC _k ^kA ^k<br>$$</p>
<p>式中，由于$D$和$A$都是对称矩阵，所以$DA&#x3D;AD$。邻接矩阵的$k$次幂的各个元素$A ^k[i][j]$表示的是从$i$到$j$路径长度为$k$的路径的条数。$D$为对角矩阵，只会改变与其相乘的矩阵的非零元素的值。因此，$L ^k$中的$k$实际上就是本次卷积感受野的大小，即距离中心顶点$k+1$-hop以内的邻居顶点都会被用来更新中心顶点的特征表示。</p>
<blockquote>
<p>接下来的两类GCN的$L$均代表正则化后的拉普拉斯矩阵。</p>
</blockquote>
<h1 id="ChebyNet"><a href="#ChebyNet" class="headerlink" title="ChebyNet"></a>ChebyNet</h1><p>不同于第二代GCN，2016年被提出的<a href="https://arxiv.org/pdf/1606.09375.pdf">ChebyNet</a>使用切比雪夫多项式来近似拟合滤波器：</p>
<p>$$<br>\begin{cases}<br>    g _w&#x3D;\sum\limits _{k&#x3D;0} ^{K}w _kT _k(\hat{\Lambda})\tag{5}\\<br>    \hat{\Lambda}&#x3D;\frac{2}{\lambda _{max}}\Lambda-I _N<br>\end{cases}<br>$$</p>
<p>其中，$\lambda _{max}$是正则化的拉普拉斯矩阵$L$的最大特征值，而正则化拉普拉斯矩阵的特征值介于0和2之间，因此，$\hat{\Lambda}$的作用是让特征值矩阵的值介于-1和1之间。切比雪夫多项式是递归定义的：</p>
<p>$$<br>\begin{cases}<br>    T _0(x)&#x3D;1 \\<br>    T _1(x)&#x3D;x \\<br>    T _{n+1}(x)&#x3D;2xT _n(x)- T _{n-1}(x)<br>\end{cases}<br>$$</p>
<p>切比雪夫多项式各个幂次项的系数实际上是余弦函数$\cos nx$展开为$\cos x$的函数后的各个幂次的$\cos x$的系数。因此，相比于普通的多项式，切比雪夫多项式具有很多特殊的性质：</p>
<ol>
<li>在$[-1,1]$有$n$个零点；</li>
<li>在$[-1,1]$的值在$[-1,1]$之间震荡。</li>
<li>...</li>
</ol>
<p><img src="/2023/07/06/SpectralApproach/1.png" alt="1"></p>
<center style="font-size:12px; font-weight:bold">Fig. 1. Chebyshev polynomials</center><br>

<p>这使得它与正弦波形十分相似，但是又更为简单。将式子带入$(3)$，可得：</p>
<p>$$<br>\begin{cases}<br>    Z&#x3D;\sigma(\sum\limits _{k&#x3D;0} ^Kw _kT _k(\hat{L})f)\\<br>    \hat{L}&#x3D;\frac{2}{\lambda _{max}}L-I _N<br>\end{cases}<br>$$</p>
<p>因为切比雪夫多项式本质上仍是多项式，所以$k$仍然表示的是感受野的大小，这与第二代GCN一致。</p>
<h1 id="GCN"><a href="#GCN" class="headerlink" title="GCN"></a>GCN</h1><p>目前常规的<a href="https://arxiv.org/pdf/1609.02907.pdf">GCN</a>源于2017年Kipf等发表的论文。它是在ChebyNet的基础上进一步简化而来的，它取$K$为1，即，只取切比雪夫多项式的前两项：</p>
<p>$$<br>g _w&#x3D;w_0+w_1\hat{L}\tag{6}<br>$$</p>
<p>进一步地，它固定$\lambda _{max}$为2，并且限制$w_0&#x3D;-w_1&#x3D;w$，于是式$(6)$变为：</p>
<p>$$<br>\begin{align*}<br>    g _w<br>    &amp;&#x3D;w-w(L-I _N)\\<br>    &amp;&#x3D;-w(L-2I _N)\\<br>    &amp;&#x3D;-w[D ^{-\frac{1}{2}}(D-A)D ^{-\frac{1}{2}}-2I _N]\\<br>    &amp;&#x3D;-w(I _N-D ^{-\frac{1}{2}}AD ^{-\frac{1}{2}}-2I _N)\\<br>    &amp;&#x3D;w(I _N+D ^{-\frac{1}{2}}AD ^{-\frac{1}{2}})\tag{7}<br>\end{align*}<br>$$</p>
<p>不难看出，如此，卷积层的感受野就只有1，即中心顶点只会聚合与其直接邻接的顶点的信息。但是，由于$A$的对角线均为0，在卷积过程中，中心顶点会丢失自己的信息。因此，GCN为每个顶点增加了自旋边，即：</p>
<p>$$<br>\tilde{A}&#x3D;A+I _N<br>$$</p>
<p>相应地，拉普拉斯矩阵$L$和和度矩阵$D$分别变成了$\tilde{L}$和$\tilde{D}$。如此，$k$为0的切比雪夫项$I _N$就不再被需要了，因为$\tilde{A}$使得中心顶点能够保留自己的信息，故式$(7)$进一步简化为：</p>
<p>$$<br>g _w&#x3D;w\tilde{D} ^{-\frac{1}{2}}\tilde{A}\tilde{D} ^{-\frac{1}{2}}<br>$$</p>
<p>为了增加可学习的参数，GCN将$w$扩充为矩阵$W$，并定义了卷积层的最终操作为：</p>
<p>$$<br>Z&#x3D;\sigma(\tilde{D} ^{-\frac{1}{2}}\tilde{A}\tilde{D} ^{-\frac{1}{2}}fW)<br>$$</p>
<p>以上就是GCN，它是ChebyNet的简化，但实际效果却远优于更复杂的ChebyNet。但是，GCN不能叠加很多层，因为它是个低通滤波器，即它会过滤掉高频的信号，因此当GCN的层数过高时，最终的信号就只剩低频信号了，相应地网络的效果也会变差：</p>
<p><img src="/2023/07/06/SpectralApproach/2.png" alt="2"></p>
<center style="font-size:12px; font-weight:bold">Fig. 2. Low pass filter</center><br>

<blockquote>
<p>图中的$h$就是文中的$g$。</p>
</blockquote>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://arxiv.org/ftp/arxiv/papers/1812/1812.08434.pdf">Graph neural networks: A review of methods and applications</a></li>
<li><a href="https://www.bilibili.com/video/BV1Me4y1c7kj">谱域图神经网络理论基础</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/121090537">GCN中的等式证明</a></li>
<li><a href="https://www.jianshu.com/p/35212baf6671">图卷积网络(GCN)原理解析</a></li>
</ul>
]]></content>
      <categories>
        <category>Math</category>
      </categories>
      <tags>
        <tag>Math</tag>
        <tag>GNN</tag>
        <tag>GCN</tag>
      </tags>
  </entry>
  <entry>
    <title>Graph Fourier Transform</title>
    <url>/2023/07/04/GraphFourierTransform/</url>
    <content><![CDATA[<span id="more"></span>


<h1 id="傅里叶变换"><a href="#傅里叶变换" class="headerlink" title="傅里叶变换"></a>傅里叶变换</h1><p>在探讨图傅里叶变换之前，我们需要先弄清楚傅里叶变换和傅里叶逆变换的本质。傅里叶逆变换本质上是傅里叶级数在非周期函数（周期为$\infty$）上的形式，因此，傅里叶逆变换<strong>将一个函数表示为若干正交基函数的线性组合</strong>。与之相应地，傅里叶变换就是<strong>求线性组合的系数</strong></p>
<p>$$<br>\begin{align*}<br>    f(t)&#x3D;\frac{1}{2\pi}\int_{-\infty} ^{+\infty}&amp;\int_{-\infty} ^{+\infty}f(t)e^{-i\omega t}dt\space e^{i\omega t}d\omega\tag{1}\\<br>    F(\omega)&#x3D;&amp;\int_{-\infty} ^{+\infty}f(t)e^{-i\omega t}dt\tag{2}<br>\end{align*}<br>$$</p>
<p>其中，式$(1)$为傅里叶逆变换，式$(2)$为傅里叶变换。</p>
<blockquote>
<p>见<a href="/2023/05/10/FourierTransform/">Fourier Transform</a>了解傅里叶变换的推导过程。</p>
</blockquote>
<h1 id="图傅里叶变换"><a href="#图傅里叶变换" class="headerlink" title="图傅里叶变换"></a>图傅里叶变换</h1><p>图的顶点$i$可表示为信号$f_i$，其中$f _i\in\mathbf{R} ^F$，假设图有$N$个顶点，则整张图可以表示为：</p>
<p>$$<br>f&#x3D;[f_1,f_2,...,f_N]^T<br>$$</p>
<p>图傅里叶逆变换，使用的是拉普拉斯矩阵$L$的特征向量作为基函数（Fourier bias）。因为拉普拉斯矩阵是个半正定对称矩阵，所以它有$N$个相互正交的特征向量$\vec{u} _i$（见<a href="/2023/07/03/Laplacian/">Laplacian</a>了解拉普拉斯变换和拉普拉斯矩阵）。若使用拉普拉斯矩阵的特征向量作为图傅里叶逆变换的基函数，则图上的任意信号就可以表示为：</p>
<p>$$<br>\begin{align*}<br>    f&amp;&#x3D;\hat{f}(\lambda _1)\vec{u} _1+...+\hat{f}(\lambda _N)\vec{u} _N\\<br>    &amp;&#x3D;U\hat{f}<br>\end{align*}<br>$$</p>
<p>式中，$U$为使拉普拉斯矩阵$L$的相似对角化的正交矩阵，$\hat{f}$为$f$的傅里叶系数，也是其在频域的信号。这本质上是一次坐标变换，即，将$f$在时域的坐标，转变为以$(\vec{u _1},...,\vec{u} _n)$为正交基的频域坐标$\hat{f}$。见<a href="/2023/05/10/FourierTransformAndCNN/">Fourier Transform And CNN </a>了解傅里叶变换和坐标变换的关系。由于$U$是正交矩阵，我们可以很容易得到$f$的傅里叶变换为：</p>
<p>$$<br>\hat{f}&#x3D;U ^Tf<br>$$</p>
<p>实际上，正交的特征向量$\vec{u _i}$就是离散形式的正（余）弦波形：</p>
<p><img src="/2023/07/04/GraphFourierTransform/1.png" alt="1"></p>
<center style="font-size:12px; font-weight:bold">Fig. 1. Fourier bias</center>

<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://zhuanlan.zhihu.com/p/137897522">图卷积神经网络系列：2. | 图傅里叶变换</a></li>
<li><a href="https://www.bilibili.com/video/BV1Me4y1c7kj">谱域图神经网络理论基础</a></li>
</ul>
]]></content>
      <categories>
        <category>Math</category>
      </categories>
      <tags>
        <tag>Math</tag>
        <tag>GNN</tag>
        <tag>GCN</tag>
      </tags>
  </entry>
  <entry>
    <title>Laplacian</title>
    <url>/2023/07/03/Laplacian/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h1><p>拉普拉斯算子$\Delta$定义为函数$f$（假设$f$为三元函数）的梯度的散度，它接受标量函数输入，并输入一个新的标量函数，即：</p>
<p>$$<br>\Delta f&#x3D;\text{div}(\text{grad}f(x,y,z))<br>$$</p>
<h2 id="哈密顿算子"><a href="#哈密顿算子" class="headerlink" title="哈密顿算子"></a>哈密顿算子</h2><p>哈密顿算子$\nabla$定义为：</p>
<p>$$<br>\nabla&#x3D;\frac{\partial}{\partial x}\vec{i}+\frac{\partial}{\partial y}\vec{j}+\frac{\partial}{\partial z}\vec{k}<br>$$</p>
<p>它本身没有意义，只是一个算子，但是又被看作是一个矢量。哈密顿算子作用于标量$f$可以得到$f$的梯度，作用于矢量$\vec{f}$可以得到$\vec{f}$的散度。</p>
<h2 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h2><p>梯度是一个矢量，它表示某一空间函数$f(x,y,z)$在点$(x,y,z)$处方向导数最大的方向或者说函数值变化最快的方向：</p>
<p>$$<br>\text{grad}f(x,y,z)&#x3D;\nabla f(x,y,z)&#x3D;\frac{\partial f}{\partial x}\vec{i}+\frac{\partial f}{\partial y}\vec{j}+\frac{\partial f}{\partial z}\vec{k}<br>$$</p>
<h2 id="散度"><a href="#散度" class="headerlink" title="散度"></a>散度</h2><p>散度是一个标量，它表示空间中的矢量场$F&#x3D;&lt;f,g,h&gt;$在该点$(x,y,z)$单位体积上的通量，即该矢量场发散的强弱程度：</p>
<p>$$<br>\text{div}(F)&#x3D;\nabla\cdot&lt;f,g,h&gt;&#x3D;\frac{\partial f}{\partial x}+\frac{\partial g}{\partial y}+\frac{\partial h}{\partial z}<br>$$</p>
<p>当散度大于0时，表示该矢量场有散发通量的正源；当散度小于0时，表示该矢量场有吸收通量的负源；当散度等于0时，表示该场无源。</p>
<h2 id="拉普拉斯算子的意义"><a href="#拉普拉斯算子的意义" class="headerlink" title="拉普拉斯算子的意义"></a>拉普拉斯算子的意义</h2><p>以下以二元函数$f(x,y)$为例，说明拉普拉斯算子的数学意义。</p>
<p>对于给定的二元函数$f(x,y)$，其梯度$\nabla f$给出了其在二维空间上各个点函数值变化最快的方向。</p>
<p><img src="/2023/07/03/Laplacian/1.png" alt="1"></p>
<center style="font-size:12px; font-weight:bold">Fig. 1. f & grad</center><br>

<p>上图下方的箭头即为$f$在各个点上的梯度方向。此时，若我们将原函数图像去掉，我们就得到一个由原函数的梯度向量形成的矢量场：</p>
<p><img src="/2023/07/03/Laplacian/2.png" alt="2"></p>
<center style="font-size:12px; font-weight:bold">Fig. 2. Vector field</center><br>

<p>对该矢量场求散度$\nabla\cdot\nabla f$，我们实际上就能够得到原函数图像的凹凸性（大于0为凸，小于0为凹），如同一元函数的凹凸性一般，多元函数的凹凸性也体现了该点的函数值与该点周围的点的函数值的大小关系，凸说明该点的函数值小于周围点函数值的平均值，凹则说明大于。</p>
<p>这就是拉普拉斯算子（$\Delta&#x3D;\nabla\cdot\nabla&#x3D;\nabla ^2$），它代表了小范围的凹凸性，这也是它能被用于图像边缘检测的原因（边缘往往显著区别于邻居）。</p>
<h1 id="拉普拉斯矩阵"><a href="#拉普拉斯矩阵" class="headerlink" title="拉普拉斯矩阵"></a>拉普拉斯矩阵</h1><p>拉普拉斯矩阵$L$，是图上的拉普拉斯算子，也即离散的拉普拉斯算子。前面提到的拉普拉斯算子，其基于的空间都是连续的，而对于图像的像素矩阵，它可以被视为一个二元的函数，只不过其空间位置（即矩阵的行和列）是离散的值。当拉普拉斯算子作用于离散的空间，拉普拉斯算子就成了拉普拉斯矩阵。</p>
<h2 id="一维和二维离散函数的导数"><a href="#一维和二维离散函数的导数" class="headerlink" title="一维和二维离散函数的导数"></a>一维和二维离散函数的导数</h2><p>假设离散空间中空间位置的最小步长为$h$：</p>
<p>$$<br>x _{i+1}- x_i&#x3D;...&#x3D;x_2-x_1&#x3D;h<br>$$</p>
<p>将$f(x _{i+1})$和$f(x _{i-1})$分别用泰勒级数在$x_i$点展开：</p>
<p>$$<br>\begin{align*}<br>    f(x _{i+1})&amp;&#x3D;f(x_i)+f&#39;(x_i)h+f&#39;&#39;(x_i)\frac{h^2}{2}+...\tag{1}\\<br>    f(x _{i-1})&amp;&#x3D;f(x_i)-f&#39;(x_i)h+f&#39;&#39;(x_i)\frac{h^2}{2}-... \tag{2}<br>\end{align*}<br>$$</p>
<p>$(1)$和$(2)$相减，可得：</p>
<p>$$<br>f&#39;(x_i)&#x3D;\frac{f(x _{i+1})-f(x _{i-1})}{2h}-O(h^2)<br>$$</p>
<p>同样地，$(1)$和$(2)$相加，可得：</p>
<p>$$<br>f&#39;&#39;(x_i)&#x3D;\frac{f(x _{i+1})+f(x _{i-1})-2f(x_i)}{h^2}-O(h^3)<br>$$</p>
<p>若忽略掉低阶无穷小$O(h^2)$和$O(h^3)$，并取$h$为1，那么二维空间的离散拉普拉斯算子便可表示为：</p>
<p>$$<br>\begin{align*}<br>    \Delta f&amp;&#x3D;f _{xx}+f _{yy}\\<br>    &amp;&#x3D;f(x+1,y)+f(x-1,y)-2f(x,y)+f(x,y+1)+f(x,y-1)-2f(x,y)\\<br>    &amp;&#x3D;f(x+1,y)+f(x-1,y)+f(x,y+1)+f(x,y-1)-4f(x,y)\tag{3}<br>\end{align*}<br>$$</p>
<p>若将3x3的卷积核的中心定义为$(x,y)$：</p>
<p>$$<br>\begin{bmatrix}<br>    (x-1,y-1)&amp;(x,y-1)&amp;(x+1,y-1)\\<br>    (x-1,y)&amp;(x,y)&amp;(x+1,y)\\<br>    (x-1,y+1)&amp;(x,y+1)&amp;(x+1,y+1)<br>\end{bmatrix}<br>$$</p>
<p>则其系数即为用于边缘检测的卷积核：</p>
<p>$$<br>\begin{bmatrix}<br>    0&amp;1&amp;0\\<br>    1&amp;-4&amp;1\\<br>    0&amp;1&amp;0<br>\end{bmatrix}<br>$$</p>
<blockquote>
<p>此处可以看出拉普拉斯算子的另外一层含义：$f(x,y)$受到微小扰动后，可能变成$f(x-1,y)$，$f(x,y-1)$，$f(x+1,y)$和$f(x,y+1)$中的任何一个。拉普拉斯算子计算的就是对该点进行微小扰动后可能获得的总增益。这对于连续的拉普拉斯算子也成立，只不过扰动后可能出现的情况变成无数个。增益越大，说明函数在该点越有可能是个凸点（谷点）。</p>
</blockquote>
<h2 id="图拉普拉斯矩阵"><a href="#图拉普拉斯矩阵" class="headerlink" title="图拉普拉斯矩阵"></a>图拉普拉斯矩阵</h2><p>对于有$N$个顶点的图，其每个顶点的最大邻接顶点数（自由度）为$N$，邻接矩阵为$A$，度矩阵为$D$，其中：</p>
<p>$$<br>D&#x3D;<br>\begin{cases}<br>    D _{ii}&#x3D;\sum\limits _{j&#x3D;1} ^{N}A _{ij}\\<br>    D _{ij}&#x3D;0,i\ne j<br>\end{cases}<br>$$</p>
<p>根据扰动与增益的定义，我们很容易将式$(3)$推广到图中。图的扰动增益可以定义为邻接的两个顶点的特征差值。假设顶点$v _i$的特征为$f_i$，那么整个图的顶点特征则为：</p>
<p>$$<br>f&#x3D;[f_1,f_2,...,f_N]^T<br>$$</p>
<p>两邻接顶点的差异可定义为：</p>
<p>$$<br>f_i-f_j<br>$$</p>
<p>若考虑加权图中边的权值，则为：</p>
<p>$$<br>w _{ij}(f_i-f_j)<br>$$</p>
<p>于是，对某一个顶点$v_i$，其拉普拉斯算子为：</p>
<p>$$<br>\begin{align*}<br>    \Delta f_i<br>    &amp;&#x3D;\sum\limits _{j&#x3D;1} ^Nw _{ij}(f_i-f_j)\\<br>    &amp;&#x3D;\sum\limits _{j&#x3D;1} ^Nw _{ij}f_i-\sum\limits _{j&#x3D;1} ^Nw _{ij}f_j\\<br>    &amp;&#x3D;D _{ii}f _i-w _{i:}f<br>\end{align*}<br>$$</p>
<p>式中，$D _{ii}$为顶点$v_i$的度（有权则为带权的度），$w _{i:}$为顶点$v_i$的邻接边的权重向量，维度为$N$。对于整张图，则为：</p>
<p>$$<br>\begin{align*}<br>    \Delta f<br>    &amp;&#x3D;<br>\begin{pmatrix}<br>    \Delta f_1\\<br>    \Delta f_2\\<br>    ...\\<br>    \Delta f_N<br>\end{pmatrix}&#x3D;<br>\begin{pmatrix}<br>    D _{11}f _1-w _{1:}f\\<br>    D _{22}f _2-w _{2:}f\\<br>    ...\\<br>    D _{NN}f _N-w _{N:}f<br>\end{pmatrix}\\<br>    &amp;&#x3D;<br>\begin{pmatrix}<br>    D _{11}&amp;\cdots&amp;0\\<br>    \vdots&amp;\ddots&amp;0\\<br>    0&amp;\cdots&amp;D _{NN}<br>\end{pmatrix}f-<br>\begin{pmatrix}<br>    W _{11}&amp;\cdots&amp;W _{1N}\\<br>    \vdots&amp;\ddots&amp;\vdots\\<br>    W _{N1}&amp;\cdots&amp;W _{NN}<br>\end{pmatrix}f\\<br>    &amp;&#x3D;(D-W)f\\<br>    &amp;&#x3D;Lf<br>\end{align*}<br>$$</p>
<blockquote>
<p>$L$称拉普拉斯矩阵。若$W$不表示权重，只表示邻接关系，则$W$即为邻接矩阵$A$。</p>
<p>拉普拉斯矩阵是半正定矩阵，即对于任意不为0的实列向量$x$，有$x^TLx\ge0$。</p>
<p>此处不以符号区别行&#x2F;列向量，均假设其自适应。</p>
</blockquote>
<h2 id="邻接矩阵探讨"><a href="#邻接矩阵探讨" class="headerlink" title="邻接矩阵探讨"></a>邻接矩阵探讨</h2><p>邻接矩阵$A$实际上也相当于一个算子，如：</p>
<p>$$<br>\begin{align*}<br>    g&amp;&#x3D;Af\\<br>    g(i)&amp;&#x3D;\sum _{j} ^{A _{ij}&#x3D;1}f _j<br>\end{align*}<br>$$</p>
<p>$g(i)$表示$i$的邻接顶点的特征和。又如：</p>
<p>$$<br>\begin{align*}<br>    f ^Tg<br>    &amp;&#x3D;f ^TAf\\<br>    &amp;&#x3D;\sum\limits _{i,j} ^{A _{ij}&#x3D;1}f _i\cdot f _j<br>\end{align*}<br>$$</p>
<p>邻接矩阵必是对称矩阵，因此这实际上是个二次形。</p>
<h2 id="拉普拉斯矩阵探讨"><a href="#拉普拉斯矩阵探讨" class="headerlink" title="拉普拉斯矩阵探讨"></a>拉普拉斯矩阵探讨</h2><p>类似地，拉普拉斯矩阵$L$和$f$也能形成二次型：</p>
<p>$$<br>\begin{align*}<br>    f^TLf<br>    &amp;&#x3D;\sum _{(i,j)\in E}(f _i - f _j)^2\tag{4}<br>\end{align*}<br>$$</p>
<p>该式具有实际的物理意义：</p>
<p><img src="/2023/07/03/Laplacian/3.png" alt="3"></p>
<center style="font-size:12px; font-weight:bold">Fig. 3. Spring network</center><br>

<p>在上图所示弹簧网络中，有5个质子，假设两端（黑球）固定，并给予中间的三个振子一定程度的扰动，则当系统重新平衡时，三个振子和两端会停留在让系统总弹性势能最小的位置。假设连接质子$i$和质子$j$的弹簧的弹性系数为$k _{ij}$（$k _{ji}$），则系统的总弹性势能为：</p>
<p>$$<br>E&#x3D;\frac{1}{2}\sum _{(i,j)\in E}k _{ij}(x(i)-x(j))^2\tag{5}<br>$$<br>式中，$i$和$j$表示质子，$x(i)$为质子$i$的绝对位置，$E$是边集（即弹簧集）。最终质子的位置将使得$E$最小。将上述弹簧网络扩展为更一般的图，那么式$(5)$便能很轻易地被扩展为式$(4)$，只不过省略了一些对结果不造成影响的项（e.g. $\frac{1}{2}$），且一些项的含义发生了变化（e.g. $k _{ij}$成了图的边的权，若图的边无权，则可忽略该项）。式$(4)$显然是一个凸函数，因此它可以作为图神经网络的损失函数（但一般作为正则项）。</p>
<h2 id="关联矩阵"><a href="#关联矩阵" class="headerlink" title="关联矩阵"></a>关联矩阵</h2><p>关联矩阵$\nabla$展现了边和其两端顶点的关系。一般地，有向图的关联矩阵更有意义，因此，对于无向图，我们可以为每条边随意规定一个方向，并定义其关联矩阵（形状为$|E|\times|V|$）为：<br>$$<br>\nabla&#x3D;<br>\begin{cases}<br>    \nabla _{ev}&#x3D;-1,&amp;\text{if v is the initial vextex of e}\\<br>    \nabla _{ev}&#x3D;1,&amp;\text{if v is the terminal vextex of e}\\<br>    \nabla _{ev}&#x3D;0,&amp;\text{otherwise}<br>\end{cases}<br>$$</p>
<p>关联矩阵与拉普拉斯矩阵存在关系：</p>
<p>$$<br>L&#x3D;\nabla^T\nabla<br>$$</p>
<h2 id="拉普拉斯谱分解"><a href="#拉普拉斯谱分解" class="headerlink" title="拉普拉斯谱分解"></a>拉普拉斯谱分解</h2><p>特征分解也叫谱分解，即将矩阵分解为特征值和特征向量的乘积。因为拉普拉斯矩阵$L$为对称矩阵，因此它有$N$个特征值，$N$个相互正交的特征向量：</p>
<p>$$<br>\begin{align*}<br>    Lu_k&amp;&#x3D;\lambda_ku_k\\<br>    L&#x3D;U^{-1}\Lambda&amp;U&#x3D;U^T\Lambda U<br>\end{align*}<br>$$</p>
<h2 id="拉普拉斯矩阵的正则化"><a href="#拉普拉斯矩阵的正则化" class="headerlink" title="拉普拉斯矩阵的正则化"></a>拉普拉斯矩阵的正则化</h2><p>首先，要了解邻接矩阵的正则化：</p>
<p>$$<br>\tilde{A}&#x3D;D ^{-\frac{1}{2}}A D ^{-\frac{1}{2}}<br>$$</p>
<p>如同一般的正则化一样，邻接矩阵的正则化也可视作一种防止过拟合的措施。但是，邻接矩阵的正则化包含了两个方向的正则：</p>
<ol>
<li>左乘$D ^{-\frac{1}{2}}$是横向正则，即对$A$的同一行用相同尺度放缩，而对$A$的不同行用不同尺度放缩，这在一定程度上使得度高的顶点和度低的顶点在聚合后值不会相差得太太，有点像BatchNorm；</li>
<li>右乘$D ^{-\frac{1}{2}}$是纵向正则，即对$A$的同一列用相同尺度放缩，而对$A$的不同列用不同尺度放缩，这相当于为邻接顶点赋予<strong>分发比例</strong>，即，对某个邻接顶点$j$，其信号聚合到本顶点$i$时，其贡献的程度应该是$1&#x2F;D _{jj}$，当然，实际上是$1&#x2F;\sqrt{D _{jj}}$，这是为了保证左右正则后的行列式值与只用$D$左正则的值相同。</li>
</ol>
<p>同样地，拉普拉斯矩阵的正则化为：</p>
<p>$$<br>\begin{align*}<br>    \tilde{L}<br>    &amp;&#x3D;D ^{-\frac{1}{2}}L D ^{-\frac{1}{2}}\\<br>    &amp;&#x3D;D ^{-\frac{1}{2}}(D-A) D ^{-\frac{1}{2}}\\<br>    &amp;&#x3D;I-D ^{-\frac{1}{2}}A D ^{-\frac{1}{2}}<br>\end{align*}<br>$$</p>
<p>上式本质上是让对角线都变成1。</p>
<blockquote>
<p>注，此处假设图不带权，故用的是邻接矩阵$A$。</p>
<p>正则化拉普拉斯矩阵仍为半正定对称矩阵，且所有特征值$\lambda\in[0,2]$</p>
</blockquote>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://www.zhihu.com/zvideo/1440673926157991936">拉普拉斯方程</a></li>
<li><a href="https://cloud.tencent.com/developer/article/1799419">【图神经网络】数学基础篇</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/85287578">拉普拉斯矩阵与拉普拉斯算子的关系</a></li>
</ul>
]]></content>
      <categories>
        <category>Math</category>
      </categories>
      <tags>
        <tag>Math</tag>
        <tag>GNN</tag>
        <tag>GCN</tag>
      </tags>
  </entry>
  <entry>
    <title>Graph Neural Networks &amp; Graph Convolutional Networks</title>
    <url>/2023/07/02/GCN/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="数据的图表示"><a href="#数据的图表示" class="headerlink" title="数据的图表示"></a>数据的图表示</h1><p>要完整地表示一张图，需要四类信息：顶点（nodes，V），边（edgs，E），全局信息（global-context，U）和连接性（connectivity，A）。其中，每个顶点（Fig 1中黄色的圈）的属性都可以用一个标量或向量表示；每条边（Fig 1中蓝色的线）的属性也可以用一个标量或向量表示；全局信息（Fig 1中红色虚线框）同样可以用一个标量或向量表示。表示图连接性的方法最简单的是邻接矩阵（Adjacency matrix），因为它很容易向量化，但是，邻接矩阵存在两个缺点：1）浪费空间，特别是对于稀疏图；2）对相同的图可以有多种邻接矩阵表示（如调整行列的顶点顺序可以得到不同的邻接矩阵）。这两个缺点，特别是第二个，使得邻接矩阵法不能得到很好的效果。更为有效、常用的方法是邻接列表法，即将每条边用顶点的二元元组表示，所有的边组成一个列表。对于Fig 1中的图，假设顶点顺时针排序，最上面的顶点编号为1，则其连接性可表示为：</p>
<p>$$<br>[[1, 2], [1, 3], [1, 4], [1, 5], [2, 5], [3, 4]]<br>$$</p>
<p><img src="/2023/07/02/GCN/1.png" alt="1"></p>
<center style="font-size:12px; font-weight:bold">Fig. 1. Graph</center>

<h2 id="图片的图表示"><a href="#图片的图表示" class="headerlink" title="图片的图表示"></a>图片的图表示</h2><p>图片是由多个像素点组成的矩阵，因此它可以很自然地被视为一个矩形的特殊的图：每个像素点为顶点，每个像素点和它周围的所有像素点（包括对角线的）具有连接性，像素点的值代表的顶点的属性。</p>
<p><img src="/2023/07/02/GCN/2.png" alt="2"></p>
<center style="font-size:12px; font-weight:bold">Fig. 2. Image graph</center>

<h2 id="文本的图表示"><a href="#文本的图表示" class="headerlink" title="文本的图表示"></a>文本的图表示</h2><p>文本作为一种序列，实际上是一种特殊的图：直线。因此，文本的图表示是一个有向图。</p>
<p><img src="/2023/07/02/GCN/3.png" alt="3"></p>
<center style="font-size:12px; font-weight:bold">Fig. 3. Text graph</center> <br>

<blockquote>
<p>实际上图片和文本连接性的表示要更加简洁，因为所有图片和文本都具有相同的连接模式：图片中的像素只和周围的像素相连；文本中的词元只指向下一个词元，邻接矩阵是条对角线。</p>
<p>由于树也是一种特殊的图，凡是能表示为树的东西，也能用GNNs的方法处理。</p>
</blockquote>
<h1 id="面向GNNs的任务"><a href="#面向GNNs的任务" class="headerlink" title="面向GNNs的任务"></a>面向GNNs的任务</h1><p>GNNs能解决的任务一般可分三个类别：图级别任务（Graph-level task），顶点级别任务（Node-level task）和边级别任务（Edge-level task）。</p>
<p>对于图级别的任务，最常规的是图分类问题，它和MNIST和CIFAR等图片分类一样，将不同类型的图分到合适的类别。</p>
<p><img src="/2023/07/02/GCN/4.png" alt="4"></p>
<center style="font-size:12px; font-weight:bold">Fig. 4. Graph-level task</center> <br>

<p>对于顶点级别的任务，最常规的亦为分类问题，即预测顶点的属性或者类别。</p>
<p><img src="/2023/07/02/GCN/5.png" alt="5"></p>
<center style="font-size:12px; font-weight:bold">Fig. 5. Node-level task</center> <br>

<p>对于边级别的任务，其可以处理的是广义上的分类问题，即预测边的属性（其属性体现着边连接的两个顶点的关系）。</p>
<p><img src="/2023/07/02/GCN/6.png" alt="6"></p>
<center style="font-size:12px; font-weight:bold">Fig. 6. Edge-level task</center>

<h1 id="图神经网络（GNNs）"><a href="#图神经网络（GNNs）" class="headerlink" title="图神经网络（GNNs）"></a>图神经网络（GNNs）</h1><p>图神经网络是一种对图的属性做变换（顶点，边以及全局信息），但是不改变图的拓扑结构的神经网络。它接受图输入，并输出一个属性变化了但拓扑结构不变的图。</p>
<p><img src="/2023/07/02/GCN/7.png" alt="7"></p>
<center style="font-size:12px; font-weight:bold">Fig. 7. The simplest GNN</center> <br>

<p>最简单的图神经网络如上图所示。所有的顶点的属性、边的属性和全局信息分别形成一个矩阵，它们分别被输入到一个MLP中。三个MLP形成一个GNN块，每个GNN块的输出都是属性变化后的顶点、边和全局信息，但是图的拓扑结构不变。GNN块可以逐个堆叠，起着特征提取的作用。GNN块实际上相当于一个特征提取器&#x2F;编码器，而具体的任务则由后续的分类器&#x2F;解码器完成。</p>
<p><img src="/2023/07/02/GCN/8.png" alt="8"></p>
<center style="font-size:12px; font-weight:bold">Fig. 8. GNN block</center> <br>

<p>需要注意的是，编码器提取的是顶点、边和全局信息的特征，而输入解码器的特征只是需要的特征，如顶点级别任务只输入顶点特征、边级别任务只输入边特征。不过有些时候给定的图可能缺乏某个方面的信息，它可能只有边属性和全局信息而没有顶点属性，也可能只有顶点属性和边属性而没有全局属性。这时，编码器的工作照常进行，而解码器在工作之前，需要从其他地方获取缺失的信息，该操作称为<em>Pooling</em>。</p>
<p>比如，若缺失顶点信息，那么，进入解码器的顶点信息则由包含该顶点的边和全局信息Pooling而来，如Fig 9所示；若缺失边信息，那么，进入编码器的边信息则由该边两侧的顶点和全局信息Pooling而来，如Fig 10所示；若缺失全局信息，那么，进入编码器的全局信息则由所有的顶点信息（也可以加上边的信息）Pooling而来，如Fig 11所示。</p>
<blockquote>
<p>顶点、边和全局信息的形状可能不一样，这时候就需要先进行投影。汇聚（Pooling）的方法有多种，如取最大（Max）、求和（Sum）、求和取平均（Average），各种方法的效果差不多。</p>
<p>全局信息相当于一个特殊顶点的属性：这个顶点和所有顶点相连、和所有边相连。因此，全局信息的汇聚会用到所有的顶点和边。</p>
</blockquote>
<p><img src="/2023/07/02/GCN/9.png" alt="9"></p>
<center style="font-size:12px; font-weight:bold">Fig. 9. Node pooling</center> <br>

<p><img src="/2023/07/02/GCN/10.png" alt="10"></p>
<center style="font-size:12px; font-weight:bold">Fig. 10. Edge pooling</center> <br>

<p><img src="/2023/07/02/GCN/11.png" alt="11"></p>
<center style="font-size:12px; font-weight:bold">Fig. 11. Graph-context pooling</center> <br>

<p>上述的简单的GNN存在一个严重的缺陷：除了汇聚层，我们没有用到任何连接性的信息。每个顶点、边、全局信息都是被独立地处理的，而它们实际上存在很强的关系。</p>
<h1 id="图卷积神经网络（GCNs）"><a href="#图卷积神经网络（GCNs）" class="headerlink" title="图卷积神经网络（GCNs）"></a>图卷积神经网络（GCNs）</h1><p>图卷积神经网络能够解决前面提到的简单图神经网络的缺点。它采用类似于图片卷积的方法，用图卷积层将某一顶点和其周围顶点的信息汇聚到下一层的同一个顶点中，如此，只要经过足够多的图卷积层，单一的顶点就能看到所有顶点的信息。</p>
<p><img src="/2023/07/02/GCN/12.png" alt="12"></p>
<center style="font-size:12px; font-weight:bold">Fig. 12. GCN layer</center> <br>

<p>更进一步地，可以将上一节中顶点、边和全局信息间的信息传递提前到图卷积层中。也就是说，边的信息会汇聚边和两边顶点的信息，顶点的信息会汇聚边以及相连顶点的信息。不同的汇聚策略会产生不同的图卷积方法，如下图所示。</p>
<p><img src="/2023/07/02/GCN/13.png" alt="13"></p>
<center style="font-size:12px; font-weight:bold">Fig. 13. Different ways to combine nodes and edges</center> <br>

<blockquote>
<p>由于顶点和边的属性向量的形状可能不同，汇聚时可以采用映射后再相加或者直接拼接等方法，不同的方法会产生不同的效果。</p>
</blockquote>
<p>当我们想要尽早地了解到整个图的情况时，也可以将全局信息加入汇聚操作中。</p>
<p><img src="/2023/07/02/GCN/14.png" alt="14"></p>
<center style="font-size:12px; font-weight:bold">Fig. 14. Pooling with global-context</center> <br>

<blockquote>
<p>最后一个图卷积层的顶点往往会包含很多信息，这使得我们不得不保留一个很大的计算图以计算梯度，这十分浪费内存和时间。常用的策略是对图进行随机采样，用随机生成的子图来完成训练。采样方法有多种，如Diffusion sampling，Random node sampling，Random walk sampling等。</p>
</blockquote>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>本文所有的内容包括图片都基于文章 <em><strong>A Gentle Introduction to Graph Neural Networks</strong></em> 以及李沐先生的讲解。</p>
<ul>
<li><a href="https://distill.pub/2021/gnn-intro/">A Gentle Introduction to Graph Neural Networks</a></li>
<li><a href="https://www.bilibili.com/video/BV1iT4y1d7zP">零基础多图详解图神经网络（GNN&#x2F;GCN）【论文精读】
</a></li>
</ul>
]]></content>
      <categories>
        <category>GNN</category>
      </categories>
      <tags>
        <tag>GNN</tag>
        <tag>GCN</tag>
      </tags>
  </entry>
  <entry>
    <title>Paper003: Dataset Distillation by Matching Training Trajectories</title>
    <url>/2023/06/14/P003/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="专家轨迹"><a href="#专家轨迹" class="headerlink" title="专家轨迹"></a>专家轨迹</h1><p>专家轨迹（Expert trajectories）$\tau^*$是<a href="https://arxiv.org/pdf/2203.11932.pdf">本文</a>对原训练集训练模型时模型参数变化过程的描述。它记录了每个时期（Epoch）所有模型参数$\theta^*$的值$\left\{\theta_t^*\right\}_0^T$，其中$T$是总的时期数。专家轨迹是在原训练集上得到的，因此其反映了用合成集训练模型时的理论最佳路线。与专家轨迹各时期值相对应地，合成集在时期$t$的模型参数称学生参数（Student parameters）$\hat{\theta}_t$。</p>
<h1 id="短期和长期参数匹配"><a href="#短期和长期参数匹配" class="headerlink" title="短期和长期参数匹配"></a>短期和长期参数匹配</h1><p>本文的核心思想是用经过了$N$步优化的$\hat{\theta} _{t+N}$去匹配同一个网络下某一个专家轨迹的$\theta ^* _{t+M}$，其中学生参数$\hat{\theta}_t$被初始化为某个时期的专家参数$\theta_t^*$且$N&lt;&lt;M$。$\hat{\theta} _{t+N}$和$\theta ^* _{t+M}$之间的差异被定义为损失函数$\mathcal{L}$：</p>
<p>$$<br>\mathcal{L}&#x3D;\frac{||\hat{\theta} _{t+N}-\theta ^* _{t+M}||_2^2}{||\theta ^* _t-\theta ^* _{t+M}||_2^2}\tag{1}<br>$$</p>
<p>式中，下标2表示$L_2$范数，即欧几里得范数，定义为向量各元素平方和的平方根；上标2表示平方。之所以要有分母是为了保证当选取的$t$为训练的较后时期、模型参数变化不大时，$L$仍能有较大的响应。以该损失函数$L$进行梯度下降，更新合成集$\mathcal{D} _{syn}$，使得经过多次迭代后，合成集$\mathcal{D} <em>{syn}$训练得到的$\hat{\theta}</em>{t+N}$能使$\mathcal{L}$最小。</p>
<p><img src="/2023/06/14/P003/1.png" alt="1"></p>
<center style="font-size:12px; font-weight:bold">Fig. 1. Expert trajectories matching (Iteration here means epoch)</center><br>

<p>从$N$和$M$的视角来看，之前的数据蒸馏方法，包括最原始的数据蒸馏——只匹配最终的模型参数和<a href="/2023/06/12/P001/">基于梯度匹配的数据缩合</a>——实际上是对每个时期的模型参数都进行匹配，都可以被视为$N$和$M$在不同取值下的特例：</p>
<p><img src="/2023/06/14/P003/2.png" alt="2"></p>
<center style="font-size:12px; font-weight:bold">Fig. 2. Left: Original dataset distillation. Mid: Gradient matching. Right: Expert trajectories</center><br>

<p>原始的数据蒸馏只考虑结果，因此性能不是太好；梯度匹配过于追求过程，是一种贪心的思想，可能不能得到全局最优且计算量偏大；本文提出的专家轨迹则可视为对前两者的折中。</p>
]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>Paper</tag>
        <tag>Dataset Distillation</tag>
      </tags>
  </entry>
  <entry>
    <title>Paper002: Dataset Condensation with Distribution Matching</title>
    <url>/2023/06/13/P002/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="核心集和以往数据压缩算法存在的问题"><a href="#核心集和以往数据压缩算法存在的问题" class="headerlink" title="核心集和以往数据压缩算法存在的问题"></a>核心集和以往数据压缩算法存在的问题</h1><p>核心集选择（Coreset selection）算法基于一定的启发式准则（Heuristic criteria），如核心集到原数据集中心的距离最小、核心集包含的样本要尽可能地多样等，从原训练集中选择一个更小的核心训练集。这种方法选择了更具有代表性的数据来进行训练，因而降低了训练的开销，但是它也存在如下缺点：</p>
<ol>
<li>核心集选择依赖的几乎都是贪心算法，这可能无法达到全局最优；</li>
<li>有效性极度依赖原训练集的有效性。</li>
</ol>
<p>数据蒸馏（Dataset distillation，或数据缩合，Dataset condensation），一定程度上解决了核心集选择的问题，但无论是最原始的数据蒸馏算法，还是前面提到的<a href="/2023/06/12/P001/">基于梯度匹配的数据缩合算法</a>都存在一定的缺陷：</p>
<ol>
<li>模型参数$\theta ^{\mathcal{S}}$和合成数据集$\mathcal{S}$的双重梯度下降十分消耗算力；</li>
<li>对合成数据集$\mathcal{S}$进行梯度下降时，要计算二阶混合偏导；</li>
<li>用于合成数据的网络的超参数不好调节（e.g. $\theta ^{\mathcal{S}}$和$\mathcal{S}$的梯度下降次数$\varsigma ^{\mathcal{\theta}}$和$\varsigma ^{\mathcal{S}}$）。</li>
</ol>
<p>这些缺陷限制了其在大数据集上的应用。</p>
<h1 id="分布匹配"><a href="#分布匹配" class="headerlink" title="分布匹配"></a>分布匹配</h1><p>分布匹配（Distribution matching）是<a href="https://arxiv.org/pdf/2110.04181.pdf">这篇文章</a>使用的用于数据缩合的方法。实际上，它更像是一种可以学习的核心集：</p>
<ol>
<li>相比于纯核心集，分布匹配学习到的$\mathcal{S}$中的数据不一定存在于原训练集中；</li>
<li>相比于基于梯度匹配的数据缩合，分布匹配学习到的$\mathcal{S}$中的数据分布更加接近原训练集，且缩合的速度更快；</li>
<li>可以将分布匹配理解为用学习的方法去获得一个更小的、与原训练集同分布的合成集。</li>
</ol>
<p><img src="/2023/06/13/P002/1.png" alt="1"></p>
<center style="font-size:12px; font-weight:bold">Fig. 1. Dataset condensation with distribution matching</center><br>

<p>要让学习到的$\mathcal{S}$与原训练集$\mathcal{T}$近似于同分布，要点在于有个确切的方法能衡量两个分布的近似情况。文中采用的是常用的<a href="/2023/06/13/MaximumMeanDiscrepancy/">最大均值差异法（Maximum mean discrepancy）</a>：</p>
<p>$$<br>\begin{align*}<br>    \sup _{\psi _{\theta}\in\mathcal{H},||\psi _{\theta}||_\mathcal{H}\le1}(\text{E}[\psi _{\theta}(\mathcal{T})]&amp;-\text{E}[\psi _{\theta}(\mathcal{S})])\tag{1}\\<br>    \min \text{E} _{\theta\sim P _{\theta}} ||\frac{1}{|\mathcal{T}|}\sum\limits _{i&#x3D;1} ^{|\mathcal{T}|}\psi _{\theta}(x_i)&amp;-\frac{1}{|\mathcal{S}|}\sum\limits _{j&#x3D;1} ^{|\mathcal{S}|}\psi _{\theta}(\mathcal{s}_j)|| ^2\tag{2}<br>\end{align*}<br>$$</p>
<p>其中，$\mathcal{T}$是原训练集，$\mathcal{S}$是合成集；$x$和$\mathcal{s}$分别是原训练集和合成集单一样本的特征；$\psi_\theta$是一个带参数的函数（实际上是一个神经网络，$\theta$为其模型参数，服从分布$P_\theta$；也是再生希尔伯特空间中的一个向量），该函数将样本的特征映射到更低的维度，便于处理（实际上是求一个高阶矩）。</p>
<p>文中用的实验样例是图片分类，因此样本的特征就是图片。作者还在式$(2)$的基础上考虑了数据增强（Data augmentation）以更好地适应训练图片数据的实际情况，因此式$(2)$又可变为：</p>
<p>$$<br>\min _{\omega\sim\Omega} \text{E} _{\theta\sim P _{\theta}}  ||\frac{1}{|\mathcal{T}|}\sum\limits _{i&#x3D;1} ^{|\mathcal{T}|}\psi _{\theta}(\mathcal{A}(x_i,\omega))-\frac{1}{|\mathcal{S}|}\sum\limits _{j&#x3D;1} ^{|\mathcal{S}|}\psi _{\theta}(\mathcal{A}(\mathcal{s}_j,\omega))|| ^2\tag{3}<br>$$</p>
<p>其中，$\Omega$是数据增强参数空间，$\mathcal{A}(x,\omega)$则是相应的增强操作，其对$x$和$\mathcal{s}$是一样的。</p>
<h2 id="重要代码"><a href="#重要代码" class="headerlink" title="重要代码"></a>重要代码</h2><p>以下只是对文中代码的简单实现（并不能运行），具体代码详见文章提供的开源部分<a href="https://github.com/VICO-UoE/DatasetCondensation">Dataset Condensation with Distribution Matching</a>。</p>
<p><img src="/2023/06/13/P002/2.png" alt="2"></p>
<center style="font-size:12px; font-weight:bold">Fig. 2. Pseudocode in paper</center> <br>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 参数定义</span></span><br><span class="line">num_exp = <span class="number">5</span>  <span class="comment"># 重复实验次数</span></span><br><span class="line">num_classes = <span class="number">10</span>  <span class="comment"># 实验为图片分类，使用数据集MNIST，共10个类</span></span><br><span class="line">ipc = <span class="number">10</span>  <span class="comment"># 为每个类训练十张合成图片</span></span><br><span class="line">channel = <span class="number">1</span>  <span class="comment"># 对黑白图片，输入通道为1</span></span><br><span class="line">im_size = [<span class="number">28</span>, <span class="number">28</span>]  <span class="comment"># MNIST图片尺寸为28x28</span></span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span>)  <span class="comment"># 用GPU训练</span></span><br><span class="line">lr_img = <span class="number">1.0</span>  <span class="comment"># 合成图片学习率</span></span><br><span class="line">K = <span class="number">1000</span>  <span class="comment"># 最外层循环，使得合成集S能适应模型参数的不同初始化方式，实际上作用同epoch</span></span><br><span class="line">batch_real = <span class="number">256</span>  <span class="comment"># 原训练集的批量大小</span></span><br><span class="line">batch_train = <span class="number">256</span>  <span class="comment"># 训练模型参数时的批量大小</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 重复实验</span></span><br><span class="line"><span class="keyword">for</span> exp <span class="keyword">in</span> <span class="built_in">range</span>(num_exp):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;对原训练集和合成集的初始化&#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># 一些获取训练集T的操作，最终得到的特征和标签依次为（MNIST）:</span></span><br><span class="line">    <span class="comment"># images_all: Tensor，shape (6000, 1, 28, 28)，其中6000为样本数，1为输入通道数（因为是黑白图片），28x28是图片像素</span></span><br><span class="line">    <span class="comment"># labels_all: Tensor，shape (6000)，6000为样本数，类别为10，包含0~9的手写数字</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 可训练的合成图片，shape (10, 1, 28, 28)</span></span><br><span class="line">    image_syn = torch.randn(size=(num_classes*ipc, channel, im_size[<span class="number">0</span>], im_size[<span class="number">1</span>]), dtype=torch.float32, requires_grad=<span class="literal">True</span>, device=device)</span><br><span class="line">    <span class="comment"># 不可训练的合成图片标签，shape (10)</span></span><br><span class="line">    label_syn = torch.tensor([np.ones(ipc)*i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_classes)], dtype=torch.long, requires_grad=<span class="literal">False</span>, device=device).view(-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;优化算法、损失函数&#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># 对合成图片的优化算法为使用动量法的SGD</span></span><br><span class="line">    optimizer_image = torch.optim.SGD([image_syn, ], lr=lr_img, momentum=<span class="number">0.5</span>)</span><br><span class="line">    optimizer_image.zero_grad()  <span class="comment"># 梯度清零</span></span><br><span class="line">    <span class="comment"># 此处先不定义损失函数，因为用的是MMD</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;for k = 0, ... ,K - 1&#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(K+<span class="number">1</span>):</span><br><span class="line">        <span class="comment"># 源代码在此处存在在特定结点分析训练效果的代码，此处省略</span></span><br><span class="line">        </span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;sample \theta&#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment"># 获取某个指定的网络同时随机初始化模型参数，原文存在参数，此处省略，仅作为示意</span></span><br><span class="line">        net = get_network().to(device)</span><br><span class="line">        net.train()  <span class="comment"># 使网络进入训练模式</span></span><br><span class="line">        <span class="comment"># PyTorch生成网络的模型参数默认是记录梯度的，但在DM中，网络只是映射函数，因此不需要训练，也就无需梯度</span></span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> <span class="built_in">list</span>(net.parameters()):</span><br><span class="line">            param.requires_grad = <span class="literal">False</span></span><br><span class="line">        <span class="comment"># net.embed是作者定义的网络类中的函数，实际上是个简化版的forward，一般是去掉最后一层的全连接层</span></span><br><span class="line">        embed = net.embed</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将所有类别的梯度差异相加再计算梯度</span></span><br><span class="line">        loss = torch.tensor(<span class="number">0.0</span>).to(device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 原文还分有BatchNorm和无BatchNorm的情况，此处只考虑无BatchNorm</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;sample mini-batch pairs for each class&#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment"># 对不同的类别单独训练</span></span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">range</span>(num_classes):</span><br><span class="line">            <span class="comment"># 从原训练集的c类中随机获取batch_real张图片，注意，get_images是原文定义的内联函数，因此可以直接访问images_all，而此时images_all已经被放到GPU中了</span></span><br><span class="line">            img_real = get_images(c, batch_real)</span><br><span class="line">            <span class="comment"># 因为网络只是映射特征，故不需要labels</span></span><br><span class="line">            <span class="comment"># 同样地获取合成集中对应类别的数据</span></span><br><span class="line">            img_syn = image_syn[c*ipc:(c+<span class="number">1</span>)*ipc].reshape(ipc, channel, im_size[<span class="number">0</span>], im_size[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 原文此处有数据增强，此处省略</span></span><br><span class="line"></span><br><span class="line">            <span class="string">&#x27;&#x27;&#x27;数据过网络&#x27;&#x27;&#x27;</span></span><br><span class="line">            <span class="comment"># 原训练集</span></span><br><span class="line">            output_real = embed(img_real).detach()  <span class="comment"># 以防万一，不要让其进入计算图中</span></span><br><span class="line">            <span class="comment"># 合成集</span></span><br><span class="line">            output_syn = embed(img_syn)</span><br><span class="line">            <span class="string">&#x27;&#x27;&#x27;MMD&#x27;&#x27;&#x27;</span></span><br><span class="line">            <span class="comment"># 计算MMD，实际值为矩阵每列平均值（行一般代表样本数）差的平方和</span></span><br><span class="line">            loss += torch.<span class="built_in">sum</span>((torch.mean(output_real, dim=<span class="number">0</span>) - torch.mean(output_syn, dim=<span class="number">0</span>))**<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;更新合成图片&#x27;&#x27;&#x27;</span></span><br><span class="line">        optimizer_img.zero_grad()  <span class="comment"># 合成图片矩阵的梯度清零</span></span><br><span class="line">        loss.backward()  <span class="comment"># 计算对合成图片矩阵的二阶混合偏导</span></span><br><span class="line">        optimizer_img.step()  <span class="comment"># 梯度下降</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>只有学习率一个可调参数。</p>
</blockquote>
<h2 id="实验设计"><a href="#实验设计" class="headerlink" title="实验设计"></a>实验设计</h2><p>后续的内容几乎与<a href="/2023/06/12/P001/">Dataset Condensation</a>一致，只不过用到了更大的数据集。实验结果也表明，相比于DC得到的合成集，DM得到的合成集的分布更加均匀、更加接近原训练集。</p>
]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>Paper</tag>
        <tag>Dataset Distillation</tag>
      </tags>
  </entry>
  <entry>
    <title>Paper001: Dataset Condensation with Gradient Matching</title>
    <url>/2023/06/12/P001/</url>
    <content><![CDATA[<span id="more"></span>


<h1 id="数据蒸馏"><a href="#数据蒸馏" class="headerlink" title="数据蒸馏"></a>数据蒸馏</h1><p>当前最先进的深度学习技术几乎都采用的是大数据训练大模型的方式。这样的方式能够得到如GPT-4等具有远超我们想象的性能的大模型，但是其训练的算力和内存开销是一般人难以承担的。</p>
<p>数据蒸馏（Dataset Distillation）是解决这种问题的一种技术。它以学习的方式，从初始的大训练数据集（the original large training set）中（e.g. MNIST, CIFAR10等经典的数据集）获得一个更小的合成数据集（synthetic set）。这个合成数据集的数据量可能只有原数据集的1%，但是，用该数据集训练得到模型的性能可以达到用原数据集训练得到模型性能的80%~100%，同时也能具有不输于原模型的泛化性能。下图显示了数据蒸馏的目标。需要注意的是，原数据集与合成数据集训练的网络是一模一样的，测试集中的数据也是一模一样的，不同的只是训练集中的数据。</p>
<p><img src="/2023/06/12/P001/1.png" alt="1"></p>
<center style="font-size:12px; font-weight:bold">Fig. 1. Dataset Distillation</center><br>

<p>除了所用的训练集不同外，合成集训练得到的模型和原集得到的模型几乎一致（实际的模型参数可能不同）。因此，以后再用同样的数据集训练模型时，完全可以使用更小、训练速度更快的合成数据集，而测试和部署时模型的输入依旧为未经合成处理的数据。</p>
<p><img src="/2023/06/12/P001/2.png" alt="2"></p>
<center style="font-size:12px; font-weight:bold">Fig. 2. Process of dataset distillation</center>

<h1 id="数据缩合"><a href="#数据缩合" class="headerlink" title="数据缩合"></a>数据缩合</h1><p>本篇论文发表于ICLR 2021，第一作者是Dr. Bo Zhao。论文中用到的数据蒸馏方法称“数据缩合”（<a href="https://arxiv.org/pdf/2006.05929.pdf">Dataset Condensation</a>）。其最基本的设想是：合成集模型要想获得可与原集模型相媲美的性能和泛化能力，只要两个集合训练出的模型具有相似的模型参数就好了，即：对于原数据集$\mathcal{T}&#x3D;\left\{(x_i,y_i)\right\}| _{i&#x3D;1} ^\mathcal{|T|}$，其中$x\in\mathbf{R}^d$，是单一样本的特征，$y\in\left\{0,1,...,C-1\right\}$，是单一样本的标签，$C$是类别个数；对于合成数据集$\mathcal{S}&#x3D;\left\{(s_i,y_i)\right\}| _{i&#x3D;1} ^\mathcal{|S|}$，其中$s\in\mathbf{R}^d$，是单一合成样本的特征，$y\in\left\{0,1,...,C-1\right\}$，是单一合成样本的标签，其类别和数量与原标签一致，它们分别独立训练相同网络的最优化模型参数分别为：</p>
<p>$$<br>\begin{align*}<br>    \theta ^{\mathcal{T}}&amp;&#x3D;\arg\min _{\theta}\mathcal{L ^T}(\theta)\tag{1}\\<br>    \theta ^{\mathcal{S}}&amp;&#x3D;\arg\min _{\theta}\mathcal{L ^S}(\theta)\tag{2}<br>\end{align*}<br>$$</p>
<p>其中，$\mathcal{L}$是代价函数（Cost Function），两个训练集采用相同的代价函数，而$\theta ^{\mathcal{T}}$和$\theta ^{\mathcal{S}}$则分别是网络达到最优状态时的模型参数，两者相似，即它们之间的距离要尽可能地小：</p>
<p>$$<br>\min _\mathcal{S}D(\theta ^{\mathcal{T}},\theta ^{\mathcal{S}})\quad \text{subject to} \quad \theta ^{\mathcal{S}}(\mathcal{S})&#x3D;\arg\min _{\theta}\mathcal{L ^S}(\theta)\tag{3}<br>$$</p>
<p>其中$\theta ^{\mathcal{S}}$是$\mathcal{S}$的函数是因为我们期望找到这么一个合成集$\mathcal{S}$，使得由它训练得到的$\theta ^{\mathcal{S}}$能够满足$(3)$。训练开始时，我们一般会把$\theta ^{\mathcal{T}}$和$\theta ^{\mathcal{S}}$初始化为相同的值$\theta_0$，其中$\theta_0\sim P _{\theta_0}$。但是不同的$\theta_0$可能会得到不同的$\theta ^{\mathcal{T}}$，因此，更进一步地，我们希望不同$\theta_0$下距离的期望是最小的，于是$(3)$转变为：</p>
<p>$$<br>\min _\mathcal{S}\text{E} _{\theta_0\sim P _{\theta_0}} [D(\theta ^{\mathcal{T}} ({\theta_0}),\theta ^{\mathcal{S}} ({\theta_0}))]\quad \text{subject to} \quad \theta ^{\mathcal{S}}(\mathcal{S})&#x3D;\arg\min _{\theta}\mathcal{L ^S}(\theta ({\theta_0})) \tag{4}<br>$$</p>
<p>一般情况下，给定$\theta_0$，$\theta ^{\mathcal{T}}$可以通过训练网络得到。此后，$\theta ^{\mathcal{T}}$便可用于训练$\mathcal{S}$，这包含两层的循环：</p>
<ol>
<li>任意初始化一个$\mathcal{S}$集；</li>
<li>内层循环：根据给定的$\theta_0$和当前的$\mathcal{S}$，训练与$\theta ^{\mathcal{T}}$相同的网络，得到当前的$\theta ^{\mathcal{S}}$，此过程对应$(4)$的后一项；</li>
<li>外层循环：初始化内层循环的$\theta_0$，内层循环结束后，计算$\theta ^{\mathcal{T}}$和$\theta ^{\mathcal{S}}$之间的距离，得到梯度（$\mathcal{S}$为自变量），更新$\mathcal{S}$，此过程对应$(4)$的前一项。</li>
</ol>
<p><img src="/2023/06/12/P001/3.png" alt="3"></p>
<center style="font-size:12px; font-weight:bold">Fig. 3. Process of dataset condensation</center> <br>

<p>不难看出，每次内层循环都相当于重新训练了一个网络，这是很浪费算力的。然而在实际操作的过程中，我们并没有必要每次都训练出当前$\mathcal{S}$下最优的$\theta ^{\mathcal{S}}$，毕竟这只是个近似值。因此，实际上，我们采用用特定的优化算法优化了几步得到的$\theta ^{\mathcal{S}}$作为近似值即可，即：</p>
<p>$$<br>\theta ^{\mathcal{S}}(\mathcal{S})&#x3D;\text{opt-alg} _{\theta}(\mathcal{L ^S}(\theta),\varsigma) \tag{5}<br>$$</p>
<p>其中$\varsigma$是优化（梯度下降）的次数，而$\text{opt-alg}$是任意的优化算法（e.g. adam, sgd, etc.），$\theta$是$\theta_0$的函数，此处为方便不写出，后面也是。</p>
<blockquote>
<p>合成的是特征$\mathcal{s}$，不对labels进行合成操作。</p>
</blockquote>
<h2 id="梯度匹配"><a href="#梯度匹配" class="headerlink" title="梯度匹配"></a>梯度匹配</h2><p>上面的数据缩合方法又称<em>参数匹配法</em>（Parameter Matching），即以让$\theta ^{\mathcal{S}}$逐渐逼近$\theta ^{\mathcal{T}}$的方式来训练得到$\mathcal{S}$。该方法有两个弊端：</p>
<ol>
<li>$\theta ^{\mathcal{T}}$与$\theta ^{\mathcal{S}}$之间可能相差很大且一个网络的模型参数空间很大，优化路径中可能存在若干局部最小值，因此难以达到最优解；</li>
<li>限定步数的$\text{opt-alg}$得到的参数过于不精确，但为了计算性能只能这么做。</li>
</ol>
<blockquote>
<p>最原始的数据蒸馏算法用的就是参数匹配法。数据蒸馏和数据缩合实际上是一回事，只不过数据缩合特指这篇文章所用到的蒸馏算法。</p>
</blockquote>
<p>这篇论文提出的能解决上述问题的方法称<em>梯度匹配</em>（Gradient Matching），其核心思想在于：<strong>不仅让$\theta ^{\mathcal{T}}$和$\theta ^{\mathcal{S}}$最终的值相近，且遵循的优化路径也相近</strong>。于是式$(4)$就变成：</p>
<p>$$<br>\begin{align*}<br>    \min _\mathcal{S}\text{E} _{\theta_0\sim P _{\theta_0}} [\sum\limits _{t&#x3D;0} ^{T-1}D&amp;(\theta _t ^{\mathcal{T}},\theta _t ^{\mathcal{S}})]\quad \text{subject to}\tag{6}\\<br>    \theta ^{\mathcal{S}} _{t+1}(\mathcal{S})&#x3D;\text{opt-alg} _{\theta}(\mathcal{L ^S}(\theta _t),\varsigma ^{\mathcal{S}}) \quad&amp;\text{and}\quad \theta ^{\mathcal{T}} _{t+1}&#x3D;\text{opt-alg} _{\theta}(\mathcal{L ^T}(\theta _t),\varsigma ^{\mathcal{T}})<br>\end{align*}<br>$$</p>
<p>其中$\varsigma ^{\mathcal{S}}$和$\varsigma ^{\mathcal{T}}$分别是$\mathcal{S}$和$\mathcal{T}$一次优化的梯度下降次数，$T$是迭代次数。整个式$(6)$表明，$\mathcal{S}$要使得整个迭代过程中的距离和最小，即$\mathcal{S}$要使得$\theta ^{\mathcal{T}}$与$\theta ^{\mathcal{S}}$几乎遵循相同的优化路径。每一次梯度下降，$\theta ^{\mathcal{T}}$与$\theta ^{\mathcal{S}}$的变化如下所示：</p>
<p>$$<br>\theta ^{\mathcal{S}} _{t+1} \leftarrow \theta ^{\mathcal{S}} _{t}-\eta_\theta\nabla_\theta \mathcal{L} ^{\mathcal{S}}(\theta _t ^{\mathcal{S}}) \quad\text{and}\quad \theta ^{\mathcal{T}} _{t+1} \leftarrow \theta ^{\mathcal{T}} _{t}-\eta_\theta\nabla_\theta \mathcal{L} ^{\mathcal{T}}(\theta _t ^{\mathcal{T}})\tag{7}<br>$$</p>
<p>其中$\eta_\theta$是学习率而其后续项为梯度。</p>
<p><img src="/2023/06/12/P001/4.png" alt="4"></p>
<center style="font-size:12px; font-weight:bold">Fig. 4. Dataset condensation with gradient matching</center> <br>

<p>由于$\theta ^{\mathcal{T}}$与$\theta ^{\mathcal{S}}$均初始化为$\theta_0$，所以在计算距离$D(\theta _t ^{\mathcal{T}},\theta _t ^{\mathcal{S}})$时：</p>
<p>$$<br>\theta ^{\mathcal{S}} _{0}-\eta_\theta\nabla_\theta \mathcal{L} ^{\mathcal{S}}(\theta _0 ^{\mathcal{S}}) -(\theta ^{\mathcal{T}} _{0}-\eta_\theta\nabla_\theta \mathcal{L} ^{\mathcal{T}}(\theta _0 ^{\mathcal{T}}))&#x3D;\eta_\theta\nabla_\theta \mathcal{L} ^{\mathcal{T}}(\theta _0 ^{\mathcal{T}})-\eta_\theta\nabla_\theta \mathcal{L} ^{\mathcal{S}}(\theta _0 ^{\mathcal{S}})\tag{8}<br>$$</p>
<p>因此，实际上我们只需要计算梯度的距离，并让其尽可能地小即可，于是式$(6)$又可转变为：</p>
<p>$$<br>\min _\mathcal{S}\text{E} _{\theta_0\sim P _{\theta_0}} [\sum\limits _{t&#x3D;0} ^{T-1}D(\nabla_\theta \mathcal{L} ^{\mathcal{S}}(\theta _t ^{\mathcal{S}}),\nabla_\theta \mathcal{L} ^{\mathcal{T}}(\theta _t ^{\mathcal{T}}))] \tag{9}<br>$$</p>
<p>即，只要梯度下降的方向相近即可。</p>
<blockquote>
<p>实际上，$\varsigma ^{\mathcal{S}}$和$\varsigma ^{\mathcal{T}}$由优化算法决定，不同的优化算法，在处理完所有训练数据后，梯度下降的次数不同。$\theta _t ^{\mathcal{T}}$和$\theta _t ^{\mathcal{S}}$实际上值相同，因为实际训练时分两个阶段：训练并更新$\mathcal{S}$和训练并更新$\theta_t$，而$\mathcal{S}$和$\mathcal{T}$经过的都是相同的网络，所以$\theta_t$也是一样的。$\theta_t$的梯度下降使用的数据是$\mathcal{S}$，这也在一定程度上造成了该方法的误差。</p>
</blockquote>
<p>由于梯度是向量，而对于多参数的梯度下降，重要的是下降的方向，因此距离$D$采用两个向量的余弦距离，而不是欧式距离：</p>
<p>$$<br>\begin{align*}<br>    D(\mathbf{A},\mathbf{B})<br>    &amp;&#x3D;1-cos&lt;\mathbf{A},\mathbf{B}&gt;\\\<br>    &amp;&#x3D;1-\frac{\mathbf{A}\cdot\mathbf{B}}{||\mathbf{A}||\space||\mathbf{B}||}\tag{10}<br>\end{align*}<br>$$</p>
<h2 id="重要代码"><a href="#重要代码" class="headerlink" title="重要代码"></a>重要代码</h2><p>以下只是对文中代码的简单实现（并不能运行），具体代码详见文章提供的开源部分<a href="https://github.com/VICO-UoE/DatasetCondensation">Dataset Condensation</a>。</p>
<p><img src="/2023/06/12/P001/5.png" alt="5"></p>
<center style="font-size:12px; font-weight:bold">Fig. 5. Pseudocode in paper</center> <br>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 参数定义</span></span><br><span class="line">num_exp = <span class="number">5</span>  <span class="comment"># 重复实验次数</span></span><br><span class="line">num_classes = <span class="number">10</span>  <span class="comment"># 实验为图片分类，使用数据集MNIST，共10个类</span></span><br><span class="line">ipc = <span class="number">1</span>  <span class="comment"># 为每个类只训练一张合成图片</span></span><br><span class="line">channel = <span class="number">1</span>  <span class="comment"># 对黑白图片，输入通道为1</span></span><br><span class="line">im_size = [<span class="number">28</span>, <span class="number">28</span>]  <span class="comment"># MNIST图片尺寸为28x28</span></span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span>)  <span class="comment"># 用GPU训练</span></span><br><span class="line">lr_img = <span class="number">0.1</span>  <span class="comment"># 合成图片学习率</span></span><br><span class="line">lr_net = <span class="number">0.01</span>  <span class="comment"># 模型参数学习率</span></span><br><span class="line">K = <span class="number">1000</span>  <span class="comment"># 最外层循环，使得合成集S能适应模型参数的不同初始化方式</span></span><br><span class="line">T = <span class="number">1</span>  <span class="comment"># 里一层循环，对不同ipc有不同的值，相当于训练合成图片时的epoch，因为合成图片在一次循环中只对一个类更新一次</span></span><br><span class="line">batch_real = <span class="number">256</span>  <span class="comment"># 原训练集的批量大小</span></span><br><span class="line">batch_train = <span class="number">256</span>  <span class="comment"># 训练模型参数时的批量大小</span></span><br><span class="line">inner_loop = <span class="number">1</span>  <span class="comment"># 训练模型参数时的迭代次数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 重复实验</span></span><br><span class="line"><span class="keyword">for</span> exp <span class="keyword">in</span> <span class="built_in">range</span>(num_exp):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;对原训练集和合成集的初始化&#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># 一些获取训练集T的操作，最终得到的特征和标签依次为（MNIST）:</span></span><br><span class="line">    <span class="comment"># images_all: Tensor，shape (6000, 1, 28, 28)，其中6000为样本数，1为输入通道数（因为是黑白图片），28x28是图片像素</span></span><br><span class="line">    <span class="comment"># labels_all: Tensor，shape (6000)，6000为样本数，类别为10，包含0~9的手写数字</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 可训练的合成图片，shape (10, 1, 28, 28)</span></span><br><span class="line">    image_syn = torch.randn(size=(num_classes*ipc, channel, im_size[<span class="number">0</span>], im_size[<span class="number">1</span>]), dtype=torch.float32, requires_grad=<span class="literal">True</span>, device=device)</span><br><span class="line">    <span class="comment"># 不可训练的合成图片标签，shape (10)</span></span><br><span class="line">    label_syn = torch.tensor([np.ones(ipc)*i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_classes)], dtype=torch.long, requires_grad=<span class="literal">False</span>, device=device).view(-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;优化算法、损失函数&#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># 对合成图片的优化算法为使用动量法的SGD</span></span><br><span class="line">    optimizer_image = torch.optim.SGD([image_syn, ], lr=lr_img, momentum=<span class="number">0.5</span>)</span><br><span class="line">    optimizer_image.zero_grad()  <span class="comment"># 梯度清零</span></span><br><span class="line">    <span class="comment"># 损失函数，因为梯度是原网络训练时模型参数产生的梯度，而本实验是个图片分类问题，因此使用的仍是softmax的损失函数</span></span><br><span class="line">    criterion = torch.nn.CrossEntropyLoss().to(device)</span><br><span class="line"></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;for k = 0, ... ,K - 1&#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(K+<span class="number">1</span>):</span><br><span class="line">        <span class="comment"># 源代码在此处存在在特定结点分析训练效果的代码，此处省略</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 获取某个指定的网络同时随机初始化模型参数，原文存在参数，此处省略，仅作为示意</span></span><br><span class="line">        net = get_network().to(device)</span><br><span class="line">        net.train()  <span class="comment"># 使网络进入训练模式</span></span><br><span class="line">        net_parameters = <span class="built_in">list</span>(net.parameters())</span><br><span class="line">        <span class="comment"># 优化模型参数的优化函数，采用简单的SGD</span></span><br><span class="line">        optimizer_net = torch.optim.SGD(net.parameters(), lr=lr_net)</span><br><span class="line">        optimizer_net.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;for t = 0, ... ,T - 1&#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(T):</span><br><span class="line">            <span class="comment"># 此处还有对BatchNorm做的优化，也省略</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 将所有类别的梯度差异相加再计算二阶混合偏导</span></span><br><span class="line">            loss = torch.tensor(<span class="number">0.0</span>).to(device)</span><br><span class="line"></span><br><span class="line">            <span class="string">&#x27;&#x27;&#x27;for c = 0, ... ,C - 1，训练合成图片&#x27;&#x27;&#x27;</span></span><br><span class="line">            <span class="comment"># 对不同的类别单独训练</span></span><br><span class="line">            <span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">range</span>(num_classes):</span><br><span class="line">                <span class="comment"># 从原训练集的c类中随机获取batch_real张图片，注意，get_images是原文定义的内联函数，因此可以直接访问images_all，而此时images_all已经被放到GPU中了</span></span><br><span class="line">                img_real = get_images(c, batch_real)</span><br><span class="line">                <span class="comment"># lab_real是新数据，要先放入GPU</span></span><br><span class="line">                lab_real = torch.ones((img_real.shape[<span class="number">0</span>]), device=device, dtype=torch.long) * c</span><br><span class="line">                <span class="comment"># 同样地获取合成集中对应类别的数据</span></span><br><span class="line">                img_syn = image_syn[c*ipc:(c+<span class="number">1</span>)*ipc].reshape(ipc, channel, im_size[<span class="number">0</span>], im_size[<span class="number">1</span>])</span><br><span class="line">                lab_syn = torch.ones((ipc), device=device, dtype=torch.long) * c</span><br><span class="line"></span><br><span class="line">                <span class="string">&#x27;&#x27;&#x27;数据过网络&#x27;&#x27;&#x27;</span></span><br><span class="line">                <span class="comment"># 原训练集</span></span><br><span class="line">                output_real = net(img_real)</span><br><span class="line">                loss_real = criterion(output_real, lab_real)</span><br><span class="line">                gw_real = torch.autograd.grad(loss_real, net_parameters)  <span class="comment"># 获得原训练集的梯度</span></span><br><span class="line">                gw_real = <span class="built_in">list</span>((_.detach().clone() <span class="keyword">for</span> _ <span class="keyword">in</span> gw_real))  <span class="comment"># 逐层处理且将梯度从计算图中分离</span></span><br><span class="line"></span><br><span class="line">                output_syn = net(img_syn)</span><br><span class="line">                loss_syn = criterion(output_syn, lab_syn)</span><br><span class="line">                gw_syn = torch.autograd.grad(loss_syn, net_parameters, create_graph=<span class="literal">True</span>)  <span class="comment"># 获得合成集的梯度，不过因为要计算二阶混合偏导，因此要保留计算图</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># 余弦距离</span></span><br><span class="line">                <span class="keyword">def</span> <span class="title function_">distance_wb</span>(<span class="params">gwr, gws</span>):</span><br><span class="line">                    shape = gwr.shape</span><br><span class="line">                    <span class="keyword">if</span> <span class="built_in">len</span>(shape) == <span class="number">4</span>:  <span class="comment"># conv_layer: out*in*h*w</span></span><br><span class="line">                        gwr =gwr.reshape(shape[<span class="number">0</span>], shape[<span class="number">1</span>]*shape[<span class="number">2</span>]*shape[<span class="number">3</span>])</span><br><span class="line">                        gws =gws.reshape(shape[<span class="number">0</span>], shape[<span class="number">1</span>]*shape[<span class="number">2</span>]*shape[<span class="number">3</span>])</span><br><span class="line">                    <span class="keyword">elif</span> <span class="built_in">len</span>(shape) == <span class="number">3</span>:  <span class="comment"># layer_norm: C*h*w</span></span><br><span class="line">                        gwr =gwr.reshape(shape[<span class="number">0</span>], shape[<span class="number">1</span>]*shape[<span class="number">2</span>])</span><br><span class="line">                        gws =gws.reshape(shape[<span class="number">0</span>], shape[<span class="number">1</span>]*shape[<span class="number">2</span>])</span><br><span class="line">                    <span class="keyword">elif</span> <span class="built_in">len</span>(shape) == <span class="number">2</span>:  <span class="comment"># linear_layer:h*w</span></span><br><span class="line">                        tmp = <span class="string">&quot;do nothing&quot;</span></span><br><span class="line">                    <span class="keyword">else</span>:  <span class="comment"># only bias</span></span><br><span class="line">                        gwr =gwr.reshape(<span class="number">1</span>, shape[<span class="number">0</span>])</span><br><span class="line">                        gws =gws.reshape(<span class="number">1</span>, shape[<span class="number">0</span>])</span><br><span class="line">                    <span class="comment"># torch.norm: 默认求F范数，即矩阵各元素平方和开根号</span></span><br><span class="line">                    <span class="comment"># 此处实际上是先求每层每个神经元的余弦距离最后再将余弦距离相加（改变形状后，gwr和gws的第一维是神经元个数）</span></span><br><span class="line">                    <span class="keyword">return</span> torch.<span class="built_in">sum</span>(<span class="number">1</span> - torch.<span class="built_in">sum</span>(gwr*gws, dim=-<span class="number">1</span>) / (torch.norm(gwr, dim=-<span class="number">1</span>) * torch.norm(gws, dim=-<span class="number">1</span>) + <span class="number">0.000001</span>))</span><br><span class="line">                </span><br><span class="line">                <span class="keyword">def</span> <span class="title function_">match_loss</span>(<span class="params">gw_syn, gw_real</span>):</span><br><span class="line">                    dis = torch.tensor(<span class="number">0.0</span>).to(device)</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">for</span> ig <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(gw_real)):</span><br><span class="line">                        gwr = gw_real[ig]</span><br><span class="line">                        gws = gw_syn[ig]</span><br><span class="line">                        dis += distance_wb(gwr, gws)</span><br><span class="line">                    </span><br><span class="line">                    <span class="keyword">return</span> dis</span><br><span class="line"></span><br><span class="line">                loss += match_loss(gw_syn, gw_real)</span><br><span class="line">            </span><br><span class="line">            <span class="string">&#x27;&#x27;&#x27;更新合成图片&#x27;&#x27;&#x27;</span></span><br><span class="line">            optimizer_img.zero_grad()  <span class="comment"># 合成图片矩阵的梯度清零</span></span><br><span class="line">            loss.backward()  <span class="comment"># 计算对合成图片矩阵的二阶混合偏导</span></span><br><span class="line">            optimizer_img.step()  <span class="comment"># 梯度下降</span></span><br><span class="line"></span><br><span class="line">            <span class="string">&#x27;&#x27;&#x27;更新模型参数&#x27;&#x27;&#x27;</span></span><br><span class="line">            <span class="comment"># 深拷贝，完全复制合成图片和label，防止在训练模型参数时合成图片生成计算图</span></span><br><span class="line">            image_syn_train, label_syn_train = copy.deepcopy(image_syn.detach()), copy.deepcopy(label_syn.detach())</span><br><span class="line">            <span class="comment"># 小批量训练的迭代器</span></span><br><span class="line">            dst_syn_train = TensorDataset(image_syn_train, label_syn_train)</span><br><span class="line">            trainloader = torch.utils.data.DataLoader(dst_syn_train, batch_size=batch_train, shuffle=<span class="literal">True</span>, num_workers=<span class="number">0</span>)</span><br><span class="line">            <span class="comment"># 我认为这一步正是DC的缺陷所在，合成集数据本身就已经不准了，用它更新的模型参数大概率偏离原来的方向</span></span><br><span class="line">            <span class="keyword">for</span> il <span class="keyword">in</span> <span class="built_in">range</span>(inner_loop):</span><br><span class="line">                epoch(trainloader)  <span class="comment"># 一次迭代：梯度下降、模型参数更新</span></span><br></pre></td></tr></table></figure>

<h2 id="实验设计"><a href="#实验设计" class="headerlink" title="实验设计"></a>实验设计</h2><p>实验主要分为三个部分：</p>
<ol>
<li>与现有核心集选择法和数据蒸馏法的性能比较；</li>
<li>缩合数据的泛化性分析；</li>
<li>缩合数据的数量与性能分析。</li>
</ol>
<p>采用的数据集分别为MNIST，SVHN，FashionMNIST和CIFAR10，网络结构有六种：MLP，ConvNet，LeNet，AlexNet，VGG-11和ResNet-18。每种结构5次重复实验，在训练$\mathcal{S}$的过程中，当k达到一定次数时，会用$S$去训练20个随机初始化的网络以分析训练效果。</p>
<h3 id="与现有方法性能比较"><a href="#与现有方法性能比较" class="headerlink" title="与现有方法性能比较"></a>与现有方法性能比较</h3><p>文中统一采用ConvNet作为所有方法的训练网络，与DC相比较的核心集选择法有Random，Herding，K-Center和Forgetting，以往的数据蒸馏方法则是DD（Dataset Distillation）。</p>
<h3 id="泛化性分析"><a href="#泛化性分析" class="headerlink" title="泛化性分析"></a>泛化性分析</h3><p>这部分实验分析的是某一网络训练出来的合成数据$\mathcal{S}$用于训练其他网络能否达到较好的性能。作者对6种网络进行了一一组合。</p>
<h3 id="缩合数据数量"><a href="#缩合数据数量" class="headerlink" title="缩合数据数量"></a>缩合数据数量</h3><p>这部分分析的是每个类别生成几张缩合图片合适，作者分析了1，10和50三种情况，结果当然是越多效果越好。</p>
<h2 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h2><p>作者对数据缩合可能存在的两大应用场景进行了分析。</p>
<h3 id="持续学习"><a href="#持续学习" class="headerlink" title="持续学习"></a>持续学习</h3><p>持续学习（Continual Learning）是一种将旧任务学习的知识应用到新的任务上、同时在旧任务上的表现不会出现太大的损失的机器学习方法。本质上是基于旧有的模型训练面向新任务的模型，使得最终的模型在新旧任务上都能表现得很好。相比于普通的数据，数据缩合后的合成数据有效信息更多，很适合持续学习。</p>
<h3 id="神经网络架构检索"><a href="#神经网络架构检索" class="headerlink" title="神经网络架构检索"></a>神经网络架构检索</h3><p>选择合适的神经网络架构需要训练大量的模型，而数据量更小的合成数据显然能大大地降低训练的算力和存储开销。</p>
]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>Paper</tag>
        <tag>Dataset Distillation</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformer: Self-Attention and Parallelization</title>
    <url>/2023/06/07/Transformer/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h1><p><a href="https://arxiv.org/pdf/1706.03762.pdf">Transformer</a> deals with sequence based on the encoder-decoder architecture. Compared to seq2seq which uses attention mechanisms as well as RNNs, transformer is purely based on the attention mechanism. Both of its encoder and decoder consist of $n$ transformer blocks.</p>
<p><img src="/2023/06/07/Transformer/1.png" alt="1"></p>
<center style="font-size:12px; font-weight:bold">Fig. 1. Architecture of transformer</center><br>

<p>The outpur of encoder is $\mathbf{y}_1,...,\mathbf{y}_n$, where $\mathbf{y}_i$ is the encoding of the $i$-th token. Except for the last encoder block, the output of other encoder blocks is the input for their next encoder block. The output of the last encoder block is the keys and values for the multi-head attention layer of all the decoder blocks whose queries are the output of masked multi-head attention layer. As a result, the shape of the output of the encoder and decoder should be the same.</p>
<p><img src="/2023/06/07/Transformer/2.png" alt="2"></p>
<center style="font-size:12px; font-weight:bold">Fig. 2. Information transfer between encoder and decoder</center><br>

<blockquote>
<p>The code of transformer in PyTorch: <a href="https://zh-v2.d2l.ai/chapter_attention-mechanisms/transformer.html">Dive into Deep Learning</a>.</p>
</blockquote>
<h2 id="Layer-Normalization"><a href="#Layer-Normalization" class="headerlink" title="Layer Normalization"></a>Layer Normalization</h2><ul>
<li>BatchNorm: The mean of each feature dimension becomes 0, and the variance becomes 1 (two-dimensional: column).</li>
<li>LayerNorm：The mean of all features of each sample becomes 0, and the variance becomes 1 (two-dimensional: row).</li>
</ul>
<p>But for sequence data, it is generally three-dimensional, that is <code>(batch_size, sequence_length, features)</code>. For different sequences, the effective length of the sequence may be different. At this time, the jitter of the mean and variance of different sequences calculated by BatchNorm will be larger, which will affect the global mean and variance, and when the length of the predicted sequence is much longer or shorter than the training sequence, the global mean and variance may not be very useful. LayerNorm normalizes all features of each sample, that is, each sample only considers itself, so it is relatively stable.</p>
<p><img src="/2023/06/07/Transformer/2_1.png" alt="2_1"></p>
<center style="font-size:12px; font-weight:bold">Fig. 3. BatchNorm and LayerNorm</center>

<h1 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h1><p>The encoder is made up of one embedding layer and $n$ encoder blocks. Each encoder block consists of two sublayers: multi-head self-attention and positionwise feed-forward networks. The shape of output of each layer is the same so hyperparameters that can be adjusted are the number of encoder block and the shape of output.</p>
<p>Inputs of encoder are keys, values and queries, which are all the source sequence. The shape of feature of the input after embedding layer is the same as the output. The input of the first encoder block is the original sequence after encoding while the others are outputs of the previous encoder block.</p>
<blockquote>
<p>All layers (except for the embedding layer) of encoder and decoder don&#39;t change the shape of samples.</p>
</blockquote>
<h2 id="Embedding-amp-Positional-Encoding"><a href="#Embedding-amp-Positional-Encoding" class="headerlink" title="Embedding &amp; Positional Encoding"></a>Embedding &amp; Positional Encoding</h2><p>The embedding layer maps the feature dimension of each key, value and query to the same dimension <code>num_hiddens</code>, which is the output dimension of the encoder, and then adds location information to all keys, values and queries.</p>
<blockquote>
<p>Since keys, values and queries are the same thing at this point, only one variable needs to be processed. <code>key_size</code>, <code>value_size</code>, and <code>query_size</code> which are the shape of features of keys, values and queries, are all the same thing.</p>
<p>Since the location information is between $-1$ and $1$, before adding the location information, it is necessary to multiply the samples processed by the embedding layer by the square root of <code>num_hiddens</code> in order to ensure that the value of the feature of each token is not much smaller than the location information.</p>
</blockquote>
<h2 id="Encoder-Block"><a href="#Encoder-Block" class="headerlink" title="Encoder Block"></a>Encoder Block</h2><p>The encoder block consists of two modules: a multi-head self-attention layer and a position-wise FFN layer. After each module, a residual connection and layer normalization are performed to train a deeper network (residual connection first and then layer normalization).</p>
<blockquote>
<p>This also reflects that in transformer, the feature dimensions of the input and output of each module are always constant. This is very different from the traditional CNNs.</p>
</blockquote>
<h3 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h3><p>Transformer uses dot-product attention. Therefore, in order to allow the multi-head attention layer to have learned parameters, before each head of keys, values, and queries, they must first be newly mapped with a fully connected layer.</p>
<p><img src="/2023/06/07/Transformer/3.png" alt="3"></p>
<center style="font-size:12px; font-weight:bold">Fig. 4. Multi-head attention and fully connected layer</center><br>

<p>The attention is actually a weighted sum of different values, so the dot-product attention does not change the shape of input. However, the fully connected layer may change the shape of input. The feature dimension of the sample after the multi-head attention layer should remain unchanged, so the number of heads is actually related to the output dimension of the feature:</p>
<p>$$<br>\text{heads}\times\text{num_hiddens_FC}&#x3D;\text{num_hiddens}<br>$$</p>
<p>Therefore, the fully connected layers before different heads can be merged into a large fully connected layer with model parameters <code>W_q</code>, <code>W_k</code> and <code>W_v</code>, which process queries, keys, and values respectively. The output size of the layer is $\text{num_hiddens}$. In this way, the parallelism will be better.</p>
<p><img src="/2023/06/07/Transformer/4.png" alt="4"></p>
<center style="font-size:12px; font-weight:bold">Fig. 5. Large fully connected layer</center><br>

<blockquote>
<p>The shape of the output obtained from the above operations is <code>(batch_size, num_queries or key-value_pairs, num_hiddens)</code>. After the deformation operation, the shape can be changed to <code>(batch_size, num_queries or key-value_pairs, heads, num_hiddens/heads)</code>. In this way, the different heads are separated again, and the output of each head can be calculated independently. After the attention is gathered and the output of each head is concatenated, a linear transformation is performed again. This transformation still does not change the feature dimension, but increases the number of learned parameters (<code>W_o</code>).</p>
</blockquote>
<h3 id="Position-wise-Feed-Forward-Networks"><a href="#Position-wise-Feed-Forward-Networks" class="headerlink" title="Position-wise Feed-Forward Networks"></a>Position-wise Feed-Forward Networks</h3><p>This module is actually an MLP with only one hidden layer, which still does not change the feature dimension of the input.</p>
<h1 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h1><p>The decoder is made up of one embedding layer and $n$ decoder blocks. The input of decoder is targeted sequence. The input of the first decoder block is the targeted sequence after embedding layer while the others are outputs of the previous decoder block.</p>
<p>When making predictions, the work of the encoder has not changed, and the original sequence can be processed directly. The decoder, on the other hand, has to work sequentially because the predictions have to be obtained one by one. That is to say, when predicting the (t+1)-th output, the keys and values of input of decoder are 0~t predicted values, while the query is the t-th predicted value.</p>
<h2 id="Embedding-amp-Positional-Encoding-1"><a href="#Embedding-amp-Positional-Encoding-1" class="headerlink" title="Embedding &amp; Positional Encoding"></a>Embedding &amp; Positional Encoding</h2><p>Functions and operations are in the same way as the encoder, except that the input is the targeted sequence.</p>
<h2 id="Decoder-Block"><a href="#Decoder-Block" class="headerlink" title="Decoder Block"></a>Decoder Block</h2><p>The decoder block consists of three modules: masked multi-head attention, multi-head self-attention and position-wise FFN. Similar to the encoder, there are also residual connections and layer normalization after each module.</p>
<h3 id="Masked-Multi-Head-Attention"><a href="#Masked-Multi-Head-Attention" class="headerlink" title="Masked Multi-Head Attention"></a>Masked Multi-Head Attention</h3><p>This module extracts the feature information of the targeted sequence. Its mode is almost the same as the multi-head self-attention of the encoder. The difference is that it adds a mask operation. The purpose of the mask operation is to make the transformer have the same autoregressive behavior mode during training and prediction. At predicting time, the decoder cannot see all targeted sequences at once, while at training time it can. Therefore, during training, a mask is used to cover the tokens after the current time step, making it invisible.</p>
<p>The mechanism is to make the attention score of the token after the current time step be negative infinity before $\text{softmax}$, so that the attention weights assigned to them are infinitely close to $0$. Then, we can achieve the purpose of shielding future tokens:</p>
<p>$$<br>\text{softmax}(\text{masked}(\mathbf{QK}^\text{T}))\mathbf{V}<br>$$</p>
<h3 id="Multi-Head-Attention-1"><a href="#Multi-Head-Attention-1" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h3><p>The same to the encoder, but the keys and values come from the final output of the encoder.</p>
<h3 id="Position-wise-Feed-Forward-Networks-1"><a href="#Position-wise-Feed-Forward-Networks-1" class="headerlink" title="Position-wise Feed-Forward Networks"></a>Position-wise Feed-Forward Networks</h3><p>The same to the encoder.</p>
<h2 id="Dense-Layer"><a href="#Dense-Layer" class="headerlink" title="Dense Layer"></a>Dense Layer</h2><p>It is a fully connected layer that implements softmax regression.</p>
<blockquote>
<p>By default, PyTorch treats the last dimension as the feature dimension.</p>
</blockquote>
]]></content>
      <categories>
        <category>Dive Into Deep Learning</category>
      </categories>
      <tags>
        <tag>Attention Mechanism</tag>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Attention Mechanisms: More Targeted Information Extraction</title>
    <url>/2023/06/06/Attention/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="Attention-in-Biology"><a href="#Attention-in-Biology" class="headerlink" title="Attention in Biology"></a>Attention in Biology</h1><p>In biology, there are two kinds of cues that affect our attention: volitional cue (自主性提示) and nonvolitional cue (非自主性提示).</p>
<p><img align="left" src="/2023/06/06/Attention/1.png" style=" width:380px; padding: 0px 0px; "> <img align="mid" src="/2023/06/06/Attention/2.png" style=" width:380px; padding: 0px 0px; "></p>
<center style="font-size:12px; font-weight:bold">Fig. 1. Nonvolitional cue (left) and Volitional cue (right)</center><br>

<p>The nonvolitional cue is based on the saliency of objects in the environment. It&#39;s unconscous. As the above picture shows, when there is a red cup among several white objects, we will unconsciously choose the red cup as its color is more prominent. The volitional cue is controlled by our awareness and our decisions are led by our consciousness. For example, after drinking a cup of coffee, we may want to relax, because of which we choose a book to read.</p>
<h1 id="Attention-Mechanisms"><a href="#Attention-Mechanisms" class="headerlink" title="Attention Mechanisms"></a>Attention Mechanisms</h1><p>The nonvolitional cue and volitional cue inspire us to take the environment as input and use volitional cue to focus the attention to a certain object or feature.</p>
<h2 id="Queries-Keys-and-Values"><a href="#Queries-Keys-and-Values" class="headerlink" title="Queries, Keys and Values"></a>Queries, Keys and Values</h2><p>In attention mechanisms, the volitional cue is also called <strong>query</strong>. It reflects the tendency of feature extraction. The nonvolitional cue is actually our perception of the environment. The characteristics of the environment can be represented by several <strong>key-value pairs</strong>. These three things consist of the input of the attention mechanism. Giving queries and keys, the attention mechanism will aggregate values and choose the most prominent and suitable value as output. Such a process is also called <em>attention pooling</em>. It is queries that distinguish attention pooling from dense layers.</p>
<p><img src="/2023/06/06/Attention/3.png" alt="3"></p>
<center style="font-size:12px; font-weight:bold">Fig. 2. Attention pooling</center>

<h2 id="Mathematical-Representation-of-Attention"><a href="#Mathematical-Representation-of-Attention" class="headerlink" title="Mathematical Representation of Attention"></a>Mathematical Representation of Attention</h2><p>Our attention to a certain object reflects how much we care about it, or, mathematically, the weight we give it. When a certain query is offered to us, it is natural for us to think of finding something similar to this query to get the answer. That is, for a key-value pair, the more similar the key is to the query, the higher proportion its value should take up in the answer. This is how <em><strong>Nadaraya-Watson kernel regression</strong></em> works:</p>
<p>$$<br>f(\mathbf{q})&#x3D;\sum\limits _{i&#x3D;1} ^n \frac{K(\mathbf{q}-\mathbf{k}_i)}{\sum\limits _{j&#x3D;1} ^n K(\mathbf{q}-\mathbf{k}_j)}\mathbf{v}_i<br>$$</p>
<p>where $K$ is a kernel. Such an estimator assigns different weight to different keys for a certain query. More generally, attention is defined as:</p>
<p>$$<br>\text{Attention}(\mathbf{q},\mathbf{k},\mathbf{v})&#x3D;\sum\limits _{i&#x3D;1} ^n\alpha(\mathbf{q},\mathbf{k}_i)\mathbf{v}_i<br>$$</p>
<p>where $\alpha(\mathbf{q},\mathbf{k}_i)$ is <em>attention weight</em>. It is non-negative and its sum is $1$. The output is a weighted sum of the values for each key-value pair.</p>
<p><img src="/2023/06/06/Attention/4.png" alt="4"></p>
<center style="font-size:12px; font-weight:bold">Fig. 3. Mathematical representation of attention pooling</center> <br>

<p>It is noteworthy that Nadaraya-Watson kernel regression is a nonparametric model (<em>nonparametric attention pooling</em>). Therefore, it is consistent. With enough data, it will converge to the optimum result. However, in machine learning, the agent needs parameters to learn so that it could improve its performance. For instance, if we set $K$ as a gaussian kernel and add learned parameter $\mathbf{w}$, we get a formula that looks like softmax:</p>
<p>$$<br>\begin{align*}<br>K(\mathbf{u})&amp;&#x3D;\frac{1}{\sqrt{2\pi}}\exp(-\frac{\mathbf{u}^2}{2})\\<br>f(\mathbf{q})<br>    &amp;&#x3D;\sum\limits _{i&#x3D;1} ^n\alpha(\mathbf{q},\mathbf{k}_i)\mathbf{v}_i\\<br>    &amp;&#x3D;\sum\limits _{i&#x3D;1} ^n\frac{\exp[-\frac{1}{2}(\mathbf{q}-\mathbf{k}_i)^2\mathbf{w}^2]}{\sum\limits _{j&#x3D;1} ^n\exp[-\frac{1}{2}(\mathbf{q}-\mathbf{k}_i)^2\mathbf{w}^2]}\mathbf{v}_i\\<br>    &amp;&#x3D;\sum _{i&#x3D;1} ^n\text{softmax}[-\frac{1}{2}(\mathbf{q}-\mathbf{k}_i)^2\mathbf{w}^2]\mathbf{v}_i<br>\end{align*}<br>$$</p>
<h2 id="Attention-Scoring-Function"><a href="#Attention-Scoring-Function" class="headerlink" title="Attention Scoring Function"></a>Attention Scoring Function</h2><p>The exponent of the gaussian kernel above ($-{\mathbf{u}^2}&#x2F;2$) is actually an attention scoring function ($a$) which evaluates the similarity between $\mathbf{q}$ and $\mathbf{k}_i$. Hence, the attention pooling can be also represented as:</p>
<p>$$<br>\begin{align*}<br>    \text{Attention}(\mathbf{q},\mathbf{k},\mathbf{v})&amp;&#x3D;\sum\limits _{i&#x3D;1} ^n\alpha(\mathbf{q},\mathbf{k}_i)\mathbf{v}_i\\<br>    &amp;&#x3D;\sum\limits _{i&#x3D;1} ^n\text{softmax}[a(\mathbf{q},\mathbf{k}_i)]\mathbf{v}_i<br>\end{align*}<br>$$</p>
<p><img src="/2023/06/06/Attention/5.png" alt="5"></p>
<center style="font-size:12px; font-weight:bold">Fig. 4. Computing the output of attention pooling</center> <br>

<p>where $\text{softmax}$ generates attention weights. The more similar the key $\mathbf{k}_i$ is to the query $\mathbf{q}$, the higher weight its value $\mathbf{v}_i$ gets. The output is the weighted sum of values.</p>
<p>More generally, query $\mathbf{q}$ and key $\mathbf{k}_i$ are both <strong>vectors</strong> of different length. The attention scoring function maps these two vectors to a scalar</p>
<h3 id="Additive-Attention"><a href="#Additive-Attention" class="headerlink" title="Additive Attention"></a>Additive Attention</h3><p>Additive attention uses MLP to evaluate the similarity between $\mathbf{q}$ and $\mathbf{k}$, where $\mathbf{q}\in\mathbf{R}^q$ and $\mathbf{k}\in\mathbf{R}^k$:</p>
<p>$$<br>a(\mathbf{q,k})&#x3D;[\tanh(\mathbf{q}\mathbf{W}_q+\mathbf{k}\mathbf{W}_k)]\mathbf{w}_v\in\mathbf{R}<br>$$</p>
<p>where $\mathbf{W}_q\in\mathbf{R} ^{q\times h}$, $\mathbf{W}_k\in\mathbf{R} ^{k\times h}$ and $\mathbf{w}_v\in\mathbf{R} ^{h}$. All of them are learned parameters (here we ignore the bias) in one dense layer.</p>
<blockquote>
<p>In practice, each batch may have several queries and key-value pairs. And there are always more than one batches. In this case, the shape of $\mathbf{q}$ is <code>(batch_size, num_queries, q)</code> and the shape of $\mathbf{k}$ is <code>(batch_size, num_kv_pairs, k)</code>. The shape of $\mathbf{q}\mathbf{W}_q$ is <code>(batch_size, num_queries, h)</code> and the shape of $\mathbf{k}\mathbf{W}_k$ is <code>(batch_size, num_kv_pairs, h)</code>. <code>num_queries</code> and <code>num_kv_pairs</code> are always different, which means that we can&#39;t add $\mathbf{q}\mathbf{W}_q$ and $\mathbf{k}\mathbf{W}_k$ directly. </p>
<p>To solve this, we always use <code>queries.unsqueeze(2)</code> and <code>keys.unsqueeze(1)</code> (<code>queries</code> and <code>keys</code> are both tensor) to expand their dimensions. After <code>unsqueeze</code>, the shape of <code>queries</code> is <code>(batch_size, num_queries, 1, h)</code> and the shape of <code>keys</code> is <code>(batch_size, 1, num_kv_pairs, h)</code>. Now broadcasting can make $\mathbf{q}\mathbf{W}_q+\mathbf{k}\mathbf{W}_k$ legal. The nature of this operation is that each query should take all the keys into account so the $2$-th dimension of <code>queries</code> is expanded to <code>num_kv_pairs</code>, so is <code>keys</code>. </p>
</blockquote>
<h3 id="Scaled-Dot-Product-Attention"><a href="#Scaled-Dot-Product-Attention" class="headerlink" title="Scaled Dot-Product Attention"></a>Scaled Dot-Product Attention</h3><p>If the shape of $\mathbf{q}$ and $\mathbf{k}$ are the same (both are $d$), we could just use dot-product to evaluate their similarity:</p>
<p>$$<br>a(\mathbf{q,k})&#x3D;\mathbf{q}^\text{T}\mathbf{k}&#x2F;\sqrt{d}<br>$$</p>
<p>where $\sqrt{d}$ is to make sure that the value of $a(\mathbf{q,k})$ will not change significantly when the shape changes.</p>
<p>If considering SGD, the output of attention pooling is:</p>
<p>$$<br>\text{softmax}(\mathbf{Q}\mathbf{K}^\text{T}&#x2F;\sqrt{d})\mathbf{V}<br>$$</p>
<p>where $d$ is still the shape of each query or each key.</p>
<h1 id="Different-Types-of-Attention"><a href="#Different-Types-of-Attention" class="headerlink" title="Different Types of Attention"></a>Different Types of Attention</h1><p>Depending on the number of attention pooling and the relationship among queries, keys and values, there are various forms of attention mechanisms.</p>
<h2 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h2><p>Just as different convolutional output channels can extract different types of features, different attention pooling will also learn various behaviors. Hence, we can use multi-head attention to generate multiple attention pooling outputs $\mathbf{h}_1...\mathbf{h}_i$ in parallel and concatenate them together to form $(\mathbf{h}_1,...,\mathbf{h}_i)$.</p>
<p><img src="/2023/06/06/Attention/6.png" alt="6"></p>
<center style="font-size:12px; font-weight:bold">Fig. 5. Multi-head attention</center>

<h2 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h2><p>Self-attention (intra-attention) is a special kind of attention, whose queries, keys and values are all the same things.</p>
<h1 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h1><p>When dealing with sequence information, RNNs operate each token in sequence. Therefore, the location information of the sequence is used. However, the attention pooling operates all the inputs in parallel, because of which, the location information is wasted.</p>
<p>Positional encoding is the location information added to the keys, values and queries. In computer science, we use binary string to represent position (e.g. 0: 000, 1: 001, ..., 7: 111). As can be seen, the bit on each position alternates at diferent frequencies respectively. Higher bits alternate less frequently than lower bits. Such alternating changes can represent discrete positional information.</p>
<h2 id="Absolute-Positional-Information"><a href="#Absolute-Positional-Information" class="headerlink" title="Absolute Positional Information"></a>Absolute Positional Information</h2><p>For the input $\mathbf{X}\in\mathbf{R} ^{n\times d}$ which contains $n$ tokens and $d$ features for each token, we can use positional embedding matrix $\mathbf{P}\in\mathbf{R} ^{n\times d}$ to offer it positional information:</p>
<p>$$<br>\begin{align*}<br>    \mathbf{X&#39;}&amp;&#x3D;\mathbf{X}+\mathbf{P}\\<br>    \mathbf{P} _{i,2j}&amp;&#x3D;\sin(\frac{i}{10000 ^{2j&#x2F;d}})\\<br>    \mathbf{P} _{i,2j+1}&amp;&#x3D;\cos(\frac{i}{10000 ^{2j&#x2F;d}})<br>\end{align*}<br>$$</p>
<p>It works because different columns, which are similar to the bit position in binary string, oscillate at different frequencies. It is just like the binary string but the value is continuous.</p>
<p><img src="/2023/06/06/Attention/7.png" alt="7"></p>
<center style="font-size:12px; font-weight:bold">Fig. 6. Oscillating frequency of different columns</center>

<h2 id="Relative-Positional-Information"><a href="#Relative-Positional-Information" class="headerlink" title="Relative Positional Information"></a>Relative Positional Information</h2><p>The positional information we add to $\mathbf{X}$ is called absolute positional information. What&#39;s more, we can also get the relative positional information for different rows in the same column:</p>
<p>$$<br>\begin{align*}<br>    (\mathbf{P} _{i,2j}, \mathbf{P} _{i,2j+1})&amp;&#x3D;(\sin\frac{i}{10000 ^{2j&#x2F;d}},\cos\frac{i}{10000 ^{2j&#x2F;d}})\\<br>    (\mathbf{P} _{i+\delta,2j}, \mathbf{P} _{i+\delta,2j+1})&amp;&#x3D;(\sin\frac{i+\delta}{10000 ^{2j&#x2F;d}},\cos\frac{i+\delta}{10000 ^{2j&#x2F;d}})\\<br>    &amp;&#x3D;({\begin{bmatrix}<br>           \cos\delta\omega_j\space\space \sin\delta\omega_j\\<br>           -\sin\delta\omega_j\space\space \cos\delta\omega_j<br>       \end{bmatrix}}<br>      {\begin{bmatrix}<br>           \mathbf{P} _{i,2j}\\<br>           \mathbf{P} _{i,2j+1}<br>       \end{bmatrix}})^\text{T}\\<br>    &amp;&#x3D;[\sin(i+\delta)\omega_j,\cos(i+\delta)\omega_j]\\<br>\end{align*}<br>$$</p>
<p>where $\omega_j&#x3D;1&#x2F;10000^{2j&#x2F;d}$.</p>
<blockquote>
<p>Positional information is information added to the input $\mathbf{X}$, including keys, values and queries, it could be fixed positional encoding like what we do above or learned one.</p>
</blockquote>
]]></content>
      <categories>
        <category>Dive Into Deep Learning</category>
      </categories>
      <tags>
        <tag>Attention Mechanism</tag>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Optimization Algorithms</title>
    <url>/2023/06/02/OptimizationAlgorithms/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="Momentum-method"><a href="#Momentum-method" class="headerlink" title="Momentum method"></a>Momentum method</h1><p>SGD introduces randomness so that gradient descent may jump out of local minimum points and saddle points. What&#39;s more, the computational complexity of SGD is lower than the regular gradient descent.</p>
<p>However, because of the randomness, SGD may oscillate around the optimum point. Besides, different model parameters may require different learning rate. When all model parameters share the same learning rate, it is likely that some model parameters with large gradient diverge and others with small gradient converge slowly. A smaller learning rate may solve this problem but it will slow the progress of the training of the whole model.</p>
<p>The momentum method helps us solve the problem above. It introduces a new parameter $\mathbf{v}_t$ (<em>speed</em>) to provide inertia for the gradient:</p>
<p>$$<br>\begin{cases}<br>    \mathbf{v}_t&#x3D;\beta\mathbf{v} _{t-1}+\mathbf{g} _t\\<br>    \mathbf{w}_t &#x3D;\mathbf{w} _{t-1}-\alpha\mathbf{v} _t<br>\end{cases}<br>$$</p>
<p>where $\beta$ is a hyperparameter that determines the magnitude of inertia. When $\beta$ is $0$, it reverts to the regular gradient descent. Compared to the regular gradient descent, the momentum method gives a certain weight to the gradient of the old moment $t_1$ at the new moment $t_2$. Under extreme conditions:</p>
<p>$$<br>\begin{cases}<br>    \tau&#x3D;t_2-t_1&#x3D;\infty\\<br>    \sum\limits _{\tau&#x3D;0} ^\infty\beta ^\tau\mathbf{g} _{t_1}&#x3D;\frac{1}{1-\beta}\mathbf{g} _{t_1}<br>\end{cases}<br>$$</p>
<p>that is, the weight of gradient is $1&#x2F;(1-\beta)$. By doing so, the size of the gradient is guaranteed and gradents are more likely to descend in a better direction.</p>
<blockquote>
<p>When using the momentum method in SGD, we should maintain a auxiliary variable $\mathbf{v}$. It is a vector and is initialized to $0$. It should record the history of all batches and iteration rounds. Namely, it is initialized only once.</p>
<p>In PyTorch, it has been included in <code>torch.optim.SGD</code>, we only need to pass the hyperparameter $\beta$ (<code>momentum</code>) to PyTorch.</p>
</blockquote>
<h1 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h1><p>Adam combines many techniques (momentum, adagrad, etc.) into one efficient learning algorithm. The mechanism of adam is quite simple, it uses the second moment to adjust the size of the gradient based on the momentum method so as to achieve the purpose of dynamically adjusting the learning rate:</p>
<p>$$<br>\begin{cases}<br>    \mathbf{v}_t&#x3D;\beta _1 \mathbf{v} _{t-1} + (1-\beta _1) \mathbf{g}_t \\<br>    \mathbf{s}_t&#x3D;\beta _2 \mathbf{s} _{t-1}+ (1-\beta _2) \mathbf{g}_t ^2<br>\end{cases}<br>$$</p>
<p>Similarly, under extreme conditions:</p>
<p>$$<br>\begin{cases}<br>    \sum\limits _{\tau&#x3D;0} ^\infty\beta _1 ^\tau(1-\beta _1)\mathbf{g} _{t_1}&#x3D;\mathbf{g} _{t_1}\\<br>    \sum\limits _{\tau&#x3D;0} ^\infty\beta _2 ^\tau(1-\beta _2) \mathbf{g} _{t_1} ^2&#x3D;\mathbf{g} _{t_1} ^2<br>\end{cases}<br>$$</p>
<p>However, when $\tau$ is not large enough, we can&#39;t get the formula above. Hence, we usually standardize $\mathbf{v}$ and $\mathbf{s}$:</p>
<p>$$<br>\begin{cases}<br>    \hat{\mathbf{v}}_t&#x3D;\frac{\mathbf{v}_t}{1-\beta_1^t}\\<br>    \hat{\mathbf{s}}_t&#x3D;\frac{\mathbf{s}_t}{1-\beta_2^t}<br>\end{cases}<br>$$</p>
<p>Now, the gradient is:</p>
<p>$$<br>\mathbf{g}_t&#39;&#x3D;\alpha\frac{\hat{\mathbf{v}}_t}{\sqrt{\hat{\mathbf{s}}_t}+\epsilon}<br>$$</p>
<p>where $\epsilon$ is to make sure that division by $0$ errors will not occur. Its value is usually $10^{-6}$.</p>
<p>And the gradient descent is:</p>
<p>$$<br>\mathbf{w}_t&#x3D;\mathbf{w} _{t-1}-\mathbf{g}_t&#39;\space\text{or}\space\mathbf{w}_t&#x3D;\mathbf{w} _{t-1}-\frac{\alpha}{\sqrt{\hat{\mathbf{s}}_t}+\epsilon}\hat{\mathbf{v}}_t<br>$$</p>
<p>where the latter reflects the fact that the adam algorithm dynamically adjusts the learning rate $\alpha$ and is not sensitive to the learning rate, just like what we talk about in <a href="/2023/04/10/NeuralNetwork/#Adam-algorithm">Adam</a>.</p>
<blockquote>
<p>In PyTorch, we can use <code>torch.optim.Adam</code> to call adam algorithm. The only hyperparameter we need to pass to it is $\alpha$.</p>
</blockquote>
]]></content>
      <categories>
        <category>Dive Into Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>The Encoder-Decoder Architecture</title>
    <url>/2023/05/31/EncoderDecoder/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="Another-view-to-ANNs"><a href="#Another-view-to-ANNs" class="headerlink" title="Another view to ANNs"></a>Another view to ANNs</h1><p>Firstly, let&#39;s explain CNNs from the perspective of encoder-decoder:</p>
<p><img src="/2023/05/31/EncoderDecoder/1.png" alt="1"></p>
<center style="font-size:12px; font-weight:bold">Fig. 1. View CNNs in a new perspective</center><br>

<p>When the agent extracts features from the input using convolutional layers and pooling layer, it encodes the image as a vector that better reflects some certain characteristics of the image. After that, the agent decodes these features and generates output.</p>
<p>Similarly, for CNNs, the embedding layer and LSTM (GRU) layer encode texts as vectors. Then, The agent decodes these message and generates output.</p>
<p><img src="/2023/05/31/EncoderDecoder/2.png" alt="2"></p>
<center style="font-size:12px; font-weight:bold">Fig. 2. View RNNs in a new perspective</center><br>

<p>This is the encoder-decoder architecture. In this architecture, the encoder deals with input , passes information to the decoder and the decoder generates output. More generally, the information the encoder passes to the decoder is called <em>state</em> which contains features of the input. The decoder can also receive new input so that it will get more information about the thing it copes with.</p>
<p><img src="/2023/05/31/EncoderDecoder/3.png" alt="3"></p>
<center style="font-size:12px; font-weight:bold">Fig. 3. Encoder-Decoder</center><br>

<h1 id="Seq2seq"><a href="#Seq2seq" class="headerlink" title="Seq2seq"></a>Seq2seq</h1><p><a href="https://arxiv.org/pdf/1409.3215.pdf">Sequence to sequence</a> (<a href="(https://arxiv.org/pdf/1406.1078.pdf)">Seq2seq</a>) is a kind of neural network using the encoder-decoder architecture to complete the task of <em>sequence to sequence learning</em>. It is applied mainly in machine translation.</p>
<p><img src="/2023/05/31/EncoderDecoder/4.png" alt="4"></p>
<center style="font-size:12px; font-weight:bold">Fig. 4. Machine translation and Seq2seq (English to French)</center><br>

<p><img src="/2023/05/31/EncoderDecoder/5.png" alt="5"></p>
<center style="font-size:12px; font-weight:bold">Fig. 5. Structure of Seq2seq</center><br>

<h2 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h2><p>The encoder is a DRNN without regression layers. It works like the other DRNNs: take the source sequence (English) as input and output vectors. What is worth noticing is the <em>embedding layer</em>. In fact, it is an improvement on one-hot encoding. When using one-hot encoding, we have to use a $t\times m$ tensor for a sentence with $t$ tokens and a vocabulary with $m$ tokens. This is space-consuming. However, the embedding layer can map $m$-dimensional vectors to $k$-dimensional vectors ($k&lt;m$). This cuts the size of space we have to spend to store a sentence.</p>
<h2 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h2><p>The decoder takes the target sequence (French) $\mathbf{X}$ as a part of input and also uses the embedding layer. In addition, it also takes the last hidden state of the last RNN layer of the encoder $\mathbf{C}$ as a part of input. $\mathbf{C}$ contains the context about the source sequence. $[\mathbf{X}, \mathbf{C}]$ form the complete input and are fed to the decoder. The last hidden states of all the RNN layers of the encoder are also used to initialize the hidden states of all the RNN layers of the decoder. Hence, the depth of RNN layer in encoder and decoder should be the same (that is $n$).</p>
<blockquote>
<p>For convenience, the length of all the sequences ($t$) is fixed. Hence, for the sentences that are longer than $t$, they are cut to $t$; for the sentences that are shorter than $t$, they are filled up to $t$. When computing the loss using softmax, we should only consider the valid length and set the filled part to $0$.</p>
</blockquote>
<h2 id="Training-and-predicting"><a href="#Training-and-predicting" class="headerlink" title="Training and predicting"></a>Training and predicting</h2><p>When training, what we feed to the decoder are the correct translation sequence, like Fig. 4. However, when predicting, we only feed the token that indicates the start of a sentence (<code>&lt;bos&gt;</code>) to the decoder and the output is fed as input to the decoder until the decoder generates the token that indicates the end of a sentence (<code>&lt;eos&gt;</code>).</p>
<p><img src="/2023/05/31/EncoderDecoder/6.png" alt="6"></p>
<center style="font-size:12px; font-weight:bold">Fig. 6. Making prediction (translation)</center>

<h2 id="Assessment"><a href="#Assessment" class="headerlink" title="Assessment"></a>Assessment</h2><p><a href="https://aclanthology.org/P02-1040.pdf">Bilingual evaluation understudy</a> (BLEU) is a method to evaluate the accuracy of machine translation or other sequence to sequence application. BLEU is always range between $0$ and $1$. The larger BLEU is, the more accurate the prediction is. It is defined as:</p>
<p>$$<br>\exp(\min(0,1-\frac{\text{len} _{label}}{\text{len} _{pred}}))\prod\limits _{n&#x3D;1}^k p _n ^{1&#x2F;2^n}<br>$$</p>
<p>where $\text{len} _{label}$ is the number of tokens in target sequence $y$ while $\text{len} _{pred}$ is the number of tokens in prediction sequence $\hat{y}$. Since shorter predictions contain less tokens, which makes $\prod\limits _{n&#x3D;1}^k p _n ^{1&#x2F;2^n}$ bigger, there will be penalty for short predictions.</p>
<blockquote>
<p>The smaller $\text{len} _{pred}$ is, the larger $\text{len} _{label}&#x2F;{\text{len} _{pred}}$ is, and thus the smaller $\exp(\min(...))$ is.</p>
</blockquote>
<p>$p_n$ is the accuracy of $n$-grams, namely, the accuracy of prediction of $n$ tokens in sequence. Due to the difficulty to predict long sequence, long sequence will get higher weight than short sequence. $1&#x2F;2^n$ help us do this. Because $p_n$ is often smaller than $1$ and the bigger $n$ is, the smaller $1&#x2F;2^n$ is, long sequence will always get larger value even though $p_n$ keeps the same.</p>
<p><img src="/2023/05/31/EncoderDecoder/7.png" alt="7"></p>
<center style="font-size:12px; font-weight:bold">Fig. 7. Curve of p<sub>n</sub><sup>1/2<sup>n</sup></sup>(p<sub>n</sub>=0.5)</center>

<h1 id="Beam-search"><a href="#Beam-search" class="headerlink" title="Beam search"></a>Beam search</h1><p>When predicting a token using softmax, we are actually using <em>Greedy search</em>. That is, for each timestep, the token we choose is the one with the highest probability.</p>
<p><img src="/2023/05/31/EncoderDecoder/8.png" alt="8"></p>
<center style="font-size:12px; font-weight:bold">Fig. 8. Greedy search</center> <br>

<p>However, local optimum does not necessarily lead to global optimum. If we choose B in timestep1 and feed it to decoder in timestep2, maybe we will get higher accuracy for the prediction in timestep2. <em>Exhaustive search</em> can solver this problem but it is time-consuming.</p>
<p>Beam search is a compromise between greedy search and Exhaustive search. In the first timestep, beam search will choose <em>k</em> tokens with high probability and finally generate <em>k</em> condidate sequence. Then, sequence with the highest probability is chosen. <em>k</em> is a hyperparameter called <em>beam size</em>.</p>
<p><img src="/2023/05/31/EncoderDecoder/9.png" alt="9"></p>
<center style="font-size:12px; font-weight:bold">Fig. 9. Beam search</center>
]]></content>
      <categories>
        <category>Dive Into Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>CNN</tag>
        <tag>RNN</tag>
      </tags>
  </entry>
  <entry>
    <title>Common RNN Models</title>
    <url>/2023/05/29/CommonRNNModels/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>RNN is a special kind of MLP that takes its output as input to remember the previous information. It is such an simple struture that it can&#39;t remember as much information as we want. Besides, since RNN updates hidden state recurrently, the cumulative chain of gradients will become rather long, which may result in gradient explosion or gradient vanishing.</p>
<h1 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h1><p><a href="https://arxiv.org/pdf/1406.1078.pdf">Gated Recurrent Unit</a> (GRU) is a method to solve gradient anomaly in RNN. Its idea is based on three facts:</p>
<ul>
<li>Some tokens have nothing to do with previous tokens.</li>
<li>Some tokens are closely related to previous tokens.</li>
<li>Current token may make no sense. It is better to just ignore its influence.</li>
</ul>
<p>Hence, GRU defines two kinds of gate to catch long-term and short-term relationship:</p>
<ul>
<li>Reset Gate $\mathbf{R}_t$, catches short-term relationship between current token and previous tokens:<br>$$\mathbf{R}_t&#x3D;\sigma(\mathbf{X}_t\mathbf{W} _{xr}+\mathbf{H} _{t-1}\mathbf{W} _{hr}+\mathbf{b}_r)$$</li>
<li>Update Gate $\mathbf{Z}_t$, catches long-term relationship between current token and previous tokens:<br>$$\mathbf{Z}_t&#x3D;\sigma(\mathbf{X}_t\mathbf{W} _{xz}+\mathbf{H} _{t-1}\mathbf{W} _{hz}+\mathbf{b}_z)$$</li>
</ul>
<p>Both values of $\mathbf{R}_t$ and $\mathbf{Z}_t$ ($\sigma$ means sigmoid) are between $0$ and $1$. $\mathbf{R}_t$ is used to generate <em>candidate hidden state</em>:</p>
<p>$$\tilde{\mathbf{H}}_t&#x3D;\tanh(\mathbf{X}_t\mathbf{W} _{xh}+(\mathbf{R}_t\odot\mathbf{H} _{t-1})\mathbf{W} _{hh}+\mathbf{b}_h)$$</p>
<p>As its name suggests, reset gate $\mathbf{R}_t$ will weigh the influence of previous tokens $\mathbf{H} _{t-1}$ on current sequence and gather the information of current token and previous tokens. Since $\mathbf{R}_t$ is always smaller than $1$, it pays more attention to current token $\mathbf{X}_t$ while weakens $\mathbf{H} _{t-1}$. That&#39;s why we say that $\mathbf{R}_t$ catches short-term relationship.</p>
<p>After generating the <em>candidate hidden state</em>, update gate $\mathbf{Z}_t$ generates real $\mathbf{H}_t$ from previous sequence $\mathbf{H} _{t-1}$ and current candidate sequence $\tilde{\mathbf{H}}_t$:</p>
<p>$$\mathbf{H}_t&#x3D;\mathbf{Z}_t\odot\mathbf{H} _{t-1}+(1-\mathbf{Z}_t)\odot\tilde{\mathbf{H}}_t$$</p>
<p>When $\mathbf{Z}_t$ is close to $0$, it means that the current token make sense and it may have something to do with previous tokens (based on $\mathbf{R}_t$). On the contrary, when $\mathbf{Z}_t$ is close to $1$, it means that the current token makes no sense. $\mathbf{Z}_t$ weighs $\mathbf{H} _{t-1}$ and $\tilde{\mathbf{H}}_t$. That&#39;s why we say that $\mathbf{Z}_t$ catches long-term relationship. </p>
<p><img src="/2023/05/29/CommonRNNModels/1.png" alt="1"></p>
<center style="font-size:12px; font-weight:bold">Fig. 1. Computing process in GRU</center><br>

<p>Compared with RNN, GRU requires more model parameters ($\mathbf{W} _{xr}$, $\mathbf{W} _{hr}$, $\mathbf{W} _{xz}$, $\mathbf{W} _{hz}$, $\mathbf{W} _{xh}$, $\mathbf{W} _{hh}$ and corresponding bias $\mathbf{b}$). All of them are learnable parameters in one GRU layer. The two gates help the agent properly forget current or previous information. This will shorten the gradient cumulative chain to a certain degree.</p>
<blockquote>
<p><code>nn.GRU</code> is the class of GRU in PyTorch. It is almost the same as <code>nn.RNN</code> though it has more model parameters.</p>
</blockquote>
<h1 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h1><p><a href="https://papers.baulab.info/Hochreiter-1997.pdf">Long-Short-Term Memory</a> (LSTM) is almost the same as GRU but it is more complicated.</p>
<p><img src="/2023/05/29/CommonRNNModels/2.png" alt="2"></p>
<center style="font-size:12px; font-weight:bold">Fig. 2. Computing process in LSTM</center><br>

<p>Compared with GRU, LSTM use memory cell $\mathbf{C}_t$ to record information. The function of $\mathbf{C}_t$ in LSTM is similar as $\mathbf{H}_t$&#39;s in GRU. But $\mathbf{C}_t$ only propagates inside LSTM and the output of LSTM is still $\mathbf{H}_t$. In addition, LSTM takse $\mathbf{Z}_t$ and $(1-\mathbf{Z}_t)$ apart to form forget gate $\mathbf{F}_t$ and input gate $\mathbf{I}_t$:</p>
<p>$$<br>\begin{cases}<br>    \mathbf{F}_t&#x3D;\sigma(\mathbf{X}_t\mathbf{W} _{xf}+\mathbf{H} _{t-1}\mathbf{W} _{hf}+\mathbf{b}_f) \\<br>    \mathbf{I}_t&#x3D;\sigma(\mathbf{X}_t\mathbf{W} _{xi}+\mathbf{H} _{t-1}\mathbf{W} _{hi}+\mathbf{b}_i)<br>\end{cases}<br>$$</p>
<p>Similarly, LSTM use $\tilde{\mathbf{C}}_t$ to record the relationship between current token and previous tokens but it doesn&#39;t reset previous tokens:</p>
<p>$$\tilde{\mathbf{C}}_t&#x3D;\tanh(\mathbf{X}_t\mathbf{W} _{xc}+\mathbf{H} _{t-1}\mathbf{W} _{hc}+\mathbf{b}_c)<br>$$</p>
<p>Then, LSTM generate memory cell $\mathbf{C}_t$ by weighing previous information $\mathbf{C} _{t-1}$ and current information $\tilde{\mathbf{C}}_t$:</p>
<p>$$\mathbf{C}_t&#x3D;\mathbf{F}_t\odot\mathbf{C} _{t-1}+\mathbf{I}_t\odot\tilde{\mathbf{C}}_t$$</p>
<p>Since $\mathbf{F}_t$ and $\mathbf{I}_t$ are no longer relevant. LSTM can combine previous information and current information more flexibly. $\mathbf{C}_t$ only flows inside LSTM. To generate the output, LSTM use output gate $\mathbf{O}_t$ for reset gate $\mathbf{R}_t$:</p>
<p>$$<br>\begin{cases}<br>    \mathbf{O}_t&#x3D;\sigma(\mathbf{X}_t\mathbf{W} _{xo}+\mathbf{H} _{t-1}\mathbf{W} _{ho}+\mathbf{b}_o)\\<br>    \mathbf{H}_t&#x3D;\mathbf{O}_t\odot\tanh(\mathbf{C}_t)<br>\end{cases}<br>$$</p>
<p>where $\tanh$ is to make sure that $\mathbf{H}_t$ is range from $-1$ to $1$ so that the agent can control gradients better.</p>
<blockquote>
<p><code>nn.LSTM</code> is the class of LSTM in PyTorch. It is almost the same as <code>nn.GRU</code>.</p>
</blockquote>
<p>Though LSTM is more complex, GRU performs as well as LSTM.</p>
<h1 id="DRNN"><a href="#DRNN" class="headerlink" title="DRNN"></a>DRNN</h1><p>Deep Recurrent Neural Networks (DRNNs) are neural networks with multiple hidden layers. Their structures are the same as MLP&#39;s.</p>
<h1 id="BRNN"><a href="#BRNN" class="headerlink" title="BRNN"></a>BRNN</h1><p><a href="https://arxiv.org/pdf/2210.08402.pdf">Bidirectional Recurrent Neural Networks</a> (BRNNs) are neural networks that consider not only the leftward context but also the rightward context. In some scenarios, a token is not only associated with its previous tokens but also its future tokens, for example, contextual analysis and cloze. In this case, it is better for the agent to consider the future tokens in addition to the previous tokens. And that&#39;s what BRNNs does:</p>
<p><img src="/2023/05/29/CommonRNNModels/3.png" alt="3"></p>
<center style="font-size:12px; font-weight:bold">Fig. 3. BRNNs</center><br>

<p>BRNNs assign two independent RNN layers with the same structure to each hidden layer, where one deals with the leftward context and another one deals with the rightward context (just takse the antisequence as input):</p>
<p>$$<br>\begin{cases}<br>    \overrightarrow{\mathbf{H}} _t&#x3D;\text{activation}(\mathbf{X}_t\mathbf{W} _{xh} ^{(f)}+\overrightarrow{\mathbf{H}} _{t-1}\mathbf{W} _{hh} ^{(f)}+\mathbf{b} _h ^{(f)})\\<br>    \overleftarrow{\mathbf{H}} _t&#x3D;\text{activation}(\mathbf{X}_t^r\mathbf{W} _{xh} ^{(b)}+\overleftarrow{\mathbf{H}} _{t-1}\mathbf{W} _{hh} ^{(b)}+\mathbf{b} _h ^{(b)})<br>\end{cases}<br>$$</p>
<p>Then, the concatenation of $\overrightarrow{\mathbf{H}} _t$ and $\overleftarrow{\mathbf{H}} _t$ is passed as output $\mathbf{H} _t&#x3D;(\overrightarrow{\mathbf{H}} _t,\overleftarrow{\mathbf{H}} _t)$ to the next layer.</p>
<p>BRNNs only work when the sequence contains information about future. Hence, BRNNs can&#39;t predict the future. Instead, BRNNs are usually used to <strong>extract features</strong> from a sequence, just like what convolutional layers do.</p>
<blockquote>
<p>There is keyword <code>bidirectional</code> in <code>nn.RNN</code>, <code>nn.GRU</code> and <code>nn.LSTM</code>. To enable BRNN, we can just set <code>bidirectional=True</code>.</p>
</blockquote>
]]></content>
      <categories>
        <category>Dive Into Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>RNN</tag>
      </tags>
  </entry>
  <entry>
    <title>RNN: a Special Kind of MLP</title>
    <url>/2023/05/25/RNN/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="Sequence-model-amp-Language-model"><a href="#Sequence-model-amp-Language-model" class="headerlink" title="Sequence model &amp; Language model"></a>Sequence model &amp; Language model</h1><p>Sequence information is the data arranged in a certain order, whose biggest feature is contexutal correlation. Language, or text, is typical sequence information. When dealing with sequence information, what the neural network does is to predict the future based on the history, that is:</p>
<p>$$x_t\sim P(x_t|x_1,...,x _{t-1})$$</p>
<p>where $x_t$ is correlated with its history $(x_1,...,x _{t-1})$. Similarly, if we wanna predict a new sequence, we deal with:</p>
<p>$$(x_1,...,x_T)\sim \prod\limits_{t&#x3D;1}^TP(x_t|x_1,...,x _{t-1})$$</p>
<p>Such a model or such a relationship is called sequence model. In sequence model, the data is not independent but sequential. We use previous data to predict future data. However, when the sequence is extremely long, the quantity of previous data will be rather large. In general, there are two methods to cope with it.</p>
<h2 id="Markov-assumption"><a href="#Markov-assumption" class="headerlink" title="Markov assumption"></a>Markov assumption</h2><p>Markov assumption (or Markov model) assumes that the future $x_t$ is only correlated with a small span of the past $(x _{t-1},...,x _{t-\tau})$ where $\tau$ is the span. In this case:</p>
<p>$$x_t\sim P(x_t|x_{t-\tau},...,x _{t-1})$$</p>
<p>$\tau$ is an import hyperparameter that determines the complexity of prediction. When $\tau&#x3D;m$, the model is called <em>mth-order Markov model</em>:</p>
<p>$$(x_1,...,x_T)\sim \prod\limits_{t&#x3D;1}^TP(x_t|x _{\max (t-\tau,0)},...,x _{t-1})$$</p>
<p>where $x_0$ has nothing to do with $x_i$, that is, it is independent from others. Such a model is also called <em>Autoregressive model</em>.</p>
<h2 id="Latent-autoregressive-models"><a href="#Latent-autoregressive-models" class="headerlink" title="Latent autoregressive models"></a>Latent autoregressive models</h2><p>In this model, we use a new parameter $h_t$ to summarize the past information:</p>
<p>$$h_t&#x3D;g(h_{t-1},x_{t-1})$$</p>
<p>$$\hat{x}_t&#x3D;P(x_t|h_t)$$</p>
<p>where $h_t$ is the latent variable.</p>
<p><img src="/2023/05/25/RNN/1.png" alt="1"></p>
<center style="font-size:12px; font-weight:bold">Fig. 1. Latent autoregressive models</center><br>

<blockquote>
<p>In a word, $(x _{t-\tau},...,x _{t-1})$ is the feature of $x_t$. The aim of RNN is to obtain the mapping relation $x_t&#x3D;f(x _{t-\tau},...,x _{t-1})$ for markov models or $h_t&#x3D;g(h _{t-1},x _{t-1})$ and $\hat{x}_t&#x3D;f(h_t)$ for latent autoregressive models.</p>
</blockquote>
<h2 id="Language-models"><a href="#Language-models" class="headerlink" title="Language models"></a>Language models</h2><p>The language model is a typical sequence model. However, because languages are in the form of <em>string</em>, it is extremely hard for computers to cope with them. Hence, we always divide a text into several <em>tokens</em>. A token is a string and a word in the original text. Then, we count the probablity of occurrence of all tokens or token sequences and use markov models to model language models. The value of $\tau$ decides the number of tokens we take into account to predict $x_t$. For instance, one token (tokens are independent from each other) is <em>Unigram</em>, two tokens are <em>Bigram</em> and three tokens are <em>Trigram</em>.</p>
<blockquote>
<p>One-hot encoding is used to turn a token into a vector so that the neural network can work.</p>
</blockquote>
<h1 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h1><p>RNNs (recurrent neural networks) are neural networks with <em>hidden state</em> which is another <em>input</em> of the hidden layer and is updated by calling the hidden layer recurrrently. </p>
<p><img src="/2023/05/25/RNN/2.png" alt="2"></p>
<center style="font-size:12px; font-weight:bold">Fig. 2. Hidden state</center><br>

<p>The hidden state <em>H</em> is actually the same as input <em>X</em>, that is, they are both inputs of hidden layer:</p>
<p>$$\text{Output}_t&#x3D;H_t&#x3D;\phi(X _tW _{xh}+H _{t-1}W _{hh}+b_h)$$</p>
<p>The following picture shows the nature of RNN better:</p>
<p><img src="/2023/05/25/RNN/3.png" alt="3"></p>
<center style="font-size:12px; font-weight:bold">Fig. 3. CNN</center><br>

<p>In each timing, a token or other sequential unit $X_t$ enters the RNN as an input. $H_{t-1}$ records the information of previous tokens. They together produce $H_t$. $H_t$ records the information of current token $X_t$ and previous tokens. Hence, it is only the output of this hidden layer at this timing but also the input of the hidden layer at the next timing. That&#39;s why such a neural network is called RNN: for a sequence with several tokens, all the tokens will be fed to RNN in sequence. Their relationship is recorded by $H$. The updated $H$ is fed to RNN recurrently. And finally, after dealing with the last token, the prediction is made.</p>
<p><img src="/2023/05/25/RNN/4.png" alt="4"></p>
<center style="font-size:12px; font-weight:bold">Fig. 4. Process of RNN</center><br>

<blockquote>
<p>Though we use the RNN layer several times (depends on the length of sequence) to generate $H_1,...,H_t$ in a single iteration, it is still a layer. Namely, during one iteration, $W_{xh}$, $W_{hh}$ and $b_h$ keep the same. What changes is $H$.</p>
</blockquote>
<p>When the batch is generated by random sampling, that is, the sequence of different batches is not continuous (e.g. batch1: [1, 2], batch2: [8, 9]), $H$ must be initialized to 0 in each iteration. Otherwise (e.g. batch1: [1, 2], batch2: [3, 4]), $H$ should be kept as the last result of the former batch. By doing so, the current batch and the former batch form a longer sequence. In practice, we use random sampling more as the text we cope with is often too long to remember all of it only using $H$.</p>
<blockquote>
<p>The api of RNN in PyTorch is <code>nn.RNN</code> .</p>
</blockquote>
<h2 id="Gradient-clipping"><a href="#Gradient-clipping" class="headerlink" title="Gradient clipping"></a>Gradient clipping</h2><p>Timesteps represent the length of a sequence or $\tau$ in markov assumption. The larger the timestep is, the more information the RNN need to remember. In addition, because the operation of a RNN hidden layer of k timesteps is equivalent to the operation of k dense layers, it is more likely for RNN to have gradient explosion.</p>
<p>To solve this, what we use is gradient clipping. For all the parameters that require gradient, we put their gradient in $\mathbf{g}$ and project $\mathbf{g}$ back to a sphere of given radius $\theta$:</p>
<p>$$\mathbf{g}\leftarrow\min(1,\frac{\theta}{\left|\mathbf{g}\right|})\mathbf{g}$$</p>
<p>Usually, $\theta$ is 5.</p>
<h2 id="Perplexity"><a href="#Perplexity" class="headerlink" title="Perplexity"></a>Perplexity</h2><p>When coping with language model using RNN, what we actually do is to make the agent choose the best token from vocabulary. This is actually a softmax regression problem. However, in NLP, we don&#39;t use the average of crossentropy loss to measure its precision. Instead, we use its exponent, <em>perplexity</em>:</p>
<p>$$\exp(-\frac{1}{n}\sum\limits_{t&#x3D;1}^n\log P(x_t|x_{t-1},...,x_1))$$</p>
<p>They are actually the same but <em>perplexity</em> shows the range of tokens that the agent can choose. Namely, when <em>perplexity</em> is 1, which means that the agent can choose a specific token without hesitation, this is the ideal result. When <em>perlexity</em> is bigger than 1, this means that there are still some tokens confusing the agent.</p>
<h1 id="Structure-of-RNN"><a href="#Structure-of-RNN" class="headerlink" title="Structure of RNN"></a>Structure of RNN</h1><p>There are many types of RNN structure. Different structures are suitable for different functions. For instance, in NLP, one to many structure is suitable for text generation, that is, predicting the subsequent text using current text. Many to one structure is suitable for text categorization. It tries to remember the whole text and only output one value. Many to many structure is fit for machine translation and question answering. Seq2seq is a typical many to many model.</p>
<p><img src="/2023/05/25/RNN/5.png" alt="5"></p>
<center style="font-size:12px; font-weight:bold">Fig. 5. Different structures of RNN</center><br>
]]></content>
      <categories>
        <category>Dive Into Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>RNN</tag>
      </tags>
  </entry>
  <entry>
    <title>Multiple GPUs and Parallelism</title>
    <url>/2023/05/19/MultipleGPUsAndParallelism/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="Batch-size"><a href="#Batch-size" class="headerlink" title="Batch size"></a>Batch size</h1><p>Batch size is related to sample diversity. When computing the loss of a batch, we actually take the average of the total loss. Therefore, no matter how large the batch size is, we only update the model parameters with one sample on average in each iteration. In this case, the diversity of samples is rather important. The more diverse the sample is, the more efficient updates we can get in each iteration. For example, if all the samples in a batch are the same, it works as if each batch has only one sample.</p>
<p>When the total number of samples is constant, the larger the batch size is, the fewer times the model parameter is updated in one iteration. Hence, to achieve the same performance, we have to enlarge learning rate when we enlarge the batch size.</p>
<blockquote>
<p>In theory, one sample in a batch will produce the best model performance but it is time-consuming.</p>
</blockquote>
]]></content>
      <categories>
        <category>Dive Into Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Common CNN Models</title>
    <url>/2023/05/14/CommonCNNModels/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="LeNet-5"><a href="#LeNet-5" class="headerlink" title="LeNet-5"></a>LeNet-5</h1><p><a href="https://ieeexplore.ieee.org/document/726791">LeNet-5</a> which was published in 1998, is one of the simplest CNN models. It was originally used to recognize handwritten numbers between 0 and 9.</p>
<p><img src="/2023/05/14/CommonCNNModels/1.png" alt="1"></p>
<center style="font-size:12px;font-weight:bold">Fig. 1. LeNet-5</center><br>

<p>Its structure is quite simple from today&#39;s perspective: start with two convolutional and pooling layers and end with three dense layers (the last layer is softmax). However, it pioneered the template of CNN, that is:</p>
<ol>
<li>Start with convolutional and pooling layers to extract features and reduce data dimensionality;</li>
<li>End with dense layers to solve the problems.</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># structure of LeNet-5</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Reshape</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> x.view((-<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>))  <span class="comment"># reshape the dimension of input data</span></span><br><span class="line"></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    Reshape(),</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>),  <span class="comment"># add padding to get 32*32</span></span><br><span class="line">    nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(<span class="number">2</span>),  <span class="comment"># windows of the pooling layer in pytorch do not overlap by default</span></span><br><span class="line">    nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>),</span><br><span class="line">    nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(<span class="number">2</span>),</span><br><span class="line">    nn.Flatten(),</span><br><span class="line">    nn.Linear(<span class="number">400</span>, <span class="number">120</span>),</span><br><span class="line">    nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">120</span>, <span class="number">84</span>),</span><br><span class="line">    nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">84</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure>

<h1 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h1><p><a href="https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">AlexNet</a> started the craze for neural networks in 2012, before which, the kernel function and SVM were more powerful in machine learning. The key of kernel function is manual feature extraction. With features, it can calculate their correlation using the kernel function and transform it into a convex optimization problem. After that, SVM will work well by feeding these features to it. It has strong beautiful theorems and can be explained mathematically.</p>
<p>AlexNet is actually a larger LeNet. It improves on LeNet and increases the depth of the neural network:</p>
<ul>
<li>Dropout: control the overfitting of deeper neural networks;</li>
<li>ReLu: make gradients larger;</li>
<li>MaxPooling: make gradients larger and the model easier to train;</li>
<li>ImageNet: larger dataset than Mnist;</li>
<li>Data augmentation: create more samples and reduce the sensitivity of convolutional layers to position.</li>
</ul>
<p>AlexNet changed the way people think about machine learning:</p>
<ul>
<li>Learning features through CNN instead of extracting manually;</li>
<li>Training classifiers (traditional ML methods) together with feature extrators (CNN).</li>
</ul>
<p><img src="/2023/05/14/CommonCNNModels/2.png" alt="2"></p>
<center style="font-size:12px;font-weight:bold">Fig. 2. AlexNet</center><br>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># structure of AlexNet</span></span><br><span class="line"><span class="comment"># size of each image: 224*224</span></span><br><span class="line">Net = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">96</span>, kernel_size=<span class="number">11</span>, stride=<span class="number">4</span>, padding=<span class="number">1</span>),  <span class="comment"># the dataset is Mnist, if ImageNet, 1 should be 3</span></span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Conv2d(<span class="number">96</span>, <span class="number">256</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Conv2d(<span class="number">256</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">384</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">384</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Flatten(),</span><br><span class="line">    nn.Linear(<span class="number">6400</span>, <span class="number">4096</span>), nn.ReLU(), nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">    nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>), nn.ReLU(), nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">    nn.Linear(<span class="number">4096</span>, <span class="number">10</span>)  <span class="comment"># dataset is Mnist</span></span><br><span class="line">    )</span><br></pre></td></tr></table></figure>

<h1 id="VGG"><a href="#VGG" class="headerlink" title="VGG"></a>VGG</h1><p><a href="https://arxiv.org/pdf/1409.1556.pdf">VGG</a> (Visual Geometry Group) encapsulates the convolutional and pooling layers in AlexNet into blocks so that they can be reused and the network will be more standardized.</p>
<p><img src="/2023/05/14/CommonCNNModels/3.png" alt="3"></p>
<center style="font-size:12px;font-weight:bold">Fig. 3. VGG</center><br>

<p>Usually, for each block, the height and width of matrices are halved, and the number of channels is doubled.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># structure of VGG</span></span><br><span class="line"><span class="comment"># size of each image: 224*224</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">vgg_block</span>(<span class="params">num_convs, in_channels, out_channels</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    definition of block.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        num_convs: the number of convolutional layer in this block</span></span><br><span class="line"><span class="string">        in_channels: the number of input channels in the first convolutional layer</span></span><br><span class="line"><span class="string">        out_channels: the number of output channels of the block</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">        nn.Sequential</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    layers = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_convs):</span><br><span class="line">        layers.append(nn.Conv2d(</span><br><span class="line">            in_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>))</span><br><span class="line">        layers.append(nn.ReLU())</span><br><span class="line">        in_channels = out_channels</span><br><span class="line">    layers.append(nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">conv_arch = ((<span class="number">1</span>, <span class="number">64</span>), (<span class="number">1</span>, <span class="number">128</span>), (<span class="number">2</span>, <span class="number">256</span>), (<span class="number">2</span>, <span class="number">512</span>), (<span class="number">2</span>, <span class="number">512</span>))</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">vgg</span>(<span class="params">conv_arch</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    definition of vgg.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        conv_arch: structure of all the blocks, including number of convolutional </span></span><br><span class="line"><span class="string">                   layer of each block and the number of output channels of each block</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">        the whole VGG network</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    conv_blks = []</span><br><span class="line">    in_channels = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> (num_convs, out_channels) <span class="keyword">in</span> conv_arch:</span><br><span class="line">        conv_blks.append(vgg_block(</span><br><span class="line">            num_convs, in_channels, out_channels))</span><br><span class="line">        in_channels = out_channels</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        *conv_blks, nn.Flatten(),</span><br><span class="line">        nn.Linear(out_channels * <span class="number">7</span> * <span class="number">7</span>, <span class="number">4096</span>), nn.ReLU(), nn.Dropout(p=<span class="number">0.5</span>),  <span class="comment"># 224 / 2^5 = 7</span></span><br><span class="line">        nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>), nn.ReLU(), nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">        nn.Linear(<span class="number">4096</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="NiN"><a href="#NiN" class="headerlink" title="NiN"></a>NiN</h1><p><a href="https://arxiv.org/pdf/1312.4400.pdf">NiN</a> (Network in Network) follows the block idea of VGG. However, it removes the dense layer completely as the dense layer need too much memory. As an alternative to the dense layer, NiN utilizes the $1\times1$ kernel and ReLU to add nonlinearity to each pixel and uses global average pooling layer to replace the last dense layer, which compute the mean of each input channel. </p>
<blockquote>
<p>The function of global average pooling layer is the same as the last dense layer but it requires less computation. Softmax is realized by loss function CrossEntropy.</p>
</blockquote>
<p><img src="/2023/05/14/CommonCNNModels/4.png" alt="4"></p>
<center style="font-size:12px;font-weight:bold">Fig. 4. NiN</center><br>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># structure of NiN</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">nin_block</span>(<span class="params">in_channels, out_channels, kernel_size, strides, padding</span>):</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.Conv2d(in_channels, out_channels, kernel_size, strides, padding), nn.ReLU(),</span><br><span class="line">        nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">        nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">1</span>), nn.ReLU())</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nin_block(<span class="number">1</span>, <span class="number">96</span>, kernel_size=<span class="number">11</span>, strides=<span class="number">4</span>, padding=<span class="number">0</span>),</span><br><span class="line">    nn.MaxPool2d(<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nin_block(<span class="number">96</span>, <span class="number">256</span>, kernel_size=<span class="number">5</span>, strides=<span class="number">1</span>, padding=<span class="number">2</span>),</span><br><span class="line">    nn.MaxPool2d(<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nin_block(<span class="number">256</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, strides=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.MaxPool2d(<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    <span class="comment"># just normalization, we can remove it</span></span><br><span class="line">    nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">    <span class="comment"># the number of labels is 10</span></span><br><span class="line">    nin_block(<span class="number">384</span>, <span class="number">10</span>, kernel_size=<span class="number">3</span>, strides=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">    <span class="comment"># the shape of output is 1x1</span></span><br><span class="line">    nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>)),  <span class="comment"># the number of input channels must be equal to the number of labels</span></span><br><span class="line">    <span class="comment"># turn the 10 labels to a vector</span></span><br><span class="line">    nn.Flatten())</span><br></pre></td></tr></table></figure>

<blockquote>
<p>The global average pooling reduces the complexity of computation but also slows convergence.</p>
</blockquote>
<h1 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h1><p><a href="https://arxiv.org/pdf/1409.4842.pdf">GoogLeNet</a> inherits the idea of block network, 1x1 convolutional layer, and global average pooling layer in NiN. The most important block in GoogLeNet is Inception block.</p>
<p><img src="/2023/05/14/CommonCNNModels/5.png" alt="5"></p>
<center style="font-size:12px;font-weight:bold">Fig. 5. Inception block(V1)</center><br>

<p>The inception block consists of multiple parallel pathways which <strong>extract different types of features</strong> from the input. Each pathway generates new output channels using all the channels of input data. Different pathways output different number of output channels but keep the shape of each channel the same as the input channel. Usually, the number of output channels is larger than the number of input channels. The output channels of different pathways are concatenated together to form the new input data.</p>
<blockquote>
<p>In the inception block, the <strong>1x1 kernel</strong> is used to merge channels so that the computaion complexity (it is equal to the number of parameters that the neural network can learn) can be reduced while <strong>the blue layer</strong> extract features from the input.</p>
<p>The output size of each pathway is the hyperparameter that can be modified in inception block.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="comment"># Inception block</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Inception</span>(nn.Module):</span><br><span class="line">    <span class="comment"># c1--c4 which may be a list, are the number of output channels of each pathway</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, c1, c2, c3, c4, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(Inception, self).__init__(**kwargs)</span><br><span class="line">        <span class="comment"># pathway1, single 1x1 convolutional layer</span></span><br><span class="line">        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># pathway2，1x1 convolutional layer + 3x3 convolutional layer</span></span><br><span class="line">        self.p2_1 = nn.Conv2d(in_channels, c2[<span class="number">0</span>], kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.p2_2 = nn.Conv2d(c2[<span class="number">0</span>], c2[<span class="number">1</span>], kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># pathway3，1x1 convolutional layer + 5x5 convolutional layer</span></span><br><span class="line">        self.p3_1 = nn.Conv2d(in_channels, c3[<span class="number">0</span>], kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.p3_2 = nn.Conv2d(c3[<span class="number">0</span>], c3[<span class="number">1</span>], kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># pathway4，3x3 maxpooling layer + 1x1 convolutional layer</span></span><br><span class="line">        self.p4_1 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># ReLU add nonlinearity for each layer</span></span><br><span class="line">        p1 = F.relu(self.p1_1(x))</span><br><span class="line">        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))</span><br><span class="line">        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))</span><br><span class="line">        p4 = F.relu(self.p4_2(self.p4_1(x)))</span><br><span class="line">        <span class="comment"># concatenate the output channels (dim 0 is batch size)</span></span><br><span class="line">        <span class="keyword">return</span> torch.cat((p1, p2, p3, p4), dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>The entire GoogLeNet uses 9 inception blocks and one global average pooling layer to generate the prediction. However, the global average pooling layer in GoogLeNet doesn&#39;t requires its output size to be equivalent with the number of labels in softmax because there is a dense layer behind it.</p>
<p><img src="/2023/05/14/CommonCNNModels/6.png" alt="6"></p>
<center style="font-size:12px;font-weight:bold">Fig. 6. GoogleNet</center><br>

<p>The whole GoogLeNet can be divided into 5 stages. After each stage, the length and width of the image are halved and the number of channels increases (often doubles). This is a idea followed by many subsequent neural networks. For instance, in the neural network we build below, the output shape of each stage is:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">64</span>, <span class="number">24</span>, <span class="number">24</span>])  <span class="comment"># the first stage makes the output a quarter</span></span><br><span class="line">Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">192</span>, <span class="number">12</span>, <span class="number">12</span>])</span><br><span class="line">Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">480</span>, <span class="number">6</span>, <span class="number">6</span>])</span><br><span class="line">Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">832</span>, <span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line">Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">1024</span>])</span><br><span class="line">Linear output shape:         torch.Size([<span class="number">1</span>, <span class="number">10</span>])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># structure of GoogLeNet</span></span><br><span class="line">b1 = nn.Sequential(nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">                   nn.ReLU(),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">b2 = nn.Sequential(nn.Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=<span class="number">1</span>),</span><br><span class="line">                   nn.ReLU(),</span><br><span class="line">                   nn.Conv2d(<span class="number">64</span>, <span class="number">192</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">                   nn.ReLU(),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">b3 = nn.Sequential(Inception(<span class="number">192</span>, <span class="number">64</span>, (<span class="number">96</span>, <span class="number">128</span>), (<span class="number">16</span>, <span class="number">32</span>), <span class="number">32</span>),</span><br><span class="line">                   Inception(<span class="number">256</span>, <span class="number">128</span>, (<span class="number">128</span>, <span class="number">192</span>), (<span class="number">32</span>, <span class="number">96</span>), <span class="number">64</span>),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">b4 = nn.Sequential(Inception(<span class="number">480</span>, <span class="number">192</span>, (<span class="number">96</span>, <span class="number">208</span>), (<span class="number">16</span>, <span class="number">48</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">160</span>, (<span class="number">112</span>, <span class="number">224</span>), (<span class="number">24</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">128</span>, (<span class="number">128</span>, <span class="number">256</span>), (<span class="number">24</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">112</span>, (<span class="number">144</span>, <span class="number">288</span>), (<span class="number">32</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">528</span>, <span class="number">256</span>, (<span class="number">160</span>, <span class="number">320</span>), (<span class="number">32</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">b5 = nn.Sequential(Inception(<span class="number">832</span>, <span class="number">256</span>, (<span class="number">160</span>, <span class="number">320</span>), (<span class="number">32</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   Inception(<span class="number">832</span>, <span class="number">384</span>, (<span class="number">192</span>, <span class="number">384</span>), (<span class="number">48</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   nn.AdaptiveAvgPool2d((<span class="number">1</span>,<span class="number">1</span>)),</span><br><span class="line">                   nn.Flatten())</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(<span class="number">1024</span>, <span class="number">10</span>))</span><br><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">96</span>, <span class="number">96</span>))</span><br></pre></td></tr></table></figure>
<p>The above is the first version of GoogLeNet. In the subsequent versions, Google made different improvements to the inception block:</p>
<ul>
<li><p>Inception-BN(V2): batch normalization;</p>
</li>
<li><p>Inception-V3: modify the size of kernel;</p>
</li>
<li><p>Inception-V4: use residual connections</p>
</li>
</ul>
<p>Among them, V3 is used more.</p>
<blockquote>
<p>Advantages of GoogLeNet: fewer model parameters and relatively low computational complexity.</p>
</blockquote>
<h2 id="Batch-normalization"><a href="#Batch-normalization" class="headerlink" title="Batch normalization"></a>Batch normalization</h2><p><a href="https://arxiv.org/pdf/1502.03167.pdf">Batch normalization</a> is a layer that will make the neural network converge faster as it keeps the input of hidden layer more stable. This is how it works:</p>
<p>$$\mathrm{BN}(\mathbf{x}) &#x3D; \boldsymbol{\gamma} \odot \frac{\mathbf{x} - \hat{\boldsymbol{\mu}}_\mathcal{B}}{\hat{\boldsymbol{\sigma}}_\mathcal{B}} + \boldsymbol{\beta}$$</p>
<p>where $\hat{\boldsymbol{\mu}}_\mathcal{B}$ is the mean of small batch $\mathcal{B}$ and $\hat{\boldsymbol{\sigma}}_\mathcal{B}$ is the standard deviation of small batch $\mathcal{B}$. However, we just artificially make the distribution of $\mathbf{x}$ into a standard normal distribution. Therefore, we reserve two learnable parameters for the machine so that it can automatically adjust the distribution of $\mathbf{x}$. And that is scale ($\boldsymbol{\gamma}$) and shift ($\boldsymbol{\beta}$), which adjust variance and mean respectively. They have the same shape as $\mathbf{x}$.</p>
<p>For $\hat{\boldsymbol{\mu}}_\mathcal{B}$, it is easy to compute for each batch:</p>
<p>$$\hat{\boldsymbol{\mu}} _\mathcal{B} &#x3D; \frac{1} {|\mathcal{B} |} \sum _{\mathbf{x} \in \mathcal{B}} \mathbf{ x }$$</p>
<p>But for $\hat{\boldsymbol{\sigma}}_\mathcal{B}$, we should add $\epsilon$ so that the formula don&#39;t divide by 0.</p>
<p>$$\hat{\boldsymbol {\sigma}} _\mathcal{B}^2 &#x3D; \frac{1} {|\mathcal{B} |} \sum _{\mathbf{ x } \in \mathcal{ B }} (\mathbf{x} - \hat{\boldsymbol{\mu}} _{\mathcal{B}})^2 + \epsilon$$</p>
<blockquote>
<p>Usually, we make $\epsilon$ 1e-5 or 1e-6.</p>
</blockquote>
<p>Similar to dropout, we only make the batch normalization layer work when training. When making prediction, we use <strong>global mean</strong> and <strong>global variance</strong> to normalize the input of hiddden layers. The method to get them is slightly similar to that we use to get RTT in computer network:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">moving_mean = momentum * moving_mean + (<span class="number">1.0</span> - momentum) * mean</span><br><span class="line">moving_var = momentum * moving_var + (<span class="number">1.0</span> - momentum) * var</span><br></pre></td></tr></table></figure>
<p>where <code>mean</code> and <code>var</code> are mean and variance the agent get in this small batch, <code>momentum</code> is a weight factor (range from 0 to 1).</p>
<blockquote>
<p>$\boldsymbol{\gamma}$ and $\boldsymbol{\beta}$ are still needed.</p>
</blockquote>
<p>It is important to understand that all the mean and variance the agent computes are mean and variance of each <strong>feature</strong> or <strong>channel</strong> (for the convolutional layer, a channel represents a feature). That&#39;s why the shape of the second dimension of $\boldsymbol{\gamma}$ and $\boldsymbol{\beta}$ is the same as the number of features for dense layers or the number of input channels for convolutional layers.</p>
<blockquote>
<p>We put the batch normalization layer before the activation layer. Its function is a bit like <em>dropout</em> so we often don&#39;t use both layers at the same time.</p>
</blockquote>
<p>It is not clear that why batch normalization works. Some researchers guess that it works as it add noise to each small batch. Those noise make the output of different layers more stable and more realistic. Hence, the gradients of bottom layers will larger and the model converge faster. But it doesn&#39;t change the accuracy of models.</p>
<blockquote>
<p>Xavier makes the initial value of the model parameters more stable. Batch normalization makes the output of hidden layers (or the input of hidden layers) stable. It&#39;s actually a linear layer that modifies the data as we wish. Namely, <strong>we give the agent some human guidance</strong>.</p>
</blockquote>
<h1 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h1><p><a href="https://arxiv.org/pdf/1512.03385.pdf">ResNet</a> combined with batch normalization enables us to train a deeper neural network. Before ResNet, the model we train is based on non-nested function classes like the figure on the left. Namely, when we add a new block to complicate the model, we establish a new mapping relationship $y&#x3D;g(f(x))$. However, we can&#39;t make sure that $g(f(x))$ is better than $f(x)$ since $g(f(x))$ can&#39;t cover all the zone that $f(x)$ cover. What&#39;s more, in a deep neural network, the gradients of the bottom layers are usually very small, which makes the bottom layers converge slowly.</p>
<p>The core idea of ResNet is to compute $y&#x3D;g(f(x))+f(x)$ instead of $y&#x3D;g(f(x))$ (Fig. 8). This small change makes a more complex network contain a simpler network (like the picture on the right). Therefore, when using ResNet, a deeper network always performs better than a simple network. In addition, ResNet provides a fast track for the bottom layer to get larger gradients (through $f(x)$). What&#39;s more, when the simple network $f(x)$ has already performed well, its gradients will be rather small, that is, its parameters will not change significantly and it will keep performing well in a deeper network.</p>
<p><img src="/2023/05/14/CommonCNNModels/7.png" alt="7"></p>
<center style="font-size:12px;font-weight:bold">Fig. 7. Non-nested and nested function classes</center><br>

<p><img src="/2023/05/14/CommonCNNModels/8.png" alt="8"></p>
<center style="font-size:12px;font-weight:bold">Fig. 8. Residual block, the weight layer could be dense or convolutional layer</center><br>

<p>The structure of ResNet is similar to GoogLeNet but it replace the inception block with residual block.</p>
<p><img src="/2023/05/14/CommonCNNModels/9.png" alt="9"></p>
<center style="font-size:12px;font-weight:bold">Fig. 9. ResNet-18</center><br>

<blockquote>
<p>When counting the number of layer, we only count the convolutional layer and the dense layer.</p>
</blockquote>
]]></content>
      <categories>
        <category>Dive Into Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title>CNN: Feature Extraction</title>
    <url>/2023/05/10/CNN/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="From-Dense-to-Convolution"><a href="#From-Dense-to-Convolution" class="headerlink" title="From Dense to Convolution"></a>From Dense to Convolution</h1><p>The convolutional layer is the key layer of CNN. It is a special kind of dense layer. CNN is used to deal with images. In this section, we take the grayscale image as example, that is, the feature of an input image is a 2-D matrix. When processing images, firstly, it is reasonable that agents&#39;s awareness of a certain object should not be overly concerned with the precise location of the object in the image because what the object looks like has nothing to do with its position. Besides, the features of one object should only be related to the pixels around it as each pixel only determines the depth of the color and we distinguish objects by their color boundaries. These two priciples are called <em>translation invariance</em> and <em>locality</em> respectively. If fulfilling both priciples, dense layers become convolutional layers.</p>
<h2 id="Constructing-a-dense-layer"><a href="#Constructing-a-dense-layer" class="headerlink" title="Constructing a dense layer"></a>Constructing a dense layer</h2><p>For convinience, we keep the dimension of a input image $X$ 2-D and make the model parameters of one neuron $w$ have the same dimension as $X$. Under such setting, the output of one neuron become:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">(X*w).<span class="built_in">sum</span>() <span class="comment"># w is a matrix</span></span><br></pre></td></tr></table></figure>

<p>Now that the output of a hidden layer $H$ should be a image that has the same shape as $X$, we have to place $m*n$ neurons in the hidden layer, where $m$ and $n$ are the dimension of $X$. Hence, for a hidden layer, its model parameter $W$ is:</p>
<p>$$<br>{\begin{bmatrix}<br>    {\begin{bmatrix}<br>        w_{0,0,0,0} &amp; w_{0,0,0,1}\\<br>        w_{0,0,1,0} &amp; w_{0,0,1,1}<br>    \end{bmatrix}<br>    \begin{bmatrix}<br>        w_{0,1,0,0} &amp; w_{0,1,0,1}\\<br>        w_{0,1,1,0} &amp; w_{0,1,1,1}<br>    \end{bmatrix}}\\<br>    {\begin{bmatrix}<br>        w_{1,0,0,0} &amp; w_{1,0,0,1}\\<br>        w_{1,0,1,0} &amp; w_{1,0,1,1}<br>    \end{bmatrix}<br>    \begin{bmatrix}<br>        w_{1,1,0,0} &amp; w_{1,1,0,1}\\<br>        w_{1,1,1,0} &amp; w_{1,1,1,1}<br>    \end{bmatrix}}<br>\end{bmatrix}} _{m\times n\times m\times n&#x3D;2\times 2\times 2\times 2}<br>$$</p>
<p>And an element of output $H$ is:</p>
<p>$$<br>[H]_{i,j}&#x3D;\sum_k \sum_l {[W]} _{i,j,k,l} {[X]} _{k,l} + {[U]} _{i,j}<br>$$</p>
<p>where $U$ contains bias of each output pixel.</p>
<h2 id="Translation-invariance"><a href="#Translation-invariance" class="headerlink" title="Translation invariance"></a>Translation invariance</h2><p>Now we can clearly see that in order to process an image with $m * n$ data, we almost use $(m * n)^2$ model parameters. This is extremely space-consuming. However, if we apply translation invariance priciple to the dense layer, we can cut it to $m * n$.</p>
<p>As mentioned above, the activation of a pixel should have nothing to do with its position. This is only possible when ${[W]} _{i,j}$ and ${[U]} _{i,j}$ are both the same for any $(i,j)$. Hence, for a certain hidden layer, we actually just need a 2-D $W$ and a scalar $u$:</p>
<p>$$<br>{[H]} _{i,j}&#x3D;\sum_k\sum_l{[W]} _{k,l} {[X]} _{k,l}+u<br>$$</p>
<h2 id="Locality"><a href="#Locality" class="headerlink" title="Locality"></a>Locality</h2><p>So far, we have significantly cut the space of model parameters but it is still too large. Now it is time for <em>locality</em>. As motivated above, there is no need to look so far away from the location of input pixel ${[X]}_ {i,j}$. We just need to consider a small window around ${[X]} _{i,j}$:</p>
<p>$${[H]}_ {i, j} &#x3D; u + \sum_{a &#x3D; -\Delta}^{\Delta} \sum_{b &#x3D; -\Delta}^{\Delta} {[V]}_ {a, b}  {[X]}_ {i+a, j+b}.$$</p>
<p>where $\Delta\times\Delta$ is the size of the window and $V$ (size $\Delta\times\Delta$) is the model parameter inside the window, a filter that generates the feature of a zone in an image or a <strong>convolution kernel</strong>.</p>
<p>Now we construct a convolutional layer (<code>nn.Conv2d</code>) with hyperparameters <em>kernel size</em> and <em>stride</em> (see below).</p>
<p>$$Y &#x3D; X \star W+b$$</p>
<p>$W$ and $b$ are model parameters that the agent can learn. $\star$ is cross-correlation operator which is slightly different from convolution but their behaviors are the same in the convolutional layer (see below). The dimensions of $Y$ are smaller than the original dimensions, which is:</p>
<p>$$(n_h-k_h+1)\times(n_w-k_w+1)$$</p>
<p>where $n_h$ and $n_w$ are the dimensions of $X$, $k_h$ and $k_w$ are the dimensions of $W$. It doesn&#39;t matter and we can solve it easily (see below).</p>
<p><img src="/2023/05/10/CNN/1.png" alt="1"></p>
<center style="font-size:12px;font-weight:bold">Fig. 1. Convolutional layer</center>

<h1 id="Convolution-Fourier-and-Neural-Networks"><a href="#Convolution-Fourier-and-Neural-Networks" class="headerlink" title="Convolution, Fourier and Neural Networks"></a>Convolution, Fourier and Neural Networks</h1><p>Convolution is an operation on two functions ($f$ and $g$) that produces a third function ($f*g$) that express how the shape of one is modified by the other:</p>
<p>$$(f*g)(t)&#x3D;\int_{-\infty} ^{+\infty}f(\tau)g(t-\tau)d\tau$$</p>
<p>Physically, we can explain it from two perspectives.</p>
<h2 id="Stock"><a href="#Stock" class="headerlink" title="Stock"></a>Stock</h2><p>Suppose there is a system with input $f(t)$ at each moment. The input starts decaying with $g(t)$ by the time it enters the system, where $g(t)$ is its remaining percentage.</p>
<p><img src="/2023/05/10/CNN/2.png" alt="2"></p>
<center style="font-size:12px;font-weight:bold">Fig. 2. f(t) and g(t)</center><br>

<p>For items that enter the system between $(t,t+\Delta t)$, their margin at $x$ is:</p>
<p>$$f(t)g(x-t)\Delta t$$</p>
<p>Therefore, for items that enter the system between $(t_1,t_2)$, their margin at $x$ is:</p>
<p>$$\int_{t_1}^{t_2}f(t)g(x-t)dt$$</p>
<p>In this case, we take $f(t)$ as the unstable input and $g(t)$ as the stable output to compute the stock of a system after some time.</p>
<h2 id="Influence"><a href="#Influence" class="headerlink" title="Influence"></a>Influence</h2><p>In the explaination above, we actually make $dt$ relate to $f(t)$. Namely, $f(t)dt$ is the input of the system during $t$ and $t+\Delta t$. $g(x-t)$ has nothing to do with $dt$. It is just a decaying factor.</p>
<p>However, we can also take $g(x-t)dt$ as a whole. This will make the $t$ and  $f * g$ more meaningful. For example, we can regard $f(t)$ as a thing happens at $t$ and $g(t)$ as its influence over time. Then $f*g$ can be regarded as the totally influence at $x$ caused by things that happen during $t_1$ and $t_2$:</p>
<p>$$\int_{t_1}^{t_2}f(t)g(x-t)dt$$</p>
<p>To go a step further, we can take $t$ as position, $f(t)$ as a thing happens at $t$ and keep moving from $t$ to $x$, $g(t)$ as its award or loss per distance. For example, some people whose weight $f(t)$ starts running as $t$ and they will loss $g(t)$ kg per meter. Then $f*g$ is the weight loss of all. (the fatter a person is, the more contribution he&#x2F;she makes).</p>
<p><img src="/2023/05/10/CNN/3.png" alt="3"></p>
<center style="font-size:12px;font-weight:bold">Fig. 3. weight and weight loss</center>

<h2 id="Convolution-in-CNN"><a href="#Convolution-in-CNN" class="headerlink" title="Convolution in CNN"></a>Convolution in CNN</h2><p>For convolution in CNN, we&#39;d better expand its dimension to 2-D and take $t$ as a discrete number which represents position $(x,y)$. Then, we can take $X$ as function $f$ and $W$ as $g$. Now the meaning of convolution in CNN can be interpreted as the influence that pixels around $f(x,y)$ exert on pixel $f(x,y)$ or the certain average feature of the pixels around $f(x,y)$. </p>
<p>$$f * g&#x3D;\sum _{i,j}f(i,j)g(x-i,y-j)$$</p>
<p><img src="/2023/05/10/CNN/4.png" alt="4"></p>
<center style="font-size:12px;font-weight:bold">Fig. 4. Convolution in CNN</center><br>

<p>The phrase certain average feature may be a little confusing. However, if we regard the convolution kernel $W$ as a feature filter that extract specific features of a region of an image, it makes sense. Different filters extract different features and their parameters are totally learnt by the agent itself.</p>
<p><img src="/2023/05/10/CNN/5.png" alt="5"></p>
<center style="font-size:12px;font-weight:bold">Fig. 5. The feature filter</center><br>

<p>There are still some differences between convolution and the convolutional layer, that is, $g$ is not the kernel we actually use. For example, convolution computes $f(x-1,y-1)g(1,1)$ rather than $f(x-1,y-1)g(-1,-1)$. However, in CNN, we actually compute $f(x-1,y-1)g(-1,-1)$. In fact, it doesn&#39;t matter. We just need to turn $g$ around and we get the $W$ we use in CNN. Hence, what the agent computes is <strong>cross-correlation</strong> rather than convolution but the results are the same so we just call it convolution.</p>
<h1 id="Hyperparameters-of-convolutional-layer"><a href="#Hyperparameters-of-convolutional-layer" class="headerlink" title="Hyperparameters of convolutional layer"></a>Hyperparameters of convolutional layer</h1><p>Kernel size, padding, stride, output (input) channels are the new hyperparameters that we can adjust in the convolutional layer. </p>
<ol>
<li>Kernel size determines the size of window. It is the simplest parameter among them. We usually set it to $3\times3$ or $5\times5$ for 2-D input.</li>
<li>Padding is used to solve the dimensionality reduction problem of the output image.</li>
<li>Stride determines the movement of window. The stride we use above is $1\times1$.</li>
<li>The number of input channels is not a hyperparameter of the convolutional layer but it determines the size of input.</li>
<li>The number of output channels determines the number of images the convolutional layer outputs.</li>
</ol>
<h2 id="Kernel-size"><a href="#Kernel-size" class="headerlink" title="Kernel size"></a>Kernel size</h2><p>It is recommended to adopt a small kernel size and deep neural networks instead of a large kernel size and shallow neural networks. Their results are almost equivalent but the speed of former is faster as the speed is inversely related to the number of data (For kernel, this is generally on the order of the square). That&#39;s why we always make its size 3, 5 instead of 11, 13.</p>
<blockquote>
<p>The size here refers to the length of every dimension.</p>
</blockquote>
<p>Such a structure is very reasonable as what the kernel does is to gather the features of the edge points together. Therefore, in the final layer, data that the kernel see is the linear combination of all pixels in the image. Namely, its kernel size is actually the whole image:</p>
<p><img src="/2023/05/10/CNN/5_1.png" alt="5_1"></p>
<center style="font-size:12px;font-weight:bold">Fig. 6. Gathering of data</center>

<h2 id="Padding"><a href="#Padding" class="headerlink" title="Padding"></a>Padding</h2><p>Padding adds extra rows and columns around the input images:</p>
<p><img src="/2023/05/10/CNN/6.png" alt="6"></p>
<center style="font-size:12px;font-weight:bold">Fig. 7. Padding</center><br>

<p>After padding, the shape of output becomes:</p>
<p>$$(n_h-k_h+1+p_h)\times(n_w-k_w+1+p_w)$$</p>
<p>Generally, we let:</p>
<p>$$<br>p_h&#x3D;k_h-1,\space p_w&#x3D;k_w-1<br>$$</p>
<p>so that the shape of output is the same as the shape of input. For convinience, we often take $k_h\And k_w$ to be odd number. Therefore, we can pad $p_h&#x2F;2$ ($p_w&#x2F;2$)<br> on both sides. If $k_h\And k_w$ are even, $\lceil p_h&#x2F;2\rceil$ on the upper side and $\lfloor p_h&#x2F;2\rfloor$ on the lower side or vice versa.</p>
<h2 id="Stride"><a href="#Stride" class="headerlink" title="Stride"></a>Stride</h2><p>Stirde refers to the step size of the kernel window on the row and column. When stride is too small, the agent will need a amount of computation to get a small ouput.</p>
<p><img src="/2023/05/10/CNN/7.png" alt="7"></p>
<center style="font-size:12px;font-weight:bold">Fig. 8. Stride (3*2)</center><br>

<p>For a certain stride $s_h\times s_w$, the shape of output becomes:</p>
<p>$$\lfloor(n_h-k_h+p_h)&#x2F;s_h+1\rfloor\times\lfloor(n_w-k_w+p_w)&#x2F;s_w+1\rfloor<br>$$</p>
<p>If:</p>
<p>$$<br>p_h&#x3D;k_h-1,\space p_w&#x3D;k_w-1<br>$$</p>
<p>The shape becomes:</p>
<p>$$\lfloor(n_h-1)&#x2F;s_h+1\rfloor\times\lfloor(n_w-1)&#x2F;s_w+1\rfloor<br>$$</p>
<blockquote>
<p>We can roughly regard that the stride will cut the shape of output by a factor of $s_h\And s_w$.</p>
</blockquote>
<h2 id="Multiple-input-channels"><a href="#Multiple-input-channels" class="headerlink" title="Multiple input channels"></a>Multiple input channels</h2><p>The role of multiple input channels is to combine different features together.</p>
<p><img src="/2023/05/10/CNN/8.png" alt="8"></p>
<center style="font-size:12px;font-weight:bold">Fig. 9. Multiple input channels</center>

<ul>
<li>$X$: $c_i\times n_h\times n_W$</li>
<li>$W$: $c_i\times k_h\times k_w$</li>
<li>$Y$: $m_h\times m_w$</li>
</ul>
<p>For each input channel, there is a kernel. The output of the multiple input channels is the <strong>weighted</strong> sum of the output of each channel. Namely, even though there are several input channels, they just generate one output. To a certain extent, a kernel can be regarded as one of the generalized $w$ of a neuron and the multiple input channels are the model parameters of a neuron.</p>
<h2 id="Multiple-output-channels"><a href="#Multiple-output-channels" class="headerlink" title="Multiple output channels"></a>Multiple output channels</h2><p>An output channel generate an output. It contains an independent batch of kernels which form a 3-D kernel. In other words, the multiple input channels are parts of an output channel. That&#39;s why the input channel is not a hyperparameter. Its size totally depends on the data. For examples, the number of input channels should 3 for RGB images and 1 for grayscale images.</p>
<p><img src="/2023/05/10/CNN/9.png" alt="9"></p>
<center style="font-size:12px;font-weight:bold">Fig. 10. Multiple input and output channels</center>

<ul>
<li>$X$: $c_i\times n_h\times n_W$</li>
<li>$W$: $c_o\times c_i\times k_h\times k_w$</li>
<li>$Y$: $c_o\times m_h\times m_w$</li>
<li>$B$ (bias): $c_o\times c_i$, one kernel one bias.</li>
</ul>
<blockquote>
<p>The computational complexity is roughly $O(c_ik_hk_wc_om_hm_w)$, where $c_ik_hk_w$ is the complexity of computing an output pixel and $c_om_hm_w$ is the number of output pixels.</p>
</blockquote>
<h3 id="1-times-1-kernel"><a href="#1-times-1-kernel" class="headerlink" title="$1\times 1$ kernel"></a>$1\times 1$ kernel</h3><p>The $1\times 1$ kernel is a special kernel which doesn&#39;t recognize spatial patterns but just fuses channels. It can be regarded as a dense layer with input $n_hn_w\times c_i$ ($c_i$ is the number of features) and weight $c_ic_o$.</p>
<p><img src="/2023/05/10/CNN/10.png" alt="10"></p>
<center style="font-size:12px;font-weight:bold">Fig. 11. 1x1 kernel</center>

<h1 id="Pooling-layer"><a href="#Pooling-layer" class="headerlink" title="Pooling layer"></a>Pooling layer</h1><p>The pooling layer is another type of layer in CNN. It has two functions:</p>
<ol>
<li>Reduce the sensitivity of convolutional layers to position;</li>
<li>Reduce the computational complexity.</li>
</ol>
<h2 id="Sensitivity-to-position"><a href="#Sensitivity-to-position" class="headerlink" title="Sensitivity to position"></a>Sensitivity to position</h2><p>The convolutional layer is highly sensitive to the position of pixels. This makes a pixel offset will cause a large change in the output image. However, we may not want such a significant change. Therefore, we need pooling layer to reduce these effects.</p>
<p>The hyperparameters of the pooling layer are almost the same as the convolutional layer. But, the pooling layer is just an operator, that is, it doesn&#39;t have any model parameters to learn. Besides, it doesn&#39;t fuse channels. Therefore, the number of input channels is equal to output channels in the pooling layer.</p>
<p>There are generally two types of pooling layer:</p>
<ol>
<li>Maximum pooling <code>nn.MaxPool2d</code>, computes the maximum value of all elements in the pooling window;</li>
<li>Average pooling <code>nn.AvgPool2d</code>, computes the average value of all elements in the pooling window.</li>
</ol>
<h2 id="Computational-complexity"><a href="#Computational-complexity" class="headerlink" title="Computational complexity"></a>Computational complexity</h2><p>Since the pooling layer windows are the same as kernels in convolutional layer, the pooling layer can reduce the dimension of data. </p>
<p>However, because of the increase in data diversities and its consistency with the convolutional layer in dimensionality reduction, the pooling layer are no longer so important.</p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul>
<li><a href="https://www.bilibili.com/video/BV1VV411478E">从“卷积”、到“图像卷积操作”、再到“卷积神经网络”，“卷积”意义的3次改变</a></li>
</ul>
]]></content>
      <categories>
        <category>Dive Into Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title>Discrete Fourier Transform</title>
    <url>/2024/09/29/DFT/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="Continuous-Fourier-Transform"><a href="#Continuous-Fourier-Transform" class="headerlink" title="Continuous Fourier Transform"></a>Continuous Fourier Transform</h1><p>See <a href="/2023/05/10/FourierTransform/">Fourier Transform</a>.</p>
<p>In brief:</p>
<p>$$f(t)&#x3D;\sum\limits_{n&#x3D;-\infty} ^{+\infty}C_ne^{in\omega_0 t}<br> &#x3D;\sum\limits_{n&#x3D;-\infty} ^{+\infty}e^{in\omega_0 t}\frac{1}{T}\int_{0}^T f(t)e^{-in\omega_0 t}dt$$</p>
<p>where $F(\omega)&#x3D; C _n&#x3D;\frac{1}{T}\int _{0} ^T f(t)e ^{-in\omega _0 t}dt$ is Fourier Transform.</p>
<h1 id="Discrete-Fourier-Transform"><a href="#Discrete-Fourier-Transform" class="headerlink" title="Discrete Fourier Transform"></a>Discrete Fourier Transform</h1><p>For $N$ discrete signals sampled at equal intervals in a continuous signal, denoted as $\mathbf{X}&#x3D;\{x _0, x _1,\cdots, x _{N-1}\}$, assume the sampling interval is $\Delta t&#x3D;0$ and consider the original sequence has a period $T$. Then, the continuous fourier transform could be converted to DFT as :</p>
<p>$$<br>\begin{align*}<br> F _k &amp;&#x3D; \frac{1}{N\Delta t}\sum\limits _{n&#x3D;0} ^{N-1}x _{n\Delta t}\cdot e ^{-i(n\omega _0)k\Delta t}\\<br> &amp;&#x3D;\frac{1}{N}\sum\limits _{n&#x3D;0} ^{N-1}x _n\cdot e ^{-i(n2\pi &#x2F; T)k}\\<br> &amp;&#x3D;\frac{1}{N}\sum\limits _{n&#x3D;0} ^{N-1}x _n\cdot e ^{-i(n2\pi &#x2F; (N\Delta t))k}\\<br> &amp;&#x3D;\frac{1}{N}\sum\limits _{n&#x3D;0} ^{N-1}x _n\cdot e ^{-i2\pi \frac{k}{N}n}<br>\end{align*}<br>$$<br>However, $\frac{1}{N}$ will make the value too small, so we always remove this term (actually it is moved to the <strong>inverse transform</strong>):</p>
<p>$$<br>\begin{align*}<br> F _k &#x3D;\sum\limits _{n&#x3D;0} ^{N-1}x _n\cdot e ^{-i2\pi \frac{k}{N}n}<br>\end{align*}<br>$$</p>
<p>Given a certain $k$, $ e ^{-i2\pi \frac{k}{N}n}$ could be treated as a signal in a certain frequency ($k&#x2F;N$). Then, $\sum\limits _{n&#x3D;0} ^{N-1}x _n\cdot e ^{-i2\pi \frac{k}{N}n}$ could be treated as calculating the <strong>similarity</strong> of a signal sequence $\{x _0, x _1,\cdots, x _{N-1}\}$ and $\{e ^{-i2\pi \frac{k}{N}\cdot0}, e ^{-i2\pi \frac{k}{N}\cdot1},\cdots, e ^{-i2\pi \frac{k}{N}\cdot(N-1)}\}$. Hence, DFT could also be represented in a matrix form:</p>
<p>$$<br>[F _0, F _1, \cdots, F _{N-1}]&#x3D;[x _0, x _1,\cdots, x _{N-1}]\cdot\begin{bmatrix}<br> k&#x3D;0,&amp; k&#x3D;1,&amp; \cdots,&amp; k&#x3D;N-1&amp;\\<br> \vdots,&amp; \vdots,&amp; \ddots,&amp; \vdots&amp;<br>\end{bmatrix}<br>$$</p>
<blockquote>
<p>Fast Fourier Transform (FFT), a method to calculate DFT efficiently,  is based on the above formula.</p>
</blockquote>
<p>If $k$ is expressed in Hz, each $k$ represents $\frac{kf _s}{N}$Hz. $\frac{f _s}{N}$ is the <strong>Frequency Resolution</strong>.</p>
<h2 id="One-sided-and-Two-sided-FFT"><a href="#One-sided-and-Two-sided-FFT" class="headerlink" title="One-sided and Two-sided FFT"></a>One-sided and Two-sided FFT</h2><p>One-sided FFT (<code>torch.fft.rfft</code>) only considers positive frequency while two-sided FFT (<code>torch.fft.fft</code>) considers both positive and negative frequency. If $x$ is real numbers and $N$ is even, then $|F _k|$ is symmetric about $|F _{N&#x2F;2}|$.</p>
<p>Hence:</p>
<ul>
<li>if $N$ is even, one-sided FFT contain $N&#x2F;2+1$ elements ($k&#x3D;0,\cdots,N&#x2F;2$) where $k&#x3D;N&#x2F;2$ is called <strong>Nyquist Frequency</strong>.</li>
<li>if $N$ is odd, one-sided FFT contain $\lfloor N&#x2F;2\rfloor$ elements ($k&#x3D;0,\cdots,\lfloor N&#x2F;2 \rfloor$-1) because <strong>Nyquist Frequency</strong> is skipped.</li>
</ul>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul>
<li><a href="https://www.bilibili.com/video/BV1rj41157C6">如何快速理解离散傅立叶变换和FFT</a></li>
</ul>
]]></content>
      <categories>
        <category>Math</category>
      </categories>
      <tags>
        <tag>Math</tag>
        <tag>Digital Signal Processing</tag>
      </tags>
  </entry>
  <entry>
    <title>Maximum Mean Discrepancy</title>
    <url>/2023/06/13/MaximumMeanDiscrepancy/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="衡量两个随机变量的差异"><a href="#衡量两个随机变量的差异" class="headerlink" title="衡量两个随机变量的差异"></a>衡量两个随机变量的差异</h1><p>对于两个不同的随机变量$x$和$y$，其概率分布函数分别为$p(x)$和$p(y)$，我们常用<em>矩</em>来衡量其相似性：</p>
<p>$$<br>\mu_n&#x3D; \int x^np(x)dx\tag{1}<br>$$</p>
<p>上述公式为$x$的$n$阶零点矩，其中一阶原点矩是均值、二阶中心（中心即均值）矩是方差。<em>矩</em>可以理解为某一分布的特征。如果两个随机变量的任意阶矩都相同，那么说明这两个分布具有相同的特征，我们就可以说这两个分布是一致的。而当两个分布不相同时，那么使得两个分布差异最大的矩就应该作为衡量两个分布差异性的标准，这也正是<em>最大均值差异</em>（Maximum mean discrepancy, MMD）的基本思想。</p>
<blockquote>
<p><em>矩</em>与傅里叶变换密不可分，它其实就是抽象的频谱。不同的矩组成了希尔伯特空间的一组正交基。这些无穷维的正交基的线性组合组成了希尔伯特空间的一个无穷维向量：<br>$$<br>\vec{v}&#x3D;v_1\vec{e_1}+v_2\vec{e_2}+...<br>$$<br>其系数$(v_1,v_2,...)$在概率论中就是矩。</p>
</blockquote>
<h1 id="随机变量的任意阶矩"><a href="#随机变量的任意阶矩" class="headerlink" title="随机变量的任意阶矩"></a>随机变量的任意阶矩</h1><p>此处的任意阶矩包括原点矩、中心矩、标准矩等。观察式$(1)$，不难看出，一个随机变量的任意阶矩，实际上就是该随机变量经过某个函数$f(x)$映射后的期望：</p>
<p>$$<br>\mu_{\text{random}}&#x3D;\int f(x)p(x)dx<br>$$</p>
<h1 id="最大均值差异"><a href="#最大均值差异" class="headerlink" title="最大均值差异"></a>最大均值差异</h1><p>于是，对于服从分布$p$的$x$，服从分布$q$的$y$，最大均值差异将两个分布的差异定义为：</p>
<p>$$<br>\text{MMD}(q,p,\mathcal{H})&#x3D;\sup _{f\in\mathcal{H},||f||_\mathcal{H}\le1}(\text{E}_q[f(x)]-\text{E}_p[f(y)])\tag{2}<br>$$</p>
<p>式中，$\sup$表求上界，$\mathcal{H}$表再生希尔伯特空间，$f$为再生希尔伯特空间的任意映射函数，且范数小于等于1（若不加限制，则总能找到一个$f$使得MMD无限大）。</p>
<p>在再生希尔伯特空间中，函数值$f(x)$可以表示为希尔伯特空间中的函数向量$f$和该空间中的向量$\phi(x)$的点积：</p>
<p>$$<br>f(x)&#x3D;&lt;f,\phi(x)&gt;_\mathcal{H}<br>$$</p>
<p>所以，期望也可以写成点积的形式：</p>
<p>$$<br>\text{E}_q[f(x)]&#x3D;&lt;f,\text{E}_q[\phi(x)]&gt;_\mathcal{H}&#x3D;&lt;f,\mu _q&gt;_\mathcal{H}<br>$$</p>
<p>此时，式$(2)$可转变为：</p>
<p>$$<br>\begin{align*}<br>    \text{MMD}(q,p,\mathcal{H})<br>    &amp;&#x3D;\sup _{f\in\mathcal{H},||f||_\mathcal{H}\le1}(\text{E}_q[f(x)]-\text{E}_p[f(y)])\\<br>    &amp;&#x3D;\sup _{f\in\mathcal{H},||f||_\mathcal{H}\le1}&lt;f,\mu _q-\mu _p&gt;\\<br>    &amp;&#x3D;\sup _{f\in\mathcal{H},||f||_\mathcal{H}\le1}||f||\space||\mu _q-\mu _p||\space \cos&lt;f,\mu _q-\mu _p&gt;\\<br>    &amp;&#x3D;||\mu _q-\mu _p||<br>\end{align*}<br>$$</p>
<p>最后一个等号成立因为$||f||\le1$且$\cos x\le1$。$\mu _q$和$\mu _p$是期望，可以用均值估计：</p>
<p>$$<br>\begin{align*}<br>    \mu _q&amp;&#x3D;\frac{1}{m}\sum\limits _{i&#x3D;1} ^m\phi(x_i)\\<br>    \mu _p&amp;&#x3D;\frac{1}{n}\sum\limits _{j&#x3D;1} ^n\phi(y_j)<br>\end{align*}<br>$$</p>
<blockquote>
<p>一般地，我们会取MMD的平方，以保证值为非负值。</p>
</blockquote>
<p>也可以用核方法计算。</p>
<h1 id="核方法"><a href="#核方法" class="headerlink" title="核方法"></a>核方法</h1><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://zhuanlan.zhihu.com/p/163839117">统计知识（一）MMD Maximum Mean Discrepancy 最大均值差异</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/471732960">Maximum Mean Discrepancy（MMD）最大均值差异</a></li>
<li><a href="https://www.zhihu.com/question/288185961">怎样理解MMD（Maximum Mean Discrepancy），它的平方怎样定义？</a></li>
</ul>
]]></content>
      <categories>
        <category>Math</category>
      </categories>
      <tags>
        <tag>Math</tag>
      </tags>
  </entry>
  <entry>
    <title>Fourier Transform and CNN</title>
    <url>/2023/05/10/FourierTransformAndCNN/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="傅里叶变换与坐标变换"><a href="#傅里叶变换与坐标变换" class="headerlink" title="傅里叶变换与坐标变换"></a>傅里叶变换与坐标变换</h1><p>二维坐标系下的一条曲线$f(t)$，可以用无穷个坐标$(t_i,y_i)$表示。若将无穷个纵坐标融合为一个无穷维的坐标对$(y_1,...,y_n)$，那么二维下的一条曲线，便可以表示为无穷维度下的一个点$(y_1,...,y_n)$。更进一步地，可以将这个点视为一个向量$\vec{f}$。</p>
<blockquote>
<p>横坐标$t$为基向量。</p>
</blockquote>
<p>向量各分量的值是与空间的基向量直接相关的，其在基向量$\vec{d} _n$上投影的值实际上就是其在相应基向量上的坐标值：</p>
<p>$$<br>u_n&#x3D;|\vec{f}|\cos\theta_n<br>$$</p>
<p>不过，更为有用的是投影向量，因此，一般再将上式乘以基向量得到投影向量：</p>
<p>$$<br>\begin{align*}<br>    \vec{u} _n<br>    &amp;&#x3D;|\vec{f}|\vec{d}_n\cos\theta_n\\<br>    &amp;&#x3D;|\vec{f}||\vec{d}_n|\cos\theta _n\frac{\vec{d}_n}{|\vec{d}_n|}\\<br>    &amp;&#x3D;&lt;\vec{f},\vec{d}_n&gt;\cdot\vec{d}_n<br>\end{align*}<br>$$</p>
<blockquote>
<p>$&lt;\vec{f},\vec{d}_n&gt;$是两向量的内积，向量间存在内积的无穷维空间称希尔伯特空间，它是欧几里得空间的拓展。</p>
</blockquote>
<p>向量的内积可以表示为向量坐标的乘积和，于是，上式可变为：</p>
<p>$$<br>\vec{u}_n&#x3D;\int _{-\infty} ^{+\infty}f(t)d_n(t)\text{d}t\cdot\vec{d}_n<br>$$<br>式中，$\int _{-\infty} ^{+\infty}f(t)d_n(t)\text{d}t$是投影长，即特征值；$\vec{d}_n$是一组基向量中的任意一个，代表一种模式或特征。</p>
<p>不同的$(d_1,...,d_n)$代表一组不同的基向量，而上述操作则是对原基向量下的$f(t)$进行了一次坐标变换。若取基向量为$e^{i\omega t}$，则上式的特征值便可转变为一次傅里叶变换：</p>
<p>$$<br>F(\omega)&#x3D;\int _{-\infty} ^{+\infty}f(t)e ^{-i\omega t}\text{d}t&#x3D;&lt;\vec{f},\vec{d}_\omega&gt;<br>$$</p>
<blockquote>
<p>$e ^{i\omega t}$取共轭是因为它是复数。</p>
</blockquote>
<p>将投影向量相加可重新得到原向量，也即傅里叶逆变换：</p>
<p>$$<br>\begin{align*}<br>    \vec{f}<br>    &amp;&#x3D;\sum F(w)\cdot\frac{\vec{d}_{\omega}}{|\vec{d} _{\omega}|}\\<br>    &amp;&#x3D;\frac{1}{2\pi}\int _{-\infty} ^{+\infty}F(\omega)e ^{i\omega t}\text{d}\omega<br>\end{align*}<br>$$</p>
<blockquote>
<p>$|\vec{d} _\omega|&#x3D;|e ^{i\omega t}|&#x3D;\frac{2\pi}{\Delta\omega}$</p>
</blockquote>
<p>不难看出，傅里叶变换实际上是将希尔伯特空间上的向量$f$投影到了新的一组基向量$e ^{i\omega t}$上。其在原基向量上对应的二维曲线即为原曲线，如Fig 1所示。在新的基向量上也存在对应的二维曲线，如Fig 2所示。从Fig 1到Fig 2，相当于将原向量做了如Fig 3所示的旋转。</p>
<p><img src="/2023/05/10/FourierTransformAndCNN/1.png" alt="1"></p>
<center style="font-size:12px; font-weight:bold">Fig. 1. 时域</center><br>

<p><img src="/2023/05/10/FourierTransformAndCNN/2.png" alt="2"></p>
<center style="font-size:12px; font-weight:bold">Fig. 2. 频域</center><br>

<p><img src="/2023/05/10/FourierTransformAndCNN/3.png" alt="3"></p>
<center style="font-size:12px; font-weight:bold">Fig. 3. 坐标变换</center>

<h1 id="加窗傅里叶变换"><a href="#加窗傅里叶变换" class="headerlink" title="加窗傅里叶变换"></a>加窗傅里叶变换</h1><p>傅里叶变换会考虑曲线全局的信息，但是对于卷积神经网络，只考虑局部信息反而能达到更好的效果。傅里叶变换的基向量$e ^{i\omega t}$是希尔伯特空间里的一个向量，因此它也有对应的二维曲线，这实际体现为一个正弦波形：</p>
<p><img src="/2023/05/10/FourierTransformAndCNN/4.png" alt="4"></p>
<center style="font-size:12px; font-weight:bold">Fig. 4. 傅里叶变换基向量</center><br>

<p>加窗傅里叶变换通过抹平某个区域以外的基向量曲线来达到只考虑局部特征的目的：</p>
<p><img src="/2023/05/10/FourierTransformAndCNN/5.png" alt="5"></p>
<center style="font-size:12px; font-weight:bold">Fig. 5. 加窗傅里叶变换</center><br>

<p>$$<br>F(\omega)&#x3D;\int _{-\infty} ^{+\infty}f(t)g(t-s)e ^{-i\omega t}\text{d}t<br>$$</p>
<p>式中$g(t-s)$称窗函数，不同的窗函数可以得到不同的变换、提取不同的特征，$s$表当前窗口的位置，是个变量。</p>
<p>有了窗函数，变换域（频域）的曲线就必须以三维的方式表示。此时，重复的局部信息不会再影响频域轴的曲线，而是在窗口位置轴$s$中体现，如下图Fig 6和Fig 7所示。</p>
<p><img src="/2023/05/10/FourierTransformAndCNN/6.png" alt="6"></p>
<center style="font-size:12px; font-weight:bold">Fig. 6. 时域</center><br>

<p><img src="/2023/05/10/FourierTransformAndCNN/7.png" alt="7"></p>
<center style="font-size:12px; font-weight:bold">Fig. 7. 频域</center>

<h1 id="CNN与加窗傅里叶"><a href="#CNN与加窗傅里叶" class="headerlink" title="CNN与加窗傅里叶"></a>CNN与加窗傅里叶</h1><p>卷积神经网络的卷积实际上就是一种加窗傅里叶变换。原图片相当于$\vec{f}$，卷积过程中像素点的移动相当于窗口$s$的移动，而卷积核的大小、值则共同充当着窗函数和基向量。卷积（实际是加窗傅里叶变换）将像素点周边的局部特征通过像素值的线性组合标定为一种模式。后续的全连接神经网络为不同卷积核识别出的不同模式赋予不同的权重，最终组合出最有可能的模式。</p>
<p><img src="/2023/05/10/FourierTransformAndCNN/8.png" alt="8"></p>
<center style="font-size:12px; font-weight:bold">Fig. 8. CNN</center>]]></content>
      <categories>
        <category>Math</category>
      </categories>
      <tags>
        <tag>CNN</tag>
        <tag>Math</tag>
        <tag>Digital Signal Processing</tag>
      </tags>
  </entry>
  <entry>
    <title>Fourier Transform</title>
    <url>/2023/05/10/FourierTransform/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="Inner-product"><a href="#Inner-product" class="headerlink" title="Inner product"></a>Inner product</h1><p>The inner product generalizes the dot product to abstract vector spaces over a field of scalars, being either the field of real numbers $R$  or the field of complex numbers $C$. It is usually denoted using angular brackets by $\braket{\vec{a}, \vec{b}}$. Each vector in the field of real numbers $R$ can be regarded as a <strong>discrete functions</strong> with domain $\{k\in N:1\le k\le n\}$. And the inner product measures the similarity of two vectors:</p>
<p>$$\vec{a}\cdot\vec{b}&#x3D;\sum\limits_{i&#x3D;0}^na_ib_i\Delta n$$</p>
<p>where $\Delta n&#x3D;1$ , can be regarded as the difference of two dimension. It is easy to generalize this to continuous functions. In this case, the dimension of a vector is <strong>infinity</strong> and the sum will diverge. Therefore, we average each term, that is $dx$:</p>
<p>$$\braket{f,g}&#x3D;\int_a^bf(x)g(x)dx$$</p>
<p>Similarly, the inner product of functions measures the similarity of two functions.</p>
<h1 id="Orthogonality-of-trigonometric-functions"><a href="#Orthogonality-of-trigonometric-functions" class="headerlink" title="Orthogonality of trigonometric functions"></a>Orthogonality of trigonometric functions</h1><p>We define:<br>$$\{\sin0x,\cos0x,\sin x,\cos x,...,\sin nx, \cos nx\}$$<br>as trigonometric space. In this space, the inner product of two different functions with domain $\{x\in R:-\pi\le x\le \pi\}$ is zero, that is, the two functions are orthogonal:</p>
<p>$$\braket{\sin nx, \sin mx}&#x3D;\int_{-\pi}^\pi\sin nx\sin mxdx&#x3D;0$$<br>where $n\ne m$ and either or both of $\sin$ could be $\cos$. If the two functions are the same and are not $0$, their inner product is $\pi$:<br>$$\braket{\sin nx, \sin nx}&#x3D;\int_{-\pi}^\pi\sin nx\sin nxdx&#x3D;\pi$$</p>
<h1 id="Fourier-series"><a href="#Fourier-series" class="headerlink" title="Fourier series"></a>Fourier series</h1><p>Fourier series is another kind of series besides power series. It uses functions in trigonometric space to represent any periodic functions.</p>
<h2 id="T-x3D-2-pi"><a href="#T-x3D-2-pi" class="headerlink" title="$T&#x3D;2\pi$"></a>$T&#x3D;2\pi$</h2><p>For a function of period $2\pi$, $f(x)&#x3D;f(x+2\pi)$, we can represent it using trigonometric functions:</p>
<p>$$f(x)&#x3D;\sum\limits_{n&#x3D;0}^\infty a_n\cos nx+\sum\limits_{n&#x3D;0}^\infty b_n\sin nx$$</p>
<p>To get $a_n\space(n\ne0)$ , multiply both sides by $\cos nx$ and integrate at $[-\pi, \pi]$:</p>
<p>$$\int_{-\pi}^\pi f(x)\cos nxdx&#x3D;\sum\limits_{m&#x3D;0}^\infty[\int_{-\pi}^\pi a_m\cos mx\cdot\cos nxdx+b_m\sin mx\cdot\cos nxdx]\tag{1}$$</p>
<p>Because of the orthogonality of trigonometric functions, formula (1) becomes:</p>
<p>$$\int_{-\pi}^\pi f(x)\cos nxdx&#x3D;\int_{-\pi}^\pi a_n\cos nx\cdot\cos nxdx&#x3D;a_n\pi$$</p>
<p>Therefore:</p>
<p>$$a_n&#x3D;\frac{1}{\pi}\int_{-\pi}^\pi f(x)\cos nxdx$$</p>
<p>If $n&#x3D;0$:</p>
<p>$$\int_{-\pi}^\pi f(x)dx&#x3D;a_02\pi\rightarrow a_0&#x3D;\frac{1}{2\pi}\int_{-\pi}^\pi f(x)dx$$</p>
<p>For unity, we usually use $2a_0\space(A_0)$ as $a_0$:</p>
<p>$$A_0&#x3D;2a_0&#x3D;\frac{1}{\pi}\int_{-\pi}^\pi f(x)dx$$</p>
<p>Similarly, if multiply both sides by $\sin nx\space(n\in N)$ and integrate at $[-\pi, \pi]$, we get $b_n$:</p>
<p>$$b_n&#x3D;\frac{1}{\pi}\int_{-\pi}^\pi f(x)\sin nxdx$$</p>
<p>So, the fourier series of $f(x)&#x3D;f(x+2\pi)$ is:</p>
<p>$$f(x)&#x3D;\frac{a_0}{2}+\sum\limits_{n&#x3D;1}^\infty [a_n\cos nx+b_n\sin nx], \begin{cases}<br>   a_0&#x3D;\frac{1}{\pi}\int_{-\pi}^\pi f(x)dx &amp;\\<br>   a_n&#x3D;\frac{1}{\pi}\int_{-\pi}^\pi f(x)\cos nxdx &amp;\\<br>   b_0&#x3D;0 &amp;\\<br>   b_n&#x3D;\frac{1}{\pi}\int_{-\pi}^\pi f(x)\sin nxdx &amp;<br>\end{cases}$$</p>
<h2 id="T-x3D-2L"><a href="#T-x3D-2L" class="headerlink" title="$T&#x3D;2L$"></a>$T&#x3D;2L$</h2><p>More generally, for any function with period $2L$, $f(t)&#x3D;f(t+2L)$, we assume:</p>
<p>$$t&#x3D;\frac{L}{\pi}x$$</p>
<p>Then:</p>
<p>$$f(t)&#x3D;f(\frac{L}{\pi}x)$$</p>
<p>$$f(t+2L)&#x3D;f(\frac{L}{\pi}x+2L)&#x3D;f[\frac{L}{\pi}(x+2\pi)]$$</p>
<p>We make:</p>
<p>$$g(x)&#x3D;f(\frac{L}{\pi}x)$$</p>
<p>Then:</p>
<p>$$g(x)&#x3D;g(x+2\pi)$$</p>
<p>We have already know the fourier series of functions with period $2\pi$ is:</p>
<p>$$g(x)&#x3D;\frac{a_0}{2}+\sum\limits_{n&#x3D;1}^\infty [a_n\cos nx+b_n\sin nx], \begin{cases}<br>   a_0&#x3D;\frac{1}{\pi}\int_{-\pi}^\pi g(x)dx &amp;\\<br>   a_n&#x3D;\frac{1}{\pi}\int_{-\pi}^\pi g(x)\cos nxdx &amp;\\<br>   b_0&#x3D;0 &amp;\\<br>   b_n&#x3D;\frac{1}{\pi}\int_{-\pi}^\pi g(x)\sin nxdx &amp;<br>\end{cases}$$</p>
<p>Since $t&#x3D;\frac{L}{\pi}x$ and $g(x)&#x3D;f(\frac{L}{\pi}x)&#x3D;f(t)$, we can replace $x$ with $t$:</p>
<p>$$dx&#x3D;\frac{\pi}{L}dt$$</p>
<p>$$x\in[-\pi,\pi]\rightarrow t\in[-L,L]$$</p>
<p>Therefore:</p>
<p>$$f(t)&#x3D;\frac{a_0}{2}+\sum\limits_{n&#x3D;1}^\infty [a_n\cos \frac{n\pi<br>}{L}t+b_n\sin \frac{n\pi}{L}t], \begin{cases}<br>   a_0&#x3D;\frac{1}{L}\int_{-L}^L f(t)dt &amp;\\<br>   a_n&#x3D;\frac{1}{L}\int_{-L}^L f(t)\cos \frac{n\pi<br>}{L}tdt &amp;\\<br>   b_0&#x3D;0 &amp;\\<br>   b_n&#x3D;\frac{1}{L}\int_{-L}^L f(t)\sin \frac{n\pi<br>}{L}tdt &amp;<br>\end{cases}$$</p>
<p>Sometimes, we use $T&#x3D;2L$ to represent period and $\omega_0$ to represent fundamental frequency $\frac{2\pi}{T}$. Since the integral of a periodic function over a period is constant, we can also write the fourier series as:</p>
<p>$$f(t)&#x3D;\frac{a_0}{2}+\sum\limits_{n&#x3D;1}^\infty [a_n\cos n\omega_0 t+b_n\sin n\omega_0 t], \begin{cases}<br>   a_0&#x3D;\frac{2}{T}\int_{0}^T f(t)dt &amp;\\<br>   a_n&#x3D;\frac{2}{T}\int_{0}^T f(t)\cos n\omega_0 tdt &amp;\\<br>   b_0&#x3D;0 &amp;\\<br>   b_n&#x3D;\frac{2}{T}\int_{0}^T f(t)\sin n\omega_0 tdt &amp;<br>\end{cases}$$</p>
<h2 id="Complex-form"><a href="#Complex-form" class="headerlink" title="Complex form"></a>Complex form</h2><p>Fourier series can be transformed into complex form using Euler&#39;s formula:</p>
<p>$$e^{i\theta}&#x3D;\cos\theta+i\sin\theta\tag{2}$$<br>$$e^{-i\theta}&#x3D;\cos\theta-i\sin\theta\tag{3}$$</p>
<p>By combining (2) and (3), we can get:</p>
<p>$$\cos\theta&#x3D;\frac{1}{2}(e^{i\theta}+e^{-i\theta})\tag{4}$$<br>$$\sin\theta&#x3D;-\frac{1}{2}i(e^{i\theta}-e^{-i\theta})\tag{5}$$</p>
<p>Put the above two formulas into fourier series, we get:</p>
<p>$$\begin{align*}<br>    f(t) &amp;&#x3D;\frac{a_0}{2}+\sum\limits_{n&#x3D;1}^\infty [\frac{a_n}{2}(e^{in\omega_0 t}+e^{-in\omega_0 t})-\frac{b_n}{2}i(e^{in\omega_0 t}-e^{-in\omega_0 t})]\\<br>         &amp;&#x3D;\frac{a_0}{2}+\sum\limits_{n&#x3D;1}^\infty [\frac{a_n-ib_n}{2}e^{in\omega_0 t}+\frac{a_n+ib_n}{2}e^{-in\omega_0 t}]\\<br>         &amp;&#x3D;\frac{a_0}{2}+\sum\limits_{n&#x3D;1} ^{+\infty} \frac{a_n-ib_n}{2}e^{in\omega_0 t}+\sum\limits_{n&#x3D;-1} ^{-\infty} \frac{a_{-n}+ib_{-n}}{2}e^{in\omega_0 t}\tag{6}\\<br>         &amp;&#x3D;\sum\limits_{n&#x3D;-\infty} ^{+\infty}C_ne^{in\omega_0 t}\tag{7}<br>\end{align*}$$</p>
<p>For (6) to (7), since $e^0&#x3D;1$, we can treat $\frac{a_0}{2}$ as $C_0$. Therefore:</p>
<p>$$<br>C_n&#x3D;<br>\begin{cases}<br>    \frac{1}{2}a_0&#x3D;\frac{1}{T}\int_{0}^T f(t)dt,&amp;n&#x3D;0\\<br>    \frac{1}{2}(a_n-ib_n)&#x3D;\frac{1}{T}\int_{0}^T f(t)(\cos n\omega_0 t-i\sin n\omega_0 t)dt&#x3D;\frac{1}{T}\int_{0}^T f(t)e^{-in\omega_0 t}dt,&amp;n&#x3D;1,2,3...\\<br>    \frac{1}{2}(a_n+ib_n)&#x3D;\frac{1}{T}\int_{0}^T f(t)(\cos n\omega_0 t+i\sin n\omega_0 t)dt&#x3D;\frac{1}{T}\int_{0}^T f(t)e^{in\omega_0 t}dt,&amp;n&#x3D;-1,-2,-3...<br>\end{cases}<br>$$<br>Since $e^0&#x3D;1$:</p>
<p>$$C_n&#x3D;\frac{1}{T}\int_{0}^T f(t)e^{-in\omega_0 t}dt,n\in Z$$</p>
<p>Therefore:</p>
<p>$$f(t)&#x3D;\sum\limits_{n&#x3D;-\infty} ^{+\infty}C_ne^{in\omega_0 t}<br>      &#x3D;\sum\limits_{n&#x3D;-\infty} ^{+\infty}e^{in\omega_0 t}\frac{1}{T}\int_{0}^T f(t)e^{-in\omega_0 t}dt$$</p>
<h1 id="Fourier-transform"><a href="#Fourier-transform" class="headerlink" title="Fourier transform"></a>Fourier transform</h1><p>In the complex form of fourier series, it is $C_n$ that defines the form of $f(t)$ in frequency domain, where $C_n&#x3D;A_n+iB_n$. Based on the difference of frequency $n\omega_0$, the curve of $f(t)$ in the time domain $[0,T]$ is transformed into a point in frequency domain:</p>
<p><img src="/2023/05/10/FourierTransform/1.png" alt="1"></p>
<center style="font-size:12px;font-weight:bold">Fig. 1. Cn in the complex domain</center><br>

<p>If function $f(t)$ is not a periodic function, that is, its period is $+\infty$, fourier series becomes fuorier transform:</p>
<p>$$T\to+\infty,\space\frac{1}{T}&#x3D;[(n+1)\omega_0-n\omega_0]\frac{1}{2\pi}&#x3D;\frac{\Delta\omega}{2\pi}&#x3D;\frac{\omega_0}{2\pi}\to0$$</p>
<p>Since the difference between $(n+1)\omega_0$ and $n\omega_0$ is rather small, that is, the fundamental frequency $\omega_0$ is rather small, we can turn the discrete $n\omega_0$ into continuous $\omega$. Then:</p>
<p>$$\Delta\omega&#x3D;d\omega\to0, \space n\omega_0\to\omega$$<br>$$<br>\begin{align*}<br>f(t)<br>  &amp;&#x3D;\sum\limits_{n&#x3D;-\infty} ^{+\infty}e^{in\omega_0 t}\frac{1}{T}\int_{0}^T f(t)e^{-in\omega_0 t}dt\\<br>  &amp;&#x3D;\sum\limits_{n&#x3D;-\infty} ^{+\infty}e^{in\omega_0 t}\frac{1}{T}\int_{-\frac{T}{2}}^\frac{T}{2} f(t)e^{-in\omega_0 t}dt\\<br>  &amp;&#x3D;\sum\limits_{n\omega_0&#x3D;-\infty} ^{+\infty}e^{in\omega_0 t}\frac{\Delta\omega}{2\pi}\int_{-\frac{T}{2}}^\frac{T}{2} f(t)e^{-in\omega_0 t}dt\\<br>  &amp;&#x3D;\frac{1}{2\pi}\int_{-\infty} ^{+\infty}\int_{-\infty} ^{+\infty}f(t)e^{-i\omega t}dt\space e^{i\omega t}d\omega<br>\end{align*}<br>$$<br>Among the formula above:</p>
<p>$$\int_{-\infty} ^{+\infty}f(t)e^{-i\omega t}dt&#x3D;F(\omega)$$</p>
<p>is called <strong>Fourier Transform</strong> and the whole formula is called <strong>Inverse Fourier Transform</strong>. It&#39;s easy for us to understand that $F(\omega)$ is $\frac{1}{2\pi} C_n$ and it is a complex number in frequency domain. More generally, we don&#39;t use the form in Fig. 1. to represent $F(\omega)$, instead, we draw the curve of <em>Amplitude</em>-$\omega$:</p>
<p>$$F(\omega)&#x3D;|F(\omega)|e^{i\phi}$$</p>
<p>It is a continuous curve in frequency domain:</p>
<p><img src="/2023/05/10/FourierTransform/2.png" alt="2"></p>
<center style="font-size:12px;font-weight:bold">Fig. 2. Amplitude-omega</center><br>

<p>$e^{i\omega t}$ represents different waveforms while $F(w)$ represents the intensity of different waveforms in the original function $f(t)$. Therefore, we can restore the original function by superimposing waveforms of different intensities. And this is <strong>Inverse Fourier Transform</strong>.</p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul>
<li><a href="https://www.bilibili.com/video/BV1Et411R78v">纯干货数学推导_傅里叶级数与傅里叶变换</a></li>
<li><a href="https://www.bilibili.com/video/BV1ce4y1p7jF">卷积神经网络的底层是傅里叶变换，傅里叶变换的底层是希尔伯特空间坐标变换</a></li>
</ul>
]]></content>
      <categories>
        <category>Math</category>
      </categories>
      <tags>
        <tag>CNN</tag>
        <tag>Math</tag>
        <tag>Digital Signal Processing</tag>
      </tags>
  </entry>
  <entry>
    <title>Numerical Stability</title>
    <url>/2023/05/07/NumericalStability/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="Numerical-instability"><a href="#Numerical-instability" class="headerlink" title="Numerical instability"></a>Numerical instability</h1><p>A numerically instable neural network is often caused by exploding or vanishing gradients, which make the model diverge or converge slowly. According to chain rules, for a neural network with $d$ layers, the gradient of layer d with respect to parameters of layer t is:</p>
<p>$$\frac{\partial{h^d}}{\partial{h^t}}&#x3D;\prod_{i&#x3D;t}^{d-1}\frac{\partial{h^{i+1}}}{\partial{h^i}}&#x3D;\prod_{i&#x3D;t}^{d-1}diag(\sigma&#39;(W^ih^{i-1}))W^i$$</p>
<p>where $h^{i}&#x3D;\sigma(W^ih^{i-1})$, $h^i$ is the input of layer i+1 and the output of layer i.</p>
<h2 id="Exploding-gradients"><a href="#Exploding-gradients" class="headerlink" title="Exploding gradients"></a>Exploding gradients</h2><p>If we use ReLU as the activation:</p>
<p>$$\sigma(x)&#x3D;\max(0,x)$$</p>
<p>Then, the element of $diag(\sigma&#39;(W^ih^{i-1}))$ will only be 1 or 0. Therefore, it is possible that the gradients will become:</p>
<p>$$\prod_{i&#x3D;t}^{d-1}W^i$$</p>
<p>which will make the gradients rather large if $d-t$ is large and cause some serious problems:</p>
<ul>
<li>Gradients are <code>nan</code> or <code>inf</code>;</li>
<li>The model is very sensitive to the learning rate $\alpha$, which makes the training harder.</li>
</ul>
<h2 id="Vanishing-gradients"><a href="#Vanishing-gradients" class="headerlink" title="Vanishing gradients"></a>Vanishing gradients</h2><p>If we use sigmoid as the activation, since the gradient of sigmoid becomes smooth when $|x|$ is larger than 4, it is likely that the gradients are rather small, even zero:</p>
<ul>
<li>Floating point underflow;</li>
<li>No progress in training, especially for the bottom layers because $d-t$ is large.</li>
</ul>
<h1 id="Mul-to-add"><a href="#Mul-to-add" class="headerlink" title="Mul to add"></a>Mul to add</h1><h2 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h2><p>See <a href="/2023/05/14/CommonCNNModels/#ResNet">ResNet</a> to know more about it.</p>
<h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><p>See <a href="/2023/05/29/CommonRNNModels/#LSTM">LSTM</a> to know more about it.</p>
<h1 id="Normalization"><a href="#Normalization" class="headerlink" title="Normalization"></a>Normalization</h1><h2 id="Gradient-normalization"><a href="#Gradient-normalization" class="headerlink" title="Gradient normalization"></a>Gradient normalization</h2><h2 id="Gradient-clipping"><a href="#Gradient-clipping" class="headerlink" title="Gradient clipping"></a>Gradient clipping</h2><p>See <a href="/2023/05/25/RNN/#Gradient-clipping">Gradient clipping</a> to know more about it.</p>
<h1 id="Model-parameter-and-activation"><a href="#Model-parameter-and-activation" class="headerlink" title="Model parameter and activation"></a>Model parameter and activation</h1><p>Another method to increase numerical stability is to keep the expectation and variance of the outputs and inputs of each layer the same:</p>
<p>$$E(h_i^t)&#x3D;0\And Var(h_i^t)&#x3D;a$$</p>
<p>and the expectation and variance of the gradients of each layer the same:</p>
<p>$$E(\frac{\partial{\ell}}{\partial{h_i^t}})&#x3D;0\And Var(\frac{\partial{\ell}}{\partial{h_i^t}})&#x3D;b$$</p>
<p>We can realize this by choosing an appropriate initail value of $w$ ($b$ is offset so we can ignore it) and a proper activation.</p>
<h2 id="Parameter-initialization"><a href="#Parameter-initialization" class="headerlink" title="Parameter initialization"></a>Parameter initialization</h2><p>For model parameters of layer t with $E&#x3D;0\And Var&#x3D;\gamma_t$, we should keep $n_{t-1}\gamma_t&#x3D;1$ to achieve the mean-variance requirements of forward propagation (inputs and outputs) and keep $n_t\gamma_t&#x3D;1$ to achieve the mean-variance requirements of backward propagation (gradients), where $n_t$ is the number of neurons of layer t.</p>
<p>We cannot satisfy both conditions so we often use <strong>Xavier</strong> to initialize $w$:</p>
<p>$$\frac{\gamma_t(n_{t-1}+n_t)}2&#x3D;1\rightarrow\gamma_t&#x3D;\frac{2}{(n_{t-1}+n_t)}$$</p>
<p>Some examples:</p>
<ul>
<li>$N(0,\sqrt{2&#x2F;(n_{t-1}+n_t)})$;</li>
<li>$U(-\sqrt{6&#x2F;(n_{t-1}+n_t)},\sqrt{6&#x2F;(n_{t-1}+n_t)})$</li>
</ul>
<h2 id="Activation-choosing"><a href="#Activation-choosing" class="headerlink" title="Activation choosing"></a>Activation choosing</h2><p>The activation should be $y&#x3D;x$ when it is around (0,0):</p>
<ul>
<li>$sigmoid(x)&#x3D;\frac{1}{2}+\frac{x}{4}+O(x^3)$</li>
<li>$tanh(x)&#x3D;x+O(x^3)$</li>
<li>$relu(x)&#x3D;x$ for $x\ge0$</li>
</ul>
<p>Therefore, we often use relu or tanh or 4$\times$sigmoid(x)-2</p>
]]></content>
      <categories>
        <category>Dive Into Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Model Selection &amp; Hyperparameter Optimization</title>
    <url>/2023/05/03/ModelSelection/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="Evaluating-a-model"><a href="#Evaluating-a-model" class="headerlink" title="Evaluating a model"></a>Evaluating a model</h1><p>See <a href="/2023/04/13/MachineLearningDiagnostics/">Machine learning diagnostics</a> to know the basic methods of evaluating a model.</p>
<p>When evaluating a model, we sometimes may not have enough data for cross validation. The method we use to solve this problem is <strong>K-fold Cross Validation</strong>.</p>
<h2 id="K-fold-Cross-Validation"><a href="#K-fold-Cross-Validation" class="headerlink" title="K-fold Cross Validation"></a>K-fold Cross Validation</h2><p>K-fold Cross Validation is a technique used for hyperparameter tuning such that the model with the most optimal value of hyperparameters can be trained in a rather small data set. The advantage of this approach is that each example is used for validation exactly once in each test fold and all examples will be used for validation finally. The following are steps in K-fold Cross Validation:</p>
<ol>
<li>Split the dataset into training set and test set;</li>
<li>Split training set into K-folds;</li>
<li>Choose K-1 folds for training and 1 fold for validation;</li>
<li>Train the model with specific hyperparameters in K-1 folds and validate it using the 1 fold;</li>
<li>Record its performance;</li>
<li>Choose another K-1 folds for training and 1 fold for validation;</li>
<li>Repeat 4, 5, 6 until each of the K-fold get used for validation purpose;</li>
<li>Evaluate the mean performance of K models;</li>
<li>Step 3-8 for different hyperparameters; (optional)</li>
<li>Choose the hyperparameters which result in best performance; (optional)</li>
<li>Train the model using the whole training set.</li>
</ol>
<p><img src="/2023/05/03/ModelSelection/1.png" alt="1"></p>
<center style="font-size:12px;font-weight:bold">Fig. 1. K-fold Cross Validation</center><br>

<blockquote>
<p>In general, we seldom use K-fold since it is time-consuming. We often set K 5 or 10.</p>
</blockquote>
<ul>
<li><a href="https://vitalflux.com/k-fold-cross-validation-python-example/">K-Fold Cross Validation – Python Example</a></li>
</ul>
<h1 id="Model-capacity-and-data"><a href="#Model-capacity-and-data" class="headerlink" title="Model capacity and data"></a>Model capacity and data</h1><p>The <strong>model capacity</strong> is a model&#39;s ability to fit the function. It&#39;s very close for model complexity. High model capacity always means a deeper and larger neural network or more complex polynomials. Data or the complexity of data is determined by the size of dataset, the features number of each data, the diversity of samples, etc. Model capacity and data together determine the performance of a model:</p>
<p><img src="/2023/05/03/ModelSelection/2.png" alt="2"></p>
<center style="font-size:12px;font-weight:bold">Fig. 2. Model capacity and data</center><br>

<p>For neural networks, a simple way to evaluate model capacity is to compute the number of model parameters that the neural network can learn, or to evaluate the range of each parameter (w, b):</p>
<p><img src="/2023/05/03/ModelSelection/3.png" alt="3"></p>
<center style="font-size:12px;font-weight:bold">Fig. 3. Evaluating model capacity</center>

<h2 id="VC-dimension"><a href="#VC-dimension" class="headerlink" title="VC dimension*"></a>VC dimension*</h2><p>The VC dimension of a model is the maximum number of points that can be arranged so that shatters them. More formally, it is the maximum cardinal such that there exists a generally positioned data point set of cardinality can be shattered by. -- From Google</p>
<p>For a perceptron with inputs of N dimensions, its VC dimension is $N+1$ or $O(N\log_2N)$, that is, it can classify $N+1$ ($O(N\log_2N)$) points. VC dimension can be used to evaluate the model capacity though it doesn&#39;t work well.</p>
<h1 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h1><p>Dropout is another method of regularization besides <a href="/2023/04/05/SupervisedLearning/#Weight-decay">weight decay</a>. The motivation of dropout is that a good model should be robust to input perturbations. Dropout achieves this by adding noise to the ouptout of hidden layers:<br>$$<br>\text{dropout}[x]&#x3D;x&#39;<br>$$<br>where:<br>$$<br>x_i&#39;&#x3D;<br>\begin{cases}<br>    0, &amp;p \\<br>    \frac{x_i}{1-p}, &amp;1-p<br>\end{cases}<br>$$<br>$p$ is a hyperparameter that controls the probability of dropout. Usually, we set $p$ to 0.1, 0.5 or 0.9. The meaning of the formula above is that dropout layer randomly sets the output of the neurons in a hidden layer to 0 (with probability $p$) or $\frac{1}{1-p}$ times as much as the origin value (with probability $1-p$). This works because when one value is set to 0, it is equivalent to remove a neuron from the hidden layer, which makes the neural network a sub neural network. Since dropout is totally random, we are actually training a model using multiple neural networks, which will make the model more robust.</p>
<p><img src="/2023/05/03/ModelSelection/4.png" alt="4"></p>
<center style="font-size:12px;font-weight:bold">Fig. 4. Dropout</center><br>

<blockquote>
<p>To ensure the accuracy, we must make sure that the expectation of dropout is equivalent to its original value:<br>$$\text{E}[x&#39;]&#x3D;x$$</p>
</blockquote>
<p>Since dropout is a kind of regularization, it only works when training models. When making predictions, we don&#39;t use dropout layer, like weight decay.</p>
<blockquote>
<p>In general, we use dropout in the dense layer of MLP.</p>
</blockquote>
]]></content>
      <categories>
        <category>Dive Into Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>D2L: Environment Configuration and Introduction</title>
    <url>/2023/04/28/DiveIntoDeepLearningIntroduction/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="Environment-configuration"><a href="#Environment-configuration" class="headerlink" title="Environment configuration"></a>Environment configuration</h1><p>Firstly, install <code>anaconda</code> or <code>miniconda</code>. Then, create a virtual environment:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">conda create -n d2l-zh -y python=3.8 pip</span><br><span class="line">conda actiave d2l-zh <span class="comment"># activate the virtual environment</span></span><br></pre></td></tr></table></figure>
<p>Install dependent packages:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pip install jupyter d2l torch torchvision</span><br></pre></td></tr></table></figure>
<p>If you want to remove a virtual environment, use:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">conda remove -n d2l-zh --all</span><br></pre></td></tr></table></figure>
<p>Download the courseware from <a href="http://zh-v2.d2l.ai/d2l-zh.zip">http://zh-v2.d2l.ai/d2l-zh.zip</a> and unzip it. Now you can enter your conda prompt, activate virtual environment <code>d2l-zh</code>, enter your courseware folder <code>d2l-zh</code> and enter <code>jupyter notebook</code> to open the folder with jupyter notebook.</p>
<blockquote>
<p><code>gym</code> and <code>scipy</code> should also be installed using <code>pip</code>.</p>
</blockquote>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>This only includes what I&#39;m interested in:</p>
<ul>
<li>Convolutional neural network (CNN): LeNet, AlexNet, VGG, Inception, ResNet</li>
<li>Recurrent neural network (RNN): RNN, GRU, LSTM, seq2seq</li>
<li>Attention mechanism: Attention, Transformet</li>
<li>Optimization: SGD, Momentum, Adam</li>
<li>High performance computing</li>
</ul>
<h1 id="Derivative-of-matrix"><a href="#Derivative-of-matrix" class="headerlink" title="Derivative of matrix"></a>Derivative of matrix</h1><p>The essence of matrix derivative: Treat each element of numerator matrix as a function. Get its partial derivatives with respect to each element in denominator matrix. This is the nature of matrix derivative. However, to represent the results better, there are two most commonly used layout:</p>
<ul>
<li>Numerator layout: Preserve the layout of elements in the numerator $Y$. Replace each element with its derivatives with respect to denominator matrix $X^T$. This the more commonly used layout in machine learning.</li>
<li>Denominator layout: Preserve the layout of elements in the denominator $X$. Replace each element with the derivatives of numerator $Y^T$ with respect to it.</li>
</ul>
<p>The numerator layout is the transpose of the denominator layout. For examples, if the dimension of $Y$ is (m, l) and the dimension of $X$ is (n, k). Then, the dimension of $\frac{\partial{Y}}{\partial{X}}$ is (m, l, k, n) in numerator layout and (n, k, l, m) in denominator layout. Usually:</p>
<ul>
<li>If a vector or matrix ($\frac{\partial{Y}}{\partial{x}}$) is differentiated with respect to a scalar, the numerator layout prevails.</li>
<li>If a scalar is derived from a vector or matrix ($\frac{\partial{y}}{\partial{X}}$), the denominator layout prevails.</li>
<li>For vector-to-vector derivatives ($\frac{\partial{Y}}{\partial{X}}$), there are some differences, but generally based on the Jacobian matrix of the <strong>numerator layout</strong>.</li>
</ul>
<p>Some interesting conclusions (numerator layout):</p>
<table>
<thead>
<tr>
<th align="center">$\vec{y}$</th>
<th align="center">$a$</th>
<th align="center">$\vec{x}$</th>
<th align="center">$A\vec{x}$</th>
<th align="center">$\vec{x}^TA$</th>
</tr>
</thead>
<tbody><tr>
<td align="center">$\frac{\partial{\vec{y}}}{\partial{\vec{x}}}$</td>
<td align="center">$0$</td>
<td align="center">$I$</td>
<td align="center">$A$</td>
<td align="center">$A^T$</td>
</tr>
</tbody></table>
<blockquote>
<p>$A\vec{x}&#x3D;(\vec{a}_1\cdot \vec{x},..., \vec{a}_n \cdot \vec{x})^T$, where $\vec{a}_i$ is the row vector of $A$</p>
<p>$\vec{x}^TA&#x3D;(\vec{b}_1\cdot \vec{x},..., \vec{b}_n \cdot \vec{x})$ or $^T$, where $\vec{b}_j$ is the column vector of $A$</p>
</blockquote>
<h2 id="Chain-rules"><a href="#Chain-rules" class="headerlink" title="Chain rules"></a>Chain rules</h2><p>See <a href="https://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&mid=2247513352&idx=2&sn=f633df720ae78007b4a6a8ec2c60e52a&chksm=ebb78ddcdcc004cac4f8d1677bb10040d01eaaf449d1034236ecd6e780d5c823e23e83b413cb&scene=27">Matrix derivative rules</a> to know more about chain rules in matrix derivative. In deep learning, the result of loss function is always a scalar (the result of softmax is actually a scalar). Therefore, it is enough for us to just master the chain rules for derivative of scalar to vector and the chain rules for detivation of vector to vector (numerator layout):</p>
<ol>
<li>Scalor to vector<br>$$\vec{x}_1\rightarrow\vec{x}_2\rightarrow...\rightarrow\vec{x}_n\rightarrow y$$<br>$$\frac{\partial{y}}{\partial\vec{x}_1}&#x3D;\frac{\partial{y}}{\partial{\vec{x}_n}}\cdot...\frac{\partial{\vec{x}_2}}{\partial{\vec{x}_1}}$$</li>
<li>Vector to vector<br> $$\vec{x}_1\rightarrow\vec{x}_2\rightarrow...\rightarrow\vec{x}_n\rightarrow\vec{y}$$<br>$$\frac{\partial{\vec{y}}}{\partial\vec{x}_1}&#x3D;\frac{\partial{\vec{y}}}{\partial{\vec{x}_n}}\cdot...\frac{\partial{\vec{x}_2}}{\partial{\vec{x}_1}}$$</li>
</ol>
<p>It is easy to prove the correctness of these rules by evaluating the dimension of each item. To get the derivative using chain rules, there are two ways:</p>
<ul>
<li>Forward propagation: Calculate from $\vec{x}_2$ to $y$. T(n)&#x3D;n, S(n)&#x3D;1.</li>
<li>Backward propagation: Calculate from $y$ to $\vec{x}_2$. T(n)&#x3D;n, S(n)&#x3D;n as we have to store the previous result for other parameters.</li>
</ul>
<p><img src="/2023/04/28/DiveIntoDeepLearningIntroduction/1.png" alt="1"></p>
<center style="font-size:12px;font-weight:bold"> Fig. 1. Forward and backward propagation</center><br>

<p>In general, backward propagation is more suitable for deep learning as it actually runs faster. In addition, a neural network can be easily represented as a computation graph as each neuron in the neural network is actually a node in computation graph:</p>
<p><img src="/2023/04/28/DiveIntoDeepLearningIntroduction/2.png" alt="2"></p>
<center style="font-size:12px;font-weight:bold"> Fig. 2. Neural network</center><br>

<p>The computation graph it forms is like this:</p>
<p><img src="/2023/04/28/DiveIntoDeepLearningIntroduction/3.png" alt="3"></p>
<center style="font-size:12px;font-weight:bold"> Fig. 3. Computation graph</center><br>

<p>For convenience, we make <code>bias=False</code>, that is, we only compute the gradients of <code>w</code>. Autograd will compute gradients from $\vec{a}^{[2]}$ to the leaf nodes. That is:</p>
<p>$$\text{grad}[\vec{w}^{[2]}]&#x3D;\frac{\partial{a^{[2]}}}{\partial{\vec w^{[2]}}}$$</p>
<p>$$\text{grad}[\vec{w}^{[1]}_{[i]}]&#x3D;\frac{\partial{a^{[2]}}}{\partial{\vec a^{[1]}}}\cdot\frac{\partial{\vec a^{[1]}}}{\partial{\vec w^{[1]} _ {[i]}}}$$</p>
<p>Autograd computes from the top to bottom and store the gradients it computes before (like $\frac{\partial{a^{[2]}}}{\partial{\vec a^{[1]}}}$, autograd will store it in node $\vec a^{[1]}$) so that it doesn&#39;t need to compute it again.</p>
<h1 id="Course-website"><a href="#Course-website" class="headerlink" title="Course website"></a>Course website</h1><ul>
<li><a href="https://zh-v2.d2l.ai/">DIVE INTO DEEP LEARNING</a></li>
<li><a href="https://courses.d2l.ai/zh-v2/">Course video</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/351687500">autograd1</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/321449610">autograd2</a></li>
</ul>
]]></content>
      <categories>
        <category>Dive Into Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Lab: Unsupervised Learning, Recommenders, Reinforcement Learning</title>
    <url>/2023/04/23/LabsOfMachineLearningByAndrewNg3/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="C3-W1-PracticeLab1"><a href="#C3-W1-PracticeLab1" class="headerlink" title="C3_W1_PracticeLab1"></a>C3_W1_PracticeLab1</h1><p>To finish this lab and make the code run efficiently, we must have a good understanding of the slicing and broadcasting of <code>numpy</code> so that we can implement vectorized code.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Exercise 1</span></span><br><span class="line">m = X.shape[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># You need to return the following variables correctly</span></span><br><span class="line">idx = np.zeros(X.shape[<span class="number">0</span>], dtype=<span class="built_in">int</span>)</span><br><span class="line"><span class="comment">### START CODE HERE ###</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">    d = np.<span class="built_in">sum</span>((centroids - X[i])**<span class="number">2</span>, axis=<span class="number">1</span>) <span class="comment"># For each point, compute its distance to each centroid</span></span><br><span class="line">    idx[i] = np.argmin(d, axis=<span class="number">0</span>) <span class="comment"># For each point, choose the closest centroid</span></span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Exercise 2</span></span><br><span class="line">centroids = np.zeros((K, n))</span><br><span class="line"><span class="comment">### START CODE HERE ###</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(K):</span><br><span class="line">    p_i = idx == i <span class="comment"># Get the index of points that were assigned to centroid i </span></span><br><span class="line">    x_i = X[p_i] <span class="comment"># Slice</span></span><br><span class="line">    centroids[i] = np.mean(x_i, axis=<span class="number">0</span>) <span class="comment"># Compute the mean value of each column</span></span><br><span class="line"><span class="comment">### END CODE HERE ##</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p><code>axis</code> is the dimension or the label, that will be taken into account. In other words, in each step, the other dimensions will not change. Therefore, for a matrix <code>A</code>, <code>np.sum(A, axis=1)</code> compute the sum of each row (The first dimension does&#39;t change). </p>
</blockquote>
<p>See <a href="/2023/04/22/NumPy/">Numpy</a> to know more about it.</p>
<h1 id="C3-W1-PracticeLab2"><a href="#C3-W1-PracticeLab2" class="headerlink" title="C3_W1_PracticeLab2"></a>C3_W1_PracticeLab2</h1><p>This lab is quite easy. However, there are some points to notice:</p>
<ul>
<li>Slicing and broadcasting of numpy;</li>
<li>Divide by 0 problem.<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Exercise 1</span></span><br><span class="line"><span class="comment">### START CODE HERE ### </span></span><br><span class="line">mu = np.mean(X, axis=<span class="number">0</span>) <span class="comment"># average</span></span><br><span class="line">var = np.mean((X- mu)**<span class="number">2</span>, axis=<span class="number">0</span>)</span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Exercise 2</span></span><br><span class="line">best_epsilon = <span class="number">0</span></span><br><span class="line">best_F1 = <span class="number">0</span></span><br><span class="line">prec = <span class="number">0.</span></span><br><span class="line">rec = <span class="number">0.</span></span><br><span class="line">F1 = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">step_size = (<span class="built_in">max</span>(p_val) - <span class="built_in">min</span>(p_val)) / <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epsilon <span class="keyword">in</span> np.arange(<span class="built_in">min</span>(p_val), <span class="built_in">max</span>(p_val), step_size):</span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ### </span></span><br><span class="line">    actual_pos_num = np.<span class="built_in">sum</span>(y_val) + <span class="number">0.</span> <span class="comment"># Number of actual positive</span></span><br><span class="line">    pred_pos = (p_val&lt;epsilon) + <span class="number">0</span> <span class="comment"># Make predictions</span></span><br><span class="line">    pred_pos_num = np.<span class="built_in">sum</span>(pred_pos) + <span class="number">0.</span> <span class="comment"># Number of predict positive</span></span><br><span class="line">    tp = np.<span class="built_in">sum</span>(y_val[pred_pos==<span class="number">1</span>]) <span class="comment"># Number of true positive</span></span><br><span class="line">    <span class="keyword">if</span> pred_pos_num != <span class="number">0</span>:</span><br><span class="line">        prec = tp / pred_pos_num</span><br><span class="line">    <span class="keyword">if</span> actual_pos_num != <span class="number">0</span>:</span><br><span class="line">        rec = tp / actual_pos_num</span><br><span class="line">    <span class="keyword">if</span> prec != <span class="number">0</span> <span class="keyword">and</span> rec != <span class="number">0</span>:</span><br><span class="line">        F1 = <span class="number">2</span> * prec * rec / (prec + rec)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h1 id="C3-W2-PracticeLab1"><a href="#C3-W2-PracticeLab1" class="headerlink" title="C3_W2_PracticeLab1"></a>C3_W2_PracticeLab1</h1><p>This lab is about collaborative filtering. We only need to implement the cost function. The other parts are almost the same as linear regression.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Vectorized</span></span><br><span class="line"><span class="comment">### START CODE HERE ###  </span></span><br><span class="line">reg = lambda_ / <span class="number">2</span> * (np.<span class="built_in">sum</span>(W**<span class="number">2</span>) + np.<span class="built_in">sum</span>(X**<span class="number">2</span>)) <span class="comment"># regularization</span></span><br><span class="line">err = (X @ W.T + b - Y)**<span class="number">2</span></span><br><span class="line">J = np.<span class="built_in">sum</span>(err[R==<span class="number">1</span>]) / <span class="number">2</span> + reg</span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure>

<h1 id="C3-W4-PracticeLab"><a href="#C3-W4-PracticeLab" class="headerlink" title="C3_W4_PracticeLab"></a>C3_W4_PracticeLab</h1><p>Install dependent libraries (it is recommended to install anaconda first):</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pip install gym==0.25.1</span><br><span class="line">pip install pyvirtualdisplay</span><br><span class="line">conda install swig <span class="comment"># or pip install swig</span></span><br><span class="line">conda install -c conda-forge gym-box2d</span><br><span class="line">pip install imageio[ffmpeg]</span><br><span class="line">pip install [pyav]</span><br></pre></td></tr></table></figure>
<p>Other configurations:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&#x27;KMP_DUPLICATE_LIB_OK&#x27;</span>]=<span class="string">&#x27;TRUE&#x27;</span></span><br><span class="line">os.environ[<span class="string">&#x27;TF_CPP_MIN_LOG_LEVEL&#x27;</span>]=<span class="string">&#x27;2&#x27;</span></span><br><span class="line"><span class="keyword">import</span> warnings <span class="comment"># ignore some warnings</span></span><br><span class="line">warnings.filterwarnings(<span class="string">&quot;ignore&quot;</span>, category=Warning)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>If there is still a bug in <code>Display(visible=0, size=(840, 480)).start()</code>, you can just comment out this code.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Exercise 1</span></span><br><span class="line"><span class="comment"># Create the Q-Network</span></span><br><span class="line">q_network = Sequential([</span><br><span class="line">    <span class="comment">### START CODE HERE ### </span></span><br><span class="line">    Dense(units=<span class="number">64</span>, activation=<span class="string">&#x27;relu&#x27;</span>, input_dim=<span class="number">8</span>),</span><br><span class="line">    Dense(units=<span class="number">64</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    Dense(units=<span class="number">4</span>, activation=<span class="string">&#x27;linear&#x27;</span>)</span><br><span class="line">    <span class="comment">### END CODE HERE ### </span></span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create the target Q^-Network</span></span><br><span class="line">target_q_network = Sequential([</span><br><span class="line">    <span class="comment">### START CODE HERE ### </span></span><br><span class="line">    Dense(units=<span class="number">64</span>, activation=<span class="string">&#x27;relu&#x27;</span>, input_dim=<span class="number">8</span>),</span><br><span class="line">    Dense(units=<span class="number">64</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    Dense(units=<span class="number">4</span>, activation=<span class="string">&#x27;linear&#x27;</span>)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line"><span class="comment">### START CODE HERE ### </span></span><br><span class="line">optimizer = Adam(learning_rate=ALPHA)</span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Exercise 2</span></span><br><span class="line"><span class="comment">### START CODE HERE ### </span></span><br><span class="line">y_targets = rewards + (<span class="number">1</span> - done_vals) * gamma * max_qsa</span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### START CODE HERE ### </span></span><br><span class="line">loss = MSE(q_values, y_targets)</span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Machine Learning by AndrewNg</category>
      </categories>
      <tags>
        <tag>Unsupervised Learning</tag>
        <tag>Reinforcement Learning</tag>
        <tag>Lab</tag>
        <tag>Recommender System</tag>
      </tags>
  </entry>
  <entry>
    <title>Lab: Advanced Learning Algorithms</title>
    <url>/2023/04/23/LabsOfMachineLearningByAndrewNg2/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="C2-Week1"><a href="#C2-Week1" class="headerlink" title="C2_Week1"></a>C2_Week1</h1><p>As usual, modify:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.style.use(<span class="string">&#x27;./deeplearning.mplstyle&#x27;</span>) -&gt; plt.style.use(<span class="string">&#x27;deeplearning.mplstyle&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>The same operation should also be applied to <code>lab_coffee_utils.py</code>, <code>lab_neurons_utils.py</code> and <code>lab_utils_common.py</code>. </p>
<p>Then, install tensorflow. Maybe you should run you anaconda prompt as a administrator. When running the code of labs, once using the module from tensorflow, jupyter notebook keeps saying: <strong>The kernel has died</strong>. When checking the command line of the prompt, there may be one warning and one error:</p>
<p><img src="/2023/04/23/LabsOfMachineLearningByAndrewNg2/1.png" alt="1"></p>
<center style="font-size:12px;font-weight:bold">Warning</center><br>

<p><img src="/2023/04/23/LabsOfMachineLearningByAndrewNg2/2.png" alt="2"></p>
<center style="font-size:12px;font-weight:bold">Error</center><br>

<p>For the warning, it indicates that tensorflow could run faster by using some CPU instructions. We can add the following code to the beginning of the first code cell to ignore this warning:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&#x27;TF_CPP_MIN_LOG_LEVEL&#x27;</span>]=<span class="string">&#x27;2&#x27;</span></span><br></pre></td></tr></table></figure>
<p>It is the error that leads the kernel to die. The error occurs because there are more than one <code>libiomap5md.dll</code> files. We can add the following code to the beginning of the first code cell to solve it:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&#x27;KMP_DUPLICATE_LIB_OK&#x27;</span>]=<span class="string">&#x27;TRUE&#x27;</span></span><br></pre></td></tr></table></figure>
<p>Now, we can run all the labs normally. Labs in C2_Week1 focus on introducing tensorflow, see <a href="/2023/04/12/Tensorflow/">Tensorflow</a> to know more about it.</p>
<h1 id="C2-W1-PracticeLab"><a href="#C2-W1-PracticeLab" class="headerlink" title="C2_W1_PracticeLab"></a>C2_W1_PracticeLab</h1><p>Exercise 1 is quite simple, it is almost the same as <a href="/2023/04/12/Tensorflow/#Create-the-model">Create tensorflow model</a>.</p>
<p>Exercise 2 and exercise 3 are quite interesting. They both implement forward propagation using numpy, that is, making predictions.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Non-vectorized</span></span><br><span class="line"><span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(units):</span><br><span class="line">        a_out[j] = g(np.dot(a_in.T, W[:, j]) + b[j])</span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Vectorized</span></span><br><span class="line"><span class="comment">### START CODE HERE ### </span></span><br><span class="line">    A_out = g(A_in @ W + b)</span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure>

<h1 id="C2-Week2"><a href="#C2-Week2" class="headerlink" title="C2_Week2"></a>C2_Week2</h1><p>Just following its instructions is okay. The last part of lab <strong>Multi-class Classification</strong>, which shows how the new features created by neurons are like, is rather thought-provoking. It reveals that neurons of hidden layers have learnt something about the problem. In fact, they have partly partition the dataset:</p>
<p><img src="/2023/04/23/LabsOfMachineLearningByAndrewNg2/3.png" alt="3"></p>
<p>See <a href="/2023/04/12/Tensorflow/#Optimization-for-softmax">Multiclass classification</a> to know more about relu, softmax and multiclass classification.</p>
<h1 id="C2-W2-PracticeLab"><a href="#C2-W2-PracticeLab" class="headerlink" title="C2_W2_PracticeLab"></a>C2_W2_PracticeLab</h1><p>Quite easy:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Exercise 1</span></span><br><span class="line"><span class="comment">### START CODE HERE ### </span></span><br><span class="line">e = np.exp(z)</span><br><span class="line">a = e / np.<span class="built_in">sum</span>(e)</span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Exercise 2</span></span><br><span class="line"><span class="comment">### START CODE HERE ###</span></span><br><span class="line">Dense(units=<span class="number">25</span>, activation=<span class="string">&#x27;relu&#x27;</span>, input_dim=<span class="number">400</span>),</span><br><span class="line">Dense(units=<span class="number">15</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">Dense(units=<span class="number">10</span>, activation=<span class="string">&#x27;linear&#x27;</span>)</span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure>

<h1 id="C2-W3-PracticeLab"><a href="#C2-W3-PracticeLab" class="headerlink" title="C2_W3_PracticeLab"></a>C2_W3_PracticeLab</h1><p>This lab is still very easy but it is quite useful. It use real data and curve to show us how to choose better $d$ and $\lambda$ using dev set and training set.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Exercise 1</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line"><span class="comment">### START CODE HERE ### </span></span><br><span class="line">    err += (yhat[i] - y[i])**<span class="number">2</span></span><br><span class="line">err /= <span class="number">2</span> * m</span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Exercise 2</span></span><br><span class="line">m = <span class="built_in">len</span>(y)</span><br><span class="line">incorrect = <span class="number">0.</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line"><span class="comment">### START CODE HERE ### </span></span><br><span class="line">    <span class="keyword">if</span> y[i] != yhat[i]:</span><br><span class="line">        incorrect += <span class="number">1</span></span><br><span class="line">cerr = incorrect / m</span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Exercise 3</span></span><br><span class="line">model = Sequential(</span><br><span class="line">    [</span><br><span class="line">        <span class="comment">### START CODE HERE ### </span></span><br><span class="line">        Dense(units=<span class="number">120</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">        Dense(units=<span class="number">40</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">        Dense(units=<span class="number">6</span>, activation=<span class="string">&#x27;linear&#x27;</span>)</span><br><span class="line">        <span class="comment">### END CODE HERE ### </span></span><br><span class="line"></span><br><span class="line">    ], name=<span class="string">&quot;Complex&quot;</span></span><br><span class="line">)</span><br><span class="line">model.<span class="built_in">compile</span>(</span><br><span class="line">    <span class="comment">### START CODE HERE ### </span></span><br><span class="line">    loss=SparseCategoricalCrossentropy(from_logits=<span class="literal">True</span>),</span><br><span class="line">    optimizer=Adam(learning_rate=<span class="number">0.01</span>),</span><br><span class="line">    <span class="comment">### END CODE HERE ### </span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Exercise 4</span></span><br><span class="line">model_s = Sequential(</span><br><span class="line">    [</span><br><span class="line">        <span class="comment">### START CODE HERE ### </span></span><br><span class="line">        Dense(units=<span class="number">6</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">        Dense(units=<span class="number">6</span>, activation=<span class="string">&#x27;linear&#x27;</span>)</span><br><span class="line">        <span class="comment">### END CODE HERE ### </span></span><br><span class="line">    ], name = <span class="string">&quot;Simple&quot;</span></span><br><span class="line">)</span><br><span class="line">model_s.<span class="built_in">compile</span>(</span><br><span class="line">    <span class="comment">### START CODE HERE ### </span></span><br><span class="line">    loss=SparseCategoricalCrossentropy(from_logits=<span class="literal">True</span>),</span><br><span class="line">    optimizer=Adam(learning_rate=<span class="number">0.01</span>),</span><br><span class="line">    <span class="comment">### START CODE HERE ### </span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Exercise 5</span></span><br><span class="line">model_r = Sequential(</span><br><span class="line">    [</span><br><span class="line">        <span class="comment">### START CODE HERE ### </span></span><br><span class="line">        Dense(units=<span class="number">120</span>, activation=<span class="string">&#x27;relu&#x27;</span>, kernel_regularizer=tf.keras.regularizers.l2(<span class="number">0.1</span>)),</span><br><span class="line">        Dense(units=<span class="number">40</span>, activation=<span class="string">&#x27;relu&#x27;</span>, kernel_regularizer=tf.keras.regularizers.l2(<span class="number">0.1</span>)),</span><br><span class="line">        Dense(units=<span class="number">6</span>, activation=<span class="string">&#x27;linear&#x27;</span>)</span><br><span class="line">        <span class="comment">### START CODE HERE ### </span></span><br><span class="line">    ], name= <span class="literal">None</span></span><br><span class="line">)</span><br><span class="line">model_r.<span class="built_in">compile</span>(</span><br><span class="line">    <span class="comment">### START CODE HERE ### </span></span><br><span class="line">    loss=SparseCategoricalCrossentropy(from_logits=<span class="literal">True</span>),</span><br><span class="line">    optimizer=Adam(learning_rate=<span class="number">0.01</span>),</span><br><span class="line">    <span class="comment">### START CODE HERE ### </span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h1 id="C2-W4-PracticeLab"><a href="#C2-W4-PracticeLab" class="headerlink" title="C2_W4_PracticeLab"></a>C2_W4_PracticeLab</h1><p>To finish this lab, <code>pydot</code> is required. Install it using:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip intsall pydot</span><br><span class="line">or</span><br><span class="line">conda install pydot</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Exercise 1</span></span><br><span class="line">entropy = <span class="number">0.</span></span><br><span class="line">length = <span class="built_in">len</span>(y)</span><br><span class="line">e_count = np.<span class="built_in">sum</span>(y) + <span class="number">0.</span></span><br><span class="line"><span class="comment">### START CODE HERE ###</span></span><br><span class="line"><span class="keyword">if</span> length != <span class="number">0</span> <span class="keyword">and</span> e_count != <span class="number">0</span> <span class="keyword">and</span> e_count != length:</span><br><span class="line">    p1 = e_count / length</span><br><span class="line">    entropy = -p1 * np.log2(p1) - (<span class="number">1</span> - p1) * np.log2(<span class="number">1</span> - p1)</span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Exercise 2</span></span><br><span class="line">left_indices = []</span><br><span class="line">right_indices = []</span><br><span class="line"><span class="comment">### START CODE HERE ###</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> node_indices:</span><br><span class="line">    <span class="keyword">if</span> X[i][feature] == <span class="number">1</span>:</span><br><span class="line">        left_indices.append(i)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        right_indices.append(i)</span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Exercise 3</span></span><br><span class="line">information_gain = <span class="number">0</span></span><br><span class="line"><span class="comment">### START CODE HERE ###</span></span><br><span class="line">length_node = <span class="built_in">len</span>(y_node) + <span class="number">0.</span></span><br><span class="line"><span class="keyword">if</span> length_node != <span class="number">0</span>:</span><br><span class="line">    length_left = <span class="built_in">len</span>(y_left)</span><br><span class="line">    weight_left = length_left / length_node</span><br><span class="line">    h_n = compute_entropy(y_node)</span><br><span class="line">    h_left = compute_entropy(y_left)</span><br><span class="line">    h_right = compute_entropy(y_right)</span><br><span class="line">    information_gain = h_n - weight_left * h_left - (<span class="number">1</span> - weight_left) * h_right</span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Exercise 4</span></span><br><span class="line">best_feature = -<span class="number">1</span></span><br><span class="line">biggest_gain = <span class="number">0.</span></span><br><span class="line"><span class="comment">### START CODE HERE ###</span></span><br><span class="line"><span class="keyword">for</span> feature <span class="keyword">in</span> <span class="built_in">range</span>(num_features):</span><br><span class="line">    current_gain = compute_information_gain(X, y, node_indices, feature)</span><br><span class="line">    <span class="keyword">if</span> (current_gain &gt; biggest_gain):</span><br><span class="line">        best_feature = feature</span><br><span class="line">        biggest_gain = current_gain</span><br><span class="line"><span class="comment">### END CODE HERE ##</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>Some bugs occur when drawing the picture, and I don&#39;t know how to fit it... Just commenting out the relevant code is ok.</p>
</blockquote>
]]></content>
      <categories>
        <category>Machine Learning by AndrewNg</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Decision Trees</tag>
        <tag>Lab</tag>
      </tags>
  </entry>
  <entry>
    <title>Lab: Supervised Machine Learning - Regression and Classification</title>
    <url>/2023/04/21/LabsOfMachineLearningByAndrewNg1/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="Optional-labs-W1"><a href="#Optional-labs-W1" class="headerlink" title="Optional labs - W1"></a>Optional labs - W1</h1><h2 id="Lab01-Lab02"><a href="#Lab01-Lab02" class="headerlink" title="Lab01 - Lab02"></a>Lab01 - Lab02</h2><p>Just follow its instructions.</p>
<h2 id="Lab03"><a href="#Lab03" class="headerlink" title="Lab03"></a>Lab03</h2><p>There is a markdown syntax error in <strong>Notation</strong> paragraph:</p>
<p><img src="/2023/04/21/LabsOfMachineLearningByAndrewNg1/1.png" alt="1"></p>
<p>To solve this, turn:</p>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">|: ------------|: ------------------------------------------------------------||</span><br></pre></td></tr></table></figure>
<p>to</p>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">|:---:|:---:|:---:|</span><br></pre></td></tr></table></figure>
<p>In <strong>Tools</strong> paragraph, if you have put <code>deeplearning.mplstyle</code> into the same folder of your labs&#39;file, but code <code>plt.style.use(&#39;./deeplearning.mplstyle&#39;)</code> still can&#39;t run, you should delete <code>./</code> in <code>./deeplearning.mplstyle</code>. If your os is Linux, this problem will not occur.</p>
<h2 id="Lab04"><a href="#Lab04" class="headerlink" title="Lab04"></a>Lab04</h2><p>In <strong>Tools</strong> paragraph, code <code>%matplotlib widget</code> can&#39;t run because of the lack of <strong>ipympl</strong> even though you installed jupyter using anaconda. Maybe the reason is that the version of ipympl is not compatible with jupyter. To solve this, run your anaconda prompt as a administrator and:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">conda install -c conda-forge ipympl</span><br></pre></td></tr></table></figure>

<p>The problem of <code>./deeplearning.mplstyle</code> will still occur, modify it as before. In addition, you should also modify the same path in <code>lab_utils_commonpy</code> and <code>lab_utils_uni.py</code>. For the following labs, you should keep doing so once you encounter <code>./deeplearning.mplstyle</code>.</p>
<h2 id="Lab05"><a href="#Lab05" class="headerlink" title="Lab05"></a>Lab05</h2><p>You may run into <code>int</code> overflow when running <code>plt_divergence(p_histm J_hist, x_train, y_train)</code>. Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># in file lab_utils_uni.py</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># line 300: add np.int64</span></span><br><span class="line">w_array = np.arange(-<span class="number">70000</span>, <span class="number">70000</span>, <span class="number">1000</span>, dtype=np.int64)</span><br><span class="line"><span class="comment"># line 301: add np.int64</span></span><br><span class="line">cost = np.zeros_like(w_array, dtype=np.int64)</span><br><span class="line"><span class="comment"># line 319: add np.int64</span></span><br><span class="line">z=np.zeros_like(tmp_b, dtype=np.int64)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="Optional-labs-W2"><a href="#Optional-labs-W2" class="headerlink" title="Optional labs - W2"></a>Optional labs - W2</h1><h2 id="Lab01"><a href="#Lab01" class="headerlink" title="Lab01"></a>Lab01</h2><p>See <a href="/2023/04/22/NumPy/">NumPy</a>.</p>
<h2 id="Lab02"><a href="#Lab02" class="headerlink" title="Lab02"></a>Lab02</h2><p>The same bugs as <a href="#Lab03">Supervised Machine Learning.Optional labs - W1.Lab03</a>.</p>
<h2 id="Lab03-1"><a href="#Lab03-1" class="headerlink" title="Lab03"></a>Lab03</h2><p>The same bugs as <a href="#Lab03">Supervised Machine Learning.Optional labs - W1.Lab03</a>.</p>
<h2 id="Lab04-1"><a href="#Lab04-1" class="headerlink" title="Lab04"></a>Lab04</h2><p>Just follow its instructions.</p>
<h2 id="Lab05-Lab06"><a href="#Lab05-Lab06" class="headerlink" title="Lab05 - Lab06"></a>Lab05 - Lab06</h2><p>See <a href="/2023/04/22/Scikit-learn/">Scikit-learn</a>.</p>
<h1 id="Optional-labs-W3"><a href="#Optional-labs-W3" class="headerlink" title="Optional labs - W3"></a>Optional labs - W3</h1><h2 id="Lab01-Lab09-Soln"><a href="#Lab01-Lab09-Soln" class="headerlink" title="Lab01 - Lab09_Soln"></a>Lab01 - Lab09_Soln</h2><p>Just follow its instructions.</p>
<h2 id="Lab01-user"><a href="#Lab01-user" class="headerlink" title="Lab01_user"></a>Lab01_user</h2><p>Answer:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">g = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))</span><br></pre></td></tr></table></figure>

<h2 id="Lab02-user"><a href="#Lab02-user" class="headerlink" title="Lab02_user"></a>Lab02_user</h2><p>Answer:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x1 = <span class="number">3</span> - x0</span><br></pre></td></tr></table></figure>
<p>Besides, there is a bug in <code>lab_utils.plot_data</code>:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Add the following codes after neg = y == 0</span></span><br><span class="line">pos = pos.reshape(-<span class="number">1</span>,)  <span class="comment"># work with 1D or 1D y vectors</span></span><br><span class="line">neg = neg.reshape(-<span class="number">1</span>,)</span><br></pre></td></tr></table></figure>

<h2 id="Lab03-user"><a href="#Lab03-user" class="headerlink" title="Lab03_user"></a>Lab03_user</h2><p>Answer:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">    <span class="comment">### START CODE HERE ### </span></span><br><span class="line">    g = sigmoid(X[i] @ w + b)</span><br><span class="line">    cost -= y[i] * np.log(g) + (<span class="number">1</span> - y[i]) * np.log(<span class="number">1</span> - g)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p><code>@</code> can represent matrix multiplication.</p>
</blockquote>
<h2 id="Lab04-user"><a href="#Lab04-user" class="headerlink" title="Lab04_user"></a>Lab04_user</h2><p>Non-vectorized answer:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">### START CODE HERE ### </span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">    err = sigmoid(X[i] @ w + b) - y[i]</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        dJdw[j] += err * X[i][j]</span><br><span class="line">    dJdb += err</span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>For each example, get its error and apply it to different $w_j$.</p>
</blockquote>
<p>Vectorized answer:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">### START CODE HERE ### </span></span><br><span class="line">z = X @ w + b</span><br><span class="line">g = sigmoid(z)</span><br><span class="line">err = g - y</span><br><span class="line">dJdw = <span class="number">1</span> / m * (np.dot(X.T, err))</span><br><span class="line">dJdb = <span class="number">1</span> / m * (np.<span class="built_in">sum</span>(err))</span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>Get error of all examples simultaneously.</p>
</blockquote>
<h2 id="Lab05-user"><a href="#Lab05-user" class="headerlink" title="Lab05_user"></a>Lab05_user</h2><p>See <a href="/2023/04/22/Scikit-learn/">Scikit-learn</a>.</p>
<h2 id="Lab06-user"><a href="#Lab06-user" class="headerlink" title="Lab06_user"></a>Lab06_user</h2><p>This lab realize multiclass classification using multiple binary classification models, that is <strong>One Vs All algorithm</strong>. In fact, its core idea is quite simple. For an example with $n$ possible $y$, represent its label using a vector with $n$ binary elements, only one of which is <code>1</code>. Then, we just need to train $n$ binary classification models and choose the biggest prediction of them as $\widehat{y}$.</p>
<p>Answer:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># step 1</span></span><br><span class="line"><span class="comment">### START CODE HERE ### </span></span><br><span class="line">w_init = np.zeros((<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">b_init = <span class="number">0.</span></span><br><span class="line"><span class="comment"># call gradient descent</span></span><br><span class="line">w_final, b_final,_,_ = gradient_descent(X_train, yc, w_init, b_init, </span><br><span class="line">                                        compute_cost_logistic_matrix,</span><br><span class="line">                                        compute_gradient_logistic_matrix,</span><br><span class="line">                                        predict_logistic_matrix, <span class="number">1e-2</span>, <span class="number">1000</span>)</span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># make prediction</span></span><br><span class="line"><span class="comment">### START CODE HERE ### </span></span><br><span class="line">z_wb = X @ W + b</span><br><span class="line">G = sigmoid(z_wb)</span><br><span class="line">pclass = np.argmax(G, axis=<span class="number">1</span>)</span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Second Test Case</span></span><br><span class="line"><span class="comment"># plot the decison boundary. Pass in our models - the w&#x27;s and b&#x27;s assocated with each model and predict_mc</span></span><br><span class="line">plot_mc_decision_boundary(X_train,<span class="number">3</span>, W_models, b_models, predict_mc)</span><br><span class="line">plt.title(<span class="string">&quot;model decision boundary vs original training data&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># add the original data to the decison boundary</span></span><br><span class="line">plot_mc_data(X_train,y_train,[<span class="string">&quot;blob one&quot;</span>, <span class="string">&quot;blob two&quot;</span>, <span class="string">&quot;blob three&quot;</span>], legend=<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<h2 id="Lab07-user"><a href="#Lab07-user" class="headerlink" title="Lab07_user"></a>Lab07_user</h2><p>Just follow its instructions. However, there is also a bug because of the update of <code>sklearn</code>. We should turn <code>penalty=&#39;none&#39;</code> to <code>penalty=None</code>.</p>
<p><code>map_feature</code> is a very interesting function:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">map_feature</span>(<span class="params">X1, X2, degree</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Feature mapping function to polynomial features    </span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    X1 = np.atleast_1d(X1)</span><br><span class="line">    X2 = np.atleast_1d(X2)</span><br><span class="line"></span><br><span class="line">    out = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, degree+<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i + <span class="number">1</span>):</span><br><span class="line">            out.append((X1**(i-j) * (X2**j)))</span><br><span class="line">           </span><br><span class="line">    <span class="keyword">return</span> np.stack(out, axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>It produces polynomials of degree up to <code>degree</code> formed by <code>X1</code> and <code>X2</code>. That is, if <code>degree=3</code>:<br>$$(x_1+x_2)+(x_1^2+x_1x_2+x_2^2)+(x_1^3+x_1^2x_2+x_1x_2^2+x_2^3)$$</p>
<h2 id="Lab08-user"><a href="#Lab08-user" class="headerlink" title="Lab08_user"></a>Lab08_user</h2><p>Answer:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">### START CODE HERE ### </span></span><br><span class="line">f_wb = sigmoid(X @ w + b) <span class="comment"># m*1</span></span><br><span class="line">cost += <span class="number">1</span> / m * (np.dot(-y, np.log(f_wb)) - np.dot(<span class="number">1</span> - y, np.log(<span class="number">1</span> - f_wb))) + lambda_ / <span class="number">2</span> * np.<span class="built_in">sum</span>((w**<span class="number">2</span>))</span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure>

<h2 id="Lab09-user"><a href="#Lab09-user" class="headerlink" title="Lab09_user"></a>Lab09_user</h2><p>Answer:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Looping version</span></span><br><span class="line"><span class="comment">### START CODE HERE ### </span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">    err = np.<span class="built_in">sum</span>(sigmoid(X[i] @ w + b) - y[i])</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        dJdw[j] += err * X[i][j]</span><br><span class="line">    dJdb += err</span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Vectorized version</span></span><br><span class="line"><span class="comment">### START CODE HERE ### </span></span><br><span class="line">f_wb = sigmoid(X @ w + b) <span class="comment"># m*1</span></span><br><span class="line">err = f_wb - y <span class="comment"># m*1</span></span><br><span class="line">dJdw = <span class="number">1</span> / m * np.dot(X.T, err) + lambda_ * w</span><br><span class="line">dJdb = np.<span class="built_in">sum</span>(err) / m</span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure>

<h1 id="PracticeLab01"><a href="#PracticeLab01" class="headerlink" title="PracticeLab01"></a>PracticeLab01</h1><p>This lab requires us to implement the <code>compute_cost</code> and <code>compute_gradient</code> function of a linear regression model with only one feature. It is quite simple:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># compute_cost</span></span><br><span class="line"><span class="comment">### START CODE HERE ###  </span></span><br><span class="line">cost = <span class="number">1</span> / (<span class="number">2</span> * m) * (w * x + b - y)**<span class="number">2</span> <span class="comment"># m*1</span></span><br><span class="line">total_cost = np.<span class="built_in">sum</span>(cost) </span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># compute_gradient</span></span><br><span class="line"><span class="comment">### START CODE HERE ### </span></span><br><span class="line">f_wb = w * x + b <span class="comment"># m*1</span></span><br><span class="line">err = f_wb - y <span class="comment"># m*1</span></span><br><span class="line">dj_dw = err.T @ x / m</span><br><span class="line">dj_db = np.<span class="built_in">sum</span>(err) / m</span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure>

<h1 id="PracticeLab02"><a href="#PracticeLab02" class="headerlink" title="PracticeLab02"></a>PracticeLab02</h1><p>This lab requires us to implement a logistic regression model with regularization. It is a bit more complicated than PracticeLab01, but it&#39;s still easy to finish.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># sigmoid</span></span><br><span class="line"><span class="comment">### START CODE HERE ### </span></span><br><span class="line">g = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))</span><br><span class="line"><span class="comment">### END SOLUTION ###</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># compute_cost</span></span><br><span class="line"><span class="comment">### START CODE HERE ###</span></span><br><span class="line">f_wb = sigmoid(X @ w + b) <span class="comment"># m*1</span></span><br><span class="line">total_cost = <span class="number">1</span> / m * (np.dot(-y.T, np.log(f_wb)) - np.dot(<span class="number">1</span> - y.T, np.log(<span class="number">1</span> - f_wb)))</span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># compute_gradient</span></span><br><span class="line"><span class="comment">### START CODE HERE ### </span></span><br><span class="line">f_wb = sigmoid(X @ w + b)</span><br><span class="line">err = f_wb - y</span><br><span class="line">dj_dw = <span class="number">1</span> / m * (X.T @ err)</span><br><span class="line">dj_db = <span class="number">1</span> / m * np.<span class="built_in">sum</span>(err)</span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># predict</span></span><br><span class="line"><span class="comment">### START CODE HERE ### </span></span><br><span class="line">y_pred = sigmoid(X @ w + b)</span><br><span class="line">p = <span class="number">0</span> + (y_pred &gt;= <span class="number">0.5</span>)</span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># compute_cost_reg</span></span><br><span class="line"><span class="comment">### START CODE HERE ###</span></span><br><span class="line">reg_cost = np.<span class="built_in">sum</span>(w**<span class="number">2</span>)</span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># compute_gradient_reg</span></span><br><span class="line"><span class="comment">### START CODE HERE ###     </span></span><br><span class="line">dj_dw += lambda_  / m * w</span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Machine Learning by AndrewNg</category>
      </categories>
      <tags>
        <tag>Lab</tag>
        <tag>Supervised Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Reinforcement Learning</title>
    <url>/2023/04/19/ReinforcementLearning/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Reinforcement learning differs from supervised learning in not needing labelled input&#x2F;output pairs to be presented, and in not needing sub-optimal actions to be explicitly corrected. Instead the focus is on finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge).</p>
<p>The environment is typically stated in the form of a Markov decision process (MDP), because many reinforcement learning algorithms for this context use dynamic programming techniques. The main difference between the classical dynamic programming methods and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the MDP and they target large MDPs where exact methods become infeasible.</p>
<blockquote>
<p><a href="https://en.wikipedia.org/wiki/Reinforcement_learning">wiki: Reinforcement learning</a></p>
</blockquote>
<h1 id="Basic-concepts"><a href="#Basic-concepts" class="headerlink" title="Basic concepts"></a>Basic concepts</h1><ul>
<li>environment $E$: The environment where a agent can perceive.</li>
<li>states $s$: A description of the environment perceived by a agent. It is a subset of state space $S$, that is $s\in S$.</li>
<li>actions $a$: The action a agent can take. All actions form action space $A$, $a\in{A}$.</li>
<li>rewards $r$: The reward a agent gets after it takes a action.</li>
<li>discount factor $\gamma$: A number close to 1. It discounts the gains that require more actions and reduces the losses that require more actions.</li>
<li>return: The sum of reward weighted by $\gamma$.<br>$$a_1+\gamma{a_2}+...+\gamma{^n}a_{n-1}$$</li>
<li>policy $\pi(s)$: A policy $a&#x3D;\pi(s)$ decides the action that a agent in $s_i$ state ought to take to get to next state $s_{i+1}$ so as to maximize the return.</li>
</ul>
<p><img src="/2023/04/19/ReinforcementLearning/1.png" alt="1"></p>
<center style="font-size:12px;font-weight:bold">Fig. 1. A loop of reinforcement learning</center><br>

<p>The figure above shows the procedure of reinforcement learning. A agent in state $s_i$ takes action to affect the environment or itself (The agent itself is actually a part of the environment), gets a certain reward and reach new state $s_{i+1}$. The goal of reinforcement learning is to fina a policy $\pi$ that tells the agent what action to take in every state so as to maximize the return. Nearly all reforcement learning assignment can be described as <strong>Markov decision process (MDP)</strong>. MDP is a process that the future only depends on the <strong>current state $s_i$</strong> and have nothing to do with the prior states or how things get to the current state.</p>
<h1 id="State-action-value-function"><a href="#State-action-value-function" class="headerlink" title="State-action value function"></a>State-action value function</h1><p>State-action value function $Q(s,a)$, which is also called Q-function, $Q^*$ or optimal Q function, is a function that computes the maximum return a agent can get if it is in state $s$ and take action $a$ (once). After the agent takes action $a$, its state will become $s&#39;$ and the actions it could take in state $s&#39;$ are $a&#39;$. It is easy to realize that the computing of $Q$ is actually a dynamic programming process. In reinforcement learning, the method we use to count $Q$ is called <strong>bellman equation</strong>:</p>
<p>$$Q(s,a)&#x3D;R(s)+{\gamma}\max_{a&#39;}Q(s&#39;,a&#39;)\tag{1}$$<br>where $R(s)$ is called immediate reward, the second part is the return weighted by $\gamma$ from behaving optimally starting from state $s&#39;$.</p>
<h2 id="Random-stochastic-environment"><a href="#Random-stochastic-environment" class="headerlink" title="Random (stochastic) environment"></a>Random (stochastic) environment</h2><p>In practice, we can&#39;t guarantee that the agent will execute the actions precisely due to the randomness of the environment. In order to get a more precise value of return, we have to take this randomness into account:</p>
<p>$$<br>Q(s,a)&#x3D;R(s)+{\gamma}E[\max_{a&#39;}Q(s&#39;,a&#39;)]\tag{2}<br>$$<br>$E$ means expectation. When taking randomness into account, the state after taking action $a$ is uncertain. Therefore, what the second term of $Q(s,a)$ computes is the weighted average of the maximum returns weighted by $\gamma$ of the different states the agent may reach after taking action $a$.</p>
<h1 id="Continuous-state"><a href="#Continuous-state" class="headerlink" title="Continuous state"></a>Continuous state</h1><p>In practice, the state of a agent is a vector whose values are continuous. For examples, in lab <em>Lunar Lander</em>, the state of a lunar lander can be described as:<br>$$<br>s&#x3D;[x,y,\dot{x},\dot{y},\theta,\dot{\theta},l,r]<br>$$<br>where $x$, $y$ and $\theta$ are its position since this is a plane problem. $\dot{x}$, $\dot{y}$, $\dot{\theta}$ are its velocity of different directions. $l$ and $r$ (value 0 or 1) indicates whether the left or right leg of lunar lander has landed.</p>
<p><img src="/2023/04/19/ReinforcementLearning/2.png" alt="2"></p>
<center style="font-size:12px;font-weight:bold">Fig. 2. Lunar Lander</center>

<h2 id="DQN-algorithm"><a href="#DQN-algorithm" class="headerlink" title="DQN algorithm"></a>DQN algorithm</h2><p>An algorithm to solve this type of reinforcement learning is DQN algorithm or Deep Q-Network. It is also called deep reinforcement learning as it uses neural networks to solve reinforcement learning problems. Its core idea is to compute $Q(s,a)$ using neural networks:</p>
<p><img src="/2023/04/19/ReinforcementLearning/3.png" alt="3"></p>
<center style="font-size:12px;font-weight:bold">Fig. 3. Deep reinforcement learning</center><br>

<p>where $s$ is the state vector and $a$ is the action vector. In lunar lander, $a$ has four dimensions: do nothing, left thruster, right thruster and main thruster. Since it is actually a supervised learning model, we have to feed it training set $(s,a,Q)$ to get the mapping relationship between $s \And a$ and $Q$. According to bellman equation:</p>
<p>$$Q(s,a)&#x3D;R(s)+{\gamma}\max_{a&#39;}Q(s&#39;,a&#39;)$$</p>
<p>$Q$ is decided by $R(s)$ and $s&#39;$ since we must take all the action $a&#39;$ into account, that is, once we get $(s,a,R(s),s&#39;)$, we get $(s,a,Q)$. Therefore, the steps of DQN are:</p>
<ol>
<li>Initialize the neural netwrok randomly as a guess of $Q(s,a)$;</li>
<li>Simulate the behaviors of the agent to get a number of $(s,a,R(s),s&#39;)$;</li>
<li>Feed $(s&#39;,a&#39;)$ to the neural network to <strong>predict</strong> $Q(s&#39;,a&#39;)$;</li>
<li>Create training set using<br>$$x&#x3D;(s,a) \And Q(s,a)&#x3D;R(s)+{\gamma}\max_{a&#39;}Q(s&#39;,a&#39;)$$</li>
<li>Train the neural network using training set created above, get $Q_{new}$;</li>
<li>Set $Q&#x3D;Q_{new}$.</li>
</ol>
<blockquote>
<p>That is, produce some $(s,a,R(s),s&#39;)$ and guess a structure of $Q$ ($Q$ is the neural network). Use this $Q$ to predict $(s,a,Q)$. Use $(s,a,Q)$ to train the neural network and fine tune $Q$.</p>
</blockquote>
<p>In the architecture above, for each value of $a$ in a certain state $s$, we have to predict it respectively. A more effective architecture is to just feed $s$ and produce $Q$ of all values of $a$:</p>
<p><img src="/2023/04/19/ReinforcementLearning/4.png" alt="4"></p>
<center style="font-size:12px;font-weight:bold">Fig. 4. Improved neural network architecture</center>

<h2 id="epsilon-greedy-policy"><a href="#epsilon-greedy-policy" class="headerlink" title="$\epsilon$-greedy policy"></a>$\epsilon$-greedy policy</h2><p>When simulating the behaviors of the agent, instead of simulating its action randomly, it is more advisable to choose a action that will maximizes $Q(s,a)$. However, choosing the action that maximizes $Q(s,a)$ all the time may reduce the robustness of the agent or miss some good choices. A policy to solve this is $\epsilon$-greedy policy:</p>
<ul>
<li>With probability $1-\epsilon$, pick the action $a$ that maximizes $Q(s,a)$;</li>
<li>With probability $\epsilon$, pick an action $a$ randomly.</li>
</ul>
<blockquote>
<p>The first option is called &#39;Exploitation&#39; and the second option is called &#39;Exploration&#39;. As what the wiki in the <a href="#Introduction">Introduction</a> says, reinforcement learning focuses on the balance between exploration and exploitation.</p>
</blockquote>
<p>It is recommended to start $\epsilon$ high to explore more so as to enrich the robustness and then slightly reduce $\epsilon$.</p>
<h2 id="Algorithm-refinement"><a href="#Algorithm-refinement" class="headerlink" title="Algorithm refinement"></a>Algorithm refinement</h2><h3 id="Mini-batch"><a href="#Mini-batch" class="headerlink" title="Mini-batch"></a>Mini-batch</h3><p>When the training set is too large, it is advisable to use different subsets instead of the whole set in each iteration. This will accelerate the computation of both $J$ and derivatives. Such methods can also be used in regression model:</p>
<p><img src="/2023/04/19/ReinforcementLearning/5.png" alt="5"></p>
<center style="font-size:12px;font-weight:bold">Fig. 5. Mini-batch</center><br>

<blockquote>
<p>Tensorflow trains model using mini-batch.</p>
</blockquote>
<h3 id="Soft-update"><a href="#Soft-update" class="headerlink" title="Soft update"></a>Soft update</h3><p>When updating $Q$, we can&#39;t guarantee that $Q_{new}$ is better than $Q$. Therefore, it is recommended to keep a part of $Q$ and update smoothly, like:<br>$$Q_{new}&#x3D;0.01Q_{new}+0.99Q\tag{3}$$</p>
]]></content>
      <categories>
        <category>Machine Learning by AndrewNg</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Reinforcement Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Recommender System</title>
    <url>/2023/04/17/RecommenderSystem/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="Collaborative-filtering"><a href="#Collaborative-filtering" class="headerlink" title="Collaborative filtering"></a>Collaborative filtering</h1><p>The core idea of collaborative filtering is that users tend to use items with high ratings. Therefore, by predicting users&#39; ratings for items that they haven&#39;t used based on ratings of users who gave similar ratings, we can recommend those high rated items to users.</p>
<h2 id="Notation"><a href="#Notation" class="headerlink" title="Notation"></a>Notation</h2><ul>
<li>$n_u$ &#x3D; number of users;</li>
<li>$n_m$ &#x3D; number of items;</li>
<li>$r(i,j)&#x3D;1$ if user $j$ has rated item $i$;</li>
<li>$y^{(i,j)}$ &#x3D; rating given by user $j$ to item $i$;</li>
<li>$w^{(j)}, b^{(j)}$ &#x3D; parameters for user $j$;</li>
<li>$x^{(i)}$ &#x3D; feature vector for item $i$;</li>
<li>$m^{(j)}$ &#x3D; number of items rated by user $j$</li>
</ul>
<h2 id="Recommending-with-features"><a href="#Recommending-with-features" class="headerlink" title="Recommending with features"></a>Recommending with features</h2><p><img src="/2023/04/17/RecommenderSystem/1.png" alt="1"></p>
<center style="font-size:12px;font-weight:bold">Fig. 1. Items with features</center><br>

<p>If we know the composition or features of items and their respective percentages, we can estimate how mush users like each feature or component. This is actually a regression model: ${rating}&#x3D;{w^{(j)}}\cdot{x}+b^{(j)}$. If we consider regularization as well as all the items and all the users together, we can get cost function $J$ like this:</p>
<p>$$J(w,b) &#x3D; \frac{1}{2}\sum\limits_{j&#x3D;1}^{n_u}\sum\limits_{i:r(i,j)&#x3D;1}(w^{(j)}\cdot x^{(i)}+b^{(j)}-y^{(i,j)})^2+\frac{\lambda}{2}\sum\limits_{j&#x3D;1}^{n_u}\sum\limits_{k&#x3D;1}^{n}(w_k^{(j)})^2$$</p>
<p>It is actually a linear regression model though we only consider the items that have been rated. Since $m^{(j)}$ is different for each user and it actually doesn&#39;t affect the final result for each user because it is only related to user $j$, we can remove it and make $2m^{(j)}$ just $2$. Each user is totally independent of others, that is, $w^{(j)}$ and $b^{(j)}$ have nothing to do with others. Therefore, $J$ is just a combination of several linear regression cost functions.</p>
<h2 id="Recommending-without-features"><a href="#Recommending-without-features" class="headerlink" title="Recommending without features"></a>Recommending without features</h2><p><img src="/2023/04/17/RecommenderSystem/2.png" alt="2"></p>
<center style="font-size:12px;font-weight:bold">Fig. 2. Items without features</center><br>

<p>More generally, we don&#39;t know the exact value of each feature. Therefore, the algorithm has to learn their values from users&#39; ratings:</p>
<p>$$J(x) &#x3D; \frac{1}{2}\sum\limits_{i&#x3D;1}^{n_m}\sum\limits_{j:r(i,j)&#x3D;1}(w^{(j)}\cdot x^{(i)}+b^{(j)}-y^{(i,j)})^2+\frac{\lambda}{2}\sum\limits_{i&#x3D;1}^{n_m}\sum\limits_{k&#x3D;1}^{n}(x_k^{(i)})^2$$</p>
<p>In this case, $x$, the values of features of each item, are the parameters that the algorithm will learn. We assume that the algorithm have already known the parameters of each user. Nonetheless, it actually doesn&#39;t know. Since the first term of $J(w,b)$ and $J(x)$ are the same, we can combine them together:</p>
<p>$$J(w,b,x) &#x3D; \frac{1}{2}\sum\limits_{(i,j):r(i,j)&#x3D;1}(w^{(j)}\cdot x^{(i)}+b^{(j)}-y^{(i,j)})^2+\frac{\lambda}{2}\sum\limits_{i&#x3D;1}^{n_m}\sum\limits_{k&#x3D;1}^{n}(x_k^{(i)})^2+\frac{\lambda}{2}\sum\limits_{j&#x3D;1}^{n_u}\sum\limits_{k&#x3D;1}^{n}(w_k^{(j)})^2$$</p>
<p>This is <strong>collaborative filtering</strong>. In addition to learning $w$ and $b$, it also learns $x$. When gradient descent, it has to modify $w$, $b$ and $x$ simultaneously. Because of the removing of $m^{(j)}$, it can combine the first term of $J(w,b)$ and $J(x)$ together. The ratings of other users are used to learn $x$ but affect the learning of $w$ and $b$ of all the users indirectly. That&#39;s why it is called collaborative filtering - all users collaborate to generate the features of items.</p>
<p>If the rating of a item is given in the form of binary label (like or dislike), we can just turn linear regression to logistic regression. Namely, turn activation function from linear function to sigmoid function and turn loss function from squared error to cross entropy.</p>
<h2 id="Mean-normalization"><a href="#Mean-normalization" class="headerlink" title="Mean normalization"></a>Mean normalization</h2><p>If there is a new user $a$ that haven&#39;t rated any items, the prediction of its rating that the collaborative filtering algorithm makes will all be zero. Because the parameters of $a$ have nothing to do with the value of the first term of $J(w,b,x)$, regularization will make them all zero and the prediction ${rating}&#x3D;{w}\cdot{x}+b$ will also be zero. Intuitively, we can use the average score of each item $\mu$ to predict the rating of a new user. And that&#39;s what mean normalization does:</p>
<ul>
<li>Subtract ${\mu}_{j}$ from each user&#39;s rating for item $j$;</li>
<li>Do collaborative filtering using normalized scores;</li>
<li>Make predictions: $\widehat{y}<em>{a}^{(i)}&#x3D;{w_a}\cdot{x}^{(i)}+b_a+\mu</em>{i}$</li>
</ul>
<h2 id="Finding-related-items"><a href="#Finding-related-items" class="headerlink" title="Finding related items"></a>Finding related items</h2><p>Features $x^{(i)}$ of item $i$ convey something about what the item is like. Therefore, we can find items similar to this item by computing:<br>$$||{x}^{(k)}-{x}^{(i)}||^2$$<br>where ${x}^{(k)}$ and ${x}^{(i)}$ are both vectors. The smaller this value is, the more similar the two items are.</p>
<h1 id="Content-based-filtering"><a href="#Content-based-filtering" class="headerlink" title="Content-based filtering"></a>Content-based filtering</h1><p>Cold start problem:</p>
<ul>
<li>How to rank new items that few users have rated?</li>
<li>How to show something reasonable to new users who have rated few items?</li>
</ul>
<p>Though mean normalization can solve the second problem to a certain extent, it is better to use content-based filtering to solve cold start problem.</p>
<p>Content-based filtering recommends items to users based on features or background of user as well as item to find good match. Each user $j$ has several background information $x_u^{(j)}$ and each item $i$ has several features $x_m^{(i)}$. With $x_u^{(j)}$, it is possible for the algorithm to compute how much user $j$ like certain components of this type of item $v_u^{(j)}$. With $x_m^{(i)}$, it is possible for the algorithm to compute the proportions of each attribute of item $i$ $v_m^{(i)}$.</p>
<blockquote>
<p>The dimensions of $x_u^{(j)}$ and $x_m^{(i)}$ can be different, but the dimensions of $v_u^{(j)}$ and $v_m^{(i)}$ must be the same since the prediction of rating is: $v_u^{(j)}\cdot v_m^{(i)}$.</p>
</blockquote>
<p>We can use neural networks to compute $v_u^{(j)}$ and $v_m^{(i)}$ respectively:</p>
<p><img src="/2023/04/17/RecommenderSystem/3.png" alt="3"></p>
<center style="font-size:12px;font-weight:bold">Fig. 3. User and item network</center><br>

<p>More generally, we will combine these two neural networks together:<br>$$J &#x3D; \sum\limits_{(i,j):r(i,j)&#x3D;1}{(v_u^{(j)}\cdot v_m^{(i)}-y^{(i,j)})}^2+{regularization\space term}$$</p>
<p><img src="/2023/04/17/RecommenderSystem/4.png" alt="4"></p>
<center style="font-size:12px;font-weight:bold">Fig. 4. Compound neural network</center><br>

<p>The cost function of the output layer depends on the value we need. It could be mean squared error for rating or cross entropy (and sigmoid activation function) for binary result. Similarly, we can find similar items using:</p>
<p>$$||v_m^{(k)}-v_m^{(i)}||^2$$</p>
<h2 id="Retrieval-amp-Ranking"><a href="#Retrieval-amp-Ranking" class="headerlink" title="Retrieval &amp; Ranking"></a>Retrieval &amp; Ranking</h2><p>For one user, the algorithm only needs to count $v_u^{(j)}$ one time but it has to count $v_m^{(i)}$ for each item. If there is a large set of items, it will be time consuming. Retrieval &amp; Ranking is one way to solve this.</p>
<p>In retrieval step, there is another algorithm that will generate a large list of plausible item candidates that is much smaller than the whole set.</p>
<p>In ranking step, the algorithm will only compute $v_m^{(i)}$ of items in the list generated in retrieval step and make recommendation.</p>
]]></content>
      <categories>
        <category>Machine Learning by AndrewNg</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Machine Learning</tag>
        <tag>Recommender System</tag>
      </tags>
  </entry>
  <entry>
    <title>Unsupervised Learning</title>
    <url>/2023/04/16/UnsupervisedLearning/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="Clustering"><a href="#Clustering" class="headerlink" title="Clustering"></a>Clustering</h1><p>Clustering is a type of unsupervised learning that automatically groups a set of objects in such a way that objects in the same group are more similar to each other than to those in other groups. A commonly used algorithm of clustering is K-means algorithm.</p>
<h2 id="K-means-algorithm"><a href="#K-means-algorithm" class="headerlink" title="K-means algorithm"></a>K-means algorithm</h2><p>K-means algorithm partition $n$ observations into $k$ clusters where each observation belongs to the cluster with the nearest mean. In other words, there is a centroid in each cluster and among all the centroids, an observation in its cluster is closer to its centroid than to other centroids, that is, observations in the same cluster are more related. The procedures of k-means are as follows:</p>
<blockquote>
<p>Cluster centroids: centers of cluster.</p>
</blockquote>
<ul>
<li>Randomly initialize $K$ cluster centroids $\mu_1,\mu_2,..., \mu_K$;<blockquote>
<p>Just randomly choose $K$ points from the training set is ok though different initialization will produce different clusters.</p>
</blockquote>
</li>
<li>Assign points to its closest cluster centroids;<blockquote>
<p>If there are no points assigned to a centroid, just reduce the number of centroids to $K-1$ or reinitialize the centroids.</p>
</blockquote>
</li>
<li>Move each cluster centroid to the average of all the points $(\bar{x},\bar{y},..., \bar{z})$ that were assigned to it;</li>
<li>Keep doing so until the movements of centroids are small enough;</li>
<li>Reinitialize all the centroids and train another set of $\mu$ 50-1000 times, choose the set with the lowest $J$ as the final result of clustering.</li>
</ul>
<p><img src="/2023/04/16/UnsupervisedLearning/1.png" alt="1"></p>
<center style="font-size:12px;font-weight:bold">Fig. 1. K-means</center>

<h3 id="Cost-function"><a href="#Cost-function" class="headerlink" title="Cost function"></a>Cost function</h3><p>Definition of some notations:</p>
<ul>
<li>$c^{(i)}$ &#x3D; index of cluster (1, 2,..., $K$) where example $x^{(i)}$ is currently assigned;</li>
<li>$\mu_k$ &#x3D; cluster centroid $k$;</li>
<li>$\mu_{c^{(i)}}$ &#x3D; cluster centroid of cluster to which $x^{(i)}$ has been assigned</li>
</ul>
<p>Distortion cost function $J$:<br>$$J(c^{(1)},...,c^{(m)},\mu_1,...,\mu_k)&#x3D;\frac{1}{m}\sum\limits_{i&#x3D;1}^{m}(x^{(i)}-\mu_{c^{(i)}})^2$$</p>
<p>The way to minimize $J$ is repeatedly moving $\mu_k$ to the centre of cluster $k$.</p>
<h3 id="Choosing-a-K"><a href="#Choosing-a-K" class="headerlink" title="Choosing a $K$"></a>Choosing a $K$</h3><p>Elbow method: Choose the value at the elbow of $J-K$ curve as $K$ on elbow is the position where the slope of a function rapidly decreases.</p>
<p><img src="/2023/04/16/UnsupervisedLearning/2.png" alt="2"></p>
<center style="font-size:12px;font-weight:bold">Fig. 2. Elbow method</center><br>

<p>More generally, the choice of $K$ depends on the downstream purpose.</p>
<h1 id="Anomaly-detection"><a href="#Anomaly-detection" class="headerlink" title="Anomaly detection"></a>Anomaly detection</h1><p>Anomaly detection is another type of unsupervised learning that automatically identifies rare or anomalous events which are suspicous or harmful because they differ significantly from standard behaviors or patterns. The key algorithm in anomaly detection is density estimation that builds the model of $p(x)$ which is the probability density of $x$. The most commonly used probability density model is Gaussian (Normal or Bell shaped) distribution:<br>$$p(x)&#x3D;\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$$</p>
<p>For multiple features $\vec{x}$, assuming features are independent of each other:<br>$$p(\vec{x})&#x3D;\prod\limits_{j&#x3D;1}^{n}p(x_j; \mu_j, \sigma_j^2)$$</p>
<p>The followings are procedures of anomaly detection:</p>
<ul>
<li>Choose $n$ features $x_j$ that you think might be indicative of anomalous examples;</li>
<li>Fit parameters $\mu_1,...,\mu_n,\sigma_1^2,...,\sigma_n^2$;<br>$$\mu_j&#x3D;\frac{1}{m}\sum\limits_{i&#x3D;1}^{m}x_j^{(i)}$$<br>$$\sigma_j&#x3D;\frac{1}{m}\sum\limits_{i&#x3D;1}^{m}(x_j^{(i)}-\mu_j)^2$$</li>
<li>Given new example $\vec{x}$, compute $p(\vec{x})$;</li>
<li>Anomaly if $p(\vec{x})&lt;\epsilon$.</li>
</ul>
<h2 id="Evaluating-anomaly-detection-system"><a href="#Evaluating-anomaly-detection-system" class="headerlink" title="Evaluating anomaly detection system"></a>Evaluating anomaly detection system</h2><p>In anomaly detection, we actually know which example is positive and which is not, that is, we have labeled data. However, we train the model using only normal examples, cross validate model with both anomalous and normal examples to determine $\epsilon$, test model with both anomalous and normal examples. When the data set is too small, it is feasible to remove test set.</p>
<p>We only need to train the model one time as $\mu$ and $\sigma$ totally depend on the training set. Dev set is used to choose the best $\epsilon$. For skewed datasets, precission and recall may also be used in addition to the accurary of prediction.</p>
<h2 id="Anomaly-detection-and-supervised-learning"><a href="#Anomaly-detection-and-supervised-learning" class="headerlink" title="Anomaly detection and supervised learning"></a>Anomaly detection and supervised learning</h2><style>
    table th {
        width: 160px;
    }
</style>
<table>
<thead>
<tr>
<th align="center">type</th>
<th align="center">Anomaly detection</th>
<th align="center">Supervised learning</th>
</tr>
</thead>
<tbody><tr>
<td align="center">number of positive examples</td>
<td align="center">small</td>
<td align="center">large</td>
</tr>
<tr>
<td align="center">number of negative examples</td>
<td align="center">large</td>
<td align="center">large</td>
</tr>
<tr>
<td align="center">focus</td>
<td align="center">detect negative examples</td>
<td align="center">detect both negative and positive examples</td>
</tr>
<tr>
<td align="center">new types of anomalies</td>
<td align="center">easy to detect</td>
<td align="center">hard to detect</td>
</tr>
</tbody></table>
<p>The core idea of anomaly detection is to learn under what circumstances one example is normal. Therefore, anomaly detection is more robust if the types of anomaly keep changing while the standard of normal is stable.</p>
<p>Though supervised learning is also feasible when the number of positive examples is small, it can&#39;t learn much from positive examples. In contrast, since supervised learning learns what positive and negative examples look like from previous examples, it can&#39;t deal with the types of anomaly that didn&#39;t occur before well.</p>
<h2 id="Choosing-good-features"><a href="#Choosing-good-features" class="headerlink" title="Choosing good features"></a>Choosing good features</h2><p>Choosing good features is more important in anomaly detection than in supervised learning since supervised learning can rescale and take the best advantage of the features we give while anomaly detection treats each feature independent and equally important. To choose good features, there are several methods: </p>
<ul>
<li><p>Choose more gaussian features or <strong>make</strong> features more gaussian (e.g. $\log(x+c)$ instead of $x$);<br><img src="/2023/04/16/UnsupervisedLearning/3.png" alt="3"></p>
<center style="font-size:12px;font-weight:bold">Fig. 3. Variable transformation</center><br>
</li>
<li><p>Error analysis of dev set to find new features or create new features based on exsiting features.</p>
</li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning by AndrewNg</category>
      </categories>
      <tags>
        <tag>Unsupervised Learning</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Decision Trees</title>
    <url>/2023/04/16/DecisionTrees/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="Structure-of-decision-trees"><a href="#Structure-of-decision-trees" class="headerlink" title="Structure of decision trees"></a>Structure of decision trees</h1><p>Decision trees are a tree model, where each internal node (decision node) is a feature that has several possibile values. The number of values defines the degree of nodes.</p>
<p><img src="/2023/04/16/DecisionTrees/1.png" alt="1"></p>
<center style="font-size:12px;font-weight:bold">Fig. 1. Decision trees</center>

<h1 id="Decision-tree-learning"><a href="#Decision-tree-learning" class="headerlink" title="Decision tree learning"></a>Decision tree learning</h1><p>First decision: How to choose features to maximize purity? Second decision: When can the tree stop splitting</p>
<ul>
<li>When a node is 100% one class.</li>
<li>When splitting a node will result in the tree exceeding a <strong>maximum depth</strong> (defined by us).</li>
<li>When improvements in purity score are below a threshold.</li>
<li>When number of examples in a node is below a threshold.</li>
</ul>
<h2 id="Measuring-purity"><a href="#Measuring-purity" class="headerlink" title="Measuring purity"></a>Measuring purity</h2><p>p$_1$: Fraction of examples that are our target.</p>
<p>H(p$_1$): Degree of impurity of one set.</p>
<p>$$p_0&#x3D;1-p_1$$<br>$$H(p_1)&#x3D;-p_1\log_2(p_1)-p_0\log_2(p_0)$$</p>
<p><img src="/2023/04/16/DecisionTrees/2.png" alt="2"></p>
<center style="font-size:12px;font-weight:bold">Fig. 2. Entropy function</center>

<h2 id="Choosing-a-split"><a href="#Choosing-a-split" class="headerlink" title="Choosing a split"></a>Choosing a split</h2><h3 id="Information-gain"><a href="#Information-gain" class="headerlink" title="Information gain"></a>Information gain</h3><p>Information gain is the redunction of entropy after one split:</p>
<p>$$IG&#x3D;H(p_1^{root})-(w^{left}H(p_1^{left})+w^{right}H(p_1^{right}))$$</p>
<p>where $w^{left}H(p_1^{left})+w^{right}H(p_1^{right})$ is the average weighted entropy of two child nodes. $w$ is the propotion of examples in left&#x2F;right node that come from root node.</p>
<h2 id="Building-a-decision-tree"><a href="#Building-a-decision-tree" class="headerlink" title="Building a decision tree"></a>Building a decision tree</h2><p>We always construct decision trees in a preorder traversal. That is:</p>
<ul>
<li>Start with all examples at root node;</li>
<li>Calculate information gain for all possible features and pick the one with the highest information gain;</li>
<li>Split dataset and build subtrees recursively until stopping criteria is met.</li>
</ul>
<blockquote>
<p>For features with more than two discrete values, we can use <strong>one-hot encoding</strong>.  That is, decompose the features with $k$ values into $k$ bool features so that the values of each new feature will only be yes (1) or no (0). One-hot encoding can also be applied to neural networks to turn multiclass classification to binary classification.</p>
<p>For features with continuous values, we try splitting values using different thresholds and choose the threshold that gives the highest information gain.</p>
<p>It is reasonable that the same features appears several times in one decision trees.</p>
</blockquote>
<h2 id="Regression-trees"><a href="#Regression-trees" class="headerlink" title="Regression trees"></a>Regression trees</h2><p>Regression trees are the generalization of decision trees, which predict a number. The differences between building a regression tree and a decision tree are:</p>
<ul>
<li>The prediction of regression trees is the average value of examples in leaf node;</li>
<li>We choose the features to splite dataset by the reduction of variance.</li>
</ul>
<p>For the second difference:</p>
<ul>
<li>Calculate the average weighted variance of one split:<br>$$w^{left}v^{left}+w^{right}v^{right}$$</li>
<li>Calcuate the reduction of variance after splitting:<br>$$v^{root}-(w^{left}v^{left}+w^{right}v^{right})$$</li>
<li>Choose the feature that gives the highest reduction and continue until meet stopping criteria.</li>
</ul>
<h1 id="Ensemble-of-decision-trees"><a href="#Ensemble-of-decision-trees" class="headerlink" title="Ensemble of decision trees"></a>Ensemble of decision trees</h1><p>A single decision tree is highly sentitive to small changes of the data. We may get two totally different decision trees even though only one example changes in the training set. An ensemble of decision trees will make the overall algorithm more robust. An ensemble of decision trees makes decision by voting, that is, the prediction of the ensemble is the value most decision trees give.</p>
<p><img src="/2023/04/16/DecisionTrees/3.png" alt="3"></p>
<center style="font-size:12px;font-weight:bold">Fig. 3. Ensemble of decision trees</center>

<h2 id="Random-forest-algorithm"><a href="#Random-forest-algorithm" class="headerlink" title="Random forest algorithm"></a>Random forest algorithm</h2><p>One algorithm to create an ensemble of decision trees is random forest algorithm. The algorithm can automatically explore a lot of small changes to the data set, which makes it more robust.</p>
<p>Random forests are generated by sampling with replacement and randomizing feature choice:</p>
<ul>
<li>Determine the number of trees $B$ we need and the size of subset $m$;</li>
<li>Use sampling with replacement to create a new training set of size $m$;</li>
<li>Randomly pick a subset of $k$ (usually $\sqrt{n}$) features from $n$ features and train a decision tree only use these features and the data set generated above;</li>
<li>Repeatedly generate $B$ trees.</li>
</ul>
<p>Sampling with replacement and randomizing feature choice allow us generate different trees as much as possible.</p>
<h2 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h2><p>XGBoost is an open-source library that makes ensemble of decision trees perform better. The core idea of XGBoost is that it is more worthwhile to train decision trees using the examples that the previously trained decision trees have failed to predict rather than using totally random examples. Trees generated like this is called <strong>boosted trees</strong>. In XGBoost, instead of picking subsets from all samples with equal probability, it makes it <strong>more likely</strong> to pick examples that the previously trained trees have misclassfied. The details of XGBoost are very complicated, but the using of it is quite simple:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> XGBClassifier <span class="comment"># or XGBRegressor for regression</span></span><br><span class="line"></span><br><span class="line">model = XGBClassifier()</span><br><span class="line"></span><br><span class="line">model.fit(X_train, Y_train)</span><br><span class="line">y_pred = model.predict(X_test)</span><br></pre></td></tr></table></figure>

<h1 id="Decision-trees-amp-Neural-Networks"><a href="#Decision-trees-amp-Neural-Networks" class="headerlink" title="Decision trees &amp; Neural Networks"></a>Decision trees &amp; Neural Networks</h1><p>For decision trees and tree ensembles:</p>
<ul>
<li>Work well on structured data; [advantage]</li>
<li>Fast to train; [advantage]</li>
<li>Small decision tress may be human interpretable; [advantage]</li>
<li>Not recommended for unstructured data (images, audio, text) [disadvantage];</li>
<li>Can&#39;t train multiple trees at a time. [disadvantage]</li>
</ul>
<p>Neural Networks:</p>
<ul>
<li>Works well on all types of data; [advantage]</li>
<li>Works with transfer learning; [advantage]</li>
<li>Multiple neural networks can be easier to string together when building a system of multiple models working together; [advantage]</li>
<li>Take long time to train. [disadvantage]</li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning by AndrewNg</category>
      </categories>
      <tags>
        <tag>Decision Trees</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Machine Learning Development Process</title>
    <url>/2023/04/15/MachineLearningDevelopmentProcess/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="Full-cycle-of-ML"><a href="#Full-cycle-of-ML" class="headerlink" title="Full cycle of ML"></a>Full cycle of ML</h1><p><img src="/2023/04/15/MachineLearningDevelopmentProcess/1.png" alt="1"></p>
<center style="font-size:12px;font-weight:bold">Fig. 1. Full cycle of ML</center><br>

<p>When developing a machine learning project, these are the four assignments we need to conduct.</p>
<h2 id="Scope-project"><a href="#Scope-project" class="headerlink" title="Scope project"></a>Scope project</h2><p>The step defines the purposes of the machine learning project and the functions it should implement.</p>
<h2 id="Collect-data"><a href="#Collect-data" class="headerlink" title="Collect data"></a>Collect data</h2><p>In this step, we define the data we need and try to collect as much data as we can. However, instead of adding data of all types, it is advisable to focus on adding more data of the types that error analysis has indicated it migh help.</p>
<h3 id="Data-augmentation"><a href="#Data-augmentation" class="headerlink" title="Data augmentation"></a>Data augmentation</h3><p>Data augmentation is a useful technique used especially for image and audio data.It creates new training examples by modifying existing training examples.For image data, we can distort the image and for audio data, we can add some noise.However, the type of noise or distortions we add to the set must be meaningful, that is, they should not be purely random but should be pertinent.</p>
<h3 id="Data-synthesis"><a href="#Data-synthesis" class="headerlink" title="Data synthesis"></a>Data synthesis</h3><p>Unlike data augmentation, data synthesis creates brand new examples. This method is used most for CV.</p>
<h3 id="Transfer-learning"><a href="#Transfer-learning" class="headerlink" title="Transfer learning"></a>Transfer learning</h3><p>Transfer learning is a research problem in machine learning that focuses on applying knowledge gained while solving one task to a related task. For example, knowledge gained while learning to recognize cars could be applied when trying to recognize trucks. This method is especially useful when we do not have enough data or computing power to train a large neural network.</p>
<p>When using transfer learning:</p>
<ol>
<li>Download neural network parameters <strong>pretrained</strong> on a large dataset with <strong>the same input type</strong> (e.g., images, audio, text) as our application;</li>
<li>Change the output layer and further train (<strong>fine tune</strong>)the network on our own data.</li>
</ol>
<p>When fine tuning, we can only train output layers parameters when our training set is small or we can train all parameters when our training set is large enough.</p>
<p>The reason why tranfer learning works is that in the hidden layers of a pretrained model, they have learnt something (maybe a subset of our target). Therefore, using the parameters of a pretrained model will make our model start from a better place.</p>
<blockquote>
<p>AI &#x3D; Code + Data. When models or algorithms are good enough, focusing on data can be an efficient way to help learning algorithms improve their performance.</p>
</blockquote>
<h2 id="Train-model"><a href="#Train-model" class="headerlink" title="Train model"></a>Train model</h2><p><img src="/2023/04/15/MachineLearningDevelopmentProcess/2.png" alt="2"></p>
<center style="font-size:12px;font-weight:bold">Fig. 2. Iterative loop of ML</center><br>

<p>Depending on bias, variance and error analysis, we may need to modify the model or collect more data.</p>
<h2 id="Deploy-in-production"><a href="#Deploy-in-production" class="headerlink" title="Deploy in production"></a>Deploy in production</h2><p>A common structure of an application that uses machine learning models is as follows:</p>
<p><img src="/2023/04/15/MachineLearningDevelopmentProcess/3.png" alt="3"></p>
<center style="font-size:12px;font-weight:bold">Fig. 3. ML APP</center><br>

<p>The job of inference server is to call the trained model in order to make predictions.Inference server expose its API to mobile app. Mobile app just need to call API and wait for the prediction from inference server.</p>
<p>In this step, if the app does not work well, we may need to retrain the model or collect more data.</p>
]]></content>
      <categories>
        <category>Machine Learning by AndrewNg</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Machine Learning Diagnostics</title>
    <url>/2023/04/13/MachineLearningDiagnostics/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="Evaluating-a-model"><a href="#Evaluating-a-model" class="headerlink" title="Evaluating a model"></a>Evaluating a model</h1><p>To evaluate a model is to measure the accuracy of the model&#39;s predictions. There are many ways to do so.</p>
<h2 id="Two-sets"><a href="#Two-sets" class="headerlink" title="Two sets"></a>Two sets</h2><p>One useful method is to splite our training set into <strong>training set</strong> and <strong>test set</strong>. Training set is used to train and evaluate the model, but test set is only applied to evaluate the model. To evaluate a trained model, we should calculate both the $J_{test}$ and $J_{train}$, both of which are without regularization term. But the model is regularized.</p>
<blockquote>
<p>For classification model, we can also count the number of misclassified examples in training set and test set respectively.</p>
</blockquote>
<h1 id="Choosing-a-model"><a href="#Choosing-a-model" class="headerlink" title="Choosing a model"></a>Choosing a model</h1><p>To choose a model or to determine the architecture of a model, using two datasets suffers from the same problems as evaluating models on one dataset.</p>
<p>When evaluating a model on one dataset, we actually get an optimistic estimate of generalization error. That is, the model only fits the training set well but can not be generalized. The same problem will happen when choosing a model with two datasets. When we use the test set to choose a model, it is likely that the model may just fit this test set well as the structure of the model is also a parameter like $W$ and $B$. In conclusion, we can not use datasets that determine the model to evaluate or choose a model.</p>
<p><img src="/2023/04/13/MachineLearningDiagnostics/1.png" alt="1"></p>
<center style="font-size:12px;font-weight:bold">Fig. 1. Model choosing</center>

<h2 id="Three-sets"><a href="#Three-sets" class="headerlink" title="Three sets"></a>Three sets</h2><p>The solution is to divide the dataset into three parts: <strong>training set</strong>, <strong>cross validation set</strong> and <strong>test set</strong>.</p>
<blockquote>
<p>Cross validation set is also called development set or dev set.</p>
</blockquote>
<p>The procedure of choosing a model is similar to evaluating a model:</p>
<ul>
<li>Train the model using training set, get $W$ and $B$;</li>
<li>Choose a model using development set, calculate $J_{cv}$ and get d (degree of polynomial);</li>
<li>Verify the model using test set, calculate $J_{test}$.</li>
</ul>
<style> table th {
    width: 160px;
}
</style>

<table>
<thead>
<tr>
<th align="center">type</th>
<th align="center">training set</th>
<th align="center">dev set</th>
<th align="center">test set</th>
</tr>
</thead>
<tbody><tr>
<td align="center">trained</td>
<td align="center">yes</td>
<td align="center">no</td>
<td align="center">no</td>
</tr>
<tr>
<td align="center">function</td>
<td align="center">get $W$ and $B$</td>
<td align="center">determine model structure</td>
<td align="center">evaluate generalization ability of model</td>
</tr>
<tr>
<td align="center">usage count</td>
<td align="center">multiple times</td>
<td align="center">multiple times</td>
<td align="center">one time</td>
</tr>
</tbody></table>
<blockquote>
<p>A vivid metaphor about these sets is: Training set is students&#39;textbook, dev set is students&#39;homework and test set is the final exam.</p>
</blockquote>
<h1 id="Diagnostics"><a href="#Diagnostics" class="headerlink" title="Diagnostics"></a>Diagnostics</h1><h2 id="Bias-and-variance"><a href="#Bias-and-variance" class="headerlink" title="Bias and variance"></a>Bias and variance</h2><p>Instead of plotting the model to judge underfitting or overfitting, a more common method is to calculate and compare $J_{train}$ and $J_{cv}$.</p>
<ul>
<li>If $J_{train}$ is high and $J_{train}\approx J_{cv}$, the algorithm(model) may have high bias;</li>
<li>If $J_{train}$ is low and $J_{train}&lt;&lt;J_{cv}$, the algorithm(model) may have high variance;</li>
<li>If $J_{train}$ is high and $J_{train}&lt;&lt;J_{cv}$, the algorithm(model) may have high bias and high variance.<blockquote>
<p>An algorithm having both high bias and high variance fits some training examples well but fits others badly.</p>
</blockquote>
</li>
</ul>
<p><img src="/2023/04/13/MachineLearningDiagnostics/2.png" alt="2"></p>
<center style="font-size:12px;font-weight:bold">Fig. 2. Relationship between D and error</center>

<h3 id="Choosing-a-good-lambda"><a href="#Choosing-a-good-lambda" class="headerlink" title="Choosing a good $\lambda$"></a>Choosing a good $\lambda$</h3><p>We can also use the dev set to choose a better regularization parameter $\lambda$.</p>
<ul>
<li>If $\lambda$ is rather small, we value fitting the data. Therefore, $J_{cv}$ may be rather high while $J_{train}$ may be rather low;</li>
<li>If $\lambda$ is rather big, we value scaling $\vec{w}$. Therefore, $\vec{w}$ is aproximate 0, $J_{cv}$ may be rather high and $J_{train}$ may also be rather high.</li>
</ul>
<p><img src="/2023/04/13/MachineLearningDiagnostics/3.png" alt="3"></p>
<center style="font-size:12px;font-weight:bold">Fig. 3. Relationship between lambda and error</center>

<h3 id="Quantitative-indicators"><a href="#Quantitative-indicators" class="headerlink" title="Quantitative indicators"></a>Quantitative indicators</h3><p>In order to judge bias or variance quantitatively, a baseline level of performance is required. It can be human level performance, competing algorithms performance or just guess based on experience.</p>
<ul>
<li>When gap between baseline and $J_{train}$ is rather high, the model may have high bias;</li>
<li>When gap between $J_{train}$ and $J_{cv}$ is rather high, the model may have high variance.</li>
</ul>
<h3 id="Learning-curve"><a href="#Learning-curve" class="headerlink" title="Learning curve"></a>Learning curve</h3><p>Learning curve is a function curve of training set size and error of training set and dev set. We can use learning curve to judge whether a model has high bias or high variance. A regular learning curve looks like:</p>
<p><img src="/2023/04/13/MachineLearningDiagnostics/4.png" alt="4"></p>
<center style="font-size:12px;font-weight:bold">Fig. 4. Regular learning curve</center><br>

<p>When a model has high bias, its learning curve looks like:</p>
<p><img src="/2023/04/13/MachineLearningDiagnostics/5.png" alt="5"></p>
<center style="font-size:12px;font-weight:bold">Fig. 5. High bias learning curve</center><br>

<p>It indicates that when a model has high bias, we can not train a good model by using more data.</p>
<p>When a model has high variance, its learning curve looks like:</p>
<p><img src="/2023/04/13/MachineLearningDiagnostics/6.png" alt="6"></p>
<center style="font-size:12px;font-weight:bold">Fig. 6. High variance learning curve</center><br>

<p>It indicates that when a model has high variance, we can train a better model by using more data because your model is complicated enough. Though learning curve gives us an intuitive visual experience of the performance of our model, it is seldom used as plotting it wastes too much time.</p>
<h3 id="Fixing-bias-variance"><a href="#Fixing-bias-variance" class="headerlink" title="Fixing bias variance"></a>Fixing bias variance</h3><p>High variance: </p>
<ul>
<li>Get more training examples;</li>
<li>Try smaller sets of features;</li>
<li>Try increasing $\lambda$.</li>
</ul>
<p>High bias:</p>
<ul>
<li>Try getting additional features;</li>
<li>Try adding polynomial features;</li>
<li>Try decreasing $\lambda$.</li>
</ul>
<h3 id="DL-and-bias-variance"><a href="#DL-and-bias-variance" class="headerlink" title="DL and bias variance"></a>DL and bias variance</h3><p>In deep learning, the contradiction between high bias and variance can be easily solved. In general, large neural networks are low bias machines as we can reduce bias by adding more layers or neurons. For variance, we can solve it by adding more data. As long as regularization is chosen appropriately, a large neural network will usually do as well or better than a smaller one.</p>
<p><img src="/2023/04/13/MachineLearningDiagnostics/7.png" alt="7"></p>
<center style="font-size:12px;font-weight:bold">Fig. 7. Training of neural networks</center><br>

<blockquote>
<p>A bigger network needs powerful computing power to train. Therefore, the development of hardwares, especially GPU, and big data contribute to the thriving of deep learning.</p>
</blockquote>
<h2 id="Error-analysis"><a href="#Error-analysis" class="headerlink" title="Error analysis"></a>Error analysis</h2><p>Error analysis is another useful diagnostics method when training a model. In error analysis, we take the examples that the model has wrongly predicted or inferred into account and group them into common themes or common properties. These categories can be overlapping. Sometimes, the error set may be too large for us to deal with. In this case, it is advisable to randomly sample a subset (usually 100 examples).</p>
<p>In the next step, we can modify the model according to these categories. It is advisable to process the categories that are large enough but just ignore those small categories.</p>
<h2 id="Skewed-datasets"><a href="#Skewed-datasets" class="headerlink" title="Skewed datasets"></a>Skewed datasets</h2><p>Skewed datasets are datasets that have an uneven subset distribution. That is, the number of one output is much more than the others. In this case, we can&#39;t simply judge the performance of model using $J$ as we may get a good result even if the model just predicts this label all the time. </p>
<p>The solution is to count <em>precision</em> and <em>recall</em> of the model. In binary regression (for multiclass classfication, we set the real label <code>1</code> and the others <code>0</code>), we can display the true value and predict value in the form of 2*2 matrix: </p>
<p><img src="/2023/04/13/MachineLearningDiagnostics/8.png" alt="8"></p>
<center style="font-size:12px;font-weight:bold">Fig. 8. Output distribution</center><br>

<p>Then <em>precision</em> is defined as the fraction that are actually 1 among all examples where we predicted $y&#x3D;1$:<br>$${Precision}&#x3D;\frac{TruePos}{TruePos+FalsePos}$$</p>
<p><em>Recall</em> is defined as the fraction that are correctly predicted among all examples that are actually 1:<br>$${Recall}&#x3D;\frac{TruePos}{TruePos+FalseNeg}$$</p>
<p>A good model should have both high <em>precision</em> and <em>recall</em>. Once <em>precision</em> or <em>recall</em> is 0, chances are that the model <code>print(&quot;y=0&quot;)</code> all the time.</p>
<h3 id="F-1-score-Trading-off-precision-and-recall"><a href="#F-1-score-Trading-off-precision-and-recall" class="headerlink" title="$F_1 score$: Trading off precision and recall"></a>$F_1 score$: Trading off precision and recall</h3><p>For logistic regression, threshold is the boundary value used to separate 0 and 1. If raising the threshold, we will get higher <em>precision</em> but lower <em>recall</em>. If decreasing the threshold, we will get higher <em>recall</em> but lower <em>precision</em>. We can&#39;t keep both <em>precision</em> and <em>recall</em> high. What&#39;s more, we can&#39;t solve this problem using dev set as threshold is defined by us.</p>
<p><img src="/2023/04/13/MachineLearningDiagnostics/9.png" alt="9"></p>
<center style="font-size:12px;font-weight:bold">Fig. 9. Precision and recall</center><br>

<p>The method to compare <em>precision</em> and <em>recall</em> is to calculate $F_1 \space score$:<br>$$F_1\space{score}&#x3D;\frac{1}{\frac{1}{2}(\frac{1}{P}+\frac{1}{R})}&#x3D;\frac{2PR}{P+R}$$</p>
<p>The larger the $F_1 \space score$ is , the better the model is.</p>
<h3 id="ROC-amp-AUC"><a href="#ROC-amp-AUC" class="headerlink" title="ROC &amp; AUC"></a>ROC &amp; AUC</h3><p>$F_1 \space score$ requires us to choose a threshold to judge the performance of the classifier while <em>ROC</em> and <em>AUC</em> are more intelligent.</p>
<p>Receiver operating characteristic curve (ROC) is a curve whose $x$ axis is False positive rate (FPR) and $y$ axis is True positive rate (TPR)：</p>
<p>$$<br>\begin{align*}<br>    TPR&#x3D;Recall&amp;&#x3D;\frac{TruePos}{TruePos+FalseNeg}\\<br>    FPR&#x3D;&amp;\frac{FalsePos}{FalsePos+TrueNeg}<br>\end{align*}<br>$$</p>
<p><img src="/2023/04/13/MachineLearningDiagnostics/10.png" alt="10"></p>
<center style="font-size:12px;font-weight:bold">Fig. 10. ROC</center><br>

<p>Each point on ROC represent a $(FPR, TPR)$ pair under a certain threshold. For example:</p>
<ul>
<li>If threshold is 0, all the samples will be predicted as 1. In this case, both FPR and TPR is 1;</li>
<li>If threshold is 1, all the samples will be predicted as 0. In this case, both FPR and TPR is 0;</li>
<li>With threshold decreasing, points on ROC moves to the top right (or right&#x2F;or up), or remains stationary;</li>
<li>When points on ROC are on $y&#x3D;x$, the classifier has no difference with random guesses. The closer the points are to the upper left corner, the better the classifier is;</li>
<li>Points on ROC should always be above $y&#x3D;x$ as when points on ROC are below $y&#x3D;x$, just letting the classifier make the opposite conclusion will make these points on top of $y&#x3D;x$.</li>
</ul>
<p>Area under curve (AUC) is the area below ROC. If we randomly pick a positive sample and a negative sample, AUC represents the probability that the classifier predicts the value of the positive sample is bigger than the value of the negative sample. Taking AUC as an indicator to measure the performance of a classifier helps us dismiss the influence of skewed datasets and threshold. In other words, AUC measures the ability of the classifier to correctly sort samples. That&#39;s why AUC is a more commonly used indicator.</p>
<p>Since we can&#39;t take all the values of threshold into account, we can only use the approximate method to calculate AUC, that is, computing the rate that the value of the positive sample is bigger than the value of the negative sample among all the postive-negative pairs. For a dataset with $M$ positive sample and $N$ negative samples, the number of positive-negative pairs is $M\times N$:</p>
<ol>
<li>Sort the values of all the positive and negative samples from largest to smallest;</li>
<li>Score them from $N+M$ to $1$;</li>
<li>Sum the score of positive samples;</li>
<li>Subtract $M(M+1)&#x2F;2$;</li>
<li>Divide by $M\times N$:<br> $$AUC&#x3D;\frac{\sum _{i\in\text{positive}}\text{score}_i-M(M+1)&#x2F;2}{MN}\tag{1}$$</li>
</ol>
<p>Such a process works as $\text{score} _i$ is the score of the $i$-th biggest positive sample ($i&#x3D;1,...,M$) and</p>
<p>$$<br>N+M-\text{score}_i-(i-1)<br>$$</p>
<p>is the number of negative samples whose score is higher than it. Then</p>
<p>$$<br>\begin{align*}<br>    N-[N+M-\text{score}_i-(i-1)]<br>    &amp;&#x3D;\text{score}_i+(i-1)-M\\<br>    &amp;&#x3D;\text{score}_i-(M+1-i)<br>\end{align*}<br>$$</p>
<p>is the number of positive-negative pairs that consist of the $i$-th positive sample and negative samples whose values are smaller than the positive sample. As a result, the number of positive-negative pairs that meet our requirement is</p>
<p>$$<br>\sum\limits _{i&#x3D;1} ^{M}[\text{score}_i-(M+1-i)]&#x3D;\sum _{i\in\text{positive}}\text{score}_i-M(M+1)&#x2F;2<br>$$</p>
<blockquote>
<p>In sklearn, function <code>sklearn.metrics.roc_auc_score</code> could calculate the AUC of a classifier.</p>
<p>From the relationship between AUC and threshold, it can be seen that: if a large threshold is used in actual deployment, a little FP can bring us large TP. As a result, the precision of the classifier will be very high. Since it is not necessary to classify all positive classes, using different threshold in training and deployment is quite reasonable.</p>
</blockquote>
]]></content>
      <categories>
        <category>Machine Learning by AndrewNg</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Neural Network</title>
    <url>/2023/04/10/NeuralNetwork/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Neural network, which can also be named <em><strong>deep learning</strong></em>, is an advanced machine learning model.</p>
<blockquote>
<p>Neural network is an algorithm suitable for nearly all kinds of machine learning. Compared to traditional models, neural network performs better when the training set is large.</p>
</blockquote>
<h1 id="Component"><a href="#Component" class="headerlink" title="Component"></a>Component</h1><h2 id="Layer"><a href="#Layer" class="headerlink" title="Layer"></a>Layer</h2><p>Neural network is consisted of different layers. A layer is a grouping of neurons which takes the same or similar features as input and in turn outputs a few numbers together. The <em><strong>first layer</strong></em> (layer 0) is called <em><strong>input layer</strong></em> where input and output are the same. The <em><strong>last layer</strong></em> is called <em><strong>output layer</strong></em> which outputs the value of the neural network. Input and output layer are the only two layers that are visible to us, therefore, the other layers are called <em><strong>hidden layer</strong></em>.</p>
<blockquote>
<p>There are different types of hidden layer:</p>
<ul>
<li>Dense layer: Each neuron output is a function of all the activation outputs of the previous layer;</li>
<li>Convolutional layer: Each neuron only looks at part of the previous layer&#39;s outputs. Different neurons may look at the same outputs.</li>
<li>...</li>
</ul>
</blockquote>
<p><img src="/2023/04/10/NeuralNetwork/1.png" alt="1"></p>
<center style="font-size:12px;font-weight:bold">Fig. 1. Multilayer perceptron</center>

<h2 id="Neuron"><a href="#Neuron" class="headerlink" title="Neuron"></a>Neuron</h2><p>Each layer is made up of several (including one) neurons. Each neuron is a traditional machine learning model, like linear regression, logistic regression and so on. The output of one neuron is called <em><strong>activation</strong></em> and the function of this neuron is called <em><strong>activation function</strong></em>, which means it activate the next neuron.</p>
<p>The magic of neural network is that it can learn new features by itself. So, we do not need to define who is the father of one neuron. Actually, each neuron will take the activations as its input, but the parameter of some activations may be zero. We just need to input the training set and define the structure of neural network. Then, the neural network will produce the most suitable new features. That is, a neuron (traditional model) is actually a new feature.</p>
<blockquote>
<p>The structure of neural network is called <em><strong>neural network architecture</strong></em>. It defines the number of layers and the number of neurons in each layer.</p>
</blockquote>
<p><img src="/2023/04/10/NeuralNetwork/2.png" alt="2"></p>
<center style="font-size:12px;font-weight:bold">Fig. 2.</center>

<h2 id="Notation"><a href="#Notation" class="headerlink" title="Notation"></a>Notation</h2><ul>
<li>$a^{[i]}$ &#x3D; output of layer i;</li>
<li>$\vec{w}^{[i]}, b^{[i]}$&#x3D;parameters of layer i.</li>
</ul>
<h1 id="Forward-propagation-algorithm"><a href="#Forward-propagation-algorithm" class="headerlink" title="Forward propagation algorithm"></a>Forward propagation algorithm</h1><p>Forward propagation is a series of steps to count $f$. It is an inference or prediction of $y$. So, it is similar to $\widehat{y}$ in traditional model.</p>
<p><img src="/2023/04/10/NeuralNetwork/3.png" alt="3"></p>
<center style="font-size:12px;font-weight:bold">Fig. 3. Handwritten digit recognition</center>

<h2 id="Numpy-and-Tensorflow"><a href="#Numpy-and-Tensorflow" class="headerlink" title="Numpy and Tensorflow"></a>Numpy and Tensorflow</h2><p>The data representation in numpy is slightly different from tensorflow. In numpy, we can represent data either in the form of matrix or in the form of vector:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">x = np.array([[<span class="number">200</span>, <span class="number">17</span>]]) <span class="comment"># array 1*2</span></span><br><span class="line">x = np.array([[<span class="number">200</span>],[<span class="number">17</span>]]) <span class="comment"># array 2*1</span></span><br><span class="line">x = np.array([<span class="number">200</span>, <span class="number">17</span>]) <span class="comment"># just a row vector</span></span><br></pre></td></tr></table></figure>
<p>But we can only represent data in the form of matrix in tensorflow. Therefore, when using numpy and tensorflow together, it is advisable to store the data in the form of matrix.</p>
<p>The followings are the implementation of a neuron network about coffee roasting using numpy and tensorflow. (Assuming the neural network has been trained)</p>
<p><img src="/2023/04/10/NeuralNetwork/4.png" alt="4"></p>
<center style="font-size:12px;font-weight:bold">Fig. 4. Coffee roasting (two inputs)</center><br>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Dense</span><br><span class="line"></span><br><span class="line">x = np.array([[<span class="number">200.0</span>, <span class="number">17.0</span>]])</span><br><span class="line">layer_1 = Dense(units=<span class="number">3</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>)</span><br><span class="line">a1 = layer_1(x)</span><br><span class="line"></span><br><span class="line">layer_2 = Dense(units=<span class="number">1</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>)</span><br><span class="line">a2 = layer_2(a1)</span><br></pre></td></tr></table></figure>
<p>The data type of <code>a1</code> and <code>a2</code> are tensor,which is a built-in type in tensorflow :</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">When print a1: </span><br><span class="line">tf.Tensor([[0.2 0.7 0.3]], shape=(1, 3), dtype=float32)</span><br></pre></td></tr></table></figure>
<p>We can also print it in the form of numpy:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">When print a1.numpy():</span><br><span class="line">array([0.2, 0.7, 0.3], dtype=float32)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>The difference between tensor and array is that tensor has shape and data while array just has data. Therefore, a tensor variable can actually be treated as an <em>image</em>. That is why tensor data can be processed in GPU.</p>
</blockquote>
<p>Instead of building a neural network layer by layer, we can directly concatenate the layers to form the neural network. That is what <code>Sequential</code> do:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> Sequential</span><br><span class="line"></span><br><span class="line">model = Sequential([Dense(units=<span class="number">3</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>), Dense(units=<span class="number">1</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>)])</span><br><span class="line">...</span><br><span class="line">model.predict(x)</span><br></pre></td></tr></table></figure>

<h1 id="Vectorization"><a href="#Vectorization" class="headerlink" title="Vectorization"></a>Vectorization</h1><p>GPU and some CPU functions are very good at doing large matrix multiplications. Neural network can be vectorized, because of which neural network can be processed rapidly.</p>
<p>For layer 1 in the neural network of fig.3, the vectorized version is:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">A = np.array([[<span class="number">200</span>, <span class="number">17</span>]])</span><br><span class="line">W = np.array([[<span class="number">1</span>, -<span class="number">3</span>, <span class="number">5</span>], [-<span class="number">2</span>, <span class="number">4</span>, -<span class="number">6</span>]])</span><br><span class="line">B = np.array([[-<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>]])</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dense</span>(<span class="params">A_in, W, B</span>) :</span><br><span class="line">    Z = np.matmul(A_in, W) + B</span><br><span class="line">    A_out = g(Z)  <span class="comment"># A_out is a row vector</span></span><br><span class="line">    <span class="keyword">return</span> A_out</span><br></pre></td></tr></table></figure>

<h1 id="Activation-function"><a href="#Activation-function" class="headerlink" title="Activation function"></a>Activation function</h1><p>Activation function is actually a reprocessing of the model $f$ and creates <strong>a new model</strong>. By using activation function, we can divide our model into two parts. The first part is <strong>uniform</strong> for all models:<br>$$z&#x3D;\vec{w}\cdot\vec{x}+b$$<br>To generate different models, we only need to select the most suitable activation function $g(z)$. And that is the second part. There are three commonly used activation functions: linear function (identity), Sigmoid (soft step) and ReLU (rectified linear unit).</p>
<h2 id="Linear-function"><a href="#Linear-function" class="headerlink" title="Linear function"></a>Linear function</h2><p><img src="/2023/04/10/NeuralNetwork/5.png" alt="5"></p>
<center style="font-size:12px;font-weight:bold">Fig. 5. Linear function</center><br>

<p>In linear function, we do not do anything to the first part of the model. Therefore, our model is just a linear regression model:<br>$$f&#x3D;g(z)&#x3D;\vec{w}\cdot\vec{x}+b$$<br>Since the linear function of a linear function is still a linear function, we actually do not use linear function in the hidden layer, otherwise, the hidden layer will be useless.</p>
<h2 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a>Sigmoid</h2><p><img src="/2023/04/10/NeuralNetwork/6.png" alt="6"></p>
<center style="font-size:12px;font-weight:bold">Fig. 6. Sigmoid</center><br>

<p>Sigmoid is useful when we the output just has two possible value. So it often be used in the output layer of binary classification.</p>
<h2 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h2><p><img src="/2023/04/10/NeuralNetwork/7.png" alt="7"></p>
<center style="font-size:12px;font-weight:bold">Fig. 7. ReLU</center><br>

<p>ReLU is one of the most commonly used activation function in <strong>hidden layer</strong>. As the slope of it does not change on the negative or positive semi-axis, the convergence speed of ReLU is much faster than Sigmoid. In addition, ReLU makes sense because it has a &quot;off&quot; point which enables neurons to stitch together to form complex non-linear functions:</p>
<p><img src="/2023/04/10/NeuralNetwork/8.png" alt="8"></p>
<center style="font-size:12px;font-weight:bold">Fig. 8. Unit0+Unit1+Unit2</center><br>

<blockquote>
<p>Andrew Ng suggests that for the output layer, we should select the activation function that produces the exact result we need, but for the hidden layer, it is advisable to choose ReLU as out default activation function.</p>
</blockquote>
<h1 id="Softmax-regression"><a href="#Softmax-regression" class="headerlink" title="Softmax regression"></a>Softmax regression</h1><p>Softmax regression or softmax activation function is used to deal with multiclass classification. Multiclass classification is an extension of binary classification. In multiclass classification, the number of output is more than two.</p>
<p><img src="/2023/04/10/NeuralNetwork/9.png" alt="9"></p>
<center style="font-size:12px;font-weight:bold">Fig. 9. Multiclass classification</center><br>

<p>In binary regression, $g(z)$ is actually the possibility of $a&#x3D;&#x3D;1$. We can also get the possibility of $a&#x3D;&#x3D;0$ which is $1-g(z)$. But in multiclass classification, we can not do that. To solve this, softmax calculates the probability of all possible values. We use $z_i$ to represent a possible value and $a_i$ to represent its possibility:<br>$$z_1&#x3D;\vec{w_1}\cdot\vec{x}+b_1;a_1&#x3D;\frac{e^{z_1}}{e^{z_1}+...+e^{z_n}}&#x3D;P(y&#x3D;1)|\vec{x})$$<br>$$...$$<br>$$z_n&#x3D;\vec{w_n}\cdot\vec{x}+b_n;a_n&#x3D;\frac{e^{z_n}}{e^<br>{z_1}+...+e^{z_n}}&#x3D;P(y&#x3D;n|\vec{x})$$<br>And the loss function is:<br>$$L(a_1,...,a_n,y)&#x3D;\begin{cases}<br>-\log{a_1},&amp;y&#x3D;1 \\<br>...&amp; \\<br>-log{a_n},&amp;y&#x3D;n<br>\end{cases}$$</p>
<p>The loss function will make $a_i$ tends to 1 when $y&#x3D;i$.Binary classfication is a special case where n&#x3D;2.</p>
<p>Softmax is a special activation in neural network as it is actually a layer. Its output is a vector whose elements are the possibility of values.</p>
<p><img src="/2023/04/10/NeuralNetwork/10.png" alt="10"></p>
<center style="font-size:12px;font-weight:bold">Fig. 10. Neural network with softmax</center>

<h1 id="Multi-label-classification"><a href="#Multi-label-classification" class="headerlink" title="Multi-label classification"></a>Multi-label classification</h1><p>Multi-label classification is another type of classification. In multi-label classification, we are required to classify a thing into as many labels as we want. To realize this, we just need to use several sigmoid functions in our output layer.</p>
<p><img src="/2023/04/10/NeuralNetwork/11.png" alt="11"></p>
<center style="font-size:12px;font-weight:bold">Fig. 11. Multi-label classification</center>

<h1 id="Adam-algorithm"><a href="#Adam-algorithm" class="headerlink" title="Adam algorithm"></a>Adam algorithm</h1><p>Adam algorithm is optimization of gradient descent, which will automatically modify $\alpha$. In adam algorithm, each neuron of the same layer has different $\alpha$ (the initial value is the same):</p>
<ul>
<li>If $w_j$ or $b$ keeps moving in the same direction, it increases $\alpha_j$;</li>
<li>If $w_j$ or $b$ keeps oscillating, it reduces $\alpha_j$.</li>
</ul>
<p><img src="/2023/04/10/NeuralNetwork/12.png" alt="12"></p>
<center style="font-size:12px;font-weight:bold">Fig. 12. Adam algorithm</center>
]]></content>
      <categories>
        <category>Machine Learning by AndrewNg</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Supervised Learning</title>
    <url>/2023/04/05/SupervisedLearning/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="Regression-model"><a href="#Regression-model" class="headerlink" title="Regression model"></a>Regression model</h1><p>The steps in regression model are as follow:</p>
<p><img src="/2023/04/05/SupervisedLearning/1.png" alt="1"></p>
<center style="font-size:12px;font-weight:bold">Fig. 1. Steps of regression model</center><br>

<p>$f$ is the <strong>function</strong> or <strong>model</strong> getting from the learning algorithm, which can be used to predict the output. $\widehat{y}$, the value of $f$, is the prediction of $y$.</p>
<h2 id="Linear-regression-model"><a href="#Linear-regression-model" class="headerlink" title="Linear regression model"></a>Linear regression model</h2><p>Linear regression model is the most simple model in regression model, in which, $f$ is just a linear function:</p>
<p>$${f}_{w,b}(x)&#x3D;wx+b$$</p>
<center>or</center>

<p>$$f(x)&#x3D;wx+b$$</p>
<p>$w$ and $b$ are the <em>parameters</em> that we (the learning algorithms) can adjust to make $f$ more accurate.</p>
<blockquote>
<p>Linear regression with single input is also called <em>univariate linear regression</em>.</p>
</blockquote>
<h3 id="Cost-function"><a href="#Cost-function" class="headerlink" title="Cost function"></a>Cost function</h3><p>Cost function compares $\widehat{y}$ to $y$. <strong>The better the model is, the smaller the value of the cost function is</strong>. The simplest and most commonly used cost function in linear regression is:</p>
<p>$$J_{(w,b)}&#x3D;\frac{1}{2m}\sum\limits_{i&#x3D;1}^{m}(\widehat{y}^{(i)}-y^{(i)})^2$$</p>
<p>which is called <em><strong>mean squared error cost function</strong></em>.</p>
<blockquote>
<p>Why $2m$?</p>
<p><em>This is for the convenience of later calculations. When deriving $J$ using the gradient descent method, if it is $2m$, there will not be any constant in the derivative function</em>:</p>
<p>$$\frac{\partial{J_{(w,b)}}}{\partial{w}}&#x3D;\frac{1}{m}\sum\limits_{i&#x3D;1}^{m}(\widehat{y}^{(i)}-y^{(i)})\frac{\partial{\widehat{y}^{(i)}}}{\partial{w}}$$</p>
</blockquote>
<p>Since the training set is constant, $J$ is just the function of $w$ and $b$. Therefore, <strong>our goal is to find $w$ and $b$ that minimize $J_{(w,b)}$</strong>.</p>
<blockquote>
<p>The 3D bowl-shaped surface plot, which is the plot of $J$, can also be visualized as a contour plot. In the contour plot, each oval contains the choices of $w$ and $b$ that result in the same value of $J$.</p>
</blockquote>
<p><img src="/2023/04/05/SupervisedLearning/2.png" alt="2"></p>
<center style="font-size:12px;font-weight:bold">Fig. 2. 3D plot and contour plot</center>

<h3 id="Gradient-descent"><a href="#Gradient-descent" class="headerlink" title="Gradient descent"></a>Gradient descent</h3><p>Gradient descent is an algorithm that can be used to minimize any function. The steps of gradient descent are:</p>
<ol>
<li><p>Start with some parameters (just set them to 0 is ok);</p>
</li>
<li><p>Keep changing the parameters to reduce $J$:<br>$$w&#x3D;w-\alpha\frac{\partial{J_{(w,b)}}}{\partial{w}}$$<br>$\alpha$: learning rate, (0,1], which is used to control the speed of gradient descent.</p>
<p>$w$: any parameter.</p>
<p>All the parameters should be updated <strong>simultaneously</strong>, which means that when updating one parameter, the value of other parameters should be their original values.</p>
<blockquote>
<p>Principle: The ${grad(f)}$ at a certain point is the direction in which the function changes the fastest.</p>
</blockquote>
</li>
<li><p>Get a minimum or near minimum $J$.</p>
</li>
</ol>
<p>To achieve this, $J$ has to be a <strong>bowl shape function (convex function)</strong> or <strong>a function with local minima</strong>.</p>
<p><img src="/2023/04/05/SupervisedLearning/3.png" alt="3"></p>
<center style="font-size:12px;font-weight:bold">Fig. 3. Function with locak minima</center><br>

<p>In linear regression, <em>mean squared error cost function</em> is always a bowl shape function because it squares the loss. If $\alpha$ is too small, the gradient descent will work but may be very slow.</p>
<p><img src="/2023/04/05/SupervisedLearning/4.png" alt="4"></p>
<center style="font-size:12px;font-weight:bold">Fig. 4. Model with small learning rate</center><br>

<p>If $\alpha$ is too large, the gradient descent may not work, which means it may fail to converge but diverge.</p>
<p><img src="/2023/04/05/SupervisedLearning/5.png" alt="5"></p>
<center style="font-size:12px;font-weight:bold">Fig. 5. Model with large learning rate</center><br>

<blockquote>
<p><em>Batch gradient descent</em>: Each step of gradient descent uses all the training examples.</p>
</blockquote>
<h2 id="Multiple-linear-regression-model"><a href="#Multiple-linear-regression-model" class="headerlink" title="Multiple linear regression model"></a>Multiple linear regression model</h2><p>When there are more than one features determining the output, it is advisable for us to use <em>vector</em>.</p>
<blockquote>
<p>Feature engineering: Using intuition to design new features by <strong>transforming</strong> or <strong>combining</strong> original features. Good features will make the model better.</p>
</blockquote>
<h3 id="Notation"><a href="#Notation" class="headerlink" title="Notation"></a>Notation</h3><ul>
<li>$x_j$ &#x3D; $j^{th}$ feature;</li>
<li>$n$ &#x3D; number of features;</li>
<li>${\vec{x}^{(i)}}$ &#x3D; features of $i^{th}$ training example.</li>
<li>$x_j^{(i)}$ &#x3D; value of feature $j$ in $i^{th}$ training example ($x$ can also be $\vec{x}$).</li>
</ul>
<h3 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h3><p>$$f_{w,b}(x_1,x_2,...,x_n)&#x3D;w_1x_1+w_2x_2+...+w_nx_n+b$$</p>
<p>is equal to:</p>
<p>$$f_{\vec{w},b}(\vec{x})&#x3D;\vec{w}\cdot\vec{x}+b$$</p>
<p>where</p>
<p>$$\vec{w}&#x3D;[w_1,w_2,...,w_n],\vec{x}&#x3D;[x_1,x_2,...,x_n]$$</p>
<h3 id="Vectorization"><a href="#Vectorization" class="headerlink" title="Vectorization"></a>Vectorization</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">w = np.array([<span class="number">1.0</span>, <span class="number">2.5</span>, -<span class="number">3.3</span>])</span><br><span class="line">b = <span class="number">4</span></span><br><span class="line">x = np.array([<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>])</span><br><span class="line"><span class="comment"># vectorization</span></span><br><span class="line">f = np.dot(w, x) + b</span><br></pre></td></tr></table></figure>

<p><code>np.dot()</code> can make use of parallel hardwares, so it is much faster than <code>for loop</code>. (<em>SIMD</em>)</p>
<blockquote>
<p>GPU will help to deal with <strong>vectorized code</strong>.</p>
</blockquote>
<h3 id="Cost-function-1"><a href="#Cost-function-1" class="headerlink" title="Cost function"></a>Cost function</h3><p>The cost function can also be represented as $J(\vec{w},b)$. All the parameters $\vec{w}$ and $b$ should also be updated simultaneously.</p>
<blockquote>
<p>Normal equation: This method only works for linear regression. It sovles for $w,b$ without iterations. However, it does not generalize to other learning algorithms and it is slow when the number of features is too large (&gt;10,000). It may be useful on the backend.</p>
</blockquote>
<h2 id="More-about-gradient-descent"><a href="#More-about-gradient-descent" class="headerlink" title="More about gradient descent"></a>More about gradient descent</h2><h3 id="Feature-scaling"><a href="#Feature-scaling" class="headerlink" title="Feature scaling"></a>Feature scaling</h3><p>Feature scaling is a mothod to accelerate gradient descent. When the value of some features is too large, what may happen is that even though $\alpha$ is small, the changes of some parameters are still <strong>too significant</strong>, which slows down the convergence speed of gradient descent.</p>
<p><img src="/2023/04/05/SupervisedLearning/6.png" alt="6"></p>
<center style="font-size:12px;font-weight:bold">Fig. 6. Unscaled features</center><br>

<p>One useful method is <em>feature scaling</em>. By scaling down some features, we can make the convergence speed of different parameters basically the same.</p>
<p><img src="/2023/04/05/SupervisedLearning/7.png" alt="7"></p>
<center style="font-size:12px;font-weight:bold">Fig. 7. Scaled features</center><br>

<p>Ways to scale features:</p>
<ul>
<li>Dividing by the maximum of the feature;</li>
<li>Mean normalization. This method may produce negative value, but the range of feature is always 1.To realize this:<ol>
<li>Get the mean value of this feature in the training set $\mu$;</li>
<li>Subtract $\mu$ from each example of this feature and divide the result by the difference between the maximum and minimum values of this feature.<br>$$x&#x3D;\frac{x-\mu}{max-min}$$</li>
</ol>
</li>
<li>Z-score normalization. $\sigma$ is the <strong>standard deviation</strong> of this feature.<br>$$x&#x3D;\frac{x-\mu}{\sigma}$$<blockquote>
<p>When predicting, the new $x$ should also be scaled using the same parameters as before.</p>
</blockquote>
</li>
</ul>
<h3 id="Ways-to-check-convergence"><a href="#Ways-to-check-convergence" class="headerlink" title="Ways to check convergence"></a>Ways to check convergence</h3><ol>
<li>Draw $J-iterations$ curve or learning curve.</li>
</ol>
<p><img src="/2023/04/05/SupervisedLearning/8.png" alt="8"></p>
<center style="font-size:12px;font-weight:bold">Fig. 8. Iterations curve</center>

<ol start="2">
<li>Automatic convergence test: Let $\epsilon$ be a small value. Once $J$ decreases by $\leqslant$ $\epsilon$ in one iteration, it converges.</li>
</ol>
<h3 id="Choose-a-better-alpha"><a href="#Choose-a-better-alpha" class="headerlink" title="Choose a better $\alpha$"></a>Choose a better $\alpha$</h3><p>Once the learning curve looks like these:</p>
<p><img src="/2023/04/05/SupervisedLearning/9.png" alt="9"></p>
<center style="font-size:12px;font-weight:bold">Fig. 9. Model with large learning rate</center><br>

<p>It indicates that $\alpha$ is too large. The way to choose a good $\alpha$ is starting with a relative <strong>small value</strong> (like 0.001). And check the astringency of $J$. If $J$ still does not converge, there may be <strong>some bugs</strong> in the code. Then, we can try to increase $\alpha$ by <strong>three times</strong>, until we find a relative large $\alpha$.</p>
<h2 id="Polynomial-regression"><a href="#Polynomial-regression" class="headerlink" title="Polynomial regression"></a>Polynomial regression</h2><p>In polynomial regression, the model $f$ is a polynomial, which means that one feature may occur several times with different powers. For example:</p>
<p>$$f(x)&#x3D;w_1x+w_2\sqrt{x}+b$$</p>
<p>In fact, by changing the power, one feature can produce infinite features. That is, $x$ and $\sqrt{x}$ are two different features.</p>
<blockquote>
<p>Since they are actually different features, we can use another <em>variable</em> to represent them. Then, the function may become: $f(x,z)&#x3D;w_1x+w_2z+b$. It is a linear function formally.</p>
</blockquote>
<p>Compared to linear regression, in polynomial regression, feature scaling and the selection of feature are more important.</p>
<h1 id="Classification-model"><a href="#Classification-model" class="headerlink" title="Classification model"></a>Classification model</h1><h2 id="Binary-classification-model"><a href="#Binary-classification-model" class="headerlink" title="Binary classification model"></a>Binary classification model</h2><p>When the output of classification model only has two possible values, such classification model can also be named <em><strong>binary classification</strong></em>. In binary classification, we always use 0 (false) and 1 (true) to represent the output. 0 is also called <em><strong>negative class</strong></em> and 1 is also called <em><strong>positive class</strong></em>.</p>
<p><strong>Logistic regression</strong> is the most commonly used model in binary classification:</p>
<p>$$f_{(\vec{w},b)}{(\vec{x})}&#x3D;g(\vec{w}\cdot\vec{x}+b)&#x3D;g(z)&#x3D;\frac{1}{1+e^{-(\vec{w}\cdot\vec{x}+b)}}$$</p>
<p>where </p>
<p>$$g(z)&#x3D;\frac{1}{1+e^{-z}},0&lt;g(z)&lt;1$$</p>
<p>is called <em><strong>sigmoid function</strong></em> or <em><strong>logistic function</strong></em>.</p>
<p><img src="/2023/04/05/SupervisedLearning/10.png" alt="10"></p>
<center style="font-size:12px;font-weight:bold">Fig. 10. Sigmoid function</center><br>

<p>Actually, the output of logistic regression can also be regarded as the possibility of <code>y==1</code>, so $f$ can also be represented as:<br>$$f_{(\vec{w},b)}{(\vec{x})}&#x3D;P(y&#x3D;1|\vec{x};\vec{w},b)$$</p>
<h3 id="Decision-boundary"><a href="#Decision-boundary" class="headerlink" title="Decision boundary"></a>Decision boundary</h3><p>Since the output of logistic regression should be either 0 or 1, we must turn the value between 0 and 1 to 0 or 1. That is, we should find a threshold. When $f \ge threshold$,$f&#x3D;1$, otherwise, $f&#x3D;0$. The threshold we choose is often 0.5. When $f \ge 0.5$ ($\widehat{y}&#x3D;1$):<br>$$g(z) \ge 0.5$$<br>$$\downarrow$$<br>$$z \ge 0$$<br>$$\downarrow$$<br>$$\vec{w}\cdot\vec{x}+b \ge 0$$<br>The curve $\vec{w}\cdot\vec{x}+b &#x3D; 0$ is called <em><strong>decision boundary</strong></em>, where $\widehat{y}$ could be 0 or 1. For example:</p>
<p><img src="/2023/04/05/SupervisedLearning/11.png" alt="11"></p>
<center style="font-size:12px;font-weight:bold">Fig. 11. Decision boundary-line</center>

<p><img src="/2023/04/05/SupervisedLearning/12.png" alt="12"></p>
<center style="font-size:12px;font-weight:bold">Fig. 12. Decision boundary-circle</center>

<h3 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h3><p>In linear regression, the cost function:</p>
<p>$$J_{(w,b)}&#x3D;\frac{1}{2m}\sum\limits_{i&#x3D;1}^{m}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})^2$$</p>
<p>can also be represented as:</p>
<p>$$J_{(w,b)}&#x3D;\frac{1}{m}\sum\limits_{i&#x3D;1}^{m}\frac{1}{2}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})^2$$</p>
<p>where:</p>
<p>$$L(f_{\vec{w},b}(\vec{x}^{(i)}),y^{(i)})&#x3D;\frac{1}{2}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})^2$$</p>
<p>is called <em><strong>loss function</strong></em>.</p>
<p>Loss function indicates the error between the prediction value and real value of one example in training set. The convexity of cost function is actually determined by loss function. In logistic regression, <em>squared error loss function</em> is a non-convex function.</p>
<p><img src="/2023/04/05/SupervisedLearning/13.png" alt="13"></p>
<center style="font-size:12px;font-weight:bold">Fig. 13. Non-convex loss function</center><br>

<p>Therefore, a new convex function is needed in logistic regression, that is:<br>$$L(f_{\vec{w},b}(\vec{x}^{(i)}),y^{(i)})&#x3D;-y^{(i)}\log(f_{\vec{w},b}(\vec{x}^{(i)}))-(1-y^{(i)})\log(1-f_{\vec{w},b}(\vec{x}^{(i)}))$$<br>where $y^{(i)}&#x3D;0,1$; $f_{\vec{w},b}(\vec{x}^{(i)})\in(0,1)$; $\log$ uses $\ln$.</p>
<p>When $y^{(i)}&#x3D;1$, the curve of $L$ is:</p>
<p><img src="/2023/04/05/SupervisedLearning/14.png" alt="14"></p>
<center style="font-size:12px;font-weight:bold">Fig. 14.</center><br>

<p>Using gradient descent will make the loss close to 0.</p>
<p>When $y^{(i)}&#x3D;0$,the curve of $L$ is:</p>
<p><img src="/2023/04/05/SupervisedLearning/15.png" alt="15"></p>
<center style="font-size:12px;font-weight:bold">Fig. 15.</center><br>

<p>Using gradient descent will also make the loss close to 0.</p>
<h3 id="Cost-function-2"><a href="#Cost-function-2" class="headerlink" title="Cost function"></a>Cost function</h3><p>$$J_{(w,b)}&#x3D;-\frac{1}{m}\sum\limits_{i&#x3D;1}^{m}[y^{(i)}\log(f_{\vec{w},b}(\vec{x}^{(i)}))+(1-y^{(i)})\log(1-f_{\vec{w},b}(\vec{x}^{(i)}))]$$<br>The derivative of $J$ in logistic regression is actually the same as that in linear regression:<br>$$w_j&#x3D;w_j-\alpha[\frac{1}{m}\sum\limits_{i&#x3D;1}^{m}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})x_j^{(i)}]$$<br>$$b&#x3D;b-\alpha[\frac{1}{m}\sum\limits_{i&#x3D;1}^{m}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})]$$</p>
<p>However, the model $f$ is different.</p>
<blockquote>
<p>Vectorization and feature scaling can also be used in losgistic regression.</p>
</blockquote>
<h2 id="Multiclass-classification-model"><a href="#Multiclass-classification-model" class="headerlink" title="Multiclass classification model"></a>Multiclass classification model</h2><p>See <a href="/2023/04/10/NeuralNetwork/#Softmax-regression">Softmax regression</a>.</p>
<h2 id="Multi-layer-classification-model"><a href="#Multi-layer-classification-model" class="headerlink" title="Multi-layer classification model"></a>Multi-layer classification model</h2><p>See <a href="/2023/04/10/NeuralNetwork/#Multi-label-classification">Multi-label classification</a>.</p>
<h1 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h1><h2 id="Underfitting"><a href="#Underfitting" class="headerlink" title="Underfitting"></a>Underfitting</h2><p>When a model does not fit the training set well, the model is underfitting,</p>
<p><img src="/2023/04/05/SupervisedLearning/16.png" alt="16"></p>
<center style="font-size:12px;font-weight:bold">Fig. 16. Underfitting model</center><br>

<p>An underfitting model is an algorithm having <strong>high bias</strong>. In this case, we need to change a model.</p>
<h2 id="Overfitting"><a href="#Overfitting" class="headerlink" title="Overfitting"></a>Overfitting</h2><p>When a model fits the training set extremely well, that is, the errors are approximate 0, the model is overfitting.</p>
<p><img src="/2023/04/05/SupervisedLearning/17.png" alt="17"></p>
<center style="font-size:12px;font-weight:bold">Fig. 17. Overfitting model</center><br>

<p>An overfitting model is an algorithm having <strong>high variance</strong>. In this case, the model just fits the training set well, but it can not be generalized. This problem often occurs when the training set is too small. So, the first method to solve this problem is to <strong>collect more training examples</strong>. Besides, <strong>selecting more suitable features</strong> is also feasible (<em><strong>Feature Selection</strong></em>). However, this method may cause our model to lose some useful features. The most feasible and commonly used method is <strong>regularization</strong>.</p>
<h2 id="Weight-decay"><a href="#Weight-decay" class="headerlink" title="Weight decay"></a>Weight decay</h2><p>Overfitting may occurs when some features have an overly large effect, that is, a little change of these features may cause large changes to the model, which can make the model unexpandable.</p>
<p>The core idea of regularization is to <strong>reduce the weight of features with large value</strong>. When regularizing, the cost function $J$ becomes:</p>
<p>$$J(\vec{w},b)&#x3D;\frac{1}{m}\sum\limits_{i&#x3D;1}^{m}L(f_{\vec{w},b}(\vec{x}^{(i)}),y^{(i)})+\frac{\lambda}{2m}\sum\limits_{j&#x3D;1}^{n}w_j^2$$</p>
<p>We just need to regularize $w_j$ as $b$ just makes $f$ move up or down.</p>
<blockquote>
<p>$\lambda$ (&gt;0) is an important parameter that balance fitting data and keeping $w_j$ small. In fact, this is <strong>Lagrange multiplier</strong>, a method to find local extremea of a multivariate function when its variables are constrained by one or more conditions. The restriction here is $\frac{1}{2m}\sum\limits_{j&#x3D;1}^{n}w_j^2&#x3D;0$. Since we have limited the value of $\lambda$, our function doesn&#39;t obey the constraint strictly but the constraint does make $w$ smaller and the curve more smooth.</p>
</blockquote>
<p>Since $J$ changes, the update of $w_j$ will also change:</p>
<p>$$w_j&#x3D;w_j-\alpha[\frac{1}{m}\sum\limits_{i&#x3D;1}^{m}\frac{\partial{L(f_{\vec{w},b}(\vec{x}^{(i)}),y^{(i)})}}{\partial{w_j}}+\frac{\lambda}{m}w_j]$$<br>or<br>$$w_j&#x3D;(1-\alpha\frac{\lambda}{m})w_j-\alpha\frac{1}{m}\sum\limits_{i&#x3D;1}^{m}\frac{\partial{L(f_{\vec{w},b}(\vec{x}^{(i)}),y^{(i)})}}{\partial{w_j}}$$</p>
<p>The second formula indicates that in each iteration, $w_j$ will be smaller than before. Therefore, this kind of normalization is also called <strong>weight decay</strong>.</p>
]]></content>
      <categories>
        <category>Machine Learning by AndrewNg</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Supervised Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Machine Learning</title>
    <url>/2023/04/05/MachineLearning/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="Machine-Learning"><a href="#Machine-Learning" class="headerlink" title="Machine Learning"></a>Machine Learning</h1><blockquote>
<p>Definition: Field of study that gives computers the ability to learn without being explicitly programmed.</p>
</blockquote>
<p>In general, there are three fields in machine learning, that is, supervised learning, unsupervised learning and reinforcement learning. Their relationships are as follow:</p>
<p><img src="/2023/04/05/MachineLearning/1.png" alt="1"></p>
<center style="font-size:12px;font-weight:bold">Fig. 1. Fields of machine learning</center>

<h2 id="Supervised-learning"><a href="#Supervised-learning" class="headerlink" title="Supervised learning"></a>Supervised learning</h2><p>Supervised learning is the most commonly used machine learning. It&#39;s definition is:</p>
<blockquote>
<p>Supervised learning refers to algorithms that learn input to output mappings. The key of it is that you give the <strong>input(x)</strong> and the <strong>correct output(y)</strong> for the algorithms to learn. Then, the learning algorithms eventually learn to get the input alone and give a reasonably accurate prediction or guess of the output. <strong>In short, supervised learning learns from data labeled with the &quot;right answers&quot;</strong>.</p>
</blockquote>
<p>There are generally two types of supervised learning, <strong>regression</strong> and <strong>classification</strong>.</p>
<h3 id="Regression"><a href="#Regression" class="headerlink" title="Regression"></a>Regression</h3><p>Regression is a kind of supervised learning that predicts <strong>a number</strong>, so its output contains <strong>infinitely</strong> many possible results. For example, housing price prediction:</p>
<p><img src="/2023/04/05/MachineLearning/2.png" alt="2"></p>
<center style="font-size:12px;font-weight:bold">Fig. 2. House price prediction</center>

<h3 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h3><p>Classification is a kind of supervised learning that predicts <strong>categories</strong> which can be <strong>number or non numeric</strong>, so its output just contains a small number of results. For example, breast cancer detection:</p>
<p><img src="/2023/04/05/MachineLearning/3.png" alt="3"></p>
<center style="font-size:12px;font-weight:bold">Fig. 3. Breast cancer detection</center><br>

<p>Both regression and classification can have <strong>more than one</strong> inputs.</p>
<h2 id="Unsupervised-learning"><a href="#Unsupervised-learning" class="headerlink" title="Unsupervised learning"></a>Unsupervised learning</h2><p>Unsupervised learning&#39;s definition:</p>
<blockquote>
<p>Unsupervised learning refers to algorithm that find something interesting in unlabed data. In unsupervised learning,the algorithms figure out all by themselves what&#39;s interesting or what patterns or structures might be in the dataset. <strong>Data only comes with input(x), but not output labels(y)</strong>. Algorithm has to find structure in the data.</p>
</blockquote>
<p>There are generally three types of unsupervised learning, <strong>clustering</strong>, <strong>anomaly detection</strong> and <strong>dimensionality reduction</strong>.</p>
<h3 id="Clustering"><a href="#Clustering" class="headerlink" title="Clustering"></a>Clustering</h3><p>Clustering is a kind of unsupervised learning that <strong>groups</strong> data into different categories by any standard. For example, grouping customers:</p>
<p><img src="/2023/04/05/MachineLearning/4.png" alt="4"></p>
<center style="font-size:12px;font-weight:bold">Fig. 4. Grouping customers</center>

<h3 id="Anomaly-detection"><a href="#Anomaly-detection" class="headerlink" title="Anomaly detection"></a>Anomaly detection</h3><p>Anomaly detection is a kind of unsupervised learning that finds unusual data points. It is very useful in financial system.</p>
<h3 id="Dimensionality-reduction"><a href="#Dimensionality-reduction" class="headerlink" title="Dimensionality reduction"></a>Dimensionality reduction</h3><p>This kind of unsupervised learning compresses data using fewer numbers.</p>
<h2 id="Reinforcement-learning"><a href="#Reinforcement-learning" class="headerlink" title="Reinforcement learning"></a>Reinforcement learning</h2><p>Reinforcement learning is an new field in machine learning that has not yet been well applied but has a promising future.</p>
<h1 id="Tool"><a href="#Tool" class="headerlink" title="Tool"></a>Tool</h1><h2 id="Jupyter-notebook"><a href="#Jupyter-notebook" class="headerlink" title="Jupyter notebook"></a>Jupyter notebook</h2><p>Jupyter notebook is the default <strong>environment</strong> that most researchers use to code up and experiment.</p>
<h2 id="PyTorch"><a href="#PyTorch" class="headerlink" title="PyTorch"></a>PyTorch</h2><p>see <a href="/2023/04/28/PyTorch/">PyTorch</a>.</p>
<h2 id="Tensorflow"><a href="#Tensorflow" class="headerlink" title="Tensorflow"></a>Tensorflow</h2><p>See <a href="/2023/04/12/Tensorflow/">Tensorflow</a>.</p>
<h2 id="NumOy"><a href="#NumOy" class="headerlink" title="NumOy"></a>NumOy</h2><p>See <a href="/2023/04/22/NumPy/">NumPy</a>.</p>
<h2 id="Scikit-learn"><a href="#Scikit-learn" class="headerlink" title="Scikit-learn"></a>Scikit-learn</h2><p>See <a href="/2023/04/22/Scikit-learn/">Scikit-learn</a>.</p>
<h1 id="Terminology"><a href="#Terminology" class="headerlink" title="Terminology"></a>Terminology</h1><ul>
<li>Training set: Data used to train a model.</li>
<li>Test set: Data used to evaluate a model.</li>
</ul>
<h1 id="Notation"><a href="#Notation" class="headerlink" title="Notation"></a>Notation</h1><ul>
<li>${x}$ &#x3D; &quot;input&quot; variable or feature;</li>
<li>${y}$ &#x3D; &quot;output&quot; variable or &quot;target&quot; variable;</li>
<li>${m}$ &#x3D; number of training examples;</li>
<li>$(x, y)$ &#x3D; a single training example;</li>
<li>$(x^i,y^i)$ &#x3D; $i^{th}$ training example;</li>
<li>$x_j$ &#x3D; $j^{th}$ feature;</li>
<li>$n$ &#x3D; number of features;</li>
<li>${\vec{x}^{(i)}}$ &#x3D; features of $i^{th}$ training example.</li>
<li>$x_j^{(i)}$ &#x3D; value of feature $j$ in $i^{th}$ training example ($x$ can also be $\vec{x}$).</li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning by AndrewNg</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Bash Script</title>
    <url>/2023/09/02/BashScript/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="Basic-Concept"><a href="#Basic-Concept" class="headerlink" title="Basic Concept"></a>Basic Concept</h1><p>一些关于Bash和Shell的基本概念。</p>
<h2 id="Bash-amp-Shell"><a href="#Bash-amp-Shell" class="headerlink" title="Bash &amp; Shell"></a>Bash &amp; Shell</h2><p>Shell是用户和操作系统的接口。它是一个命令解释器。Bash（Bourne Again SHell）是Shell的一个类型，由GNU开发，是对Bourne Shell（sh）的改进和扩展，同时也是当今Linux和MacOS的默认Shell。</p>
<blockquote>
<p>更通俗地来说，Shell相当于Linux操作系统，而Bash则是Linux操作系统的一个发行版，如Ubuntu。</p>
</blockquote>
<h2 id="Bash-Script-amp-Shell-Script"><a href="#Bash-Script-amp-Shell-Script" class="headerlink" title="Bash Script &amp; Shell Script"></a>Bash Script &amp; Shell Script</h2><p>脚本（Script），也就是我们常说的解释性编程语言，常见的Python、JavaScript都是脚本语言。Bash脚本和Shell脚本都是让Shell能够执行一连串命令行指令的程序，它们的关系就类似于Bash和Shell的关系：</p>
<ul>
<li>Bash脚本只能用Bash语言编写，其语法也是Bash的语法，而Shell脚本则能用任何的Shell语法编写；</li>
<li>Bash脚本的文件后缀为<code>.sh</code>，而Shell脚本就是一个普通的文本文件，不需要任何后缀，只要可执行即可。</li>
</ul>
<blockquote>
<p>无论是Bash脚本还是Shell脚本，都需要相应的Shell解释器来执行。使用<code>cat /etc/shells</code>能够查看本机所拥有的Shell解释器。</p>
</blockquote>
<h1 id="Execution-amp-Comment"><a href="#Execution-amp-Comment" class="headerlink" title="Execution &amp; Comment"></a>Execution &amp; Comment</h1><p>对于任何一个Bash脚本，编写时首先要指定其要使用的解释器：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#! /bin/bash</span></span><br></pre></td></tr></table></figure>

<p>上述行指定该Bash脚本将使用解释器<code>/bin/bash</code>运行。运行Bash脚本时，我们可以以文件名运行，也可以以<code>bash</code>加文件名运行，如一个最简单的Bash脚本：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[zclzcl@localhost Playground]$ <span class="built_in">touch</span> helloScript.sh</span><br><span class="line">[zclzcl@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">helloScript.sh</span><br><span class="line">[zclzcl@localhost Playground]$ vim helloScript.sh</span><br><span class="line"><span class="comment">#! /bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;Goodbye, world&quot;</span></span><br><span class="line">[zclzcl@localhost Playground]$ ./helloScript.sh</span><br><span class="line">Goodbye, world</span><br><span class="line">[zclzcl@localhost Playground]$ bash ./helloScript.sh</span><br><span class="line">Goodbye, world</span><br></pre></td></tr></table></figure>

<blockquote>
<p>最简单的Bash脚本就是一系列Linux命令行指令的组合。此时，运行该Bash脚本相当于在当前目录下串行地运行这些指令。要想运行Bash脚本，首先要确保脚本文件是可执行的，若用户无执行脚本文件的权限，则需要用<code>chmod +x [Script File]</code>增加权限。</p>
</blockquote>
<p>与其他编程语言一样，我们也能在脚本文件中增加注释来增强脚本文件的可读性。在Bash脚本中，单行注释用<code>#</code>，多行注释用<code>: &#39;&#39;</code>（引号内是注释内容）。</p>
]]></content>
      <categories>
        <category>Programming Language</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Tool</tag>
      </tags>
  </entry>
  <entry>
    <title>Regular Expressions</title>
    <url>/2023/08/04/RegularExpressions/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="基本格式与匹配范围"><a href="#基本格式与匹配范围" class="headerlink" title="基本格式与匹配范围"></a>基本格式与匹配范围</h1><p>基本格式为<code>/[regx]/[g\i\m\s\u\y]</code>。其中<code>[g\i\m\s\u\y]</code>每个字母表示一种匹配范围，可同时出现：</p>
<p><code>/g</code>：输出所有匹配结果；<br><code>/i</code>：不区分大小写；<br><code>/m</code>：<code>^</code>或<code>$</code>定位符在场时才发挥作用；<br><code>/</code>：只输出第一个匹配结果。</p>
<blockquote>
<p><a href="https://regexr.com/">Regexr Website</a>是一个很好的练习正则表达式的网站，它的界面简洁：上方是正则表达式，中间是待匹配文本，下方则会详细给出每个表达式的匹配细节同时还提供了一些实用工具。</p>
</blockquote>
<p><img src="/2023/08/04/RegularExpressions/1.png" alt="1"></p>
<center style="font-size:12px; font-weight:bold">Fig. 1. Regexr Website</center>

<hr>
<h1 id="运算规则与优先级"><a href="#运算规则与优先级" class="headerlink" title="运算规则与优先级"></a>运算规则与优先级</h1><p>所有的正则表达式，无论是基本运算符还是复杂运算符，都遵循优先级从左到右匹配。基本的优先级从高到低顺序为：</p>
<table>
<thead>
<tr>
<th align="center">运算符</th>
<th align="center">描述</th>
</tr>
</thead>
<tbody><tr>
<td align="center">\\</td>
<td align="center">转义字符</td>
</tr>
<tr>
<td align="center">(), []</td>
<td align="center">圆括号，方括号</td>
</tr>
<tr>
<td align="center">*, +, ?, {}</td>
<td align="center">限定符</td>
</tr>
<tr>
<td align="center">^, $, \w等元字符, 任何字符</td>
<td align="center">定位点和字符</td>
</tr>
<tr>
<td align="center">|</td>
<td align="center">逻辑运算符，或</td>
</tr>
</tbody></table>
<p>对于待匹配字符串，也遵循从左到右、从上到下的匹配顺序。因此，一旦某子串符合正则表示式的匹配要求，该子串便会立即被“摘出”字符串，不再参与后续的匹配。</p>
<hr>
<h1 id="简单的运算符"><a href="#简单的运算符" class="headerlink" title="简单的运算符"></a>简单的运算符</h1><ul>
<li><code>+</code>：对<code>+</code>的前一个字符匹配1个以上；</li>
<li><code>?</code>：对<code>?</code>的前一个字符匹配0或1个；</li>
<li><code>*</code>：对<code>*</code>的前一个字符匹配0或多个；</li>
<li><code>.</code>：匹配任何1个字符（除换行符）；</li>
<li><code>\</code>：转义字符，使运算符失去正则意义；</li>
<li><code>\d</code>：匹配数字；</li>
<li><code>\w</code>：匹配单词；</li>
<li><code>\s</code>：匹配空白符，包括space、tab、line break。</li>
</ul>
<blockquote>
<p><code>\w</code>等的大写表示匹配与原小写相反的项目，如<code>\W</code>匹配所有的非单词，实际相当于对小写的匹配结果取非。</p>
</blockquote>
<hr>
<h1 id="较复杂的运算符"><a href="#较复杂的运算符" class="headerlink" title="较复杂的运算符"></a>较复杂的运算符</h1><h2 id="括号类"><a href="#括号类" class="headerlink" title="括号类"></a>括号类</h2><ul>
<li><code>&#123;[minLength],[maxLength]&#125;</code>：对<code>&#123;[minLength],[maxLength]&#125;</code>前的所有匹配项，限制每个匹配项的长度为<code>minLength</code>-<code>maxLength</code>，其中匹配项长度小于<code>[minLength]</code>的将会被剔除，而长度大于<code>[maxLength]</code>的则会只取<code>[maxLength]</code>部分。如<code>&#123;4,5&#125;</code>只会匹配其之前的匹配项中所有长度大于4的，并对大于5的限定长度为5，限定长度后的剩下部分若仍符合前面的匹配要求，则当作新的匹配项继续参与<code>&#123;[minLength],[maxLength]&#125;</code>。若只有<code>&#123;[minLength]&#125;</code>则默认<code>[maxLength]=[minLength]</code>；若是<code>&#123;[minLength],&#125;</code>则将匹配所有长度大于<code>[minLength]</code>的；</li>
<li><code>[[Groups]]</code>：匹配任何一个<code>[[Groups]]</code>中的字符。这很像通配符中<code>[]</code>的用法，但是不同的是，正则表达式中没有组的概念，因此<code>[Groups]</code>只能是用户自定的一组备选的字符或序列，如<code>[abg]</code>将匹配<code>a</code>或<code>b</code>或<code>g</code>；<code>a-f</code>将匹配<code>a</code>到<code>f</code>的任意一个字符；<code>1-9</code>将匹配<code>1</code>到<code>9</code>的任意一个字符。<code>[]</code>支持组合使用，如<code>a-f1-9</code>将匹配<code>a</code>到<code>f</code>或<code>1</code>到<code>9</code>的任意一个字符；</li>
<li><code>()</code>：<code>()</code>表将里面的内容视作一个整体，即里面的内容在匹配时实际上是一个字符，如<code>(re)&#123;2,3&#125;</code>将匹配所有的<code>re</code>重复超过2次的连续串中的<code>rere</code>或<code>rerere</code>部分，此时<code>re</code>整体被视为一个字符；</li>
</ul>
<h2 id="逻辑运算符与定位符"><a href="#逻辑运算符与定位符" class="headerlink" title="逻辑运算符与定位符"></a>逻辑运算符与定位符</h2><ul>
<li><code>|</code>：逻辑运算符<code>or</code>，匹配结果是符合<code>|</code>左边或右边内容的字符串。若无括号将整个<code>|</code>表达式括起，<code>|</code>会默认将左边和右边的视为一个整体，因为<code>|</code>的优先级最低，如<code>tre|it</code>只会匹配<code>tre</code>或<code>it</code>，而<code>t(re|it)</code>则会匹配<code>&#39;tre</code>或<code>tit</code>；</li>
<li><code>^</code>：需放在正则表达式的开头才起作用，表示匹配字符串的开头，也就是说<code>^</code>后面的表达式只有出现在字符串的开头才会被匹配。默认情况下，整篇文章会被视作一个字符串，但是若使用<code>/m</code>选项，则每一行会被视作一个字符串，此时<code>^</code>的意义就是匹配每一行的开头；</li>
<li><code>$</code>：需放在正则表达式的最后才起作用，类似于<code>^</code>，表匹配文章或行的末尾；</li>
<li><code>?&lt;=</code>：向前看，一般而言，这个表达式必须和括号<code>()</code>一起使用。它是<code>^</code>的拓展版，表示从以括号内的表达式开头的部分开始匹配，但是不匹配括号内的内容，如<code>(?&lt;=[tT]he).</code>将匹配<code>the</code>或<code>The</code>后面的任意一个字符；</li>
<li><code>?&lt;!</code>：<code>?&lt;=</code>的取反，如<code>(?&lt;![tT]he).</code>将匹配除<code>(?&lt;=[tT]he).</code>匹配结果的所有字符；</li>
<li><code>?=</code>：向后看，<code>?&lt;=</code>的倒装版，是<code>$</code>的拓展，表示从以括号内的表达式结尾的部分开始匹配，如<code>.(?=[tT]he)</code>将匹配<code>the</code>或<code>The</code>前面的任意一个字符；</li>
<li><code>?!</code>：<code>?=</code>的取反。</li>
</ul>
<blockquote>
<p>需要注意的是，并不是所有的正则表达式标准都包含了<code>?&lt;=</code>和<code>?&lt;!</code>。</p>
</blockquote>
<hr>
<h1 id="实战-电话号码匹配"><a href="#实战-电话号码匹配" class="headerlink" title="实战-电话号码匹配"></a>实战-电话号码匹配</h1><p>对于一般的11位电话号码，其最常规的写法有如下这三种：</p>
<figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">1234567890</span><br><span class="line">123-456-7890</span><br><span class="line">123 456 7890</span><br></pre></td></tr></table></figure>

<p>不难看出，上面的写法都可以统一成334写法，因此我们可以先按334将电话号码分组：</p>
<figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">/(\d&#123;3&#125;)(\d&#123;3&#125;)(\d&#123;4&#125;)/g</span><br></pre></td></tr></table></figure>

<p>三者的区别就在于<code> </code>、<code>-</code>以及无字符，对于<code> </code>和<code>-</code>，我们可以使用中括号表任选其一<code>[ -]</code>，而对于无字符则可以采用<code>?</code>来匹配前方的0或1个字符：</p>
<figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">/(\d&#123;3&#125;)[ -]?(\d&#123;3&#125;)[ -]?(\d&#123;4&#125;)/g</span><br></pre></td></tr></table></figure>

<p>上述式子已经可以匹配所有的这3种写法。有时，还会出现加了国际区号和使用括号的写法，如：</p>
<figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">(123) 456-7890</span><br><span class="line">+1 123 456 7890</span><br></pre></td></tr></table></figure>

<p>对于加括号的，用<code>\(?</code>和<code>\)?</code>处理即可。对于带区号的，则可以<code>(\+\d )?</code>处理。所有的合起来就是：</p>
<figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">/(\+\d )?(\(?\d&#123;3&#125;\)?)[ -]?(\d&#123;3&#125;)[ -]?(\d&#123;4&#125;)/g</span><br></pre></td></tr></table></figure>

<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://youtu.be/rhzKDrUiJVk">Learn Regular Expressions In 20 Minutes</a></li>
</ul>
]]></content>
      <categories>
        <category>Tool</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Tool</tag>
      </tags>
  </entry>
  <entry>
    <title>Wildcards and Pattern Matching</title>
    <url>/2023/08/03/WildcardsPatternMatching/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="：匹配任意个数的任意字符"><a href="#：匹配任意个数的任意字符" class="headerlink" title="*：匹配任意个数的任意字符"></a><code>*</code>：匹配任意个数的任意字符</h1><p><code>*</code>可以代表任意个数的任意的连续字符：</p>
<ul>
<li><code>*.txt</code>：匹配所有以<code>.txt</code>结尾的字符串；</li>
<li><code>g*</code>：匹配所有以<code>g</code>开头的字符串；</li>
<li><code>g*.txt</code>：匹配所有以<code>g</code>开头、<code>.txt</code>结尾的字符串。</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[zclzcl@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">file1.txt  file2.txt  gd  gg  ground  ground.txt  g.txt</span><br><span class="line">[zclzcl@localhost Playground]$ <span class="built_in">ls</span> *.txt</span><br><span class="line">file1.txt  file2.txt  ground.txt  g.txt</span><br><span class="line">[zclzcl@localhost Playground]$ <span class="built_in">ls</span> g*</span><br><span class="line">gd  gg  ground  ground.txt  g.txt</span><br><span class="line">[zclzcl@localhost Playground]$ <span class="built_in">ls</span> g*.txt</span><br><span class="line">ground.txt  g.txt</span><br></pre></td></tr></table></figure>

<h1 id="：匹配一个字符"><a href="#：匹配一个字符" class="headerlink" title="?：匹配一个字符"></a><code>?</code>：匹配一个字符</h1><p><code>?</code>可以代表任意的1个字符。需要注意的是，<code>?</code>必须代表1个字符，也就是说既不能是0个，也不能多于1个：</p>
<ul>
<li><code>?.txt</code>：匹配所有以<code>.txt</code>结尾且<code>.txt</code>之前只有一个字符的字符串；</li>
<li><code>g?</code>：匹配所有以<code>g</code>开头且只有两个字符的字符串；</li>
<li><code>g?.txt</code>：匹配所有以<code>g</code>开头、<code>.txt</code>结尾且<code>g</code>和<code>.txt</code>之间只有一个字符的字符串。</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[zclzcl@localhost Playground]$ <span class="built_in">ls</span> ?.txt</span><br><span class="line">g.txt</span><br><span class="line">[zclzcl@localhost Playground]$ <span class="built_in">ls</span> g?</span><br><span class="line">gd  gg</span><br><span class="line">[zclzcl@localhost Playground]$ <span class="built_in">ls</span> g?.txt</span><br><span class="line"><span class="built_in">ls</span>: cannot access g?.txt: No such file or directory</span><br></pre></td></tr></table></figure>

<h1 id="：匹配方框中的任意一个字符"><a href="#：匹配方框中的任意一个字符" class="headerlink" title="[]：匹配方框中的任意一个字符"></a><code>[]</code>：匹配方框中的任意一个字符</h1><p><code>[]</code>会匹配内部的任意一个且必须是一个字符，比如：</p>
<ul>
<li>与<code>g[dg]</code>相匹配的只有<code>gd</code>与<code>gg</code>。</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[zclzcl@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">file1.txt  file2.txt  gd  gg  ground  ground.txt  g.txt</span><br><span class="line">[zclzcl@localhost Playground]$ <span class="built_in">ls</span> g[dg]</span><br><span class="line">gd  gg</span><br><span class="line">[zclzcl@localhost Playground]$ <span class="built_in">ls</span> g[rd]*</span><br><span class="line">gd  ground  ground.txt</span><br></pre></td></tr></table></figure>

<h1 id="：匹配任意不在方框中的一个字符"><a href="#：匹配任意不在方框中的一个字符" class="headerlink" title="[^]：匹配任意不在方框中的一个字符"></a><code>[^]</code>：匹配任意不在方框中的一个字符</h1><p><code>[^]</code>表明其所在位置（只占一位）的匹配结果要不包含方框内的字符，比如：</p>
<ul>
<li>与<code>g[^dg]</code>相匹配的是任何第二个字符不是<code>d</code>或<code>g</code>的以<code>g</code>开头的两字符字符串。</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[zclzcl@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">file1.txt  file2.txt  gd  gg  ground  ground.txt  g.txt</span><br><span class="line">[zclzcl@localhost Playground]$ <span class="built_in">ls</span> g[^dg]*</span><br><span class="line">ground  ground.txt  g.txt</span><br></pre></td></tr></table></figure>

<h1 id="start-end-：匹配-start-和-end-之间的任意一个字符"><a href="#start-end-：匹配-start-和-end-之间的任意一个字符" class="headerlink" title="[[start]-[end]]：匹配[start]和[end]之间的任意一个字符"></a><code>[[start]-[end]]</code>：匹配<code>[start]</code>和<code>[end]</code>之间的任意一个字符</h1><p><code>[[start]-[end]]</code>允许我们匹配任何一个在<code>[start]</code>和<code>[end]</code>之间的字符，其中<code>[start]</code>和<code>[end]</code>必须是两个存在顺序关系的字符，比如数字、字母等：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[zclzcl@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">file1.txt  file2.txt  gd  gg  ground  ground.txt  g.txt</span><br><span class="line">[zclzcl@localhost Playground]$ <span class="built_in">ls</span> g[g-r]*</span><br><span class="line">gg  ground  ground.txt</span><br></pre></td></tr></table></figure>

<p>上面指令会匹配所有以<code>g</code>开头，且第二个字符在<code>g</code>和<code>r</code>之间的字符串。</p>
<blockquote>
<p>以上这些涉及<code>[]</code>的匹配，<code>[]</code>内的内容除了可以是用户指定的字符串外，还可以是一类字符串，如：</p>
<ul>
<li><code>[:alnum:]</code>表任意一个字符或字母；</li>
<li><code>[:alpha:]</code>表任意一个字母；</li>
<li><code>[:digit:]</code>表任意一个数字；</li>
<li><code>[:lower:]</code>表任意一个小写字母；</li>
<li><code>[:upper:]</code>表任意一个大写字母。</li>
</ul>
<p><code>g[[:lower:]]</code>将匹配任意数量的以<code>g</code>开头、第二个字符为小写字母的两字符字符串。</p>
</blockquote>
<h1 id="：转义字符"><a href="#：转义字符" class="headerlink" title="\：转义字符"></a><code>\</code>：转义字符</h1><p>有时，我们会想要匹配名字中带有通配符字符，如<code>*</code>和<code>?</code>的字符串，这时候就需要使用转义字符<code>\</code>让通配符失去通配符意义而转为普通的字符串：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[zclzcl@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">chap?  file1.txt  file2.txt  gd  gg  ground  ground.txt  g.txt  what?</span><br><span class="line">[zclzcl@localhost Playground]$ <span class="built_in">ls</span> *\?</span><br><span class="line">chap?  what?</span><br></pre></td></tr></table></figure>

<p>上面的命令，<code>\?</code>代表字符<code>?</code>，因此其匹配的是所有以<code>?</code>结尾的字符串。</p>
<h1 id="花括号展开"><a href="#花括号展开" class="headerlink" title="花括号展开"></a>花括号展开</h1><p>花括号展开十分类似于<code>[[start]-[end]]</code>形式的通配符模式匹配。通过花括号展开，我们可以从一个包含花括号的模式中创建出多个字符串，其基本使用模式为：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&#123;[stringA],[stringB],...,[string]&#125;</span><br><span class="line">或</span><br><span class="line">&#123;[start]..[end]&#125;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>需要注意的是，花括号的内容只能用<code>,</code>分割，而不能有空白符，否则花括号会被视为一个普通的字符串。</p>
</blockquote>
<p>比如，<code>&#123;AB,CB,CC&#125;</code>将产生3个字符串<code>AB</code>、<code>CB</code>和<code>CC</code>：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[zclzcl@localhost Playground]$ <span class="built_in">rm</span> *</span><br><span class="line">[zclzcl@localhost Playground]$ <span class="built_in">touch</span> &#123;AB,CB,CC&#125;</span><br><span class="line">[zclzcl@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">AB  CB  CC</span><br></pre></td></tr></table></figure>

<p>而<code>&#123;2..6&#125;</code>将产生5个字符串<code>2</code>、<code>3</code>、<code>4</code>、<code>5</code>和<code>6</code>：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[zclzcl@localhost Playground]$ rm *</span><br><span class="line">[zclzcl@localhost Playground]$ touch &#123;2..6&#125;</span><br><span class="line">[zclzcl@localhost Playground]$ ls</span><br><span class="line">2  3  4  5  6</span><br></pre></td></tr></table></figure>

<blockquote>
<p>注意是两个<code>.</code>。</p>
</blockquote>
<p>花括号展开还可以嵌套。嵌套时，每个花括号被视为一个整体，依次展开，如：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[zclzcl@localhost Playground]$ rm *</span><br><span class="line">[zclzcl@localhost Playground]$ touch &#123;00&#123;1..9&#125;,0&#123;10..99&#125;&#125;</span><br><span class="line">[zclzcl@localhost Playground]$ ls</span><br><span class="line">001  007  013  019  025  031  037  043  049  055  061  067  073  079  085  091  097</span><br><span class="line">002  008  014  020  026  032  038  044  050  056  062  068  074  080  086  092  098</span><br><span class="line">003  009  015  021  027  033  039  045  051  057  063  069  075  081  087  093  099</span><br><span class="line">004  010  016  022  028  034  040  046  052  058  064  070  076  082  088  094</span><br><span class="line">005  011  017  023  029  035  041  047  053  059  065  071  077  083  089  095</span><br><span class="line">006  012  018  024  030  036  042  048  054  060  066  072  078  084  090  096</span><br></pre></td></tr></table></figure>

<p>上面的式子将一次性生成99个文件。其基本原理是：最外层花括号内的两项<code>00&#123;1..9&#125;</code>和<code>0&#123;10..99&#125;</code>分别单独展开。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://youtu.be/LH4eF75Z_7U?si=bxCeRYpXGUmuKGXc">Linux Tutorials | Wildcards and Pattern Matching | GeeksforGeeks</a></li>
<li><a href="https://www.kancloud.cn/thinkphp/linux-command-line/39435">第五章：操作文件和目录</a></li>
<li><a href="https://www.kancloud.cn/thinkphp/linux-command-line/39438">第八章：从shell眼中看世界</a></li>
</ul>
]]></content>
      <categories>
        <category>Tool</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Tool</tag>
      </tags>
  </entry>
  <entry>
    <title>Program Compilation</title>
    <url>/2023/07/25/ProgramCompilation/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="Compile-Assembly-and-Link"><a href="#Compile-Assembly-and-Link" class="headerlink" title="Compile, Assembly and Link"></a>Compile, Assembly and Link</h1><p>一个C&#x2F;C++程序从源文件<code>.c/.cpp</code>到可执行文件<code>.exe</code>一般要经过以下四个步骤：</p>
<p><img src="/2023/07/25/ProgramCompilation/1.png" alt="1"></p>
<center style="font-size:12px; font-weight:bold">Fig. 1. 从源文件到可执行文件</center>

<ol>
<li>预处理阶段：主要完成源文件的宏替换；</li>
<li>编译（Compile）阶段：将高级语言翻译为汇编语言、源程序翻译为汇编程序；</li>
<li>汇编（Assembly）阶段：将汇编语言翻译为机器能识别的二进制机器语言，生成的<code>.o</code>文件称可重定位目标文件，用于后续的链接操作；</li>
<li>链接（Link）阶段：将程序用到的库程序、自定义的依赖程序等与程序链接到一起，形成最终的可执行文件以及逻辑地址。</li>
</ol>
<blockquote>
<p>事实上，现在大多数编译器（Compiler）会同时完成编译和汇编的任务。</p>
</blockquote>
<h1 id="GCC"><a href="#GCC" class="headerlink" title="GCC"></a>GCC</h1><p>GCC，全称GNU C Compiler或CNU Compiler Collection，前者是其最初的称呼，是GNU Project的发起者为完善类Unix操作系统（即Linux）而开发的C&#x2F;C++编译器，后来随着GCC的发展，其支持的语言也逐渐增多，如Java、Go等，由此才有了后面的称呼。通常，Linux发行版的操作系统都会自带GCC，如果没有，则需要手动安装。我们可以使用<code>gcc --version</code>或<code>g++ --version</code>来查看本机的GCC版本。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ gcc --version</span><br><span class="line">gcc (GCC) 11.2.1 20220127 (Red Hat 11.2.1-9)</span><br><span class="line">Copyright (C) 2021 Free Software Foundation, Inc.</span><br><span class="line">This is free software; see the <span class="built_in">source</span> <span class="keyword">for</span> copying conditions.  There is NO</span><br><span class="line">warranty; not even <span class="keyword">for</span> MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.</span><br><span class="line"></span><br><span class="line">[meme@localhost Playground]$ g++ --version</span><br><span class="line">g++ (GCC) 11.2.1 20220127 (Red Hat 11.2.1-9)</span><br><span class="line">Copyright (C) 2021 Free Software Foundation, Inc.</span><br><span class="line">This is free software; see the <span class="built_in">source</span> <span class="keyword">for</span> copying conditions.  There is NO</span><br><span class="line">warranty; not even <span class="keyword">for</span> MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<blockquote>
<p><code>gcc</code>是C编译程序，而<code>g++</code>是C++编译程序。本节将以<code>gcc</code>为例记录一些GCC编译器的用法。</p>
</blockquote>
<p>在使用<code>gcc</code>前，我们先创建一个简单的C程序：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ <span class="built_in">cat</span> &gt; main.c</span><br><span class="line"><span class="comment">#include &lt;stdio.h&gt;</span></span><br><span class="line"></span><br><span class="line">int <span class="function"><span class="title">main</span></span>() &#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Goodbye, world!\n&quot;</span>);</span><br><span class="line">    <span class="built_in">return</span> 0;</span><br><span class="line">&#125;</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">main.c</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">cat</span> main.c</span><br><span class="line"><span class="comment">#include &lt;stdio.h&gt;</span></span><br><span class="line"></span><br><span class="line">int <span class="function"><span class="title">main</span></span>() &#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Goodbye, world!\n&quot;</span>);</span><br><span class="line">    <span class="built_in">return</span> 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="a-out"><a href="#a-out" class="headerlink" title="a.out"></a><code>a.out</code></h2><p>若我们不为<code>gcc</code>提供任何选项而直接使用<code>gcc</code>编译文件，<code>gcc</code>会生成<code>a.out</code>作为该程序的可执行文件。需要注意的是，在Linux操作系统中，默认路径并不包含当前工作目录，因此需要使用<code>./a.out</code>来运行<code>a.out</code>。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ gcc main.c</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">a.out  main.c</span><br><span class="line">[meme@localhost Playground]$ ./a.out</span><br><span class="line">Goodbye, world!</span><br></pre></td></tr></table></figure>

<blockquote>
<p>若<code>a.out</code>无法运行，则需要检查一下当前用户是否有运行<code>a.out</code>的权限。若无，则需用<code>chmod a+x a.out</code>来赋予当前用户权限。</p>
</blockquote>
<h2 id="c-o-g"><a href="#c-o-g" class="headerlink" title="-c, -o, -g"></a><code>-c</code>, <code>-o</code>, <code>-g</code></h2><p>若我们想要指定可执行文件的名字，我们就需要指定选项来逐步编译。</p>
<ul>
<li><code>-c</code>选项示意<code>gcc</code>完成除Link以外的全部步骤，生成可重定位的<code>.o</code>文件：<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ gcc -c main.c</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">a.out  main.c  main.o</span><br></pre></td></tr></table></figure></li>
<li><code>-o</code>选项示意<code>gcc</code>完成可重定位文件及其库文件的Link。其对象可以是<code>.o</code>文件，也可以是<code>.c</code>文件。若为<code>.o</code>则<code>gcc</code>只完成Link；若为<code>.c</code>则<code>gcc</code>将完成从源文件到可执行文件的所有步骤。需要注意的是，可执行文件的名字应<strong>严格</strong>置于<code>-o</code>之后：<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ gcc -o main main.o  <span class="comment"># 等价于gcc main.o -o main</span></span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">a.out  main  main.c  main.o</span><br><span class="line">[meme@localhost Playground]$ ./main</span><br><span class="line">Goodbye, world!</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">rm</span> main main.o</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">a.out  main.c</span><br><span class="line">[meme@localhost Playground]$ gcc -o main main.c</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">a.out  main  main.c</span><br><span class="line">[meme@localhost Playground]$ ./main</span><br><span class="line">Goodbye, world!</span><br></pre></td></tr></table></figure></li>
<li><code>-g</code>选项使得程序以Debug模式编译，以该方式编译的程序可以使用GDB来进行Debug：<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ <span class="built_in">rm</span> a.out main</span><br><span class="line">[meme@localhost Playground]$ gcc -g main.c</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">a.out  main.c</span><br><span class="line">[meme@localhost Playground]$ gdb a.out</span><br><span class="line">GNU gdb (GDB) Red Hat Enterprise Linux 10.2-6.el7</span><br><span class="line">...</span><br><span class="line">(gdb) q</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">a.out  main.c</span><br><span class="line">[meme@localhost Playground]$ gcc -g -o main main.c</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">a.out  main  main.c</span><br><span class="line">[meme@localhost Playground]$ gdb main</span><br><span class="line">GNU gdb (GDB) Red Hat Enterprise Linux 10.2-6.el7</span><br><span class="line">...</span><br><span class="line">(gdb) q</span><br></pre></td></tr></table></figure></li>
</ul>
<p>以上这3个就是GCC的3个基本选项，还有其他的选项如<code>-l</code>用于加入不在标准库中的第三方库等。</p>
<h1 id="Make"><a href="#Make" class="headerlink" title="Make"></a>Make</h1><p>一个项目往往会有多个相互包含的文件，如，我们移除之前生成的<code>a.out</code>以及<code>main</code>文件，并重新创建两个新文件<code>add.c</code>和<code>add.h</code>，同时修改<code>main.c</code>的内容让<code>main.c</code>引用<code>add.c</code>中的函数<code>add</code>：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ ls</span><br><span class="line">main.c</span><br><span class="line">[meme@localhost Playground]$ cat &gt; add.c</span><br><span class="line">int add(int a, int b) &#123;</span><br><span class="line">    return a + b;</span><br><span class="line">&#125;</span><br><span class="line">[meme@localhost Playground]$ cat &gt; add.h</span><br><span class="line">int add(int a, int b);</span><br><span class="line">[meme@localhost Playground]$ ls</span><br><span class="line">add.c  add.h  main.c</span><br><span class="line">[meme@localhost Playground]$ vim main.c</span><br><span class="line">[meme@localhost Playground]$ cat main.c</span><br><span class="line">#include &lt;stdio.h&gt;</span><br><span class="line">#include &quot;add.h&quot;</span><br><span class="line"></span><br><span class="line">int main() &#123;</span><br><span class="line">    printf(&quot;Goodbye, world!\n&quot;);</span><br><span class="line">    printf(&quot;%d\n&quot;, add(5, 3));</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>由于两个文件的关系很简单，所以我们仍可以简单地生成<code>a.out</code>：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ gcc main.c add.c</span><br><span class="line">[meme@localhost Playground]$ ./a.out</span><br><span class="line">Goodbye, world!</span><br><span class="line">8</span><br></pre></td></tr></table></figure>

<p>或者生成自命名的文件：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ gcc -o main_add main.c add.c</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">add.c  add.h  a.out  main_add  main.c</span><br><span class="line">[meme@localhost Playground]$ ./main_add</span><br><span class="line">Goodbye, world!</span><br><span class="line">8</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">rm</span> main_add</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">add.c  add.h  a.out  main.c</span><br><span class="line">[meme@localhost Playground]$ gcc -c main.c add.c</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">add.c  add.h  add.o  a.out  main.c  main.o</span><br><span class="line">[meme@localhost Playground]$ gcc -o main_add main.o add.o</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">add.c  add.h  add.o  a.out  main_add  main.c  main.o</span><br><span class="line">[meme@localhost Playground]$ ./main_add</span><br><span class="line">Goodbye, world!</span><br><span class="line">8</span><br></pre></td></tr></table></figure>

<p>上述两个程序很简单，因此手动地生成可执行文件仍是可行的。但是对于复杂的项目，其包含的程序可能有十几二十，甚至上百个，此时再手动地编译、链接就不太现实了。</p>
<p><code>make</code>工具可以帮助我们省去每次重新编译时敲打文件名的麻烦。<code>make</code>基于用户预先编写的<code>Makefile</code>文件，实现自动编译、链接。使用<code>make --version</code>可以查看本机的<code>make</code>版本，在此我们先创建我们的<code>Makefile</code>文件：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ make --version</span><br><span class="line">GNU Make 4.3</span><br><span class="line">Built <span class="keyword">for</span> x86_64-redhat-linux-gnu</span><br><span class="line">Copyright (C) 1988-2020 Free Software Foundation, Inc.</span><br><span class="line">License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;</span><br><span class="line">This is free software: you are free to change and redistribute it.</span><br><span class="line">There is NO WARRANTY, to the extent permitted by law.</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">rm</span> a.out main_add main.o add.o</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">add.c  add.h  main.c</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">touch</span> Makefile</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">add.c  add.h  main.c  Makefile</span><br></pre></td></tr></table></figure>

<p><code>Makefile</code>是<code>make</code>指定使用的文件名，它只是一个普通的文本文件，其内部的内容用于指导<code>make</code>完成编译操作，一个<code>Makefile</code>文件的基本内容有：</p>
<figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">all: main</span><br><span class="line"></span><br><span class="line">main: main.o add.o</span><br><span class="line">    gcc -o main main.o add.o</span><br><span class="line"></span><br><span class="line">main.o: main.c</span><br><span class="line">    gcc -c main.c</span><br><span class="line"></span><br><span class="line">add.o: add.c</span><br><span class="line">    gcc -c add.c</span><br><span class="line"></span><br><span class="line">clean:</span><br><span class="line">    rm main main.o add.o</span><br></pre></td></tr></table></figure>

<p>其中，<code>all</code>后面的是最终要生成的可执行文件的名称，其后续的<code>main</code>&amp;<code>main.o</code>&amp;<code>add.o</code>、冒号后的部分及下方的指令分别代表<strong>要生成的文件</strong>、<strong>生成这些文件要依赖的其他文件</strong>和<strong>相应的GCC指令</strong>。最后的<code>clean</code>使得我们能执行<code>make clean</code>来清除部分或所有生成的文件。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ vim Makefile</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">cat</span> Makefile</span><br><span class="line">all: main</span><br><span class="line"></span><br><span class="line">main: main.o add.o</span><br><span class="line">	gcc -o main main.o add.o</span><br><span class="line"></span><br><span class="line">main.o: main.c</span><br><span class="line">	gcc -c main.c</span><br><span class="line"></span><br><span class="line">add.o: add.c</span><br><span class="line">	gcc -c add.c</span><br><span class="line"></span><br><span class="line">clean:</span><br><span class="line">	<span class="built_in">rm</span> main main.o add.o</span><br><span class="line">[meme@localhost Playground]$ make</span><br><span class="line">gcc -c main.c</span><br><span class="line">gcc -c add.c</span><br><span class="line">gcc -o main main.o add.o</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">add.c  add.h  add.o  main  main.c  main.o  Makefile</span><br><span class="line">[meme@localhost Playground]$ ./main</span><br><span class="line">Goodbye, world!</span><br><span class="line">8</span><br><span class="line">[meme@localhost Playground]$ make clean</span><br><span class="line"><span class="built_in">rm</span> main main.o add.o</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">add.c  add.h  main.c  Makefile</span><br></pre></td></tr></table></figure>

<p><code>make</code>的另一个优点在于：在一次编译过后再次编译时，它只会编译被修改过的文件。比如，若我们将<code>main.c</code>中的<code>add(5, 3)</code>修改为<code>add(5, 4)</code>再重新编译，我们将得到如下结果（第一个<code>make</code>编译的是未修改前的程序）：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ make</span><br><span class="line">gcc -c main.c</span><br><span class="line">gcc -c add.c</span><br><span class="line">gcc -o main main.o add.o</span><br><span class="line">[meme@localhost Playground]$ vim main.c</span><br><span class="line">[meme@localhost Playground]$ make</span><br><span class="line">gcc -c main.c</span><br><span class="line">gcc -o main main.o add.o</span><br><span class="line">[meme@localhost Playground]$ ./main</span><br><span class="line">Goodbye, world!</span><br><span class="line">9</span><br></pre></td></tr></table></figure>

<p>可见，<code>add.o</code>并没有被重新生成。以上是<code>make</code>及<code>Makefile</code>的一些基本操作。想要了解更多有关GCC和Make的知识可以看南洋理工大学的一份指南：<a href="https://www3.ntu.edu.sg/home/ehchua/programming/cpp/gcc_make.html#zz-2.">Compiling, Linking and Building C&#x2F;C++ Applications</a>。</p>
<h1 id="CMake"><a href="#CMake" class="headerlink" title="CMake"></a>CMake</h1><p>即便有了<code>make</code>，我们仍会遇到一些仅仅是编写<code>Makefile</code>就很麻烦的项目。<code>cmake</code>就是为了解决这项问题而出现的。类似于<code>make</code>，<code>cmake</code>也有其特有的文件<code>CMakeLists.txt</code>。但是不同于<code>make</code>的是，<code>cmake</code>的特有文件是用于<strong>生成</strong><code>Makefile</code>的。<code>cmake</code>、<code>make</code>和<code>gcc</code>的关系如下所示：</p>
<figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">              cmake           make       gcc</span><br><span class="line">CMakeLists.txt -----&gt; Makefile ----&gt; Cmds ---&gt; Binary</span><br></pre></td></tr></table></figure>

<p>同样地，我们可以使用<code>cmake --version</code>查看本系统的<code>cmake</code>版本（若没有则需要安装）。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ cmake --version</span><br><span class="line">cmake version 3.15.3</span><br><span class="line"></span><br><span class="line">CMake suite maintained and supported by Kitware (kitware.com/cmake).</span><br></pre></td></tr></table></figure>

<p>安装好<code>cmake</code>后，我们就可以在当前目录下创建<code>CMakeLists.txt</code>文件：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ <span class="built_in">touch</span> CMakeLists.txt</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">add.c  add.h  add.o  CMakeLists.txt  main  main.c  main.o  Makefile</span><br></pre></td></tr></table></figure>

<p><code>CMakeLists.txt</code>的编写比<code>Makefile</code>要更加复杂，事实上，其编写的方式本身就可以被视为一种新的语言。此处只记录一些基本的语法，更多的要去看官方文档<a href="https://cmake.org/cmake/help/latest/guide/tutorial/index.html">CMake Tutorial</a>。</p>
<p>一个最基本的<code>CMakeLists.txt</code>会包含3个基本命令：</p>
<ul>
<li><code>cmake_minimum_required()</code>：参数为该<code>CMakeLists.txt</code>文件所要求的最低<code>cmake</code>版本，是为了程序的可移植性考虑；</li>
<li><code>project()</code>：参数为最后生成的可执行文件名；</li>
<li><code>add_executable()</code>：参数为可执行文件名及其需要的源文件。</li>
</ul>
<p>以<code>make</code>中使用的<code>main.c</code>和<code>add.c</code>为例，其<code>CMakeLists.txt</code>应为：</p>
<figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">cmake_minimum_required(VERSION 3.10)</span><br><span class="line"></span><br><span class="line"># set the project name</span><br><span class="line">project(main)</span><br><span class="line"></span><br><span class="line"># add the executable</span><br><span class="line">add_executable(main main.c add.c)</span><br></pre></td></tr></table></figure>

<p>由于<code>cmake</code>利用<code>CMakeLists.txt</code>最终生成的是<code>Makefile</code>文件以及一些附属文件，我们通常会新建一个文件夹来执行<code>cmake</code>，一般我们会将该文件夹命名为<code>build</code>（也可自由命名）：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ <span class="built_in">mkdir</span> build</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">add.c  add.h  add.o  build  CMakeLists.txt  main  main.c  main.o  Makefile</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">cd</span> build</span><br></pre></td></tr></table></figure>

<p>然后，在<code>build</code>文件夹中执行我们的<code>cmake</code>指令。由于<code>CMakeLists.txt</code>存在于父目录中，我们应使用<code>cmake ..</code>而不是单单的<code>cmake</code></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost build]$ cmake ..</span><br><span class="line">-- The C compiler identification is GNU 11.2.1</span><br><span class="line">...</span><br><span class="line">-- Build files have been written to: /home/meme/Playground/build</span><br><span class="line">[meme@localhost build]$ <span class="built_in">ls</span></span><br><span class="line">CMakeCache.txt  CMakeFiles  cmake_install.cmake  Makefile</span><br></pre></td></tr></table></figure>

<p>得到<code>Makefile</code>后再执行<code>make</code>即可生成相应的可执行文件：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost build]$ make</span><br><span class="line">make[1]: Entering directory <span class="string">&#x27;/home/meme/Playground/build&#x27;</span></span><br><span class="line">...</span><br><span class="line">make[1]: Leaving directory <span class="string">&#x27;/home/meme/Playground/build&#x27;</span></span><br><span class="line">[meme@localhost build]$ <span class="built_in">ls</span></span><br><span class="line">CMakeCache.txt  CMakeFiles  cmake_install.cmake  main  Makefile</span><br><span class="line">[meme@localhost build]$ ./main</span><br><span class="line">Goodbye, world!</span><br><span class="line">9</span><br></pre></td></tr></table></figure>

<blockquote>
<p><code>cmake</code>能跨目录执行，但是<code>make</code>只能在有<code>Makefile</code>的目录执行。</p>
</blockquote>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://www.kancloud.cn/thinkphp/linux-command-line/39455">The Linux Command Line 中文版 第二十四章：编译程序</a></li>
<li><a href="https://youtu.be/qON2D3vDIt8?si=MdJ6vMEBxWEojYbl">[Environment Setup 8] Compile programs from source code using GCC, GNU Make, and CMake</a></li>
<li><a href="https://www3.ntu.edu.sg/home/ehchua/programming/cpp/gcc_make.html">Compiling, Linking and Building C&#x2F;C++ Applications</a></li>
<li><a href="https://cmake.org/cmake/help/latest/guide/tutorial/index.html">CMake Tutorial</a></li>
</ul>
]]></content>
      <categories>
        <category>Tool</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Tool</tag>
      </tags>
  </entry>
  <entry>
    <title>System Security and Permissions</title>
    <url>/2023/07/25/SystemSecurityPermission/</url>
    <content><![CDATA[<span id="more"></span>


<h1 id="用户"><a href="#用户" class="headerlink" title="用户"></a>用户</h1><p>计算机启动时的密码输入界面就是通常的用户登录界面。以什么用户登录计算机，我们就只能以该用户的身份创建文件、访问该用户能访问文件。每个Linux操作系统都会有一个超级用户<code>root</code>，<code>root</code>用户拥有对所有文件的访问权限，并且能够执行普通用户无法执行的“特权”命令。不过，Linux也为我们提供了除重新登陆外的切换用户身份的方式。</p>
<h2 id="su：Switch-User，以其他用户身份运行shell"><a href="#su：Switch-User，以其他用户身份运行shell" class="headerlink" title="su：Switch User，以其他用户身份运行shell"></a><code>su</code>：Switch User，以其他用户身份运行shell</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">su [options] [-] [USER [arg]...]</span><br></pre></td></tr></table></figure>

<p><code>su</code>命令允许我们在正确输入目标用户的登录密码的情况下以该用户的身份启动shell。若<code>su</code>后面的用户选项为空，则会默认切换为<code>root</code>用户：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ su</span><br><span class="line">Password: </span><br><span class="line">[root@localhost Playground]<span class="comment"># exit</span></span><br><span class="line"><span class="built_in">exit</span></span><br><span class="line">[meme@localhost Playground]$ su bob</span><br><span class="line">Password: </span><br><span class="line">[bob@localhost Playground]$ <span class="built_in">exit</span></span><br><span class="line"><span class="built_in">exit</span></span><br><span class="line">[meme@localhost Playground]$ </span><br></pre></td></tr></table></figure>

<blockquote>
<p>输入<code>exit</code>后我们将退出目标用户并恢复原用户身份。</p>
</blockquote>
<p>若我们只是想以其他用户的身份执行一个命令，我们也可以使用<code>su -c &#39;command&#39;</code>来传递命令：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ su -c &#x27;ls&#x27;</span><br><span class="line">Password: </span><br><span class="line">foo.txt</span><br><span class="line">[meme@localhost Playground]$ </span><br></pre></td></tr></table></figure>

<p>执行完命令后，我们将立马返回原用户。需要注意的是，传递的命令必须用引号括起。</p>
<h2 id="sudo：以另一个用户身份执行命令"><a href="#sudo：以另一个用户身份执行命令" class="headerlink" title="sudo：以另一个用户身份执行命令"></a><code>sudo</code>：以另一个用户身份执行命令</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo [<span class="built_in">command</span>]</span><br></pre></td></tr></table></figure>

<p><code>sudo</code>在一定程度上很像<code>su -c &#39;command&#39;</code>，都是使得我们能以其他用户的身份执行命令。但是<code>sudo</code>通常只用于执行<code>root</code>用户的命令，且能够执行的命令也只能由<code>root</code>用户配置，因而是一种比<code>su</code>更加安全可控的方式：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ sudo <span class="built_in">ls</span></span><br><span class="line">[sudo] password <span class="keyword">for</span> meme: </span><br><span class="line">foo.txt</span><br><span class="line">[meme@localhost Playground]$ </span><br></pre></td></tr></table></figure>

<blockquote>
<p>可用<code>sudo -l</code>查看所有<code>sudo</code>能够执行的命令（的库地址）。一次<code>sudo</code>后，系统会信任用户几分钟，使得后续的<code>sudo</code>不用再输入密码。</p>
</blockquote>
<h1 id="拥有者，组成员，其他人"><a href="#拥有者，组成员，其他人" class="headerlink" title="拥有者，组成员，其他人"></a>拥有者，组成员，其他人</h1><p>在Linux的安全架构下，用户对文件的访问权限分为三个层次：拥有者（owner）、组成员（groups）、其他人（others）。一般来说，文件拥有者的访问权限最高，它不仅可以更改不同层次的用户对本文件的访问范围，也可以以更改文件所属组的形式授予新的用户以组成员访问权限。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span> -l ./foo.txt</span><br><span class="line">-rw-rw-r--. 1 meme meme 0 Jul 27 20:01 ./foo.txt</span><br></pre></td></tr></table></figure>

<p>对于上面的<code>foo.txt</code>文件，其拥有者和访问组依次为<code>meme</code>、<code>meme</code>。</p>
<blockquote>
<p>除了以组成员的形式赋予其他用户权限，文件拥有者还可以单独为某个用户赋予权限。</p>
</blockquote>
<h2 id="组"><a href="#组" class="headerlink" title="组"></a>组</h2><p>Linux用一串数字来标记不同的用户和组。使用<code>id</code>命令可以查看当前用户的用户标识和组标识：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ <span class="built_in">id</span></span><br><span class="line">uid=1001(meme) gid=1001(meme) <span class="built_in">groups</span>=1001(meme),10(wheel)</span><br></pre></td></tr></table></figure>

<p>其中，<code>uid</code>是当前用户的用户标识，<code>gid</code>是组标识，此处显示的<code>gid</code>是<strong>当前用户所在组</strong>（即当前用户创建文件时文件所属的组）的标识，最后的<code>groups</code>则是所有包含了当前用户的组及其标识。使用<code>groups</code>同样可以查询所有包含当前用户的组：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ <span class="built_in">groups</span></span><br><span class="line">meme wheel</span><br></pre></td></tr></table></figure>

<p>使用<code>newgrp [-] [group]</code>可以切换当前用户的工作组为指定组：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ newgrp wheel</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">id</span></span><br><span class="line">uid=1001(meme) gid=10(wheel) <span class="built_in">groups</span>=10(wheel),1001(meme) </span><br></pre></td></tr></table></figure>

<blockquote>
<p>为了方便权限分配，Linux会为每个用户创建一个独一无二的组作为默认工作组，如上面的<code>10001(meme)</code>。所有的用户信息被保持在了文件<code>/etc/passwd</code>中，组信息则被保持在了<code>/etc/group</code>中，这两个文件随着<code>/etc/shadow</code>的变动而修改，<code>/etc/shadow</code>包含了前面两个文件没有的用户信息，如用户密码的信息等。</p>
</blockquote>
<h2 id="常用组命令"><a href="#常用组命令" class="headerlink" title="常用组命令"></a>常用组命令</h2><p>只有<code>root</code>用户能够创建和管理组，因此对于组的管理都要在<code>root</code>用户下进行。</p>
<h3 id="groupadd：新建组"><a href="#groupadd：新建组" class="headerlink" title="groupadd：新建组"></a><code>groupadd</code>：新建组</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">groupadd [options] GROUP</span><br></pre></td></tr></table></figure>

<p><code>groupadd</code>允许我们创建指定名称的组：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ sudo groupadd photo</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">cat</span> /etc/group</span><br><span class="line">root:x:0:</span><br><span class="line">...</span><br><span class="line">music:x:1002:meme,bob</span><br><span class="line">photo:x:1003:</span><br></pre></td></tr></table></figure>

<p>组一旦被创建，其信息就会被加入到<code>/etc/group</code>中。</p>
<h3 id="gpasswd：Group-Password，组管理"><a href="#gpasswd：Group-Password，组管理" class="headerlink" title="gpasswd：Group Password，组管理"></a><code>gpasswd</code>：Group Password，组管理</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">gpasswd [options] groupname</span><br></pre></td></tr></table></figure>
<p><code>gpasswd</code>命令是Linux下<code>/etc/group</code>和<code>/etc/shadow</code>的管理工具，实际上起着管理组的作用。其常用选项<code>-a</code>用于为指定组增加用户，<code>-d</code>用于为指定组删减用户：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ sudo gpasswd -a bob photo</span><br><span class="line">Adding user bob to group photo</span><br><span class="line">[meme@localhost Playground]$ sudo gpasswd -a meme photo</span><br><span class="line">Adding user meme to group photo</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">cat</span> /etc/group</span><br><span class="line">root:x:0:</span><br><span class="line">...</span><br><span class="line">photo:x:1003:bob,meme</span><br></pre></td></tr></table></figure>

<h1 id="权限"><a href="#权限" class="headerlink" title="权限"></a>权限</h1><p>Linux系统的文件通常有三种基本访问权限：读<code>r</code>、写<code>w</code>和执行<code>x</code>。</p>
<blockquote>
<p>对于普通文件，这些权限很清晰明了地定义了不同用户能对其进行的操作，但是对于目录文件，则有些许不同：</p>
<ul>
<li><code>r</code>表示可以用<code>ls</code>列出该目录下的文件，即可以读取目录列表；</li>
<li><code>w</code>表示可以在目录中增加或者删减文件；</li>
<li><code>x</code>表示允许用户在目录中查找，且可以以<code>cd</code>命令切换到该目录。</li>
</ul>
<p>说白了，<code>rwx</code>都表示的是对文件内容的使用权力。</p>
</blockquote>
<p>当我们用<code>ls -l</code>列出文件的详细元数据时，前面的<strong>10</strong>个字符是文件的属性，后面的2个字符串分别是文件的拥有者和访问组，然后是文件的大小、文件的修改时间以及文件名称。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span> -l foo.txt</span><br><span class="line">-rw-rw-r--. 1 meme meme 6 Jul 27 22:22 foo.txt</span><br></pre></td></tr></table></figure>

<ul>
<li>第1个字符表文件的<strong>类型</strong>，<code>-</code>表普通文件，<code>d</code>表目录文件；</li>
<li>第2~10个字符的每三个字符依次表明文件拥有组、访问组和其他用户对该文件的访问权限，顺序为<code>rwx</code>。</li>
</ul>
<h2 id="chmod：Change-Mode，更改文件权限"><a href="#chmod：Change-Mode，更改文件权限" class="headerlink" title="chmod：Change Mode，更改文件权限"></a><code>chmod</code>：Change Mode，更改文件权限</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">chmod</span> [OPTION]... MODE[,MODE]... FILE...</span><br></pre></td></tr></table></figure>

<p><code>chmod</code>以3个八进制数字来分别代表文件拥有者、访问组和其他用户的目标访问权限，如<code>6</code>表示<code>110</code>，权限为<code>rw-</code>；<code>4</code>表示<code>100</code>，权限为<code>r--</code>；<code>0</code>表示<code>000</code>，权限为<code>---</code>：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ <span class="built_in">chmod</span> 600 foo.txt</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span> -l foo.txt</span><br><span class="line">-rw-------. 1 meme meme 6 Jul 27 22:22 foo.txt</span><br></pre></td></tr></table></figure>

<p>除了以数字来直接代表目标权限，<code>chmod</code>也支持符号形式的表示：</p>
<ul>
<li><code>u</code>、<code>g</code>、<code>o</code>分别表示文件拥有者、访问组和其他用户；</li>
<li><code>+</code>，<code>-</code>，<code>=</code>分别表示添加、减少和指定权限；</li>
<li><code>r</code>、<code>w</code>、<code>x</code>分别表示3种权限。</li>
<li>如：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ <span class="built_in">chmod</span> u-r,go=rw foo.txt</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span> -l foo.txt</span><br><span class="line">--w-rw-rw-. 1 meme meme 6 Jul 27 22:22 foo.txt</span><br></pre></td></tr></table></figure>

<h2 id="umask：User-File-Creation-Mode-Mask，设置默认权限"><a href="#umask：User-File-Creation-Mode-Mask，设置默认权限" class="headerlink" title="umask：User File-Creation Mode Mask，设置默认权限"></a><code>umask</code>：User File-Creation Mode Mask，设置默认权限</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">umask</span> [-p] [-S] [mode]</span><br></pre></td></tr></table></figure>

<p><code>umask</code>让我们能够设置在当前用户下创建文件时，各用户的初始权限：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ <span class="built_in">umask</span> 002</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">touch</span> f.txt</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span> -l f.txt</span><br><span class="line">-rw-rw-r--. 1 meme meme 0 Jul 27 22:50 f.txt</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">umask</span> 022</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">touch</span> d.txt</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span> -l d.txt</span><br><span class="line">-rw-r--r--. 1 meme meme 0 Jul 27 22:51 d.txt</span><br></pre></td></tr></table></figure>

<p>其中每位数字（依次代表文件拥有者、访问组和其他用户）都代表一个二进制形式的<strong>掩码</strong>，即<code>2</code>等价于<code>010</code>，表示遮盖<code>rwx</code>中的第二位<code>w</code>。</p>
<blockquote>
<p>此处<code>x</code>也被遮盖的原因是<code>.txt</code>文件无法被执行，也就是说它没有<code>x</code>操作。<br><code>umask</code>设置的掩码不是永久的，它只在当前的shell上生效。</p>
</blockquote>
<h2 id="chown：Change-Owner，更改文件所有者和访问组"><a href="#chown：Change-Owner，更改文件所有者和访问组" class="headerlink" title="chown：Change Owner，更改文件所有者和访问组"></a><code>chown</code>：Change Owner，更改文件所有者和访问组</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">chown</span> [options] owner[:group] file...</span><br></pre></td></tr></table></figure>

<p>如：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ sudo <span class="built_in">chown</span> bob:photo foo.txt</span><br><span class="line">[sudo] password <span class="keyword">for</span> meme: </span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span> -l foo.txt</span><br><span class="line">--w-rw-rw-. 1 bob photo 6 Jul 27 22:22 foo.txt</span><br><span class="line">[meme@localhost Playground]$ sudo <span class="built_in">chown</span> meme:photo foo.txt</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span> -l foo.txt</span><br><span class="line">--w-rw-rw-. 1 meme photo 6 Jul 27 22:22 foo.txt</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">chown</span> :meme foo.txt</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span> -l foo.txt</span><br><span class="line">--w-rw-rw-. 1 meme meme 6 Jul 27 22:22 foo.txt</span><br></pre></td></tr></table></figure>

<p>其中，若文件的所有者是自己且要将该文件的访问组改回自己，则无需<code>root</code>权限，如最后一个命令所示。</p>
<h2 id="特殊权限"><a href="#特殊权限" class="headerlink" title="特殊权限"></a>特殊权限</h2><p>除了上面的3种权限<code>rwx</code>，Linux还提供另外3种特殊权限<code>setuid</code>、<code>setgid</code>和<code>sticky</code>。</p>
<h3 id="setuid-amp-setgid"><a href="#setuid-amp-setgid" class="headerlink" title="setuid &amp; setgid"></a><code>setuid</code> &amp; <code>setgid</code></h3><p><code>setuid</code>和<code>setgid</code>的符号均为<code>s</code>，只不过前者只存在于文件拥有者，而后者只存在于文件的访问组。它们的作用相似：当拥有<code>s</code>权限的<strong>可执行文件</strong>被运行时，该可执行文件会以拥有<code>s</code>权限的文件拥有者或者拥有<code>s</code>权限的访问组的身份运行，即，若文件夹<code>dir</code>（文件夹是一种特殊的可执行文件）的访问组<code>photo</code>没有<code>s</code>权限，<code>meme</code>用户在<code>dir</code>下创建文件的访问组仍然是<code>meme</code>：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost <span class="built_in">dir</span>]$ <span class="built_in">touch</span> foo.txt</span><br><span class="line">[meme@localhost <span class="built_in">dir</span>]$ <span class="built_in">ls</span> -al</span><br><span class="line">total 0</span><br><span class="line">drwxr-xr-x. 2 meme photo  21 Jul 27 23:12 .</span><br><span class="line">...</span><br><span class="line">-rw-r--r--. 1 meme meme  0 Jul 27 23:12 foo.txt</span><br></pre></td></tr></table></figure>

<p>反之，若赋予<code>dir</code>的访问组以<code>s</code>权限（要在<code>root</code>下才能赋予成功），则<code>meme</code>用户在<code>dir</code>下创建的文件的访问组仍然是<code>photo</code>：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost <span class="built_in">dir</span>]$ sudo <span class="built_in">chmod</span> g+s .</span><br><span class="line">[meme@localhost <span class="built_in">dir</span>]$ <span class="built_in">ls</span> -al</span><br><span class="line">total 0</span><br><span class="line">drwxr-sr-x. 2 meme photo  21 Jul 27 23:17 .</span><br><span class="line">...</span><br><span class="line">-rw-r--r--. 1 meme meme  0 Jul 27 23:12 foo.txt</span><br><span class="line">[meme@localhost <span class="built_in">dir</span>]$ <span class="built_in">touch</span> ff.txt</span><br><span class="line">[meme@localhost <span class="built_in">dir</span>]$ <span class="built_in">ls</span> -al</span><br><span class="line">total 0</span><br><span class="line">drwxr-sr-x. 2 meme photo  35 Jul 27 23:22 .</span><br><span class="line">...</span><br><span class="line">-rw-r--r--. 1 meme photo   0 Jul 27 23:22 ff.txt</span><br><span class="line">-rw-r--r--. 1 meme meme  0 Jul 27 23:12 foo.txt</span><br></pre></td></tr></table></figure>

<p>在上面的代码框中，当前文件夹<code>dir</code>的访问组权限由<code>r-x</code>变为了<code>r-s</code>。之后新创建的<code>ff.txt</code>的访问组也相应地变成了<code>dir</code>的访问组。</p>
<h3 id="sticky"><a href="#sticky" class="headerlink" title="sticky"></a><code>sticky</code></h3><p>为文件附加<code>sticky</code>（标识<code>t</code>）位的操作为：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">chmod</span> +t FILE</span><br></pre></td></tr></table></figure>

<p>这个权限用处不大，一般只用于目录文件，它能阻止用户删除或重命名该目录下的文件，除非用户是这个目录的所有者，或者是文件所有者，或是是超级用户<code>root</code>。</p>
<h2 id="passwd：修改用户密码"><a href="#passwd：修改用户密码" class="headerlink" title="passwd：修改用户密码"></a><code>passwd</code>：修改用户密码</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">passwd [user]</span><br></pre></td></tr></table></figure>

<p>我们可以使用该命令修改指定用户的登陆密码</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://www.kancloud.cn/thinkphp/linux-command-line/39440">The Linux Command Line 中文版 第十章：权限</a></li>
</ul>
]]></content>
      <categories>
        <category>Tool</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Tool</tag>
      </tags>
  </entry>
  <entry>
    <title>Process Management</title>
    <url>/2023/07/25/ProcessManagement/</url>
    <content><![CDATA[<span id="more"></span>

<ul>
<li><a href="https://www.kancloud.cn/thinkphp/linux-command-line/39441">The Linux Command Line 中文版 第十一章：进程</a></li>
</ul>
<p><img src="/1.png" alt="1"></p>
<center style="font-size:12px; font-weight:bold">Fig. 1. </center><br>]]></content>
      <categories>
        <category>Tool</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Tool</tag>
      </tags>
  </entry>
  <entry>
    <title>Network and Remote Interaction</title>
    <url>/2023/07/25/NetworkRemoteInteraction/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="netstat：Network-Statistics，查看本机网络信息"><a href="#netstat：Network-Statistics，查看本机网络信息" class="headerlink" title="netstat：Network Statistics，查看本机网络信息"></a><code>netstat</code>：Network Statistics，查看本机网络信息</h1><p><code>netstat</code>程序被用来检测各种各样的本机网络设置。通过选用不同的选项，我们可以查看有关本机的不同网络信息。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">netstat [OPTION]</span><br></pre></td></tr></table></figure>

<p>如，<code>-ie</code>选项使得我们可以查看本系统的所有网络接口：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost ~]$ netstat -ie</span><br><span class="line">Kernel Interface table</span><br><span class="line">ens33: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500</span><br><span class="line">        inet 192.168.115.128  netmask 255.255.255.0  broadcast 192.168.115.255</span><br><span class="line">        inet6 fe80::9b04:dd5e:b9d5:202f  prefixlen 64  scopeid 0x20&lt;<span class="built_in">link</span>&gt;</span><br><span class="line">        ether 00:0c:29:0a:87:bc  txqueuelen 1000  (Ethernet)</span><br><span class="line">        RX packets 3440  bytes 258371 (252.3 KiB)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 654  bytes 80314 (78.4 KiB)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br><span class="line"></span><br><span class="line">lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt;  mtu 65536</span><br><span class="line">        inet 127.0.0.1  netmask 255.0.0.0</span><br><span class="line">        inet6 ::1  prefixlen 128  scopeid 0x10&lt;host&gt;</span><br><span class="line">        loop  txqueuelen 1  (Local Loopback)</span><br><span class="line">        RX packets 456  bytes 95634 (93.3 KiB)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 456  bytes 95634 (93.3 KiB)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>上述两个网络接口中，第一个<code>ens33</code>是因特网接口，主要信息有IPv4地址、掩码以及IPv6地址等；第二个接口<code>lo</code>就是我们常说的内部回环网络接口，其IPv4地址以<code>127</code>开头。</p>
<blockquote>
<p>除<code>-ie</code>外还有许多选项，如<code>-r</code>选项可以查看本机的路由表等。</p>
</blockquote>
<h1 id="wget：World-Wide-Web-Get，从网页中下载文件"><a href="#wget：World-Wide-Web-Get，从网页中下载文件" class="headerlink" title="wget：World Wide Web Get，从网页中下载文件"></a><code>wget</code>：World Wide Web Get，从网页中下载文件</h1><p><code>wget</code>程序将互联网上的文件下载到当前目录，它的使用很简单，一般为<code>wget</code>+目标文件的域名（URL）。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget [OPTION]... [URL]...</span><br></pre></td></tr></table></figure>

<h1 id="ssh：Secure-Shell，安全的远程登陆"><a href="#ssh：Secure-Shell，安全的远程登陆" class="headerlink" title="ssh：Secure Shell，安全的远程登陆"></a><code>ssh</code>：Secure Shell，安全的远程登陆</h1><p><code>ssh</code>既是一个互联网协议，又是Linux命令行中的一个程序。在SSH之前，常用的远程登录协议有Rlogin和Telnet，这两个协议的重大缺陷是所有的通讯信息均以<strong>明文</strong>的方式传播，因而十分地不安全。SSH从两个方面解决了这个问题：</p>
<ol>
<li>SSH客户端所连接的远程主机必须为客户端所知，由此阻止了中间人攻击；</li>
<li>SSH对本地客户端主机和远程主机之间的通信进行了加密。</li>
</ol>
<p>SSH由两部分组成，运行在本地的SSH客户端和运行在远端主机的SSH服务器。一般来说，Linux发行版会自带一个提供SSH功能的软件包OpenSSH，它包括OpenSSH-client和OpenSSH-server两个分别提供客户端和服务器服务的程序，但是有些发行版（如Ubuntu）的OpenSSH只包含了OpenSSH-client。要想让系统能够接受远程客户端的连接，则必须手动安装OpenSSH-server。</p>
<h2 id="SSH配置"><a href="#SSH配置" class="headerlink" title="SSH配置"></a>SSH配置</h2><p>无论是想让系统充当远程的服务器还是想让系统能够作为客户端连接远程主机，都必须要正确地配置相关的SSH文件。对于Linux系统，SSH服务器的配置文件通常是<code>/etc/ssh/sshd_config</code>，SSH客户端的配置文件通常是<code>~/.ssh/config</code>，以上两个文件若没有则需要自己创建。</p>
<h3 id="SSH客户端配置与连接"><a href="#SSH客户端配置与连接" class="headerlink" title="SSH客户端配置与连接"></a>SSH客户端配置与连接</h3><p>对于一般用户来说，SSH客户端是更常用的一方，因此只介绍SSH客户端的配置。通常，有4个主要配置项：</p>
<figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="string">Host</span> [<span class="string">HOST</span> <span class="string">NAME</span>]</span><br><span class="line">    <span class="string">HostName</span> [<span class="string">HOST</span> <span class="string">IP</span>]</span><br><span class="line">    <span class="string">User</span> [<span class="string">USER</span> <span class="string">NAME</span>]</span><br><span class="line">    <span class="string">Port</span> [<span class="string">PORT</span>]</span><br></pre></td></tr></table></figure>
<p>其中，<code>Host</code>是用户为该远程主机指定的标识符，可任意；<code>HostName</code>是该远程主机的公网IP地址；<code>User</code>则是用户登录远程主机时的默认身份，该身份必须是远程主机上有且允许远程登录的用户；<code>Port</code>是要连接的SSH服务器监听的端口，要与远程主机SSH服务器配置文件中的端口匹配。配置完成后，我们便可用命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ssh [HOST NAME] 或</span><br><span class="line">ssh [HOST IP]</span><br></pre></td></tr></table></figure>

<p>连接远程主机。当然，我们也可以以远程主机允许的其他用户身份登录：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ssh [OTHER USER NAME]@[HOST NAME]</span><br></pre></td></tr></table></figure>

<p>一般来说，我们登录远程主机的验证方式默认为输入登录用户的密码，当然也可以在配置文件中增加<code>IdentityFile</code>选项使得我们能以私钥文件的形式实现免密登录。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost ~]$ ssh remote-sys</span><br><span class="line">The authenticity of host <span class="string">&#x27;remote-sys (xxx.xxx.xxx.xxx)&#x27;</span> can<span class="string">&#x27;t be</span></span><br><span class="line"><span class="string">established.</span></span><br><span class="line"><span class="string">ECDSA key fingerprint is xxxxxxxxxxxxxxxxxxx.</span></span><br><span class="line"><span class="string">ECDSA key fingerprint is xxxxxxxxxxxxxxxxxxx.</span></span><br><span class="line"><span class="string">Are you sure you want to continue connecting (yes/no)?yes</span></span><br><span class="line"><span class="string">Warning: Permanently added &#x27;</span>remote-sys,xxx.xxx.xxx.xxx<span class="string">&#x27; (RSA) to the list</span></span><br><span class="line"><span class="string">of known hosts.</span></span><br><span class="line"><span class="string">Bob@xxx.xxx.xxx.xxx&#x27;</span>s password: </span><br></pre></td></tr></table></figure>

<blockquote>
<p>第一次登录未曾登录过的远程主机时，客户端会启动保护机制，如上面的<code>Are you sure...</code>。成功输入密码后，我们便进入了远端的shell会话，直到我们输入<code>exit</code>，该会话都会一直存在。</p>
<p>需要注意的是，有时远程登录会报错<code>Bad owner or permissions on ~/.ssh/config</code>，这是因为有其他的用户拥有了对<code>~/.ssh/config</code>的使用权限，我们只要用<code>chmod 600 ~/.ssh/config</code>使得该文件只能由文件拥有者操作即可。</p>
</blockquote>
<p>若没有可用的远程主机，则可以用<code>localhost</code>作为远端主机名，此时计算机会和自己建立“远程连接”且无需输入密码。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost ~]$ ssh localhost</span><br><span class="line">Last login: Tue Aug  1 03:58:03 2023 from localhost</span><br><span class="line">[meme@localhost ~]$ <span class="built_in">date</span></span><br><span class="line">Tue Aug  1 04:00:53 PDT 2023</span><br><span class="line">[meme@localhost ~]$ <span class="built_in">exit</span></span><br><span class="line"><span class="built_in">logout</span></span><br><span class="line">Connection to localhost closed.</span><br><span class="line">[meme@localhost ~]$ <span class="built_in">date</span></span><br><span class="line">Tue Aug  1 04:01:04 PDT 2023</span><br></pre></td></tr></table></figure>

<p>有时，我们只想用远程主机执行个位数的命令，那么我们就没有必要登录又退出远程主机，而是可以直接在连接过程中就发送我们想要执行的命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost ~]$ ssh remote-sys <span class="string">&#x27;ls ..&#x27;</span></span><br><span class="line">Bob@xxx.xxx.xxx.xxx<span class="string">&#x27;s password: </span></span><br><span class="line"><span class="string">dev</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"><span class="string">var</span></span><br><span class="line"><span class="string">Killed by signal 1.</span></span><br><span class="line"><span class="string">[meme@localhost ~]$ </span></span><br></pre></td></tr></table></figure>

<p>上面的<code>ls ..</code>让远程主机执行<code>ls</code>命令列出默认登录目录的父目录的内容，然后就退出登录。特别需要注意的是，引号表示该命令由远程主机执行，因此下面两个命令得到的是完全不同的结果：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost ~]$ ssh remote-sys <span class="string">&#x27;ls ..&#x27;</span> &gt; dirlist.txt</span><br><span class="line">[meme@localhost ~]$ ssh remote-sys <span class="string">&#x27;ls .. &gt; dirlist.txt&#x27;</span></span><br></pre></td></tr></table></figure>

<p>其中，第一个命令是让远程主机列出父目录的内容并将结果重定向至本地的<code>dirlist.txt</code>文件；而第二个命令则是让远程主机列出父目录的内容并将结果重定向至<strong>远程主机</strong>的<code>dirlist.txt</code>文件。</p>
<h2 id="scp：Secure-Copy，安全的远程复制"><a href="#scp：Secure-Copy，安全的远程复制" class="headerlink" title="scp：Secure Copy，安全的远程复制"></a><code>scp</code>：Secure Copy，安全的远程复制</h2><p><code>scp</code>是OpenSSH的内置程序，它基于SSH实现安全的远程复制：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">scp [[USER@]HOST:]FILE</span><br></pre></td></tr></table></figure>

<p>它的用法很简单，<code>scp</code>+SSH远程服务器+目标文件。程序运行的结果是远程的目标文件被复制到本地目录。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost ~]$ scp remote-sys:document.txt</span><br></pre></td></tr></table></figure>

<h2 id="sftp：Secure-FTP，安全的FTP"><a href="#sftp：Secure-FTP，安全的FTP" class="headerlink" title="sftp：Secure FTP，安全的FTP"></a><code>sftp</code>：Secure FTP，安全的FTP</h2><p><code>sftp</code>是用SSH强化了安全性的FTP协议，它也是OpenSSH的内置程序，同样也是基于SSH实现的。其用法与常规的<code>ftp</code>命令完全一样，只不过远程的SSH服务器充当了FTP服务器：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost ~]$ sftp remote-sys</span><br><span class="line">Connecting to remote-sys...</span><br><span class="line">me@remote-sys<span class="string">&#x27;s password:</span></span><br><span class="line"><span class="string">sftp&gt; ls</span></span><br><span class="line"><span class="string">ubuntu-8.04-desktop-i386.iso</span></span><br><span class="line"><span class="string">sftp&gt; lcd Desktop</span></span><br><span class="line"><span class="string">sftp&gt; get ubuntu-8.04-desktop-i386.iso</span></span><br><span class="line"><span class="string">Fetching /home/me/ubuntu-8.04-desktop-i386.iso to ubuntu-8.04-</span></span><br><span class="line"><span class="string">desktop-i386.iso</span></span><br><span class="line"><span class="string">/home/me/ubuntu-8.04-desktop-i386.iso 100% 699MB 7.4MB/s 01:35</span></span><br><span class="line"><span class="string">sftp&gt; bye</span></span><br></pre></td></tr></table></figure>

<p>输入密码后，我们将登录远程的shell。<code>lcd</code>指令即“Local Change Directory”，是指切换本地主机，即客户端主机的目录，而<code>cd</code>才是切换远程服务器的目录。<code>get</code>使得远程的文件被传输到本地主机的<code>Desktop</code>目录下。输入<code>bye</code>或<code>exit</code>或<code>quit</code>我们将退出远程服务器。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://www.kancloud.cn/thinkphp/linux-command-line/39448">The Linux Command Line 中文版 第十七章：网络系统</a></li>
</ul>
]]></content>
      <categories>
        <category>Tool</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Tool</tag>
      </tags>
  </entry>
  <entry>
    <title>Package Management and Processing</title>
    <url>/2023/07/25/PackageProcessing/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="Linux的软件包管理系统"><a href="#Linux的软件包管理系统" class="headerlink" title="Linux的软件包管理系统"></a>Linux的软件包管理系统</h1><p>Linux发行版本质量最重要的决定因素是软件包管理系统和其支持社区的持久性。其中，软件包管理系统即Linux的打包系统，它负责从其支持的社区，即资源库（类似于手机上的软件商店）中搜索用户想要的软件包，并替用户完成从软件包到软件的安装。软件包管理系统通常由两种工具类型组成：</p>
<ul>
<li>安装和删除软件包文件的<strong>底层工具</strong>；</li>
<li>在资源库中完成元数据、软件包搜索和依赖解析（软件很少是独立运行的，其运行过程中需要用到的其他软件就是它的依赖程序，某一软件包安装的同时也要确保依赖程序也被正确安装）的<strong>上层工具</strong>，其作用类似于Python的<code>pip</code>和<code>conda</code>。</li>
</ul>
<p>不同的Linux发行版本使用的软件包管理系统不尽相同，但是主流的基本都属于Debian的<code>.deb</code>或Red Hat的<code>.rpm</code>阵营：</p>
<style> table th {
    width: 10px;
}
</style>

<table>
<thead>
<tr>
<th align="center">包管理系统</th>
<th align="center">底层工具</th>
<th align="center">上层工具</th>
<th align="center">发行版本</th>
</tr>
</thead>
<tbody><tr>
<td align="center">Debian Style(.deb)</td>
<td align="center">dpkg</td>
<td align="center">apt, apt-get, apt-cache, aptitude</td>
<td align="center">Ubuntu, Debian, Xandros, Linspire</td>
</tr>
<tr>
<td align="center">Red Hat Style (.rpm)</td>
<td align="center">rpm</td>
<td align="center">yum</td>
<td align="center">CentOS, Fedora, Red Hat Enterprise Linux, OpenSUSE, Mandriva, PCLinuxOS</td>
</tr>
</tbody></table>
<h2 id="Debian-Style"><a href="#Debian-Style" class="headerlink" title="Debian Style"></a>Debian Style</h2><p>当前的主流Linux发行版本，如Ubuntu、Debian，使用的都是Debian Style的包管理系统。相比于Red Hat Style的包管理系统，Debian Style的上层工具要更加多样，但也相应更加复杂。<code>apt-get</code>和<code>apt-cache</code>是最早的支持Debian包管理系统的上层工具；<code>apt</code>包含了<code>apt-get</code>和<code>apt-cache</code>中最常用命令选项的集合，但是<code>apt</code>本身要更加简洁；<code>aptitude</code>功能更加全面，且在搜索上的表现优于<code>apt</code>。一些常用的指令有：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查找资源库软件包</span></span><br><span class="line">apt-cache search [search_string] 或 apt search [search_string]</span><br><span class="line"><span class="comment"># 从资源库中安装一个软件包 </span></span><br><span class="line">apt-get install [package_name] 或 apt install [package_name] </span><br><span class="line"><span class="comment"># 卸载软件</span></span><br><span class="line">apt-get remove [package_name] 或 apt remove [package_name]</span><br><span class="line"><span class="comment"># 自动删除不需要的包</span></span><br><span class="line">apt-get autoremove 或 apt autoremove</span><br><span class="line"><span class="comment"># 经过资源库升级所有可升级的软件包</span></span><br><span class="line">apt-get upgrade 或 apt upgrade</span><br><span class="line"><span class="comment"># 在升级软件包的时候自动处理依赖关系</span></span><br><span class="line">apt-get dist-upgrade 或 apt full-upgrade</span><br><span class="line"><span class="comment"># 显示所安装软件包的说明信息</span></span><br><span class="line">apt-cache show [package_name] 或 apt show [package_name]</span><br></pre></td></tr></table></figure>

<blockquote>
<p>事实上，在<strong>查找</strong><code>search</code>、<strong>安装</strong><code>install</code>和<strong>更新</strong><code>upgrade</code>软件包时，上层工具都会去搜索软件包列表，软件包列表里包含了各类软件包的信息。但是，软件包列表会随时间变化，如添加新的包、删除旧的包等都会导致列表发生变化，而上层工具搜索的往往是软件包列表的<strong>缓存</strong>，因此，在进行前面的3项操作前，应先用<code>apt-get update</code>或<code>apt update</code>刷新软件包列表缓存。</p>
<p><code>search</code>所用到的<code>[search_string]</code>可以是软件包的名字，也可以是软件包的说明信息。</p>
<p><code>aptitude</code>的指令名字与使用方式和其他3个基本一致。</p>
</blockquote>
<p>通常，以指令调用上层工具后，上层工具会自动调用下层工具帮助我们完成后续的安装任务，不过有时我们可能会直接得到软件包<code>.deb</code>文件，这时我们就可以直接使用底层工具来处理它们：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 通过软件包安装软件</span></span><br><span class="line">dpkg --install [package_file]</span><br><span class="line"><span class="comment"># 通过软件包升级软件</span></span><br><span class="line">dpkg --install [package_file]</span><br><span class="line"><span class="comment"># 列出所有安装的软件包</span></span><br><span class="line">dpkg --list</span><br><span class="line"><span class="comment"># 确定是否安装了某个软件</span></span><br><span class="line">dpkg --status [package_name]</span><br><span class="line"><span class="comment"># 查找安装了指定文件的软件包</span></span><br><span class="line">dpkg --search file_name</span><br></pre></td></tr></table></figure>

<blockquote>
<p>需要注意的是，直接通过软件包安装文件时，底层工具不会帮助我们完成依赖解析，因此若其发现缺少了某个依赖程序，它会直接报错并退出。一旦发生这种情况，我们就需要手动地安装依赖程序。</p>
<p><code>apt</code>的全称是Advanced Packaging Tool，而<code>dpkg</code>的全称是Debian Packager。</p>
</blockquote>
<h2 id="Red-Hat-Style"><a href="#Red-Hat-Style" class="headerlink" title="Red Hat Style"></a>Red Hat Style</h2><p>Red Hat Style的上层工具只有<code>yum</code>：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查找资源库软件包</span></span><br><span class="line">yum search [search_string]</span><br><span class="line"><span class="comment"># 从资源库中安装一个软件包 </span></span><br><span class="line">yum install [package_name]</span><br><span class="line"><span class="comment"># 卸载软件</span></span><br><span class="line">yum erase [package_name]</span><br><span class="line"><span class="comment"># 经过资源库升级所有可升级的软件包</span></span><br><span class="line">yum update</span><br><span class="line"><span class="comment"># 显示所安装软件包的说明信息</span></span><br><span class="line">yum info show [package_name]</span><br></pre></td></tr></table></figure>

<p>同样地，Red Hat Style也支持用底层工具直接安装<code>.rpm</code>文件：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 通过软件包安装软件</span></span><br><span class="line">rpm -i [package_file]</span><br><span class="line"><span class="comment"># 通过软件包升级软件</span></span><br><span class="line">rpm -U [package_file]</span><br><span class="line"><span class="comment"># 列出所有安装的软件包</span></span><br><span class="line">rpm -qa</span><br><span class="line"><span class="comment"># 确定是否安装了某个软件</span></span><br><span class="line">rpm -q [package_name]</span><br><span class="line"><span class="comment"># 查找安装了指定文件的软件包</span></span><br><span class="line">rpm -qf file_name</span><br></pre></td></tr></table></figure>

<blockquote>
<p>使用<code>rpm</code>直接安装软件包同样需要我们自己处理依赖程序。</p>
<p><code>yum</code>的全称是Yellow dog Updater, Modified，<code>rpm</code>的全称是Red Hat Package Manager。</p>
</blockquote>
<h1 id="压缩，归档，备份"><a href="#压缩，归档，备份" class="headerlink" title="压缩，归档，备份"></a>压缩，归档，备份</h1><p>压缩包、文件包是另外两种常见的“包文件”。不像软件包是用于装载某个可执行程序的，它们是用于更好地存储、管理、归档和备份文件以保护重要数据的。</p>
<h2 id="压缩文件"><a href="#压缩文件" class="headerlink" title="压缩文件"></a>压缩文件</h2><p>压缩（Zip、Compress），顾名思义就是通过压缩算法，使得计算机能以尽可能小的空间来存储未压缩文件。压缩算法分两大类：</p>
<ul>
<li><strong>无损压缩</strong>：无损压缩保留了原文件的所有数据。当还原一个被压缩文件时，还原文件将与原文件一模一样；</li>
<li><strong>有损压缩</strong>：有损压缩允许压缩操作时删除一些数据，使得文件得到更大的压缩率。当一个有损压缩文件被还原时，它与原文件将存在差异。有损压缩一般用于少量的损失不影响原文件效果的文件，如图片、视频等。JPEG和MP3就是常见的有损压缩文件。</li>
</ul>
<h3 id="gzip：强大的无损压缩程序"><a href="#gzip：强大的无损压缩程序" class="headerlink" title="gzip：强大的无损压缩程序"></a><code>gzip</code>：强大的无损压缩程序</h3><p><code>gzip</code>，全称GNUzip，是GNU计划实现的一款Linux系统常用的无损压缩程序。<code>gunzip</code>则是其对应的解压程序。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">gzip [OPTION]... [FILE]...</span><br><span class="line">gunzip [OPTION]... [FILE]...</span><br></pre></td></tr></table></figure>

<p><code>gzip</code>在执行压缩操作时，原文件会被压缩文件<code>[FILE].gz</code>替代；解压时，<code>[FILE].gz</code>又会被还原为原文件：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span> -l /etc | <span class="built_in">cat</span> &gt; foo.txt</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span> -l foo.*</span><br><span class="line">--w-rw-rw-. 1 meme meme 16387 Jul 31 00:45 foo.txt</span><br><span class="line">[meme@localhost Playground]$ sudo gzip foo.txt</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span> -l foo.*</span><br><span class="line">--w-rw-rw-. 1 meme meme 3219 Jul 31 00:45 foo.txt.gz</span><br><span class="line">[meme@localhost Playground]$ sudo gunzip foo.txt.gz</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span> -l foo.*</span><br><span class="line">--w-rw-rw-. 1 meme meme 16387 Jul 31 00:45 foo.txt</span><br></pre></td></tr></table></figure>

<p>不难看出，压缩后的<code>[FILE].gz</code>文件大小约为原文件的1&#x2F;5。解压后的文件与原文件一模一样，包括权限、修改时间等。<code>gzip</code>和<code>gunzip</code>的<code>[OPTION]</code>选项有很多，常用的有：</p>
<ul>
<li><code>-c</code>：把程序输出写入到标准输出，并且保留原始文件。用在<code>gzip</code>上会使得此次<code>gzip</code>操作将压缩后的文件内容（一堆乱码）直接输出在标准输出上，而原文件不发生变化：<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line"><span class="built_in">dir</span>  foo.txt</span><br><span class="line">[meme@localhost Playground]$ sudo gzip -c foo.txt</span><br><span class="line">�f�d</span><br><span class="line">...</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line"><span class="built_in">dir</span>  foo.txt</span><br></pre></td></tr></table></figure>
用在<code>gunzip</code>上会使得<code>gunzip</code>直接输出解压后文件的内容，而并不解压文件：<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ sudo gzip foo.txt</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line"><span class="built_in">dir</span>  foo.txt.gz</span><br><span class="line">[meme@localhost Playground]$ sudo gunzip -c foo.txt.gz</span><br><span class="line">total 1424</span><br><span class="line">...</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line"><span class="built_in">dir</span>  foo.txt.gz</span><br></pre></td></tr></table></figure>
<code>zcat</code>（zip &amp; cat）命令相当于使用了<code>-c</code>的<code>gunzip</code>。</li>
<li><code>-d</code>：意为decompress，只用于<code>gzip</code>，带<code>-d</code>选项的<code>gzip</code>作用相当于<code>gunzip</code>。</li>
<li><code>-f</code>：意为force，指强制压缩原文件，即便原始文件已经被压缩。</li>
<li><code>-r</code>：意为recursive，若命令行中有一个或多个参数为目录，则递归地压缩目录中文件。</li>
</ul>
<h3 id="bzip2：慢但更彻底的gzip"><a href="#bzip2：慢但更彻底的gzip" class="headerlink" title="bzip2：慢但更彻底的gzip"></a><code>bzip2</code>：慢但更彻底的<code>gzip</code></h3><p><code>bzip2</code>与<code>gzip</code>相似，但是使用了不同的压缩算法，使得<code>bzip2</code>可以做到更高的压缩级别，但速度就稍微慢了一点。除了<code>bzip2</code>的压缩文件名字后缀为<code>.bz2</code>，<code>bzip2</code>与<code>gzip</code>的功能几乎一模一样，包括压缩文件替代原文件、除<code>-r</code>以外的选项以及<code>bunzip2</code>和<code>bzcat</code>完成解压缩。</p>
<h2 id="归档文件"><a href="#归档文件" class="headerlink" title="归档文件"></a>归档文件</h2><p>相比于压缩文件包，我们更常见的是普通的文件包，即归档文件包，Windows上的<code>.zip</code>文件包就是一种常见的归档文件包。所谓归档（Archive），即把一群文件捆绑成一个大的文件的过程。归档通常只是将目标文件群集中地放在一个归档文件包中，但有些归档程序，如<code>zip</code>，还包含着压缩的功能。</p>
<h3 id="tar：经典Linux归档工具"><a href="#tar：经典Linux归档工具" class="headerlink" title="tar：经典Linux归档工具"></a><code>tar</code>：经典Linux归档工具</h3><p><code>tar</code>，全称Tape Archive，是Linux系统中最常用的归档工具。一个<code>tar</code>包可以由一组独立的文件、一个或者多个目录或前两者的混合体组成。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tar MODE[OPTION...] [PATH]...</span><br></pre></td></tr></table></figure>

<p>上边的<code>MODE</code>指<code>tar</code>的不同操作模式，常用的有：</p>
<ul>
<li><code>c</code>：为文件、目录、目录列表创建归档文件；</li>
<li><code>x</code>：抽取归档文件；</li>
<li><code>r</code>：追加具体的路径到归档文件末尾；</li>
<li><code>t</code>：列出归档文件的内容。</li>
</ul>
<p>而常用的<code>[OPTION]</code>有：</p>
<ul>
<li><code>-f</code>：意为file，指定归档文件包的名字；</li>
<li><code>-v</code>：意为verbose，显示命令的整个执行过程；</li>
<li><code>-z</code>：意为gzip，对归档文件包使用<code>gzip</code>压缩；</li>
<li><code>-j</code>：意为bip2，对归档文件包使用<code>bzip2</code>压缩。</li>
</ul>
<blockquote>
<p>一般来说，模式只能选择一个，而选项可以选择多个。确定了模式后，选项可以直接跟在模式后面，而无需用空格或<code>-</code>分割。</p>
<p> <code>tar</code>包的后缀名为<code>.tar</code>，经过<code>gzip</code>压缩的<code>tar</code>包后缀名为<code>.tgz</code>，经过<code>bzip2</code>压缩的<code>tar</code>包后缀名为<code>.tbz</code>。</p>
</blockquote>
<p>在进一步了解<code>tar</code>之前，我们先在当前目录下创建<code>playground</code>目录，并在其下创建100个子目录，在每个子目录下创建24个普通文件：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ <span class="built_in">mkdir</span> -p ./playground/dir-&#123;00&#123;1..9&#125;,0&#123;10..99&#125;,100&#125;</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span> ./playground</span><br><span class="line">dir-001  ...  dir-100</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">touch</span> ./playground/dir-&#123;00&#123;1..9&#125;,0&#123;10..99&#125;,100&#125;/file-&#123;A..Z&#125;</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span> ./playground/dir-001</span><br><span class="line">file-A  ...  file-Z</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span> ./playgrounddir-002</span><br><span class="line">file-A  ...  file-Z</span><br></pre></td></tr></table></figure>

<p><code>tar</code>一个很重要的特性是它在归档时保留了原文档的目录结构。如，若我们在当前目录下归档<code>playground</code>文件夹：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">foo.txt.gz  playground</span><br><span class="line">[meme@localhost Playground]$ tar cf playground.tar playground</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">foo.txt.gz  playground  playground.tar</span><br></pre></td></tr></table></figure>

<p>使用<code>tar tf</code>可以列出归档文件的内容，<code>tar tvf</code>可以列出归档文件的详细内容：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ tar tf playground.tar playground</span><br><span class="line">playground/</span><br><span class="line">playground/dir-001/</span><br><span class="line">playground/dir-001/file-A</span><br><span class="line">playground/dir-001/file-B</span><br><span class="line">...</span><br><span class="line">[meme@localhost Playground]$ tar tvf playground.tar playground</span><br><span class="line">drwxrwxr-x meme/meme     0 2023-07-31 01:25 playground/</span><br><span class="line">drwxrwxr-x meme/meme     0 2023-07-31 01:30 playground/dir-001/</span><br><span class="line">-rw-rw-r-- meme/meme     0 2023-07-31 01:28 playground/dir-001/file-A</span><br><span class="line">-rw-rw-r-- meme/meme     0 2023-07-31 01:28 playground/dir-001/file-B</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>若我们使用同样的命令，但是将待归档文件夹的路径更改为其绝对路径，则其归档文件的结构也将是绝对路径的结构：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ <span class="built_in">pwd</span></span><br><span class="line">/home/meme/Playground</span><br><span class="line">[meme@localhost Playground]$ tar cf playground2.tar /home/meme/Playground/playground</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">foo.txt.gz  playground  playground2.tar  playground.tar</span><br></pre></td></tr></table></figure>

<p>我们不妨将<code>playground.tar</code>和<code>playground2.tar</code>用<code>tar xf</code>同时提取在新建文件夹<code>foo</code>中：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ <span class="built_in">mkdir</span> foo</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">cd</span> foo</span><br><span class="line">[meme@localhost foo]$ tar xf ../playground.tar</span><br><span class="line">[meme@localhost foo]$ tar xf ../playground2.tar</span><br><span class="line">[meme@localhost foo]$ <span class="built_in">ls</span> ./playground</span><br><span class="line">dir-001</span><br><span class="line">...</span><br><span class="line">dir-100</span><br><span class="line">[meme@localhost foo]$ <span class="built_in">ls</span> ./home/meme/Playground/playground</span><br><span class="line">dir-001</span><br><span class="line">...</span><br><span class="line">dir-100</span><br></pre></td></tr></table></figure>

<p>可见，<code>playground.tar</code>和<code>playground2.tar</code>内部的内容分别按相对路径和绝对路径的目录结构存储。对<code>playground.tar</code>，是<code>playground/</code>；对<code>playground2.tar</code>则是<code>home/meme/Playground/playground/</code>。这样的设计可以保证我们归档目录结构与原目录结构的完全统一，使得我们可以在任何地方抽取归档文件。比如，<code>playground2.tar</code>就可以完全在另一台主机的根目录下抽取，使得其在新主机的路径仍为<code>home/meme/Playground/playground</code>。</p>
<p>需要注意的是，<code>tar xf</code>默认抽取所有文件，但是若其后面有指定的文件路径，则其只会抽取指定文件，如，此处我们先删除<code>home</code>，再只从<code>playground2.tar</code>中抽取一个目录<code>dir-001</code>：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost foo]$ <span class="built_in">ls</span></span><br><span class="line">home  playground</span><br><span class="line">[meme@localhost foo]$ <span class="built_in">rm</span> -r home</span><br><span class="line">[meme@localhost foo]$ <span class="built_in">ls</span></span><br><span class="line">playground</span><br><span class="line">[meme@localhost foo]$ tar xf ../playground2.tar home/meme/Playground/playground/dir-001</span><br><span class="line">[meme@localhost foo]$ <span class="built_in">ls</span></span><br><span class="line">home  playground</span><br><span class="line">[meme@localhost foo]$ <span class="built_in">ls</span> ./home/meme/Playground/playground</span><br><span class="line">dir-001</span><br></pre></td></tr></table></figure>

<blockquote>
<p>在有些版本的Linux发行版中，在命令的<code>.tar</code>文件名后加入<code>--wildcards</code>可以使得我们的路径能支持通配符，但是路径最好用引号引起。</p>
<p>已抽出文件不会继承打包文件的权限，其只拥有将其抽出的用户的权限，也就是说，抽出文件的拥有者是抽取人，除非打包文件的是根用户。</p>
</blockquote>
<h4 id="与find的结合"><a href="#与find的结合" class="headerlink" title="与find的结合"></a>与<code>find</code>的结合</h4><p>由于<code>find</code>搜索指定文件夹中的文件时给出的搜索结果是相对于该文件夹的相对路径，因此<code>tar</code>常常可以与<code>find</code>结合使用，使得<code>find</code>的搜索结果可以作为<code>tar</code>的待归档文件，并用<code>tar rf</code>直接在原归档文件的基础上增加新内容，如：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ find playground -name <span class="string">&#x27;file-A&#x27;</span></span><br><span class="line">playground/dir-001/file-A</span><br><span class="line">playground/dir-002/file-A</span><br><span class="line">...</span><br><span class="line">playground/dir-099/file-A</span><br><span class="line">playground/dir-100/file-A</span><br><span class="line">[meme@localhost Playground]$ find playground -name <span class="string">&#x27;file-A&#x27;</span> -<span class="built_in">exec</span> tar rf playground.tar <span class="string">&#x27;&#123;&#125;&#x27;</span> <span class="string">&#x27;+&#x27;</span></span><br><span class="line">[meme@localhost Playground]$ tar tf playground.tar</span><br><span class="line">playground/</span><br><span class="line">playground/dir-001/</span><br><span class="line">playground/dir-001/file-A</span><br><span class="line">playground/dir-001/file-B</span><br><span class="line">...</span><br><span class="line">playground/dir-100/file-Z</span><br><span class="line">playground/dir-001/file-A</span><br><span class="line">playground/dir-002/file-A</span><br><span class="line">...</span><br><span class="line">playground/dir-099/file-A</span><br><span class="line">playground/dir-100/file-A</span><br></pre></td></tr></table></figure>

<p>将直接将所有的<code>file-A</code>文件归档、增加至<code>playground.tar</code>中。</p>
<h4 id="与标准输入和输出结合"><a href="#与标准输入和输出结合" class="headerlink" title="与标准输入和输出结合"></a>与标准输入和输出结合</h4><p>通常地，<code>tar cf [FILE.tar] [PATH]</code>不会产生输出，而是直接生成<code>FILE.tar</code>文件，但是，若我们将<code>[FILE.tar]</code>替换为<code>-</code>，则上述指令会将归档文件包以<strong>标准输出的形式输出</strong>；同样地，<code>tar xf [FILE.tar]</code>不会接受输入，只会生成提取文件，而我们若将<code>[FILE.tar]</code>替换为<code>-</code>，则<code>tar xf</code>会<strong>接受标准输入的输入</strong>：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost foo]$ <span class="built_in">ls</span></span><br><span class="line">home  playground</span><br><span class="line">[meme@localhost foo]$ <span class="built_in">rm</span> -r playground</span><br><span class="line">[meme@localhost foo]$ <span class="built_in">ls</span></span><br><span class="line">home</span><br><span class="line">[meme@localhost foo]$ tar cf - ../playground | tar xf -</span><br><span class="line">[meme@localhost foo]$ <span class="built_in">ls</span></span><br><span class="line">home  playground</span><br></pre></td></tr></table></figure>

<p>上述命令将<code>tar cf</code>对<code>../playground</code>的归档输出到<code>-</code>标准输出中，该标准输出又被管道至<code>tar xf</code>的<code>-</code>标准输入，<code>tar xf</code>将<code>-</code>提取，于是我们就在当前文件夹得到父文件夹的<code>playground</code>文件夹。</p>
<blockquote>
<p><code>-</code>代替标准输入、输出的惯例被很多程序使用。</p>
</blockquote>
<p>由此，前面<code>find</code>与<code>tar</code>的结合也可以通过管道<code>|</code>进行：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">foo  foo.txt.gz  playground  playground2.tar</span><br><span class="line">[meme@localhost Playground]$ find playground -name <span class="string">&#x27;file-A&#x27;</span> | tar cf playground.tar -T-</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">foo  foo.txt.gz  playground  playground2.tar  playground.tar</span><br><span class="line">[meme@localhost Playground]$ tar tf playground.tar</span><br><span class="line">playground/dir-001/file-A</span><br><span class="line">playground/dir-002/file-A</span><br><span class="line">...</span><br><span class="line">playground/dir-099/file-A</span><br><span class="line">playground/dir-100/file-A</span><br></pre></td></tr></table></figure>

<p><code>-T</code>全称<code>--file-from</code>，它将导致<code>tar</code>从一个文件（此处为标准输出文件）而不是命令行中读入路径名，<code>-T</code>后面的<code>-</code>示意该文件为管道过来的<strong>标准输入文件</strong>。最终整个命令达到的效果就是将<code>playground</code>中所有的<code>file-A</code>文件归档。</p>
<blockquote>
<p><code>-T-</code>可以用<code>--file-from=-</code>代替。</p>
</blockquote>
<h4 id="tgz和-tbz"><a href="#tgz和-tbz" class="headerlink" title=".tgz和.tbz"></a><code>.tgz</code>和<code>.tbz</code></h4><p><code>tar czf [FILE.tgz] [PATH]</code>和<code>tar cjf [FILE.tbz] [PATH]</code>将分别把得到的<code>.tar</code>文件一步压缩为<code>.tgz</code>或<code>.tbz</code>文件，此处不再赘述。</p>
<h3 id="zip：打包与压缩"><a href="#zip：打包与压缩" class="headerlink" title="zip：打包与压缩"></a><code>zip</code>：打包与压缩</h3><p><code>zip</code>更常用于Windows系统中。相比于<code>tar</code>，<code>zip</code>兼具打包和压缩的功能。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">zip [OPTION] [ZIPFILE] [FILE]</span><br></pre></td></tr></table></figure>

<p><code>zip</code>的使用很简单，基本格式为包文件名+待打包文件路径，对于目录文件则要加上<code>-r</code>选项，否则只有目录被存储：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">foo  foo.txt.gz  playground  playground2.tar  playground.tar</span><br><span class="line">[meme@localhost Playground]$ zip -r dir-001.zip playground/dir-001</span><br><span class="line">  adding: playground/dir-001/ (stored 0%)</span><br><span class="line">  adding: playground/dir-001/file-A (stored 0%)</span><br><span class="line">  ...</span><br><span class="line">  adding: playground/dir-001/file-Z (stored 0%)</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">dir-001.zip  foo  foo.txt.gz  playground  playground2.tar  playground.tar</span><br></pre></td></tr></table></figure>

<p>上述指令将<code>playground</code>中的<code>dir-001</code>文件夹打包压缩为<code>dir-001.zip</code>。值得注意的是，<code>zip</code>有压缩功能，<code>stored</code>后的百分比即表示<strong>压缩量</strong>。因为各个<code>file-[A..Z]</code>文件均为空文件，所以没有压缩。</p>
<p><code>zip</code>的打包是更新式的打包而不是替代式的打包，也就是说，若已经存在了文件包<code>xx.zip</code>，再以相同的名字<code>xx.zip</code>打包某群文件不会再生成一个<code>.zip</code>文件，而是在原<code>xx.zip</code>文件包的基础上进行更新，如：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ zip -r dir-001.zip playground/dir-002</span><br><span class="line">  adding: playground/dir-002/ (stored 0%)</span><br><span class="line">  ...</span><br><span class="line">  adding: playground/dir-002/file-Z (stored 0%)</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">dir-001.zip  foo  foo.txt.gz  playground  playground2.tar  playground.tar</span><br><span class="line">[meme@localhost Playground]$ unzip -l dir-001.zip</span><br><span class="line">Archive:  dir-001.zip</span><br><span class="line">  Length      Date    Time    Name</span><br><span class="line">---------  ---------- -----   ----</span><br><span class="line">        0  07-31-2023 01:30   playground/dir-001/</span><br><span class="line">        ...</span><br><span class="line">        0  07-31-2023 01:28   playground/dir-001/file-Z</span><br><span class="line">        0  07-31-2023 01:30   playground/dir-002/</span><br><span class="line">        ...</span><br><span class="line">        0  07-31-2023 01:28   playground/dir-002/file-Z</span><br><span class="line">---------                     -------</span><br><span class="line">        0                     54 files</span><br></pre></td></tr></table></figure>

<p><code>dir-002</code>直接被加进了<code>dir-001.zip</code>中，类似的操作在<code>tar</code>中要指定<code>r</code>模式。</p>
<p>上面用到的<code>unzip</code>是与<code>zip</code>对应的解压程序。<code>unzip -l [FILE.zip] [PATH]</code>将只列出文件包内某文件的信息，<code>[PATH]</code>缺省则列出所有。而去掉<code>-l</code>则将从包中抽取所有或指定文件。</p>
<blockquote>
<p><code>zip</code>也能结合标准输入输出，此处不再赘述。</p>
</blockquote>
<h2 id="同步与备份文件"><a href="#同步与备份文件" class="headerlink" title="同步与备份文件"></a>同步与备份文件</h2><p>同步与备份，即保持一个或多个目录与另一个本地或远程目录保持同步。常见的<code>git</code>仓库就是一个这样的远程托管同步系统。在Linux中，更常被使用的同步备份工具是<code>rsync</code>，全称Remote Synchronize。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rsync [OPTION] SOURCE DESTINATION</span><br></pre></td></tr></table></figure>

<p>其中<code>SOURCE</code>是待备份文件（可多个），而<code>DESTINATION</code>是即将存储<code>SOURCE</code>全部文件内容的文件夹，两者必须是下列3个选项之一且至少一个是本地文件&#x2F;目录：</p>
<ul>
<li>一个本地文件&#x2F;目录；</li>
<li>一个远程文件&#x2F;目录，以<code>[user@]host:path</code>的形式存在；</li>
<li>一个远程rsync服务器，由<code>rsync://[user@]host[:port]/path</code>指定。</li>
</ul>
<blockquote>
<p>其中带<code>[]</code>的表示可选项，因此对于远程文件&#x2F;目录，我们只要写上远程主机名<code>host</code>以及存放目录<code>path</code>；对于远程rsync服务器，我们只要写上远程服务器的主机名（url的形式）<code>host</code>及其存放目录<code>path</code>。</p>
</blockquote>
<p>实际使用时，我们一般会用<code>rsync -ac -delete</code>，其中<code>-a</code>表示递归地备份并保护文件属性，<code>-v</code>表示输出备份信息，<code>-delete</code>表示删除备份设备中已经存在但是不存在于源设备中的文件。若<code>SOURCE</code>和<code>DESTINATION</code>中有一个为远程ssh主机，还要增加选项<code>--rsh==ssh</code>。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://www.kancloud.cn/thinkphp/linux-command-line/39446">The Linux Command Line 中文版 第十五章：软件包管理</a></li>
<li><a href="https://www.kancloud.cn/thinkphp/linux-command-line/39450">The Linux Command Line 中文版 第十九章：归档和备份</a></li>
</ul>
]]></content>
      <categories>
        <category>Tool</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Tool</tag>
      </tags>
  </entry>
  <entry>
    <title>File Searching</title>
    <url>/2023/07/25/FileSearching/</url>
    <content><![CDATA[<span id="more"></span>


<h1 id="locate：通过名字查找文件"><a href="#locate：通过名字查找文件" class="headerlink" title="locate：通过名字查找文件"></a><code>locate</code>：通过名字查找文件</h1><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">locate [OPTION]... [PATTERN]...</span><br></pre></td></tr></table></figure>

<p><code>locate</code>程序会快速地搜索其内部的<strong>路径名数据库</strong>，然后输出每个绝对路径中包含了<code>[PATTERN]</code>的文件的绝对路径。<code>[PATTERN]</code>一般为普通的字符串，但有些版本还会包括正则表达式匹配和通配符匹配。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]<span class="comment"># locate bin/zip</span></span><br><span class="line">/usr/bin/zip</span><br><span class="line">/usr/bin/zipcloak</span><br><span class="line">/usr/bin/zipgrep</span><br><span class="line">/usr/bin/zipinfo</span><br><span class="line">/usr/bin/zipnote</span><br><span class="line">/usr/bin/zipsplit</span><br></pre></td></tr></table></figure>

<p><code>locate</code>的路径名数据库由<code>updatedb</code>程序创建。该程序会在特定的时间间隔（如每次开机）被守护进程<code>cron</code>执行，因此一些新创建的文件可能无法用<code>locate</code>进行搜索，但是我们也可以以超级用户的身份手动运行<code>updatedb</code>：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@localhost Playground]# updatedb</span><br></pre></td></tr></table></figure>

<h1 id="find：在目录层次结构中搜索文件"><a href="#find：在目录层次结构中搜索文件" class="headerlink" title="find：在目录层次结构中搜索文件"></a><code>find</code>：在目录层次结构中搜索文件</h1><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">find [path...] [expression]</span><br></pre></td></tr></table></figure>

<p><code>locate</code>可以让我们简单地用文件名找到文件的位置，而<code>find</code>则可以让我们在指定的目录<code>[path]</code>（绝对或相对路径）下，用文件的各种<strong>属性</strong>匹配相应的文件。在了解<code>find</code>之前，我们先在当前目录下创建100个子目录，并在每个子目录下创建24个普通文件：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ <span class="built_in">mkdir</span> <span class="built_in">dir</span>-&#123;00&#123;1..9&#125;,0&#123;10..99&#125;,100&#125;</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">dir-001  ...  dir-100</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">touch</span> <span class="built_in">dir</span>-&#123;00&#123;1..9&#125;,0&#123;10..99&#125;,100&#125;/file-&#123;A..Z&#125;</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span> dir-001</span><br><span class="line">file-A  ...  file-Z</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span> dir-002</span><br><span class="line">file-A  ...  file-Z</span><br></pre></td></tr></table></figure>

<p>在最基本的用法里，<code>find</code>会输出所查目录及其子目录的所有文件的路径，其中，相对路径查找输出相对路径，绝对路径查找则输出绝对路径：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ find ./dir-001</span><br><span class="line">./dir-001</span><br><span class="line">...</span><br><span class="line">./dir-001/file-Z</span><br><span class="line">[meme@localhost Playground]$ find /home/meme/Playground/dir-001</span><br><span class="line">/home/meme/Playground/dir-001</span><br><span class="line">...</span><br><span class="line">/home/meme/Playground/dir-001/file-Z</span><br></pre></td></tr></table></figure>

<h2 id="Tests：测试条件"><a href="#Tests：测试条件" class="headerlink" title="Tests：测试条件"></a>Tests：测试条件</h2><p><code>find</code>拥有众多的测试条件，每个测试条件都一定程度上对应着文件的一项属性。<code>find</code>在指定目录中遍历文件元数据时，会使用用户指定的测试条件来筛选文件。</p>
<h3 id="type"><a href="#type" class="headerlink" title="-type"></a><code>-type</code></h3><p><code>-type</code>选择文件的类别：<code>b</code>块设备文件、<code>c</code>字符设备文件、<code>d</code>目录、<code>f</code>普通文件、<code>l</code>符号链接：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ find ./dir-001 -<span class="built_in">type</span> f</span><br><span class="line">./dir-001/file-A</span><br><span class="line">...</span><br><span class="line">./dir-001/file-Z</span><br><span class="line">[meme@localhost Playground]$ find ./dir-001 -<span class="built_in">type</span> d</span><br><span class="line">./dir-001</span><br></pre></td></tr></table></figure>

<h3 id="name"><a href="#name" class="headerlink" title="-name"></a><code>-name</code></h3><p><code>-name</code>选择文件的名字。此处的名字只是名字，而不包含路径：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ find ./dir-001 -name <span class="string">&#x27;*1&#x27;</span></span><br><span class="line">./dir-001</span><br></pre></td></tr></table></figure>

<h3 id="size"><a href="#size" class="headerlink" title="-size"></a><code>-size</code></h3><p><code>-size</code>选择文件的大小，后面跟着的字符串，<code>+1M</code>表示只选择大于1M的文件，<code>-1M</code>表示只选择小于1M的文件，<code>1M</code>表示只选择大小等于1M的文件：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ find ./dir-001 -size <span class="string">&#x27;1M&#x27;</span></span><br><span class="line">./dir-001</span><br><span class="line">[meme@localhost Playground]$ find ./dir-001 -size <span class="string">&#x27;-1M&#x27;</span></span><br><span class="line">./dir-001/file-A</span><br><span class="line">...</span><br><span class="line">./dir-001/file-Z</span><br></pre></td></tr></table></figure>

<h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><p>除了上面这三个常用的测试条件，<code>find</code>还支持很多其他的测试条件，如<code>-perm mode</code>选择指定权限<code>mode</code>的文件等。</p>
<h2 id="Operators：操作符"><a href="#Operators：操作符" class="headerlink" title="Operators：操作符"></a>Operators：操作符</h2><p><code>find</code>的测试条件实际上相当于程序语言中的条件语句，因此，它很自然地也能支持各种条件语句的逻辑组合。这些组合与编程语言中的一样，通过逻辑操作符<em>与</em><code>-and</code>、<em>或</em><code>-or</code>、<em>非</em><code>-not</code>来实现。这些逻辑操作符的运算逻辑和执行特性和一般程序语言中的没有任何区别。通常地，逻辑操作符连接的两个测试条件应该分别用<code>()</code>括起来。在命令行中，<code>()</code>有特殊的意义，因此实际上要为<code>()</code>加上转义字符，即实际应使用<code>\(\)</code>。</p>
<ul>
<li><code>-and</code>：<code>find</code>的默认操作符，即若测试条件之间没有操作符，那么其连接符号默认是<code>-and</code>。<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ find . -<span class="built_in">type</span> f -name <span class="string">&#x27;*A&#x27;</span></span><br><span class="line"> ./dir-001/file-A</span><br><span class="line"> ...</span><br><span class="line"> ./dir-100/file-A</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ find . \( -<span class="built_in">type</span> f \) -and \( -name <span class="string">&#x27;*A&#x27;</span> \)</span><br><span class="line">./dir-001/file-A</span><br><span class="line">...</span><br><span class="line">./dir-100/file-A</span><br></pre></td></tr></table></figure>
上面两个命令是等价的。</li>
<li><code>-or</code>：表<em>或</em>，用法与<code>-and</code>相同。</li>
<li><code>-not</code>：表<em>非</em>，用在想要取<em>非</em>的测试条件之前。<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ find ./dir-001 -not -<span class="built_in">type</span> f</span><br><span class="line">./dir-001</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="Actions：行为"><a href="#Actions：行为" class="headerlink" title="Actions：行为"></a>Actions：行为</h2><p><code>find</code>允许用户对筛选结果立即进行操作，即，将筛选结果作为其他<strong>命令</strong>的输入参数。</p>
<ul>
<li><code>-delete</code>：删除当前匹配到的文件；</li>
<li><code>-ls</code>：对当前匹配到的文件执行<code>ls</code>操作；</li>
<li><code>-print</code>：将当前匹配文件的路径打印出来，是<code>find</code>的默认操作；</li>
<li><code>-quit</code>：一旦找到一个匹配项就退出。</li>
</ul>
<p>以上四个是<code>find</code>内部自带的操作，如<code>-delete</code>：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ find ./dir-001 -<span class="built_in">type</span> f -delete</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span> ./dir-001</span><br></pre></td></tr></table></figure>

<blockquote>
<p>需要注意的是，<code>Tests</code>和<code>Actions</code>在<code>find</code>看来是同优先级的，而<code>find</code>又是顺序过滤的，因此不同的<code>Tests</code>和<code>Actions</code>的顺序很重要。</p>
</blockquote>
<h3 id="用户自定义行为"><a href="#用户自定义行为" class="headerlink" title="用户自定义行为"></a>用户自定义行为</h3><p>除了<code>find</code>自带的这几种行为，<code>find</code>还允许用户自由地使用所有合法的Linux命令，一般的格式为：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">-<span class="built_in">exec</span> [<span class="built_in">command</span>] <span class="string">&#x27;&#123;&#125;&#x27;</span> <span class="string">&#x27;;&#x27;</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>因为<code>&#123;&#125;</code>和<code>;</code>在命令行中有特殊意义，所以必须被引用为字符串或者转义。</p>
</blockquote>
<p>如，对筛选出来的文件使用<code>rm -r</code>操作：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ find . -name <span class="string">&#x27;*1&#x27;</span></span><br><span class="line">./dir-001</span><br><span class="line">...</span><br><span class="line">./dir-091</span><br><span class="line">[meme@localhost Playground]$ find . -name <span class="string">&#x27;*1&#x27;</span> -<span class="built_in">exec</span> <span class="built_in">rm</span> -r <span class="string">&#x27;&#123;&#125;&#x27;</span> <span class="string">&#x27;;&#x27;</span></span><br><span class="line">find: <span class="string">&#x27;./dir-001&#x27;</span>: No such file or directory</span><br><span class="line">...</span><br><span class="line">find: <span class="string">&#x27;./dir-091&#x27;</span>: No such file or directory</span><br></pre></td></tr></table></figure>

<p>有时为了安全起见，我们会想要一个个地确认<code>Actions</code>的效果，此时我们可以使用<code>-ok</code>来代替<code>-exec</code>。使用<code>-ok</code>后，<code>[command]</code>在执行前会先向用户进行确认：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ find . -name <span class="string">&#x27;*2&#x27;</span> -ok <span class="built_in">rm</span> -r <span class="string">&#x27;&#123;&#125;&#x27;</span> <span class="string">&#x27;;&#x27;</span></span><br><span class="line">&lt; <span class="built_in">rm</span> ... ./dir-002 &gt; ? n</span><br><span class="line">&lt; <span class="built_in">rm</span> ... ./dir-012 &gt; ? n</span><br><span class="line">&lt; <span class="built_in">rm</span> ... ./dir-022 &gt; ? n</span><br><span class="line">&lt; <span class="built_in">rm</span> ... ./dir-032 &gt; ? y</span><br><span class="line">find: <span class="string">&#x27;./dir-032&#x27;</span>: No such file or directory</span><br><span class="line">&lt; <span class="built_in">rm</span> ... ./dir-042 &gt; ? n</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>当我们使用<code>&#39;;&#39;</code>时，我们在告诉<code>find</code>：对于每一个匹配的文件，运行一次<code>[command]</code>，这样会导致当匹配结果有$n$个文件时，<code>[command]</code>被运行$n$次。要想避免这种情况，我们可以将<code>&#39;;&#39;</code>替换为<code>+</code>，这样就是在告诉<code>find</code>：将所有的结果合成一个列表再执行命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ find . -name <span class="string">&#x27;*2&#x27;</span> -<span class="built_in">exec</span> <span class="built_in">rm</span> -r <span class="string">&#x27;&#123;&#125;&#x27;</span> +</span><br></pre></td></tr></table></figure>

<blockquote>
<p>注意，此时只能用<code>-exec</code>而不能用<code>-ok</code>。</p>
</blockquote>
<h1 id="辅助命令"><a href="#辅助命令" class="headerlink" title="辅助命令"></a>辅助命令</h1><p>有许多的辅助命令能够让<code>locate</code>和<code>find</code>的功能更强大。</p>
<h2 id="xargs"><a href="#xargs" class="headerlink" title="xargs"></a><code>xargs</code></h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">xargs [OPTION]... COMMAND INITIAL-ARGS...</span><br></pre></td></tr></table></figure>

<p><code>xargs</code>从标准输入中接受输入，并将输入转换为一个<em>命令行参数列表</em>，这个参数列表可以直接作为其他指令的参数，如：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ find . -name <span class="string">&#x27;*13&#x27;</span></span><br><span class="line">./dir-013</span><br><span class="line">[meme@localhost Playground]$ find . -name <span class="string">&#x27;*13&#x27;</span> | xargs <span class="built_in">ls</span> -l</span><br><span class="line">total 0</span><br><span class="line">-rw-rw-r--. 1 meme meme 0 Jul 26 19:50 file-A</span><br><span class="line">...</span><br><span class="line">-rw-rw-r--. 1 meme meme 0 Jul 26 19:50 file-Z</span><br></pre></td></tr></table></figure>

<p><code>xargs</code>将<code>./dir-013</code>管道为了<code>ls -l</code>的命令行参数。</p>
<h2 id="touch"><a href="#touch" class="headerlink" title="touch"></a><code>touch</code></h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">touch [OPTION]... FILE...</span><br></pre></td></tr></table></figure>

<p>当<code>touch</code>后的文件不存在时，<code>touch</code>相当于一个创建文件操作：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ locate taylor</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">touch</span> taylor</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span> taylor</span><br><span class="line">taylor</span><br></pre></td></tr></table></figure>

<p>若文件已存在，则<code>touch</code>会将其修改时间设置为当前时间：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ <span class="built_in">stat</span> taylor</span><br><span class="line">  File: ‘taylor’</span><br><span class="line">...</span><br><span class="line">Access: 2023-07-26 22:32:26.809464671 -0700</span><br><span class="line">Modify: 2023-07-26 22:32:26.809464671 -0700</span><br><span class="line">Change: 2023-07-26 22:32:26.809464671 -0700</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">touch</span> taylor</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">stat</span> taylor</span><br><span class="line">  File: ‘taylor’</span><br><span class="line">...</span><br><span class="line">Access: 2023-07-26 22:33:54.850140719 -0700</span><br><span class="line">Modify: 2023-07-26 22:33:54.850140719 -0700</span><br><span class="line">Change: 2023-07-26 22:33:54.850140719 -0700</span><br></pre></td></tr></table></figure>

<blockquote>
<p><code>stat</code>相当于加强版的<code>ls</code>。</p>
</blockquote>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://www.kancloud.cn/thinkphp/linux-command-line/39449">The Linux Command Line 中文版 第十八章：查找文件</a></li>
</ul>
]]></content>
      <categories>
        <category>Tool</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Tool</tag>
      </tags>
  </entry>
  <entry>
    <title>Text, File and Data Wrangling</title>
    <url>/2023/07/25/TextFileDataWrangling/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h1><p>下面这些常用的命令可以帮助我们快速地处理文本、在文件中传递信息、进行数据整理。</p>
<h2 id="I-x2F-O重定向"><a href="#I-x2F-O重定向" class="headerlink" title="I&#x2F;O重定向"></a>I&#x2F;O重定向</h2><p>与C语言中的I&#x2F;O重定向相同，Linux的I&#x2F;O重定向允许我们自由地更改标准输入、标准输出和标准错误文件。它们的默认对象分别为键盘、显示器和显示器，文件句柄为0，1和2。</p>
<h3 id="gt-and-gt-gt-：输出重定向"><a href="#gt-and-gt-gt-：输出重定向" class="headerlink" title="&gt; and &gt;&gt;：输出重定向"></a><code>&gt;</code> and <code>&gt;&gt;</code>：输出重定向</h3><p><code>&gt;</code>重定向符允许我们将程序的输出重定向到指定的文件：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span> -l /dev/null</span><br><span class="line">crw-rw-rw-. 1 root root 1, 3 Jul 25 18:19 /dev/null</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span> -l /dev/null &gt; ls-output.txt</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">cat</span> ls-output.txt</span><br><span class="line">crw-rw-rw-. 1 root root 1, 3 Jul 25 18:19 /dev/null</span><br></pre></td></tr></table></figure>

<blockquote>
<p>将<code>cat</code>视作一个查看文件内容的命令即可，后面会有介绍。</p>
</blockquote>
<p>使用<code>&gt;</code>后，原本会被输出到显示器的信息被输出到了新文件<code>ls-output.txt</code>中。需要注意的是，作为重定向目标的文件如果不存在，则Linux会先在当前目录下创建该文件，若目标文件已存在，则Linux会先将文件内容清空：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ &gt; ls-output.txt</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">cat</span> ls-output.txt</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span> -l ls-output.txt</span><br><span class="line">-rw-rw-r--. 1 meme meme 0 Jul 25 19:52 ls-output.txt</span><br></pre></td></tr></table></figure>

<p>由于<code>&gt;</code>前没有程序输出，所以目标文件的内容会被直接清零。若想保持原文件内容不变，则应该使用<code>&gt;&gt;</code>重定向符，让输出被添加在目标文件末尾：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span> -l /dev/null &gt;&gt; ls-output.txt</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">cat</span> ls-output.txt</span><br><span class="line">crw-rw-rw-. 1 root root 1, 3 Jul 25 18:19 /dev/null</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span> -l /dev/null &gt;&gt; ls-output.txt</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">cat</span> ls-output.txt</span><br><span class="line">crw-rw-rw-. 1 root root 1, 3 Jul 25 18:19 /dev/null</span><br><span class="line">crw-rw-rw-. 1 root root 1, 3 Jul 25 18:19 /dev/null</span><br></pre></td></tr></table></figure>

<blockquote>
<p>此处用到的文件<code>/dev/null</code>是Linux中的一个特殊文件，它是一个系统设备，称“位存储桶”，它可以接受输入，但是不输入任何东西。有时一些输出信息如果我们不想看到，我们可以直接把它们重定向到<code>/dev/null</code>。</p>
</blockquote>
<h3 id="标准错误重定向"><a href="#标准错误重定向" class="headerlink" title="标准错误重定向"></a>标准错误重定向</h3><p>标准错误显示的是程序执行过程中出现的错误信息，比如，若我们想用<code>ls</code>查看一个不存在的文件夹的信息，<code>ls</code>输出的就是标准错误：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span> -l /ddd/ds</span><br><span class="line"><span class="built_in">ls</span>: cannot access /ddd/ds: No such file or directory</span><br></pre></td></tr></table></figure>

<p>标准错误缺乏专用的重定向符，我们只能用文件句柄+输出重定向来实现对标准错误的重定向，即：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span> -l /ddd/ds 2&gt; ls-output-error.txt</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">cat</span> ls-output-error.txt</span><br><span class="line"><span class="built_in">ls</span>: cannot access /ddd/ds: No such file or directory</span><br></pre></td></tr></table></figure>

<p><code>2</code>是Linux内部的标准错误文件句柄，<code>2&gt;</code>或<code>2&gt;&gt;</code>（添加到文件末尾）能够实现标准错误的重定向。</p>
<h3 id="标准输出与错误同步"><a href="#标准输出与错误同步" class="headerlink" title="标准输出与错误同步"></a>标准输出与错误同步</h3><p>有时，我们会希望将标准输出和标准错误都重定向到一个文件。对于这个操作，Linux提供了两种方式。</p>
<ol>
<li>在输出重定向后将标准错误重定向到标准输出：<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span> -l /ddd/ds &gt;&gt; ls-output-error.txt 2&gt;&amp;1</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">cat</span> ls-output-error.txt</span><br><span class="line"><span class="built_in">ls</span>: cannot access /ddd/ds: No such file or directory</span><br><span class="line"><span class="built_in">ls</span>: cannot access /ddd/ds: No such file or directory</span><br></pre></td></tr></table></figure>
上面的<code>2&gt;&amp;1</code>将标准错误<code>2</code>重定向至标准输出<code>1</code>，而前面的<code>&gt;&gt;</code>则同时决定了两者的重定向方式。<code>&gt;&gt;</code>和<code>2&gt;&amp;1</code>的顺序不能颠倒，否则标准错误会被先重定向到原来的标准输出（显示器）。</li>
<li>标准输出与标准错误同时重定向，这是一种更简洁的方法：<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span> -l /ddd/ds &amp;&gt; ls-output-error.txt</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">cat</span> ls-output-error.txt</span><br><span class="line"><span class="built_in">ls</span>: cannot access /ddd/ds: No such file or directory</span><br></pre></td></tr></table></figure>
<code>&amp;&gt;</code>或<code>&amp;&gt;&gt;</code>同时将标准输出和标准错误重定向到同一个文件。</li>
</ol>
<h3 id="lt-：输入重定向"><a href="#lt-：输入重定向" class="headerlink" title="&lt;：输入重定向"></a><code>&lt;</code>：输入重定向</h3><p>在了解<code>&lt;</code>输入重定向符号之前，我们要先知道一个可以接受标准输入的命令，见<a href="/2023/07/25/TextFileDataWrangling/#cat：Concatenate，连接文件"><code>cat</code>：Concatenate，连接文件</a>。</p>
<p>既然<code>cat</code>能够接受标准输入，那么我们就可以用<code>&lt;</code>将标准输入重定向为我们想要的文件：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ <span class="built_in">cat</span> &lt; ls-output.txt</span><br><span class="line">crw-rw-rw-. 1 root root 1, 3 Jul 25 18:19 /dev/null</span><br><span class="line">crw-rw-rw-. 1 root root 1, 3 Jul 25 18:19 /dev/null</span><br></pre></td></tr></table></figure>

<p>进一步地，可以同时进行输入和输出重定向：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ <span class="built_in">cat</span> &lt; ls-output.txt &gt; ls-output-copy.txt</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">cat</span> ls-output-copy.txt</span><br><span class="line">crw-rw-rw-. 1 root root 1, 3 Jul 25 18:19 /dev/null</span><br><span class="line">crw-rw-rw-. 1 root root 1, 3 Jul 25 18:19 /dev/null</span><br></pre></td></tr></table></figure>

<h2 id="cat：Concatenate，连接文件"><a href="#cat：Concatenate，连接文件" class="headerlink" title="cat：Concatenate，连接文件"></a><code>cat</code>：Concatenate，连接文件</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cat</span> [OPTION]... [FILE]...</span><br></pre></td></tr></table></figure>

<p><code>cat</code>，如其字面意思，是一个连接操作。它将依次读取输入文件<code>[FILE]</code>的内容，并复制到标准输出。由于<code>cat</code>输出的不同文件的内容之间不会分页，也不会有分隔符，所以如果此时把输出重定向到另一个文件，那么其作用就好像是把这几个文件拼合成了一个文件：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ <span class="built_in">cat</span> ls-output.txt</span><br><span class="line">crw-rw-rw-. 1 root root 1, 3 Jul 25 18:19 /dev/null</span><br><span class="line">crw-rw-rw-. 1 root root 1, 3 Jul 25 18:19 /dev/null</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">cat</span> ls-output.txt ls-output-error.txt</span><br><span class="line">crw-rw-rw-. 1 root root 1, 3 Jul 25 18:19 /dev/null</span><br><span class="line">crw-rw-rw-. 1 root root 1, 3 Jul 25 18:19 /dev/null</span><br><span class="line"><span class="built_in">ls</span>: cannot access /ddd/ds: No such file or directory</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">cat</span> ls-output.txt ls-output-error.txt &gt; ls-output-cat.txt</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">cat</span> ls-output-cat.txt</span><br><span class="line">crw-rw-rw-. 1 root root 1, 3 Jul 25 18:19 /dev/null</span><br><span class="line">crw-rw-rw-. 1 root root 1, 3 Jul 25 18:19 /dev/null</span><br><span class="line"><span class="built_in">ls</span>: cannot access /ddd/ds: No such file or directory</span><br></pre></td></tr></table></figure>

<p><code>cat</code>后面也可以不跟文件名，此时它将从标准输入中读取信息并复制到标准输出：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ <span class="built_in">cat</span></span><br><span class="line">I don<span class="string">&#x27;t know about you but I&#x27;</span>m feeling 22.</span><br><span class="line">I don<span class="string">&#x27;t know about you but I&#x27;</span>m feeling 22.</span><br></pre></td></tr></table></figure>

<p>上面的指令中，我只输入了一次<code>I don&#39;t know about you but I&#39;m feeling 22.</code>，但是回车后<code>cat</code>将会复制并再次输出一行相同的文本，直到我们<code>Ctrl+D</code>示意标准输入已经到达文件末尾。当然，我们也可以将<code>cat</code>输出的标准输入内容重定向到指定文件：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ <span class="built_in">cat</span> &gt; 22.txt</span><br><span class="line">I don<span class="string">&#x27;t know about you but I&#x27;</span>m feeling 22.</span><br><span class="line">Everything will be alright <span class="keyword">if</span> you keep me next to you.</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">cat</span> 22.txt</span><br><span class="line">I don<span class="string">&#x27;t know about you but I&#x27;</span>m feeling 22.</span><br><span class="line">Everything will be alright <span class="keyword">if</span> you keep me next to you.</span><br></pre></td></tr></table></figure>

<blockquote>
<p><code>cat</code>有许多有趣的<code>[OPTION]</code>选项，如<code>-n</code>会给输出加入行号。用<code>cat --help</code>来查看更多有趣的选项操作。</p>
</blockquote>
<h3 id="Here-document"><a href="#Here-document" class="headerlink" title="Here-document"></a>Here-document</h3><p>Here-document是Shell中一种特殊形式的重定向。Here-document，可以解释为“立即文档”。在这种特殊形式的重定向中，Here-document就是一段字符串，这段字符串的长度很短，以至于将其单独放在一个文件中有点“大动干戈”，不如直接放在命令或者脚本代码中。Here-document的基本用法为：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[<span class="built_in">command</span>] &lt;&lt; [Here Tag]</span><br><span class="line">[Your Here-document]</span><br><span class="line">[Here Tag] <span class="comment"># 终止符[Here Tag]必须独占一行，且要顶格写</span></span><br></pre></td></tr></table></figure>

<p>其中<code>[command]</code>是任意的能够接受输入文档的指令，<code>[Your Here-document]</code>是用户想要输入的字符串，<code>[Here Tag]</code>则用于标志用户输入的开始和终止，可以是任何的字符串，一般用<code>EOF</code>。以<code>cat</code>为例：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ <span class="built_in">cat</span> &lt;&lt; <span class="string">EOF</span></span><br><span class="line"><span class="string">&gt; Hello World</span></span><br><span class="line"><span class="string">&gt; EOF</span></span><br><span class="line">Hello World</span><br></pre></td></tr></table></figure>

<p>此时，<code>Hello World</code>充当了输入文件的内容。</p>
<blockquote>
<p>Here-document常被用于Bash脚本中。</p>
</blockquote>
<h2 id="：管道线"><a href="#：管道线" class="headerlink" title="|：管道线"></a><code>|</code>：管道线</h2><p><code>|</code>运算符可用于连接两个命令，它会自动地将前一个命令的输出管道为后一个命令的输入：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ <span class="built_in">cat</span> 22.txt | <span class="built_in">cat</span> &gt;&gt; 22.txt</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">cat</span> 22.txt</span><br><span class="line">I don<span class="string">&#x27;t know about you but I&#x27;</span>m feeling 22.</span><br><span class="line">Everything will be alright <span class="keyword">if</span> you keep me next to you.</span><br><span class="line">I don<span class="string">&#x27;t know about you but I&#x27;</span>m feeling 22.</span><br><span class="line">Everything will be alright <span class="keyword">if</span> you keep me next to you.</span><br></pre></td></tr></table></figure>

<p>第一个<code>cat</code>输出<code>22.txt</code>的内容，<code>|</code>将其管道为第二个<code>cat</code>的输入，而第二个<code>cat</code>的输出又被重定向到<code>22.txt</code>，因此最后<code>22.txt</code>的前两行被复制了一份。</p>
<blockquote>
<p>管道线可以用来实现很复杂的操作。有时会把多个命令用多个<code>|</code>连接，这样形成的复合命令一般被称为“过滤器”，因为它可以把原始的输出通过<code>|</code>的逐层过滤得到我们想要的输出形式。</p>
</blockquote>
<h2 id="uniq：Unique，报道或忽略重复行"><a href="#uniq：Unique，报道或忽略重复行" class="headerlink" title="uniq：Unique，报道或忽略重复行"></a><code>uniq</code>：Unique，报道或忽略重复行</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">uniq</span> [OPTION]... [INPUT [OUTPUT]]</span><br></pre></td></tr></table></figure>

<p><code>uniq</code>从标准输入或者单个文件名参数中接受<strong>有序列表</strong>，并在默认情况下删除有序列表中任何重复的行。由于输入<code>uniq</code>的参数必须是有序的<code>uniq</code>才能其作用（因为它只能检测到连续重复的行），因此<code>uniq</code>通常会结合<code>sort</code>一起使用：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ <span class="built_in">cat</span> 22.txt</span><br><span class="line">I don<span class="string">&#x27;t know about you but I&#x27;</span>m feeling 22.</span><br><span class="line">Everything will be alright <span class="keyword">if</span> you keep me next to you.</span><br><span class="line">I don<span class="string">&#x27;t know about you but I&#x27;</span>m feeling 22.</span><br><span class="line">Everything will be alright <span class="keyword">if</span> you keep me next to you.</span><br><span class="line">You don<span class="string">&#x27;t know about me but I&#x27;</span>ll bet you want to.</span><br><span class="line">Everything will be alright <span class="keyword">if</span> we just keep dancing like we<span class="string">&#x27;re</span></span><br><span class="line"><span class="string">22 22</span></span><br><span class="line"><span class="string">[meme@localhost Playground]$ cat 22.txt | sort | uniq</span></span><br><span class="line"><span class="string">22 22</span></span><br><span class="line"><span class="string">Everything will be alright if we just keep dancing like we&#x27;</span>re</span><br><span class="line">Everything will be alright <span class="keyword">if</span> you keep me next to you.</span><br><span class="line">I don<span class="string">&#x27;t know about you but I&#x27;</span>m feeling 22.</span><br><span class="line">You don<span class="string">&#x27;t know about me but I&#x27;</span>ll bet you want to.</span><br></pre></td></tr></table></figure>

<blockquote>
<p>此处稍稍增添了点《22》的歌词。</p>
</blockquote>
<p>选项<code>-d</code>会使得<code>uniq</code>只输出重复的行：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ <span class="built_in">cat</span> 22.txt | <span class="built_in">sort</span> | <span class="built_in">uniq</span> -d</span><br><span class="line">Everything will be alright <span class="keyword">if</span> you keep me next to you.</span><br><span class="line">I don<span class="string">&#x27;t know about you but I&#x27;</span>m feeling 22.</span><br></pre></td></tr></table></figure>

<h2 id="wc：Word-Count，打印行、单词和字节数"><a href="#wc：Word-Count，打印行、单词和字节数" class="headerlink" title="wc：Word Count，打印行、单词和字节数"></a><code>wc</code>：Word Count，打印行、单词和字节数</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">wc</span> [OPTION]... [FILE]...</span><br></pre></td></tr></table></figure>

<p><code>wc</code>会统计标准输入或者文件内容的总行数、总单词数和总字节数（字符数）并输出：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ <span class="built_in">cat</span> 22.txt | <span class="built_in">sort</span> | <span class="built_in">uniq</span> -d | <span class="built_in">wc</span></span><br><span class="line">      2      20      98</span><br></pre></td></tr></table></figure>

<p><code>wc</code>默认会将三个数字一起输出，不过<code>-l</code>、<code>-w</code>、<code>-m</code>选项会让其分别只输出行、单词、字符数。</p>
<h2 id="head-x2F-tail：打印文件的开头或结尾"><a href="#head-x2F-tail：打印文件的开头或结尾" class="headerlink" title="head&#x2F;tail：打印文件的开头或结尾"></a><code>head</code>&#x2F;<code>tail</code>：打印文件的开头或结尾</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">head</span>/tail [OPTION]... [FILE]...</span><br></pre></td></tr></table></figure>

<p><code>head</code>和<code>tail</code>是两个对称的命令，前者默认输出输入的前10行，后者输出输入的后10行。<code>-n</code>选项可用于调整打印的行数：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ <span class="built_in">cat</span> 22.txt | <span class="built_in">sort</span> | <span class="built_in">uniq</span> -d | <span class="built_in">head</span> -n1</span><br><span class="line">Everything will be alright <span class="keyword">if</span> you keep me next to you.</span><br></pre></td></tr></table></figure>

<blockquote>
<p>若在使用<code>tail</code>的同时使用<code>-f</code>选项，那么除非人为地中止，<code>tail</code>程序不会退出。这使得我们可以用<code>tail -f [FILE]</code>来实时的监测文件内容。</p>
</blockquote>
<h2 id="tee：T-splitter，左右开弓"><a href="#tee：T-splitter，左右开弓" class="headerlink" title="tee：T-splitter，左右开弓"></a><code>tee</code>：T-splitter，左右开弓</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">tee</span> [OPTION]... [FILE]...</span><br></pre></td></tr></table></figure>

<p><code>tee</code>，如其名字的由来，T型分流器（T-splitter），可以接受标准输入读入的数据，并将其同时复制到标准输出和一个或多个文件中：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ <span class="built_in">cat</span> 22.txt | <span class="built_in">sort</span> | <span class="built_in">uniq</span> -d | <span class="built_in">tee</span> t1.txt t2.txt | <span class="built_in">head</span> -n1</span><br><span class="line">Everything will be alright <span class="keyword">if</span> you keep me next to you.</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">cat</span> t1.txt</span><br><span class="line">Everything will be alright <span class="keyword">if</span> you keep me next to you.</span><br><span class="line">I don<span class="string">&#x27;t know about you but I&#x27;</span>m feeling 22.</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">cat</span> t2.txt</span><br><span class="line">Everything will be alright <span class="keyword">if</span> you keep me next to you.</span><br><span class="line">I don<span class="string">&#x27;t know about you but I&#x27;</span>m feeling 22.</span><br></pre></td></tr></table></figure>

<p><code>tee</code>获取了<code>uniq</code>的输出，并将其输出到<code>t1.txt</code>和<code>t2.txt</code>中，同时还会输入到标准输出中。<code>tee</code>的这种不中断管道流的特性使得它很适合被用在过滤器中以提取中间信息。</p>
<h2 id="grep：Global-Regular-Expression-Print，打印匹配行"><a href="#grep：Global-Regular-Expression-Print，打印匹配行" class="headerlink" title="grep：Global Regular Expression Print，打印匹配行"></a><code>grep</code>：Global Regular Expression Print，打印匹配行</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">grep [OPTION]... PATTERN [FILE]...</span><br></pre></td></tr></table></figure>

<p><code>grep</code>是一个很强大的程序，它可以按照<code>PATTERN</code>的形式匹配文件或标准输入中的指定模式并打印其所在的行。<code>PATTERN</code>可以是我们需要检索的文本，也可以是更加复杂通用的<em>正则表达式</em>。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[meme@localhost Playground]$ <span class="built_in">cat</span> 22.txt | grep Everything</span><br><span class="line">Everything will be alright <span class="keyword">if</span> you keep me next to you.</span><br><span class="line">Everything will be alright <span class="keyword">if</span> you keep me next to you.</span><br><span class="line">Everything will be alright <span class="keyword">if</span> we just keep dancing like we<span class="string">&#x27;re</span></span><br></pre></td></tr></table></figure>

<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://www.kancloud.cn/thinkphp/linux-command-line/39437">The Linux Command Line 中文版 第七章：重定向</a></li>
<li><a href="https://www.kancloud.cn/thinkphp/linux-command-line/39451">The Linux Command Line 中文版 第二十章：正则表达式</a></li>
<li><a href="https://www.kancloud.cn/thinkphp/linux-command-line/39452">The Linux Command Line 中文版 第二十一章：文本处理</a></li>
<li><a href="https://blog.csdn.net/oqqHuTu12345678/article/details/129282059">Shell高级——Here Document、Here String</a></li>
<li><a href="https://www.zadmei.com/bzdc.html">Bash 中的 Cat EOF</a></li>
</ul>
]]></content>
      <categories>
        <category>Tool</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Tool</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux Command Line</title>
    <url>/2023/07/08/LinuxCommandLine/</url>
    <content><![CDATA[<span id="more"></span>

<p>总的参考：</p>
<ul>
<li><a href="https://www.kancloud.cn/thinkphp/linux-command-line">The Linux Command Line 中文版</a></li>
</ul>
<p>目录：</p>
<ul>
<li><a href="/2023/07/25/TextFileDataWrangling/">Text, File and Data Wrangling：文本，文件与数据整理</a></li>
<li><a href="/2023/07/25/FileSearching/">File Searching：文件查找</a></li>
<li><a href="/2023/07/25/PackageProcessing/">Package Processing：软件包处理</a></li>
<li><a href="/2023/07/25/NetworkRemoteInteraction/">Network and Remote Interaction：网络系统与远程交互</a></li>
<li><a href="/2023/07/25/ProcessManagement/">Process Management：进程管理</a></li>
<li><a href="/2023/07/25/SystemSecurityPermission/">System Security and Permissions：系统安全与权限</a></li>
<li><a href="/2023/07/25/ProgramCompilation/">Program Compilation：程序编译</a></li>
<li><a href="/2023/08/03/WildcardsPatternMatching/">Wildcards and Pattern Matching：通配符</a></li>
<li><a href="/2023/08/04/RegularExpressions/">Regular Expressions：正则表达式</a></li>
<li><a href="/2023/09/02/BashScript/">Bash Script：Bash脚本</a></li>
</ul>
]]></content>
      <categories>
        <category>Tool</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Tool</tag>
      </tags>
  </entry>
  <entry>
    <title>找论文，读论文，用论文，写论文</title>
    <url>/2023/06/07/PaperReading/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="找论文"><a href="#找论文" class="headerlink" title="找论文"></a>找论文</h1><p>Source of papers</p>
<ul>
<li>Twitter: 关注学者的账号;</li>
<li>ML Subreddit (machine learning)</li>
<li>NIPS&#x2F;ICML&#x2F;ICLR (machine learning) [顶会]</li>
<li>Arxiv: 会有只有摘要的未完成文章, 占坑专用;</li>
<li>dblp: 几乎是计算机领域最全的数据库;</li>
<li>IEEE: 也较全。</li>
</ul>
<p>Reading list</p>
<ul>
<li>Get a set of papers(5 is enough);</li>
<li>Skim all of them.If one paper is nonsense, just forget it.Find the most valuable paper,try to understand it and add some of its references to the reading list;</li>
<li>Jump to the other papers of reading list.</li>
</ul>
<h1 id="读论文"><a href="#读论文" class="headerlink" title="读论文"></a>读论文</h1><p>How to read a paper</p>
<ul>
<li><p>The first time: Read the title&#x2F;abstract&#x2F;figures [ie.the graph or table];</p>
</li>
<li><p>The second time: Read intro&#x2F;conclu&#x2F;figures&#x2F;skim the other parts. Actually, to persuade reviewer to accept your paper, the abstract&#x2F;intro&#x2F;conclu are always the most  clear part.</p>
</li>
<li><p>The third time: Read but skim the maths;</p>
</li>
<li><p>The forth time: Read the whole paper but jump over the nonsense part. Which part is nonsense?Parts that are not quite relative to the main topic.</p>
</li>
<li><p>标题+作者</p>
<blockquote>
<p>*表同等贡献</p>
</blockquote>
</li>
<li><p>摘要</p>
<blockquote>
<p>代码的链接一般放摘要的最后一句话</p>
</blockquote>
</li>
<li><p>结论</p>
</li>
<li><p>导言</p>
<blockquote>
<p>摘要的扩充： 一般介绍已有的工作和本文的工作</p>
</blockquote>
</li>
<li><p>相关工作</p>
</li>
</ul>
<blockquote>
<p>本工作运用到的已有工作： 联系、区别</p>
</blockquote>
<ul>
<li><p>模型</p>
<blockquote>
<p>重要章节</p>
</blockquote>
</li>
<li><p>实验</p>
</li>
</ul>
<blockquote>
</blockquote>
<ul>
<li><p>评论</p>
<blockquote>
<p>文章要讲好故事?</p>
</blockquote>
</li>
</ul>
<h1 id="用论文"><a href="#用论文" class="headerlink" title="用论文"></a>用论文</h1><p>用论文, 即理解论文的数学公式和代码.</p>
<p>How to deal with maths and code</p>
<ul>
<li>Read&#x2F;download it, run it and try to reproduce it</li>
</ul>
<p>What should I get after reading the whole paper?</p>
<ul>
<li>What did the authors try to accomplish?</li>
<li>What were the key element of the approach?</li>
<li>What can I use?</li>
<li>What other references do I want(not) to follow?</li>
</ul>
<p>Frequency</p>
<ul>
<li>Reading two or three papers a week is enough;</li>
</ul>
<h1 id="写论文"><a href="#写论文" class="headerlink" title="写论文"></a>写论文</h1><h2 id="深度学习代码"><a href="#深度学习代码" class="headerlink" title="深度学习代码"></a>深度学习代码</h2><p>深度学习的代码几乎都由3个小组件组成：<code>dataset</code>、<code>model</code>和<code>train</code>。</p>
<h3 id="dataset"><a href="#dataset" class="headerlink" title="dataset"></a>dataset</h3><p><code>dataset</code>，即对训练和测试用的数据集进行<strong>预处理</strong>，它构建了整个AI模型的输入和输出格式。在这一组件中，我们要对数据集的内容、架构有足够的了解，并对数据的<strong>存储方式</strong>和<strong>存储位置</strong>（如是否要分布式等）。<code>dataset</code>组件作为一个主要处理数据的组件，其写作与<code>train</code>组件息息相关。如，基于一次能存入gpu的数据的大小，我们要选择合适的<strong>batchsize</strong>，而batchsize直接影响了<strong>learning rate</strong>的大小。因此，<code>dataset</code>组件和<code>model</code>组件都应该放在<code>train</code>组件之前完成。</p>
<blockquote>
<p>当论文的主要工作为数据处理时，<code>dataset</code>组件更要好好写。</p>
</blockquote>
<h3 id="model"><a href="#model" class="headerlink" title="model"></a>model</h3><p><code>model</code>，是整个深度学习代码的骨架，也决定的AI模型的输入和输出格式。由于当前很多数据集的输入和输入模式都是有现成的框架可以参考的，如GNN有PyG、CV有ImageNet等，所以更加推荐的是先写好<code>dataset</code>，在写<code>model</code>，并对<code>model</code>进行测试。</p>
<h3 id="train"><a href="#train" class="headerlink" title="train"></a>train</h3><p><code>train</code>，构建了模型的训练策略和评估方法。无论论文的创新点在何处，<code>train</code>组件都是最重要和最复杂的组件。除网络架构以外的<strong>超参数</strong>，基本都要在<code>train</code>模块中调节。此外，代码作为论文的重要部分，它为论文提供的不仅仅是想法的实现，还有润色论文的<strong>可视化信息</strong>，如数据、表格、曲线等，而这些都是<code>train</code>组件的一部分。</p>
<blockquote>
<p>有时，当训练一个模型要很长时间时，我们可以将每次的模型参数都存储下来，而这也是在<code>train</code>模块进行的。</p>
</blockquote>
]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>Paper</tag>
      </tags>
  </entry>
  <entry>
    <title>PyTorch</title>
    <url>/2023/04/28/PyTorch/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="Installation-and-import"><a href="#Installation-and-import" class="headerlink" title="Installation and import"></a>Installation and import</h1><p>Since PyTorch is based on the Torch library, the name of package is actually called <em>torch</em>:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Installation</span></span><br><span class="line">pip install torch</span><br><span class="line">or</span><br><span class="line">conda install torch</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># import</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure>

<h1 id="Basic-concept"><a href="#Basic-concept" class="headerlink" title="Basic concept"></a>Basic concept</h1><p>The basic data type in PyTorch is also tensor which is almost the same as the tensor in TensorFlow. PyTorch provides two high-level features:</p>
<ul>
<li>Tensor computing with strong acceleration via GPUs (NumPy only runs on CPUs);</li>
<li>Deep neural networks built on tape-based automatic differentiation system.</li>
</ul>
<p>Both features have much in common with TensorFlow. However, compared to TensorFlow, the api provided by PyTorch are more closer to NumPy.</p>
<h1 id="Data-manipulation"><a href="#Data-manipulation" class="headerlink" title="Data manipulation"></a>Data manipulation</h1><p>To create a tensor, the operations we used are almost the same as those in NumPy:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x = torch.arange(<span class="number">12.</span>) <span class="comment"># float32</span></span><br><span class="line">x = torch.arange(<span class="number">12</span>) <span class="comment"># int64</span></span><br><span class="line">x = torch.zeros((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">x = torch.ones((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">x = torch.randn(<span class="number">3</span>, <span class="number">4</span>) <span class="comment"># random elements drawn from a standard normal distribution</span></span><br><span class="line">x = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">3</span>]])</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Unless otherwise specified, new tensors are stored in main memory and designated for CPU-based computation. See more indexing, slicing, operations and broadcasting in <a href="/2023/04/22/NumPy/">NumPy</a>.</p>
</blockquote>
<p>Though most operations are similar, there are still some differences. We use <code>torch.cat</code> to concatenate multiple tensors together:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.arange(<span class="number">12</span>, dtype=torch.float32).reshape(-<span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">Y = torch.tensor([[<span class="number">2.0</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">3</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]])</span><br><span class="line">torch.cat((X, Y), dim=<span class="number">0</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">We get:</span></span><br><span class="line"><span class="string">tensor([[ 0.,  1.,  2.,  3.],</span></span><br><span class="line"><span class="string">        [ 4.,  5.,  6.,  7.],</span></span><br><span class="line"><span class="string">        [ 8.,  9., 10., 11.],</span></span><br><span class="line"><span class="string">        [ 2.,  1.,  4.,  3.],</span></span><br><span class="line"><span class="string">        [ 1.,  2.,  3.,  4.],</span></span><br><span class="line"><span class="string">        [ 4.,  3.,  2.,  1.]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>In addition to <code>.reshape</code>, we can also use <code>.view</code> to get object of different shapes. There are some differences between the two operations. <code>.view</code> returns a view of the original tensor while <code>.shape</code> may return a copy of the original tensor. <code>.shape</code> only returns the view when the inputs are contiguous in memory.</p>
<h2 id="Saving-memory"><a href="#Saving-memory" class="headerlink" title="Saving memory"></a>Saving memory</h2><p><code>X = X +Y</code> will create a new object and allocate memory while <code>X[:] = X + Y</code> or <code>X += Y</code> will perform in-place operation.</p>
<h2 id="Conversion-to-other-python-objects"><a href="#Conversion-to-other-python-objects" class="headerlink" title="Conversion to other python objects"></a>Conversion to other python objects</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A = X.numpy() <span class="comment"># torch.Tensor-&gt;numpy.ndarray</span></span><br><span class="line">B = torch.from_numpy(A) <span class="comment"># numpy.ndarray-&gt;torch.Tensor</span></span><br><span class="line">b = torch.tensor([<span class="number">2.2</span>]).item() <span class="comment"># size-1 tensor-&gt;python scalar</span></span><br></pre></td></tr></table></figure>

<h1 id="Linear-algebra-function"><a href="#Linear-algebra-function" class="headerlink" title="Linear algebra function"></a>Linear algebra function</h1><ul>
<li><code>A.T</code> shares memory with <code>A</code>;</li>
<li><code>A.clone()</code> returns a new object with the same elements of <code>A</code>;</li>
<li><code>A.mean([axes])</code> and <code>A.sum([axes])</code> return the mean or sum of [axes] in <code>A</code>. In general, [axes] will missing from the shape of the output, but we can add <code>keepdims=True</code> which will make the shape of [axes] 1, to remain [axes];</li>
<li><code>A.cumsum([axes])</code> calculate the cumulative sum of elements of <code>A</code> along some axes;</li>
<li><code>torch.dot(), torch.mv(), torch.mm()</code> (<code>A @ B</code> is also legal) calculate v-v products, m-v products and m-m products respectively;</li>
</ul>
<blockquote>
<p><code>A*B</code> or $A\odot B$ is called <strong>Hadamard product</strong>.</p>
</blockquote>
<h2 id="Norms"><a href="#Norms" class="headerlink" title="Norms"></a>Norms</h2><p>Norms ($||x||$) are often used to measure the length or size of each vector in a vector space (or matrix). They are scalars that satisfy:</p>
<ul>
<li>Non-negativity;</li>
<li>Homogeneity;</li>
<li>Triangle inequality</li>
</ul>
<p>For vectors, $\ell{_2}$ norms measure the (Eucilidean) length of vectors:</p>
<p>$$\vert |x|\vert_2&#x3D;\sqrt{\sum\limits_{i&#x3D;1}^{n}x_i^2}$$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.norm(x)</span><br></pre></td></tr></table></figure>
<p>$\ell{_1}$ norms are called Manhattan distance:</p>
<p>$$\vert |x|\vert_1&#x3D;\sqrt{\sum\limits_{i&#x3D;1}^{n}|x|}$$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.<span class="built_in">abs</span>(x).<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure>
<p>For matrices, we often use <strong>the Frobenius norm</strong>, which is the same as the $\ell_2$ norm:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.norm(X)</span><br></pre></td></tr></table></figure>

<h1 id="Autograd"><a href="#Autograd" class="headerlink" title="Autograd"></a>Autograd</h1><p>See <a href="/2023/04/28/DiveIntoDeepLearningIntroduction/#Derivative-of-matrix
">Matrix derivative</a> to know more about matrix derivative. </p>
<p>Unlike TensorFlow, PyTorch use implicit construction to produce a computation graph, which allows us to simply use its api and don&#39;t have to declare the computation graph explicitly like <a href="/2023/04/12/Tensorflow/#Adam-algorithm">Autograd in Tensorflow</a>. Whenever we want to compute the derivative of a certain argument, we only need 4 steps in PyTorch:</p>
<ol>
<li>Attach gradients to those variables with respect to which we desire derivatives:<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Create independent variable and the space to store derivatives</span></span><br><span class="line">x = torch.arange(<span class="number">4.0</span>, requires_grad=<span class="literal">True</span>) <span class="comment"># or x.requires_grad_(True)</span></span><br></pre></td></tr></table></figure></li>
<li>Record the computation of the target value (dependent variable):<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y = f(a) <span class="comment"># f is the function we define</span></span><br></pre></td></tr></table></figure></li>
<li>Execute the back propagation function:<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y.backward()</span><br></pre></td></tr></table></figure></li>
<li>Access the resulting gradient:<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(x.grad)</span><br></pre></td></tr></table></figure></li>
</ol>
<p>The steps above only work when the dependent variable is a scalar. For non-scalar variables, we sometimes turn them to scalar variables by summing all elements together, like <code>y.sum().backward()</code>. This works because we add the gradients of a specific model parameter together finally. More commonly, we will use a certain row vector $v^T$ to turn $\vec{y}$ to a scalar:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># this is the same as: y.sum().backward(), because torch.dot(torch.ones(len(y)), y) = y.sum()</span></span><br><span class="line">y.backward(gradient=torch.ones(<span class="built_in">len</span>(y)))</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Since neural networks are always computed batch by batch, the result of <code>x.grad</code> will be accumulated. To reset the gradient buffer, we can call <code>x.grad.zero()</code>.</p>
</blockquote>
<h2 id="Detaching-computation"><a href="#Detaching-computation" class="headerlink" title="Detaching computation"></a>Detaching computation</h2><p>If $z&#x3D;f(x,y)$ and $y&#x3D;g(x)$, but we only want to focus on the <strong>direct influence</strong> of $x$ on $z$, we can create a new variable that detaches the connection between $x$ and $y$:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y = g(x)</span><br><span class="line">u = y.detach() <span class="comment"># remove y from the computation graph of z</span></span><br><span class="line">z = x * u <span class="comment"># u is no longer a function of x but y is still a function of x</span></span><br><span class="line"></span><br><span class="line">z.<span class="built_in">sum</span>().backward()</span><br><span class="line">x.grad == u <span class="comment"># the value is true</span></span><br></pre></td></tr></table></figure>

<h2 id="Updating-parameters"><a href="#Updating-parameters" class="headerlink" title="Updating parameters"></a>Updating parameters</h2><p>The Computation graph (tree) will be built implicitly (if <code>requires_grad=True</code>) whenever we operate the parameters that we want to optimize. Since the parameters are always in leaf nodes, we have to detach it from the graph, otherwise, the whole graph will go wrong.</p>
<blockquote>
<p>PyTorch implements a dynamic graph mechanism. Specifically, the computation graph is constructed during forward propagation, and is destroyed during back propagation. More specifically, the computation graph is destroyed when calling <code>backward()</code>, leaving only parameters in leaf nodes.</p>
</blockquote>
<p>The method <code>PyTorch</code> used is <code>with torch.no_grad():</code>, which makes <code>requires_grad=False</code> when entering and <code>requires_grad=True</code> when leaving:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># General usage</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">  <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">      param -= lr * param.grad / batch_size</span><br><span class="line">      param.grad.zero_()</span><br></pre></td></tr></table></figure>

<h1 id="Net-constructing"><a href="#Net-constructing" class="headerlink" title="Net constructing"></a>Net constructing</h1><p>All layers, blocks or nets in PyTorch are subclasses of <code>nn.Module</code>. We can define our own blocks by inheriting <code>nn.Module</code> and overloading the <code>__init__</code> and <code>forward</code> functions.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()  <span class="comment"># we must call the __init__ function of Module so that we can inherit its parameters</span></span><br><span class="line">        self.hidden = nn.Linear(<span class="number">20</span>, <span class="number">256</span>)  <span class="comment"># a hidden layer</span></span><br><span class="line">        self.out = nn.Linear(<span class="number">256</span>, <span class="number">10</span>)  <span class="comment"># output layer</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># forward propagation: get X and produce output</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="comment"># F defines some basic functions</span></span><br><span class="line">        <span class="keyword">return</span> self.out(F.relu(self.hidden(X)))</span><br><span class="line">net = MLP()</span><br><span class="line">net(X)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>In general, we just need to define the structure of our blocks in <code>__init__</code> and compute the output in <code>forward</code>. <code>forward</code> points to <code>__call__</code>, so <code>net(X)</code> is euqal to <code>net.forward(X)</code>.</p>
</blockquote>
<h2 id="Sequential"><a href="#Sequential" class="headerlink" title="Sequential"></a>Sequential</h2><p><code>nn.Sequential</code> is the built-in subclass of <code>nn.Module</code>. Its working principle is very simple, which just simply connects different blocks. We can define it by ourselves:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MySequential</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, *args</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> args:</span><br><span class="line">            self._modules[block] = block</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> self._modules.values():</span><br><span class="line">            X = block(X)</span><br><span class="line">        <span class="keyword">return</span> X</span><br><span class="line"></span><br><span class="line">net = MySequential(nn.Linear(<span class="number">20</span>, <span class="number">256</span>), nn.ReLU(), nn.Linear(<span class="number">256</span>, <span class="number">10</span>))</span><br><span class="line">net(X)</span><br></pre></td></tr></table></figure>
<p><code>MySequential</code> will compute in the order of <code>nn.Linear</code>, <code>nn.ReLU</code> and <code>nn.Linear</code>.</p>
<blockquote>
<p><code>._modules</code> is an <code>OrderedDict</code> defined in <code>nn.Module</code>. It will store our blocks in order. All the arguments of <code>Sequential</code> should be the subclass of <code>nn.Module</code>.</p>
<p>In PyTorch, the activation is also a layer though it doesn&#39;t have any model parameters.</p>
</blockquote>
<h1 id="Parameter-management"><a href="#Parameter-management" class="headerlink" title="Parameter management"></a>Parameter management</h1><p>The type of model parameters in PyTorch is <code>nn.Parameter</code> which is a compound object containing values (<code>Tensor</code>), gradients (<code>grad</code>) and extra information. <code>grad</code> works when we call <code>.backward</code> and <code>requires_grad=True</code>.</p>
<h2 id="Parameter-visiting"><a href="#Parameter-visiting" class="headerlink" title="Parameter visiting"></a>Parameter visiting</h2><p>For blocks that define model parameters, we can use <code>.state_dict()</code> which returns a dictionary to visit the model parameters:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = nn.Sequential(nn.Linear(<span class="number">4</span>, <span class="number">8</span>), nn.ReLU(), nn.Linear(<span class="number">8</span>, <span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].state_dict())</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">OrderedDict([(&#x27;weight&#x27;, tensor([[ 0.3016, -0.1901, -0.1991, -0.1220,  0.1121, -0.1424, -0.3060,  0.3400]])), (&#x27;bias&#x27;, tensor([-0.0291]))])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(net[<span class="number">2</span>].bias))</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].bias)</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].bias.data)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&lt;class &#x27;torch.nn.parameter.Parameter&#x27;&gt;</span></span><br><span class="line"><span class="string">Parameter containing:</span></span><br><span class="line"><span class="string">tensor([-0.0291], requires_grad=True)</span></span><br><span class="line"><span class="string">tensor([-0.0291])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p><code>Sequential</code> allows use to visit each block like a <code>list</code>. If we just <code>print(net)</code>, python will output the structure of <code>net</code>.</p>
</blockquote>
<h2 id="Parameter-initialization"><a href="#Parameter-initialization" class="headerlink" title="Parameter initialization"></a>Parameter initialization</h2><p>We can define our own function to initialize the model parameters:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_normal</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br><span class="line">        nn.init.zeros_(m.bias)</span><br><span class="line">net.apply(init_normal)</span><br></pre></td></tr></table></figure>
<blockquote>
<p><code>m</code> is the subclass of <code>nn.Module</code>. <code>net.apply</code> asks PyTorch to call <code>init_normal</code> for each block. Only the block with model parameters will initialize its parameters. (So far, only the built-in <code>nn.Linear</code> has defined model parameters).</p>
</blockquote>
<p>Since each block is the subclass of <code>nn.Module</code>, we could also initialize each block respectively:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net[<span class="number">0</span>].apply(init_normal)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Direct initialization is also possible, like:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">my_init</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.uniform_(m.weight, -<span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line">        m.weight.data *= m.weight.data.<span class="built_in">abs</span>() &gt;= <span class="number">5</span></span><br></pre></td></tr></table></figure>
<p>We must not initialize all <code>weight</code> using the same value. If we do so, all the neurons in a layer are computing the same thing, that is, they will output the same value and their gradients will also be the same.</p>
</blockquote>
<h2 id="Shared-layer"><a href="#Shared-layer" class="headerlink" title="Shared layer"></a>Shared layer</h2><p>Since instances of custom classes are mutable objects in python, we can make two layers share their parameters by passing the same object to PyTorch:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">shared = nn.Linear(<span class="number">8</span>, <span class="number">8</span>)</span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">4</span>, <span class="number">8</span>), nn.ReLU(),</span><br><span class="line">                    shared, nn.ReLU(),</span><br><span class="line">                    shared, nn.ReLU(),</span><br><span class="line">                    nn.Linear(<span class="number">8</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure>

<h2 id="Block-with-parameters"><a href="#Block-with-parameters" class="headerlink" title="Block with parameters"></a>Block with parameters</h2><p>We could define parameters for our own blocks. Since <code>backward</code> will work for all <code>Tensor</code>, we don&#39;t have to deal with <code>backward</code>. We just need to define our parameters and set <code>requires_grad=True</code>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyLinear</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_units, units</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.weight = nn.Parameter(torch.randn(in_units, units))</span><br><span class="line">        self.bias = nn.Parameter(torch.randn(units,))</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        linear = torch.matmul(X, self.weight.data) + self.bias.data</span><br><span class="line">        <span class="keyword">return</span> F.relu(linear)</span><br></pre></td></tr></table></figure>

<h1 id="Saving-amp-loading-parameters"><a href="#Saving-amp-loading-parameters" class="headerlink" title="Saving &amp; loading parameters"></a>Saving &amp; loading parameters</h1><p>We can save a tensor, a tensor list or a dictionary using <code>torch.save</code> and load them using <code>torch.load</code>:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.arange(<span class="number">4</span>)</span><br><span class="line">y = torch.arange(<span class="number">5</span>)</span><br><span class="line">z = &#123;<span class="string">&#x27;x&#x27;</span>: x, <span class="string">&#x27;y&#x27;</span>: y&#125;</span><br><span class="line">torch.save(x, <span class="string">&#x27;x-file&#x27;</span>)</span><br><span class="line">torch.save([x, y], <span class="string">&#x27;xy-file&#x27;</span>)</span><br><span class="line">torch.save(z, <span class="string">&#x27;z-dict&#x27;</span>)</span><br><span class="line"></span><br><span class="line">x1 = torch.load(<span class="string">&#x27;x-file&#x27;</span>)</span><br><span class="line">xx, yy = torch.load(<span class="string">&#x27;xy-file&#x27;</span>)</span><br><span class="line">zz = torch.load(<span class="string">&#x27;z-dict&#x27;</span>)</span><br><span class="line"></span><br><span class="line">x1 == x, xx == x, yy == y, zz[<span class="string">&#x27;x&#x27;</span>] == x, zz[<span class="string">&#x27;y&#x27;</span>] == y</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">(tensor([True, True, True, True]),</span></span><br><span class="line"><span class="string"> tensor([True, True, True, True]),</span></span><br><span class="line"><span class="string"> tensor([True, True, True, True, True]),</span></span><br><span class="line"><span class="string"> tensor([True, True, True, True]),</span></span><br><span class="line"><span class="string"> tensor([True, True, True, True, True]))</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<p>For a model, PyTorch will save its parameters rather than the whole model:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.hidden = nn.Linear(<span class="number">20</span>, <span class="number">256</span>)</span><br><span class="line">        self.output = nn.Linear(<span class="number">256</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.output(F.relu(self.hidden(x)))</span><br><span class="line"></span><br><span class="line">net = MLP()</span><br><span class="line">torch.save(net.state_dict(), <span class="string">&#x27;mlp.params&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>When we need to load the model, we should rebuild the same structure and load the parameters:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">clone = MLP()</span><br><span class="line">clone.load_state_dict(torch.load(<span class="string">&#x27;mlp.params&#x27;</span>))</span><br></pre></td></tr></table></figure>

<h1 id="Training-in-GPUs"><a href="#Training-in-GPUs" class="headerlink" title="Training in GPUs"></a>Training in GPUs</h1><p>Tensors are created on CPU by default.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">x.device</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">device(type=&#x27;cpu&#x27;)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>To create a tensor in GPU, we must specify the GPU we use:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.ones(<span class="number">2</span>, <span class="number">3</span>, device=torch.device(<span class="string">&#x27;cuda&#x27;</span>))</span><br><span class="line">X.device</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">device(type=&#x27;cuda&#x27;, index=0)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p><code>cuda</code> means GPU in PyTorch. All Nvidia GPUs owned by the computer are organized into an array. <code>cuda</code> or <code>cuda:0</code> represent the first GPU and <code>cuda:1</code> represent the second GPU. We can use <code>!nvidia-smi</code> to get the information of Nvidia GPUs in our computer and <code>torch.cuda.device_count()</code> to get the number of Nvidia GPUs of our computer.</p>
</blockquote>
<p>If we want to operate different tensors together, we must make sure that they are stored in the same GPU, otherwise, PyTorch will throw an exception <strong>because moving data from CPU to GPU or from GPU to another GPU is time-consuming</strong>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Y = torch.rand(<span class="number">2</span>, <span class="number">3</span>, device=torch.device(<span class="string">&#x27;cuda&#x27;</span>))</span><br><span class="line">Z = torch.rand(<span class="number">2</span>, <span class="number">3</span>)  <span class="comment"># create in CPU</span></span><br><span class="line">X = Z.cuda(<span class="number">0</span>)  <span class="comment"># create a new tensor in GPU 0</span></span><br><span class="line"><span class="built_in">print</span>(X)</span><br><span class="line"><span class="built_in">print</span>(Z)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[0.3206, 0.4276, 0.8653],</span></span><br><span class="line"><span class="string">        [0.3276, 0.4867, 0.1320]], device=&#x27;cuda:0&#x27;)</span></span><br><span class="line"><span class="string">tensor([[0.3206, 0.4276, 0.8653],</span></span><br><span class="line"><span class="string">        [0.3276, 0.4867, 0.1320]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># now we can add Y and X</span></span><br><span class="line">X + Y</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[0.8401, 0.6486, 1.8582],</span></span><br><span class="line"><span class="string">        [1.1932, 0.8063, 1.1152]], device=&#x27;cuda:0&#x27;)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># if we add Y and Z</span></span><br><span class="line">Y + Z</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<h2 id="Neural-networks-in-GPU"><a href="#Neural-networks-in-GPU" class="headerlink" title="Neural networks in GPU"></a>Neural networks in GPU</h2><p>Similarly, we can put the parameters of neural networks in GPU:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = nn.Sequential(nn.Linear(<span class="number">3</span>, <span class="number">1</span>))</span><br><span class="line">net = net.to(device=torch.device(<span class="string">&#x27;cuda&#x27;</span>))</span><br><span class="line">net(X)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[-0.1712],</span></span><br><span class="line"><span class="string">        [ 0.1713]], device=&#x27;cuda:0&#x27;, grad_fn=&lt;AddmmBackward0&gt;)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<p>Both parameters and data should be stored in the same device. Inadvertently moving data from one device to another can significantly degrade performance, which is what we need to pay attention to. For example, report data to the user on the command line or log it in a NumPy ndarray, both of which will cause data movement from GPU to CPU.</p>
<blockquote>
<p>To enable training the model in GPUs, we should install CUDA and NVIDIA Driver in <a href="https://developer.nvidia.com/cuda-downloads">CUDA Toolkit</a> and install the corresponding GPU version of pytorch in <a href="https://pytorch.org/get-started/locally/">PyTorch</a>.</p>
</blockquote>
]]></content>
      <categories>
        <category>Tool</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Deep Learning</tag>
        <tag>Tool</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Tensorflow</title>
    <url>/2023/04/12/Tensorflow/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="Basic-data-structure"><a href="#Basic-data-structure" class="headerlink" title="Basic data structure"></a>Basic data structure</h1><p>See <a href="/2023/04/10/NeuralNetwork/#Numpy-and-Tensorflow
">Numpy and Tensorflow</a>.</p>
<h1 id="Neural-networks"><a href="#Neural-networks" class="headerlink" title="Neural networks"></a>Neural networks</h1><h2 id="Normalize-data"><a href="#Normalize-data" class="headerlink" title="Normalize data"></a>Normalize data</h2><p>Fitting the weights (<code>w</code>, <code>b</code>) to the data will proceed more quickly if the data is normalized. In tensorflow, we can use <code>Normalization</code> to normalize data. It uses z-score. However, it is a preprocessing layer rather than an independent layer of model.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Normalization</span><br><span class="line">norm_l = Normalization(axis=-<span class="number">1</span>)</span><br><span class="line">norm_l.adapt(X) <span class="comment"># learns mean, variance</span></span><br><span class="line">Xn = norm_l(X)</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li><code>axis=-1</code> means that it will normalize the last dimension of <code>X</code>. For 2-D arrays, it will normalize the <strong>column</strong>. In this case, <code>axis=-1</code> is equal to <code>axis=1</code>.</li>
<li>After normalizing, the test set should also be normalized: <code>X_testn = norm_l(X_test)</code>.</li>
</ul>
</blockquote>
<h2 id="Create-the-model"><a href="#Create-the-model" class="headerlink" title="Create the model"></a>Create the model</h2><p>For a neural network like this:</p>
<p><img src="/2023/04/12/Tensorflow/1.png" alt="1"></p>
<center style="font-size:12px;font-weight:bold"> Fig. 1. Sample</center><br>

<p>we can create the neural network using tensorflow in Python:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Dense</span><br><span class="line"></span><br><span class="line"><span class="comment"># non-regularization</span></span><br><span class="line">model = Sequential([</span><br><span class="line">    <span class="comment"># We can use the following code to specify the shape of input layer</span></span><br><span class="line">    <span class="comment">## tf.keras.Input(shape=(x,))</span></span><br><span class="line">    Dense(units=<span class="number">25</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>),</span><br><span class="line">    Dense(units=<span class="number">15</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>),</span><br><span class="line">    Dense(units=<span class="number">1</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>)</span><br><span class="line">                   ])</span><br><span class="line"><span class="comment"># regularization</span></span><br><span class="line">model = Sequential([</span><br><span class="line">    Dense(units=<span class="number">25</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>, kernel_regularizer=L2(<span class="number">0.01</span>)),</span><br><span class="line">    Dense(units=<span class="number">15</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>, kernel_regularizer=L2(<span class="number">0.01</span>)),</span><br><span class="line">    Dense(units=<span class="number">1</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>, kernel_regularizer=L2(<span class="number">0.01</span>))</span><br><span class="line">                   ])  <span class="comment"># 0.01 is the value of lambda</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>Keras is an open-source software library that provides a Python interface for neural networks. It is now a package under tensorflow,which acts as an interface.</p>
<ul>
<li><code>units</code> represents the number of neuron in this layerr.</li>
<li><code>activation</code> represents the <code>activation function</code> you choose for this layer.</li>
<li><code>Dense</code> is a kind of hidden layer that make use of all the inputs for each neuron.</li>
<li>We can also assign names to each layer using <code>name=&#39;xx&#39;</code>. For the first layer, using <code>input_dim=xx</code> is another way to specify the shape of input layer.</li>
<li>When specifying the shape of input layer, <code>model</code> will be instantiated. That is, parameters of all layers will be initialized.</li>
<li>Use <code>.summary</code> to get the structure of model. Use <code>.get_layer(&#39;layer_name&#39;)</code> to get the certain layer.</li>
</ul>
</blockquote>
<p>We can also create the neural network layer by layer:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.array([[<span class="number">10</span>, <span class="number">11</span>]])</span><br><span class="line">layer_1 = Dense(units=<span class="number">25</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>)</span><br><span class="line">layer_2 = Dense(units=<span class="number">15</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>)</span><br><span class="line">layer_3 = Dense(units=<span class="number">1</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>)</span><br><span class="line"></span><br><span class="line">a1 = layer_1(x)</span><br><span class="line">a2 = layer_2(a1)</span><br><span class="line">a3 = layer_3(a2)  <span class="comment"># the final output</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>For each layer, we can use <code>.get_weights()</code> to get its <code>w</code> and <code>b</code>. Feed some examples to instantiate them or use <code>.set_weights([w, b])</code> to initialize them.</p>
</blockquote>
<p>The first way is more convenient and universal, as it builds a neural network at once.</p>
<blockquote>
<p>Name of activation functions in keras:</p>
<ul>
<li>relu</li>
<li>sigmoid</li>
<li>linear</li>
<li>softmax</li>
<li>...</li>
</ul>
</blockquote>
<h2 id="Loss-and-cost-functions"><a href="#Loss-and-cost-functions" class="headerlink" title="Loss and cost functions"></a>Loss and cost functions</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> losses</span><br><span class="line"></span><br><span class="line"><span class="comment"># for binary classificaiton</span></span><br><span class="line">model.<span class="built_in">compile</span>(loss=losses.BinaryCrossentropy())</span><br><span class="line"><span class="comment"># for linear regression</span></span><br><span class="line">model.<span class="built_in">compile</span>(loss=losses.MeanSquaredError())</span><br><span class="line"><span class="comment"># for Adam algorithm</span></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=tf.keras.optimizers.Adam(learning_rate=<span class="number">1e-3</span>), loss=YourChoiceOfFunction)</span><br></pre></td></tr></table></figure>
<p>See <a href="/2023/04/10/NeuralNetwork/#Adam-algorithm">Adam algorithm</a> to know more about Adam.</p>
<p>At this step, the structure of the neural network has already been established.</p>
<blockquote>
<p>The name of some loss functions in <code>losses</code>:</p>
<ul>
<li>BinaryCrossentropy: Binary classification</li>
<li>MeanSquaredError: Linear regression</li>
<li>SparseCategoricalCrossentropy: Multiclass classification</li>
</ul>
</blockquote>
<p>If we regard the whole neural network as a generalized model, the model and cost function can be represent as:<br>$$<br>f(W,B)(\vec{x})<br>$$<br>$$<br>J(W,B)&#x3D;\frac{1}{m}\sum\limits_{i&#x3D;1}^{m}L(f(\vec{x}^{(i)},y^{(i)}))<br>$$</p>
<h2 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h2><p>When regularization, we just need to add:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">kernel_regularizer=tf.keras.regularizers.l2(<span class="number">0.1</span>) <span class="comment"># 0.1 is the value of lambda</span></span><br></pre></td></tr></table></figure>
<p>to the layer that we want to regularize.</p>
<h2 id="Gradient-descent"><a href="#Gradient-descent" class="headerlink" title="Gradient descent"></a>Gradient descent</h2><p>This is the final step to train a neural network. In this step, we use gradient descent to determine $W$ and $B$.</p>
<blockquote>
<p>Capital means matrix</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.fit(X, y, epochs=<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<p>The algorithm used in <code>fit</code> is called <strong>Backpropagation</strong>. <code>X,y</code> is our training set and <code>epochs</code> represents the number of iterations.</p>
<p>After finishing the training of neural network, we can use <code>model.predict()</code> or <code>model()</code> to predict or infer.</p>
<blockquote>
<p>The type of <code>model.predict()</code> is <code>tf</code> while the type of <code>model()</code> is <code>np</code>.</p>
</blockquote>
<h2 id="Optimization-for-softmax"><a href="#Optimization-for-softmax" class="headerlink" title="Optimization for softmax"></a>Optimization for softmax</h2><p>In multiclass classfication, the following code will work, but the value is not very accurate. See <a href="/2023/04/10/NeuralNetwork/#Softmax-regression">Softmax regression</a> to know more about multiclass classification and softmax.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Dense</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.losses <span class="keyword">import</span> SparseCategoricalCrossentropy</span><br><span class="line"></span><br><span class="line">model = Sequential([</span><br><span class="line">    Dense(units=<span class="number">25</span>, activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">    Dense(units=<span class="number">15</span>, activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">    Dense(units=<span class="number">10</span>, activation=<span class="string">&#x27;softmax&#x27;</span>)</span><br><span class="line">                   ])</span><br><span class="line">model.<span class="built_in">compile</span>(loss=SparseCategoricalCrossentropy())</span><br><span class="line">model.fit(X, y, epochs=<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<p>Due to the limitations in precision when using binary to represent decimals, the more intermediate values we use, the greater the loss of precision we get. Before we calculate $loss$, we must get the value of $a$. $a$ is a intermediate value, so it is advisable for us to replace $a$ in $loss$ with its original formula. That is:<br>$$a_i&#x3D;g(z)&#x3D;\frac{e^{z_i}}{e^{z_1}+...+e^{z_i}}$$<br>$$loss&#x3D;-\log{a_1}, y&#x3D;1...$$<br>$$\downarrow$$<br>$$loss&#x3D;-\log\frac{e^{z_i}}{e^{z_1}+...+e^{z_i}},y&#x3D;1...$$<br>To realize this, we just need to set the activation function of output layer to <code>linear</code> and set <code>from_logits</code> to <code>True</code>:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = Sequential([</span><br><span class="line">    Dense(units=<span class="number">25</span>, activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">    Dense(units=<span class="number">15</span>, activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">    Dense(units=<span class="number">10</span>, activation=<span class="string">&#x27;linear&#x27;</span>)</span><br><span class="line">                   ])</span><br><span class="line">model.<span class="built_in">compile</span>(loss=SparseCategoricalCrossentropy(from_logits=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure>
<p>The principle of this code is that $g(z)$ is just $z$, and tensorflow will replace $a$ with its original formula in loss function. <code>from_logits</code> actuallly means use $z$ rather than $a$. This gives tensorflow more flexibility to rearrange the terms to get more accurate values.</p>
<blockquote>
<p>In this case, the output of our neural network is actually a vector of $z$. Therefore, we have to add </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">logit = model(X)</span><br><span class="line">f_x = tf.nn.softmax(logit)</span><br></pre></td></tr></table></figure>
<p>to get the real values. However, to get the label, we just need to use <code>np.argmax(y_pred, axis=0)</code>.</p>
</blockquote>
<p>The same optimization can also be applied to other neural networks, like binary classification.</p>
<h1 id="Regression-model"><a href="#Regression-model" class="headerlink" title="Regression model"></a>Regression model</h1><p>For regression models, Auto Diff (Auto Grad) of Tensorflow can help us get partial derivatives automatically.</p>
<h2 id="Gradient-descent-1"><a href="#Gradient-descent-1" class="headerlink" title="Gradient descent"></a>Gradient descent</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="comment"># for J = (wx-1)^2</span></span><br><span class="line"></span><br><span class="line">w = tf.Variable(<span class="number">3.0</span>) <span class="comment"># The parameters we want to optimize, they can also be vector</span></span><br><span class="line">x = <span class="number">1.0</span> <span class="comment"># Features of trainng set</span></span><br><span class="line">y = <span class="number">1.0</span> <span class="comment"># Target value of training set</span></span><br><span class="line">alpha = <span class="number">0.01</span> <span class="comment"># Learning rate</span></span><br><span class="line"></span><br><span class="line">iterations = <span class="number">30</span> <span class="comment"># Number of iteration</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">iter</span> <span class="keyword">in</span> <span class="built_in">range</span>(iterations):</span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape: <span class="comment"># Record the steps</span></span><br><span class="line">        fwb = w * x <span class="comment"># Model</span></span><br><span class="line">        costJ = (fwb - y)**<span class="number">2</span> <span class="comment"># Cost function which should be implemented by ourselves</span></span><br><span class="line">    [dJdw] = tape.gradient(costJ, [w]) <span class="comment"># Calculate derivatives</span></span><br><span class="line"></span><br><span class="line">    w.assign_add(-alpha * dJdw) <span class="comment"># Update w</span></span><br></pre></td></tr></table></figure>

<h2 id="Adam-algorithm"><a href="#Adam-algorithm" class="headerlink" title="Adam algorithm"></a>Adam algorithm</h2><p>The example used here is about collaborative filtering. See <a href="/2023/04/17/RecommenderSystem/#Collaborative-filtering">Collaborative filtering</a> to know more about it.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"></span><br><span class="line"><span class="comment"># Instantiate an adam optimizer</span></span><br><span class="line">W = tf.Variable(tf.random.normal((num_users,  num_features),dtype=tf.float64),  name=<span class="string">&#x27;W&#x27;</span>)</span><br><span class="line">X = tf.Variable(tf.random.normal((num_items, num_features),dtype=tf.float64),  name=<span class="string">&#x27;X&#x27;</span>)</span><br><span class="line">b = tf.Variable(tf.random.normal((<span class="number">1</span>,          num_users),   dtype=tf.float64),  name=<span class="string">&#x27;b&#x27;</span>)</span><br><span class="line"></span><br><span class="line">optimizer = keras.optimizers.Adam(learning_rate=<span class="number">1e-1</span>)</span><br><span class="line"></span><br><span class="line">iterations = <span class="number">200</span></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">iter</span> <span class="keyword">in</span> <span class="built_in">range</span>(iterations):</span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        <span class="comment"># features, w, b, mean normalization, R (which item have a rating), number of users, number of items, lambda</span></span><br><span class="line">        coast_value = cofiCostFuncV(X, W, b, Ynorm, R, num_users, num_items, <span class="keyword">lambda</span>) <span class="comment"># Compute cost which should be implemented by ourselves</span></span><br><span class="line">    grads = tape.gradient(cost_value, [X, W, b]) <span class="comment"># Calculate derivatives</span></span><br><span class="line">    optimizer.apply_gradients(<span class="built_in">zip</span>(grads, [X, W, b])) <span class="comment"># Update X, W, b</span></span><br></pre></td></tr></table></figure>

<h1 id="More-information"><a href="#More-information" class="headerlink" title="More information"></a>More information</h1><ul>
<li><a href="https://www.tensorflow.org/api_docs/python/tf">Tensorflow API</a></li>
<li><a href="https://www.tensorflow.org/guide/function">Better performance with tf.function</a></li>
</ul>
]]></content>
      <categories>
        <category>Tool</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Deep Learning</tag>
        <tag>Tool</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Matplotlib</title>
    <url>/2023/04/24/Matplotlib/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="Installation"><a href="#Installation" class="headerlink" title="Installation"></a>Installation</h1><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pip install matplotlib</span><br><span class="line">or</span><br><span class="line">conda install matplotlib</span><br></pre></td></tr></table></figure>
<p>It is recommended to install <code>matplotlib</code> together with <code>pandas</code> (matplotlib will intall numpy at the same time). Then, start to plot:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>

<h1 id="Loading-data"><a href="#Loading-data" class="headerlink" title="Loading data"></a>Loading data</h1><p>pandas; CSV; date series data; real-time data</p>
<h1 id="Plot-types"><a href="#Plot-types" class="headerlink" title="Plot types"></a>Plot types</h1><h2 id="Line-plot"><a href="#Line-plot" class="headerlink" title="Line plot"></a>Line plot</h2><p>To plot a line plot, the API used is <code>plt.plot</code>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ages_x = [<span class="number">18</span>, <span class="number">19</span>, <span class="number">20</span>, <span class="number">21</span>, <span class="number">22</span>]</span><br><span class="line"></span><br><span class="line">py_dev_y = [<span class="number">20046</span>, <span class="number">20060</span>, <span class="number">21000</span>, <span class="number">24744</span>, <span class="number">30500</span>]</span><br><span class="line">plt.plot(ages_x, py_dev_y, label=<span class="string">&#x27;Python&#x27;</span>)</span><br><span class="line"></span><br><span class="line">js_dev_y = [<span class="number">16446</span>, <span class="number">16791</span>, <span class="number">18942</span>, <span class="number">21780</span>, <span class="number">25704</span>]</span><br><span class="line">plt.plot(ages_x, js_dev_y, label=<span class="string">&#x27;JavaScript&#x27;</span>)</span><br><span class="line"></span><br><span class="line">dev_y = [<span class="number">17784</span>, <span class="number">16500</span>, <span class="number">18012</span>, <span class="number">20628</span>, <span class="number">25206</span>]</span><br><span class="line">plt.plot(ages_x, dev_y, color=<span class="string">&#x27;#444444&#x27;</span>, linestyle=<span class="string">&#x27;--&#x27;</span>, label=<span class="string">&#x27;All Devs&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.xlabel(<span class="string">&#x27;Ages&#x27;</span>) <span class="comment"># set lable of x-axis</span></span><br><span class="line">plt.ylabel(<span class="string">&#x27;Median Salary (USD)&#x27;</span>) <span class="comment"># set label of y-axis</span></span><br><span class="line">plt.title(<span class="string">&#x27;Median Salary (USD) by Age&#x27;</span>) <span class="comment"># set name of plot</span></span><br><span class="line"></span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line"></span><br><span class="line">plt.savefig(<span class="string">&#x27;plot.png&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<img align="right" src="/2023/04/24/Matplotlib/1.png" style=" width:380px; padding: 0px 0px; ">

<p>When running the code above, we get a line plot as shown. All line plots using the same object <code>plt.plot</code> will be drawn together. Once we wanna show the current plot, we should use <code>plt.show()</code>. The only two necessary arguments are <code>x</code> and <code>y</code>, that is, <code>ages_x</code> and <code>py_dev_y</code> (<code>js_dev_y</code>, <code>dev_y</code>).</p>
<p><code>plot.legend()</code> asks <code>pyplot</code> to show the label of each line. There are two ways to add label for each curve, the way above and:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.legend([<span class="string">&#x27;Python&#x27;</span>, <span class="string">&#x27;JavaScript&#x27;</span>, <span class="string">&#x27;All devs&#x27;</span>])</span><br></pre></td></tr></table></figure>
<p>In general, the first way is better as we don&#39;t need to arrange the labels in the order of things being added to plot. However, no matter which one we choose, we must use <code>plt.legend()</code>, otherwise, the label will not be showed.</p>
<p>In addition to <code>label</code> argument, there are many arguments that let us format each plot. For examples, <code>color</code>, <code>linestyle</code> and etc.</p>
<blockquote>
<ul>
<li>For <code>color</code>, we can use the initial letter of color, like <code>&#39;r&#39;</code> or <strong>hex color values</strong>, like what we use above.</li>
<li>We can also use one parameter to set <code>color</code>, <code>linestyle</code> and <code>marker</code> together, like <code>&#39;b--&#39;</code>, that is <code>color=&#39;b&#39;</code> and <code>linestyle=&#39;--&#39;</code>.</li>
</ul>
</blockquote>
<p><code>plt.tight_layout()</code> will automatically adjust the layout of chart to reduce the overlap of chart&#39;s x and y labels. It is useful when a chart is consisted of several subcharts. <code>plt.savefig(&#39;plot.png&#39;)</code> will save the plot as an image named <code>plot.png</code> in the specified folder (current folder by default).</p>
<blockquote>
<p>Absolute path is required if you want to save the plot to a specified folder.</p>
</blockquote>
<p>Other interesting setting:</p>
<p><img align="left" src="/2023/04/24/Matplotlib/2.png" style=" width:380px; padding: 0px 0px; "> <img align="mid" src="/2023/04/24/Matplotlib/3.png" style=" width:380px; padding: 0px 0px; "></p>
<p>For the first one, use <code>plt.grid(True)</code>. For the second one, use <code>plt.xkcd()</code>. By the way, <code>plt.xkcd()</code> should be placed before all the <code>plt.plot()</code> so that it can change the style of all line plots. The font family used in <code>xkcd()</code> is <code>&#39;xkcd&#39;</code>, <code>&#39;xkcd Script&#39;</code>, <code>&#39;Humor Sans&#39;</code>, <code>&#39;Comic Neue&#39;</code> and <code>&#39;Comic Sans Ms&#39;</code>. Windows has installed <code>Comic Sans Ms</code>. Therefore, pyplot will plot using <code>Comic Sans Ms</code> if other fonts can&#39;t be found. The font used in the second picture is <code>Humor Sans</code>. After installing a new font, delete the cache file <code>.matplotlib</code>.</p>
<blockquote>
<p>Use <code>print(matplotlib.get_cachedir())</code> to get the location of <code>.matplotlib</code>.</p>
</blockquote>
<p>Apart from <code>skcd()</code>, there are many different styles in <code>plt.style</code>.</p>
<blockquote>
<ul>
<li>Use <code>print(plt.style.available)</code> to check the styles that can be used.</li>
<li>Use <code>plt.style.use(&#39;Yourchoice&#39;)</code> to enable a style (<code>seaborn-paper</code>, <code>fivethirtyeight</code> and <code>ggplot</code> are recommended).</li>
</ul>
</blockquote>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html">pyplot.plot</a></li>
<li><a href="https://github.com/shreyankg/xkcd-desktop">Humor Sans font</a></li>
</ul>
<h2 id="Fill-between"><a href="#Fill-between" class="headerlink" title="Fill_between"></a>Fill_between</h2><p>To show the difference between two line plots or one line plot and a benchmark, <code>plt.fill_between</code> is rather useful. It can fill the area between two horizontal curves. It is actually a special type of &quot;curve&quot; so it can be formatted like other <code>plt</code> function:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">plt.style.use(<span class="string">&quot;seaborn-v0_8-paper&quot;</span>)</span><br><span class="line"></span><br><span class="line">ages_x = [<span class="number">18</span>, <span class="number">19</span>, <span class="number">20</span>, <span class="number">21</span>, <span class="number">22</span>]</span><br><span class="line"></span><br><span class="line">py_dev_y = np.array([<span class="number">20046</span>, <span class="number">20060</span>, <span class="number">21000</span>, <span class="number">24744</span>, <span class="number">30500</span>])</span><br><span class="line">plt.plot(ages_x, py_dev_y, color=<span class="string">&#x27;#fc4f30&#x27;</span>, label=<span class="string">&#x27;Python&#x27;</span>)</span><br><span class="line"></span><br><span class="line">dev_y = np.array([<span class="number">17784</span>, <span class="number">21000</span>, <span class="number">22000</span>, <span class="number">22628</span>, <span class="number">25206</span>])</span><br><span class="line">plt.plot(ages_x, dev_y, color=<span class="string">&#x27;#008fd5&#x27;</span>, linestyle=<span class="string">&#x27;--&#x27;</span>, label=<span class="string">&#x27;All Devs&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># fill_between</span></span><br><span class="line">plt.fill_between(ages_x, py_dev_y, dev_y, alpha=<span class="number">0.25</span>,</span><br><span class="line">                 where=(py_dev_y &gt; dev_y), interpolate=<span class="literal">True</span>,</span><br><span class="line">                 label=<span class="string">&#x27;Above average&#x27;</span>, color=<span class="string">&#x27;#6d904f&#x27;</span>)</span><br><span class="line">plt.fill_between(ages_x, py_dev_y, dev_y, alpha=<span class="number">0.25</span>,</span><br><span class="line">                 where=(py_dev_y &lt;= dev_y), interpolate=<span class="literal">True</span>,</span><br><span class="line">                 label=<span class="string">&#x27;Below average&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.xlabel(<span class="string">&#x27;Ages&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Median Salary (USD)&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Median Salary (USD) by Age&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.legend()</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<img align="left" src="/2023/04/24/Matplotlib/4.png" style=" width:380px; padding: 0px 0px; ">
<img align="mid" src="/2023/04/24/Matplotlib/5.png" style=" width:380px; padding: 0px 0px; ">

<p>The first picture shows the result we get. </p>
<ul>
<li>The only two necessary arguments of <code>plt.fill_between</code> is <code>x</code> and <code>y1</code>, that is <code>ages_x</code> and <code>py_dev_y</code>, which form the curve <code>fill_between</code> will stuff;</li>
<li><code>y2</code> is 0 by default so <code>fill_between</code> will fill the space between 0 and y1-x. <code>y2</code> could be a scalar (e.g. 0) or a vector. In the first chart, <code>y2</code> is a vector;</li>
<li><code>alpha</code> (0~1) is the transparency of the patch in the coverage area. <code>1</code> means opaque;</li>
<li><code>where</code> is an important argument. It defines the scope or condition of filling. To make <code>where</code> work, lists must can be compared, that&#39;s why we use the vector of numpy;</li>
<li><code>interpolate</code> only works when <code>where</code> exists. When <code>interpolate</code> is <code>False</code>, the filling area near the intersection of <code>y1</code> and <code>y2</code> will be removed, like the second picture. It is recommended to set <code>interpolate</code> to <code>True</code> when using <code>where</code>.</li>
</ul>
<p>The following is a chart drawn by only using:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.fill_between(ages_x, py_dev_y, dev_y, alpha=<span class="number">0.25</span>,</span><br><span class="line">                 label=<span class="string">&#x27;Difference&#x27;</span>, color=<span class="string">&#x27;#6d904f&#x27;</span>)</span><br></pre></td></tr></table></figure>
<img src="/2023/04/24/Matplotlib/5-1.png" style=" width:380px; padding: 0px 0px; ">

<h3 id="Reference-1"><a href="#Reference-1" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.fill_between.html">pyplot.fill_between</a></li>
</ul>
<h2 id="Bar-chart"><a href="#Bar-chart" class="headerlink" title="Bar chart"></a>Bar chart</h2><p>To draw a bar chart, what we need to do is to use <code>plt.bar</code> instead of <code>plt.plot</code>. It is legal to use <code>plt.bar</code> and <code>plt.plot</code> together as they can coexist. If we draw <code>JavaScript</code> and <code>All devs</code> using <code>plt.bar</code> while draw <code>Python</code> using <code>plt.plot</code>, we get the chart below:  </p>
<p><img src="/2023/04/24/Matplotlib/6.png" alt="6"></p>
<p>It seems that <code>JavaScript</code> and <code>All devs</code> have mixed together. To solve this, we should use <code>numpy</code>. This time, we draw three bar charts:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">plt.style.use(<span class="string">&quot;seaborn-v0_8-paper&quot;</span>)</span><br><span class="line"></span><br><span class="line">ages_x = [<span class="number">18</span>, <span class="number">19</span>, <span class="number">20</span>, <span class="number">21</span>, <span class="number">22</span>]</span><br><span class="line"></span><br><span class="line">x_idnexex = np.arange(<span class="built_in">len</span>(ages_x)) <span class="comment"># x_indexex = [0, 1, 2, 3, 4]</span></span><br><span class="line">width = <span class="number">0.25</span></span><br><span class="line"></span><br><span class="line">py_dev_y = [<span class="number">20046</span>, <span class="number">20060</span>, <span class="number">21000</span>, <span class="number">24744</span>, <span class="number">30500</span>]</span><br><span class="line">plt.bar(x_idnexex - width, py_dev_y, width=width, color=<span class="string">&#x27;#fc4f30&#x27;</span>, label=<span class="string">&#x27;Python&#x27;</span>)</span><br><span class="line"></span><br><span class="line">js_dev_y = [<span class="number">16446</span>, <span class="number">16791</span>, <span class="number">18942</span>, <span class="number">21780</span>, <span class="number">25704</span>]</span><br><span class="line">plt.bar(x_idnexex, js_dev_y, width=width, color=<span class="string">&#x27;#008fd5&#x27;</span>, label=<span class="string">&#x27;JavaScript&#x27;</span>)</span><br><span class="line"></span><br><span class="line">dev_y = [<span class="number">17784</span>, <span class="number">16500</span>, <span class="number">18012</span>, <span class="number">20628</span>, <span class="number">25206</span>]</span><br><span class="line">plt.bar(x_idnexex + width, dev_y, width=width, color=<span class="string">&#x27;#6d904f&#x27;</span>, label=<span class="string">&#x27;All Devs&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.xlabel(<span class="string">&#x27;Ages&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Median Salary (USD)&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Median Salary (USD) by Age&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.legend()</span><br><span class="line">plt.xticks(ticks=x_idnexex, labels=ages_x)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<img align="left" src="/2023/04/24/Matplotlib/7.png" style=" width:380px; padding: 0px 0px; ">

<p>Then, we get the chart as shown. The reason why we use <code>numpy</code> is that numpy supports vector operations and it could be the parameter of <code>plt.xticks()</code>. <code>x_indexes</code> is actually a list. By adding or subtracting <code>width</code>, we put each bar of the certain bar chart forward or backward. Then, use argument <code>width=width</code> to control the width of each bar. For beauty, the value of <code>width</code> should be:</p>
<p>$$width&#x3D;\frac{Distance\space between\space two\space scales}{Number\space of\space chart+1}$$</p>
<p><code>plt.xticks()</code> is a function that modifies the scale and label of axis-x. <code>ticks</code> is the scales that will be shown on axis-x. For the chart we draw, it should be <code>[0, 1, 2, 3, 4]</code>. <code>lables</code> is the display values of the scales, which overrides the true values of scales. If you don&#39;t wanna show axis-x, you can use <code>plt.xticks([])</code>.</p>
<img align="right" src="/2023/04/24/Matplotlib/8.png" style=" width:380px; padding: 0px 0px; ">

<p>We can also draw horizontal bar chart using <code>plot.barh()</code>, like the chart as shown. However, the first parameter of <code>plot.barh()</code> is now vertical axis, that is <strong>axis-y</strong>, and the second is <strong>axis-x</strong>. Therefore, we should use <code>plot.yticks()</code> to modify the scale. Besides, the width of bar is now the <strong>height</strong> of bar, so the argument is <code>height=xx</code>. </p>
<h3 id="Reference-2"><a href="#Reference-2" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.bar.html">pyplot.bar</a></li>
<li><a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.barh.html">pyplot.barh</a></li>
<li><a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.xticks.html">pyplot.xticks</a></li>
</ul>
<h2 id="Pie-chart"><a href="#Pie-chart" class="headerlink" title="Pie chart"></a>Pie chart</h2><p>If the number of data is less than 5 and we want to display their proportion, the pie chart is a good choice. To draw a pie chart, the function we use is <code>plt.pie</code>:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.style.use(<span class="string">&quot;seaborn-v0_8-paper&quot;</span>)</span><br><span class="line"></span><br><span class="line">slices = [<span class="number">59219</span>, <span class="number">55466</span>, <span class="number">47544</span>, <span class="number">36443</span>, <span class="number">35917</span>]</span><br><span class="line">labels = [<span class="string">&#x27;JavaScript&#x27;</span>, <span class="string">&#x27;HTML/CSS&#x27;</span>, <span class="string">&#x27;SQL&#x27;</span>, <span class="string">&#x27;Python&#x27;</span>, <span class="string">&#x27;Java&#x27;</span>]</span><br><span class="line">colors = [<span class="string">&#x27;#008fd5&#x27;</span>, <span class="string">&#x27;#fc4f30&#x27;</span>, <span class="string">&#x27;#e5ae37&#x27;</span>, <span class="string">&#x27;#6d904f&#x27;</span>, <span class="string">&#x27;#999999&#x27;</span>]</span><br><span class="line">explode = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0.1</span>, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">plt.pie(slices, labels=labels, explode=explode, shadow=<span class="literal">True</span>,</span><br><span class="line">        startangle=<span class="number">90</span>, autopct=<span class="string">&#x27;%1.1f%%&#x27;</span>, colors=colors,</span><br><span class="line">        wedgeprops=&#123;<span class="string">&#x27;edgecolor&#x27;</span>: <span class="string">&#x27;black&#x27;</span>&#125;)</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">&quot;My Awesome Pie Chart&quot;</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<img align="right" src="/2023/04/24/Matplotlib/9.png" style=" width:380px; padding: 0px 0px; ">

<p>The picture as shown is the pie chart we get. For <code>plt.pie</code>, the only necessary argument is <code>slices</code>, that is the data. Others are formatting arguments:</p>
<ul>
<li><code>explode</code>: List, which specifies the fraction of the radius with which to offset each wedge;</li>
<li><code>shadow</code>: Draw a shadow beneath pie;</li>
<li><code>startangle</code>: The angle by which the start of the pie is rotated (counterclockwise);</li>
<li><code>autopct</code>: A string or function used to label each wedge. If string, it should start with <code>%</code>;</li>
<li><code>wedgepros</code>: A dictionary, which modifies the features of wedge;</li>
<li>...</li>
</ul>
<h3 id="Reference-3"><a href="#Reference-3" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.pie.html">pyplot.pie</a></li>
</ul>
<h2 id="Stack-plot"><a href="#Stack-plot" class="headerlink" title="Stack plot"></a>Stack plot</h2><p>Stack plot (area plot), which shows the change of items over something (e.g. time) as well as the quantitative relationship between them, is a very useful kind of chart, especially when the sum of items is constant. To draw a stack plot, what we use is <code>plt.stackplot</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.style.use(<span class="string">&quot;seaborn-v0_8-paper&quot;</span>)</span><br><span class="line"></span><br><span class="line">minutes = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]</span><br><span class="line"></span><br><span class="line">player1 = [<span class="number">8</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line">player2 = [<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>]</span><br><span class="line">player3 = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>]</span><br><span class="line"></span><br><span class="line">labels = [<span class="string">&#x27;player1&#x27;</span>, <span class="string">&#x27;player2&#x27;</span>, <span class="string">&#x27;player3&#x27;</span>]</span><br><span class="line">colors = [<span class="string">&#x27;#6d904f&#x27;</span>, <span class="string">&#x27;#fc4f30&#x27;</span>, <span class="string">&#x27;#008fd5&#x27;</span>]</span><br><span class="line"></span><br><span class="line">plt.stackplot(minutes, player1, player2, player3, labels=labels, colors=colors)</span><br><span class="line"></span><br><span class="line">plt.legend(loc=(<span class="number">0.07</span>, <span class="number">0.05</span>))</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">&quot;My Awesome Stack Plot&quot;</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<img align="right" src="/2023/04/24/Matplotlib/10.png" style=" width:400px; padding: 0px 0px; ">

<p>The picture shows the stack plot we get. Compared to other charts, it is quite simple and does&#39;t have many things to format. One thing worth noticing is <code>plt.legend(loc(0.07, 0.05))</code>. It means putting the tab to the position that 7% from left and 5% from bottom.</p>
<h3 id="Reference-4"><a href="#Reference-4" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.stackplot.html">pyplot.stackplot</a></li>
<li><a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.legend.html">pyplot.legend</a></li>
</ul>
<h2 id="Histogram"><a href="#Histogram" class="headerlink" title="Histogram"></a>Histogram</h2><p>A histogram is a chart that plots the distribution of a numeric variable&#39;s values as a series of bins. Each bin covers a range of numeric values. It is clearer than the bar chart when the range of x is too large.  To draw a histogram, what we use is <code>plt.hist</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.style.use(<span class="string">&#x27;seaborn-v0_8-paper&#x27;</span>)</span><br><span class="line"></span><br><span class="line">ages = [<span class="number">18</span>, <span class="number">19</span>, <span class="number">21</span>, <span class="number">25</span>, <span class="number">26</span>, <span class="number">26</span>, <span class="number">30</span>, <span class="number">32</span>, <span class="number">38</span>, <span class="number">45</span>, <span class="number">55</span>, <span class="number">68</span>, <span class="number">15</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">31</span>, <span class="number">44</span>, <span class="number">56</span>]</span><br><span class="line">bins = [<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">40</span>, <span class="number">50</span>, <span class="number">60</span>, <span class="number">70</span>]</span><br><span class="line"></span><br><span class="line">plt.hist(ages, bins=bins, edgecolor=<span class="string">&#x27;black&#x27;</span>)</span><br><span class="line"></span><br><span class="line">median_age = <span class="number">31</span> <span class="comment"># Just a random number</span></span><br><span class="line">color = <span class="string">&#x27;#fc4f30&#x27;</span></span><br><span class="line"></span><br><span class="line">plt.axvline(median_age, color=color, label=<span class="string">&#x27;Age Median&#x27;</span>, linewidth=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">&#x27;Ages of Respondents&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Ages&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Total Respondents&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img align="left" src="/2023/04/24/Matplotlib/11.png" style=" width:380px; padding: 0px 0px; "> <img align="mid" src="/2023/04/24/Matplotlib/12.png" style=" width:380px; padding: 0px 0px; "></p>
<p>The only necessary argument of <code>plt.hist</code> is <code>x</code>, that is <code>ages</code>. <code>bins</code> is an important argument, which defines the range of each bin. It could be a scalar (the second picture) or a list (the first picture). If it is a scalar, its value represents the number of bins. The range of each bin is decided by <code>plt.hist</code> itself. Others are just some formatting arguments. <code>plt.axvline</code> which draws a vertical line, is often used with histogram.</p>
<h3 id="Reference-5"><a href="#Reference-5" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.hist.html">pyplot.hist</a></li>
<li><a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.axvline.html">pyplot.axvline</a></li>
</ul>
<h2 id="Scatter-plot"><a href="#Scatter-plot" class="headerlink" title="Scatter plot"></a>Scatter plot</h2><p>The scatter plot can show the correlation of two parameters intuitively. However, in addition to the quantitative relationship, <code>pyplot</code> can show more information. To draw a scatter plot, what we use is <code>plt.scatter</code>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.style.use(<span class="string">&#x27;seaborn-v0_8-paper&#x27;</span>)</span><br><span class="line"></span><br><span class="line">x = [<span class="number">5</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">9</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">6</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">6</span>, <span class="number">4</span>, <span class="number">1</span>]</span><br><span class="line">y = [<span class="number">7</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">9</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">8</span>, <span class="number">7</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">7</span>, <span class="number">7</span>, <span class="number">5</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">colors = [<span class="number">10</span>, <span class="number">15</span>, <span class="number">1</span>, <span class="number">7</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">3</span>, <span class="number">7</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">8</span>, <span class="number">1</span>, <span class="number">9</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">5</span>]</span><br><span class="line"></span><br><span class="line">sizes = [<span class="number">209</span>, <span class="number">486</span>, <span class="number">381</span>, <span class="number">255</span>, <span class="number">191</span>, <span class="number">315</span>, <span class="number">185</span>, <span class="number">228</span>, <span class="number">174</span>,</span><br><span class="line">         <span class="number">538</span>, <span class="number">239</span>, <span class="number">394</span>, <span class="number">399</span>, <span class="number">153</span>, <span class="number">273</span>, <span class="number">293</span>, <span class="number">436</span>, <span class="number">501</span>, <span class="number">397</span>, <span class="number">539</span>]</span><br><span class="line"></span><br><span class="line">plt.scatter(x, y, c=colors, s=sizes, cmap=<span class="string">&#x27;twilight_shifted&#x27;</span>,</span><br><span class="line">            edgecolor=<span class="string">&#x27;black&#x27;</span>, linewidth=<span class="number">1</span>, alpha=<span class="number">0.75</span>)</span><br><span class="line"></span><br><span class="line">cbar = plt.colorbar()</span><br><span class="line">cbar.set_label(<span class="string">&#x27;Color range&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># plt.xscale(&#x27;log&#x27;)</span></span><br><span class="line"><span class="comment"># plt.yscale(&#x27;log&#x27;)</span></span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<img align="right" src="/2023/04/24/Matplotlib/13.png" style=" width:380px; padding: 0px 0px; ">

<p><code>x</code> and <code>y</code> are the only two arguments we need. Other formatting arguments are almost the same as other plots&#39;:</p>
<ul>
<li><code>edgecolor</code>: The cloer of marker edges;</li>
<li><code>linewidth</code>: The linewidth of marker edges;</li>
<li><code>alpha</code>: Transparency of color;</li>
<li>...</li>
</ul>
<p>However, the color (<code>c</code>) and size (<code>s</code>) of the scatter plot are slightly different from others&#39;. Compared to other plots, <code>c</code> and <code>s</code> in the scatter plot can be used to show extra information. The value of <code>c</code> or <code>s</code> doesn&#39;t count, but their quantitative relationship counts. <strong>That is, the size of <code>c</code> and <code>s</code> list is equal to the number of scatters and their relative quantitative relationship reflects the shades of color and the size of dots</strong>. For <code>c</code>, a kind of colormap (e.g. <code>&#39;twilight_shifted&#39;</code>) is needed to determine the gradient colors.</p>
<p><code>plt.colorbar</code> returns a bar that shows the gradation range of colors and the values represented by different colors. If the range of <code>x</code> or <code>y</code> label is too large, we can use <code>xscale</code> or <code>yscale</code> to scale the label. We can also use <code>marker</code> to change the form of scatter.</p>
<h3 id="Reference-6"><a href="#Reference-6" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.scatter.html">pyplot.scatter</a></li>
<li><a href="https://matplotlib.org/stable/tutorials/colors/colormaps.html">colormaps</a></li>
<li><a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.colorbar.html">pyplot.colorebar</a></li>
<li><a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.xscale.html">pyplot.xscale</a></li>
</ul>
<h1 id="Data-types"><a href="#Data-types" class="headerlink" title="Data types"></a>Data types</h1><h2 id="Time-series-data"><a href="#Time-series-data" class="headerlink" title="Time series data"></a>Time series data</h2><p><code>pyplot</code> also allows us to draw plots with time series data. To do this:</p>
<ol>
<li>Store data in the form of <code>datetime</code> in python;</li>
<li>Plot using <code>plt.plot_date</code>;</li>
<li>Format the form of displaying <code>datetime</code> data (optional).</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> dates <span class="keyword">as</span> mpl_dates</span><br><span class="line"></span><br><span class="line">plt.style.use(<span class="string">&#x27;seaborn-v0_8-paper&#x27;</span>)</span><br><span class="line"></span><br><span class="line">dates = [</span><br><span class="line">    datetime(<span class="number">2019</span>, <span class="number">5</span>, <span class="number">24</span>),</span><br><span class="line">    datetime(<span class="number">2019</span>, <span class="number">5</span>, <span class="number">25</span>),</span><br><span class="line">    datetime(<span class="number">2019</span>, <span class="number">5</span>, <span class="number">26</span>),</span><br><span class="line">    datetime(<span class="number">2019</span>, <span class="number">5</span>, <span class="number">27</span>),</span><br><span class="line">    datetime(<span class="number">2019</span>, <span class="number">5</span>, <span class="number">28</span>),</span><br><span class="line">    datetime(<span class="number">2019</span>, <span class="number">5</span>, <span class="number">29</span>),</span><br><span class="line">    datetime(<span class="number">2019</span>, <span class="number">5</span>, <span class="number">30</span>),</span><br><span class="line">    datetime(<span class="number">2019</span>, <span class="number">5</span>, <span class="number">23</span>)</span><br><span class="line">]</span><br><span class="line">y = [<span class="number">10</span>, <span class="number">13</span>, <span class="number">23</span>,<span class="number">34</span>, <span class="number">46</span>, <span class="number">55</span>, <span class="number">67</span>, <span class="number">5</span>]</span><br><span class="line"></span><br><span class="line">plt.plot_date(dates, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># .gcf(): get current figure</span></span><br><span class="line">plt.gcf().autofmt_xdate()</span><br><span class="line">date_format = mpl_dates.DateFormatter(<span class="string">&#x27;%a %d %b %Y&#x27;</span>) <span class="comment"># weekday, day, month in en, year</span></span><br><span class="line"><span class="comment"># .gca(): get current axes</span></span><br><span class="line">plt.gca().xaxis.set_major_formatter(date_format)</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img align="left" src="/2023/04/24/Matplotlib/14.png" style=" width:380px; padding: 0px 0px; "> <img align="mid" src="/2023/04/24/Matplotlib/15.png" style=" width:380px; padding: 0px 0px; "></p>
<p><code>plt.gcf()</code> and <code>plt.gca()</code> are notable. <code>plt.gcf()</code> returns the current figure. <code>autofmt_xdate()</code> will make xlabel more readable (the first picture uses <code>autofmt_xdate()</code> while the second doesn&#39;t). What&#39;s more, we can also change the form of displaying date by passing a <code>DateFormatter</code> object to <code>set_major.formatter</code>. This object should conform to the python <strong>datetime format</strong>.</p>
<h3 id="Reference-7"><a href="#Reference-7" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot_date.html">pyplot.plot_date</a></li>
<li><a href="https://matplotlib.org/stable/api/dates_api.html">matplotlib.dates</a></li>
<li><a href="https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior">datetime format</a></li>
</ul>
<h2 id="Real-time-data"><a href="#Real-time-data" class="headerlink" title="Real-time data"></a>Real-time data</h2><p><code>animation</code> module in <code>matplotlib</code> is used to draw plot with real-time data. In the following code, <code>random</code> and <code>count</code> are just for producing random data to simulate real-time data.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> count</span><br><span class="line"><span class="keyword">from</span> matplotlib.animation <span class="keyword">import</span> FuncAnimation</span><br><span class="line"></span><br><span class="line">plt.style.use(<span class="string">&#x27;seaborn-v0_8-paper&#x27;</span>)</span><br><span class="line"></span><br><span class="line">x_vals = []</span><br><span class="line">y_vals = []</span><br><span class="line">z_vals = []</span><br><span class="line"></span><br><span class="line">index = count()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">animate</span>(<span class="params">i</span>):</span><br><span class="line">    x_vals.append(<span class="built_in">next</span>(index))</span><br><span class="line">    y_vals.append(random.randint(<span class="number">0</span>, <span class="number">5</span>))</span><br><span class="line">    z_vals.append(random.randint(<span class="number">0</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">    plt.cla()</span><br><span class="line">    plt.plot(x_vals, y_vals, color=<span class="string">&#x27;#fc4f30&#x27;</span>, label=<span class="string">&#x27;Curve1&#x27;</span>)</span><br><span class="line">    plt.plot(x_vals, z_vals, color=<span class="string">&#x27;#008fd5&#x27;</span>, label=<span class="string">&#x27;Curve2&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    plt.legend(loc=<span class="string">&#x27;upper left&#x27;</span>)</span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">ani = FuncAnimation(plt.gcf(), animate, interval=<span class="number">1000</span>, cache_frame_data=<span class="literal">False</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img align="left" src="/2023/04/24/Matplotlib/16.png" style=" width:380px; padding: 0px 0px; "> <img align="mid" src="/2023/04/24/Matplotlib/17.png" style=" width:380px; padding: 0px 0px; "></p>
<p>These two figures were drawn at 16 and 26 seconds respectively. In <code>FuncAnimation</code>, only <code>plt.gcf()</code> which is the figure we want to plot and <code>animate</code>  which is the plotting function, are needed. <code>interval</code> (in milliseconds) defines <code>FuncAnimation</code>&#39;s time interval for calling <code>animate</code>. <code>animate</code> is the most important part. <strong>It is actually a function of drawing a kind of plot</strong>. Because the data it uses changes whenever it is called, the figure changes accordingly. Pyplot realizes this by drawing a totally new figure that overwrites the previous figures. Therefore, we should use <code>plt.cla()</code> to delete the previous figure, otherwise the color of figure will change continuously.</p>
<blockquote>
<p>Some bugs may occur if you use pycharm as your IDE:</p>
<ul>
<li>Can&#39;t display the figure: To solve this, untick <code>Show plots in tool window</code>:<br><img src="/2023/04/24/Matplotlib/18.png" alt="18"></li>
<li>User warning: frames&#x3D;None: To solve this, add <code>cache_frame_data=False</code>.</li>
</ul>
</blockquote>
<h3 id="Reference-8"><a href="#Reference-8" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.animation.FuncAnimation.html">FuncAnimation</a></li>
</ul>
<h1 id="Subplot"><a href="#Subplot" class="headerlink" title="Subplot"></a>Subplot</h1><p>In general, a figure draw by Matplotlib contains the following components:</p>
<p><img src="/2023/04/24/Matplotlib/19.png" alt="19"></p>
<p>where a figure is a canvas, a axes is a subgraph object. A figure can contain several axes. In the above, we only use one figure with one axes. To draw more complicated figures, we can use <code>plt.subplots</code>. <code>plt.subplots</code> returns a figure object and several axes objects. With axes objects, we can plot several subgraphs in one figure. With figure objects, we can produce multiple figures:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.style.use(<span class="string">&quot;seaborn-v0_8-notebook&quot;</span>)</span><br><span class="line"></span><br><span class="line">ages_x = [<span class="number">18</span>, <span class="number">19</span>, <span class="number">20</span>, <span class="number">21</span>, <span class="number">22</span>]</span><br><span class="line">py_dev_y = [<span class="number">20046</span>, <span class="number">20060</span>, <span class="number">21000</span>, <span class="number">24744</span>, <span class="number">30500</span>]</span><br><span class="line">dev_y = [<span class="number">17784</span>, <span class="number">16500</span>, <span class="number">18012</span>, <span class="number">20628</span>, <span class="number">25206</span>]</span><br><span class="line"></span><br><span class="line">fig1, (ax1, ax2) = plt.subplots(nrows=<span class="number">2</span>, ncols=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># fig2, (ax3, ax4) = plt.subplots(nrows=2, ncols=1) ## another figure</span></span><br><span class="line"></span><br><span class="line">ax1.plot(ages_x, dev_y, color=<span class="string">&#x27;purple&#x27;</span>,</span><br><span class="line">         linestyle=<span class="string">&#x27;--&#x27;</span>, label=<span class="string">&#x27;All Devs&#x27;</span>)</span><br><span class="line">ax1.legend()</span><br><span class="line">ax1.set_title(<span class="string">&#x27;Median Salary (USD) by Age&#x27;</span>)</span><br><span class="line">ax1.set_ylabel(<span class="string">&#x27;Median Salary (USD)&#x27;</span>)</span><br><span class="line"></span><br><span class="line">ax2.plot(ages_x, py_dev_y, color=<span class="string">&#x27;#6A5ACD&#x27;</span>, label=<span class="string">&#x27;Python&#x27;</span>)</span><br><span class="line">ax2.legend()</span><br><span class="line">ax2.set_xlabel(<span class="string">&#x27;Ages&#x27;</span>)</span><br><span class="line">ax2.set_ylabel(<span class="string">&#x27;Median Salary (USD)&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br><span class="line">fig1.savefig(<span class="string">&#x27;fig1.png&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img align="left" src="/2023/04/24/Matplotlib/20.png" style=" width:380px; padding: 0px 0px; "> <img align="mid" src="/2023/04/24/Matplotlib/21.png" style=" width:380px; padding: 0px 0px; "></p>
<p>We can plot using <code>ax1</code> and <code>ax2</code> as <code>plt</code>. However, <code>plt.title</code>, <code>plt.xlabel</code> and <code>plt.ylabel</code> should be replaced with <code>ax.set_title</code>, <code>ax.set_xlabel</code> and <code>ax.set_ylabel</code>. If we want to modify figure, like saving a figure, we should use the figure object we get which is the same type as <code>plt.gcf()</code>. </p>
<p>The arguments of <code>plt.subplots</code> define the layout of subgraphs. If we set <code>nrows=2, ncols=1</code>, we get the first figure. If we set <code>nrows=1, ncols=2</code>, we geet the second figure.</p>
<blockquote>
<p>The second return value of <code>plt.subplots</code> is actually a <strong>matrix</strong>. Therefore, we can simply use:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fig1, ax = plt.subplots(nrows=<span class="number">2</span>, ncols=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>and visit ax1 and ax2 using <code>ax[0]</code> ans <code>ax[1]</code>. A more complicated example:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fig1, ax = plt.subplots(nrows=<span class="number">2</span>, ncols=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># is equal to</span></span><br><span class="line">fig1, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=<span class="number">2</span>, ncols=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
</blockquote>
<h2 id="Reference-9"><a href="#Reference-9" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html">pyplot.subplots</a></li>
</ul>
<h1 id="More-information"><a href="#More-information" class="headerlink" title="More information"></a>More information</h1><ul>
<li><a href="https://youtu.be/UO98lJQ3QGI?si=AIbHWXO0NH7m5G_x">Matplotlib Tutorials by Corey Schafer</a></li>
<li><a href="https://matplotlib.org/stable/plot_types/index.html">plot types</a></li>
<li><a href="https://matplotlib.org/stable/gallery/index.html">plot examples</a></li>
<li><a href="https://matplotlib.org/stable/api/pyplot_summary.html">pyplot api</a></li>
</ul>
]]></content>
      <categories>
        <category>Tool</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Tool</tag>
        <tag>Data Analysis</tag>
      </tags>
  </entry>
  <entry>
    <title>Scikit-learn</title>
    <url>/2023/04/22/Scikit-learn/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="Installation"><a href="#Installation" class="headerlink" title="Installation"></a>Installation</h1><p>We should install <code>scikit-learn</code> though the package we used is called <code>sklearn</code>.</p>
<h1 id="Linear-regression"><a href="#Linear-regression" class="headerlink" title="Linear regression"></a>Linear regression</h1><p>Scikit-learn has a gradient descent linear regression model <code>SGDRegressor</code> that performs well with normalized inputs. <code>StandardScaler</code> will perform z-score normalization as we learnt.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># import module</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="comment"># Scale training set</span></span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">X_norm = scaler.fit_transform(X_train)</span><br><span class="line"><span class="comment"># Create and fit model</span></span><br><span class="line">sgdr = SGDRegressor(max_iter=<span class="number">1000</span>)</span><br><span class="line">sgdr.fit(X_norm, y_train)</span><br><span class="line"><span class="comment"># View parameters</span></span><br><span class="line">b_norm = sgdr.intercept_</span><br><span class="line">w_norm = sgdr.coef_</span><br><span class="line"><span class="comment"># Make predictions</span></span><br><span class="line">y_pred = sgdr.predict(X_norm)</span><br></pre></td></tr></table></figure>

<p>In addition to linear regression using gradient descent, scikit-learn also implements another linear regression model using normal equation, that is <code>LinearRegression</code>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># import module</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="comment"># Create and fit model</span></span><br><span class="line">linear_model = LinearRegression()</span><br><span class="line"><span class="comment">## X must be a 2-D matrix</span></span><br><span class="line">linear_model.fit(X_train.reshape(-<span class="number">1</span>, <span class="number">1</span>), y_train)</span><br><span class="line"><span class="comment"># View parameters</span></span><br><span class="line">b = linear_model.intercept_</span><br><span class="line">w = linear_model.coef_</span><br><span class="line"><span class="comment"># Make predictions</span></span><br><span class="line">y_pred = linear_model.predict(X_train.reshape(-<span class="number">1</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure>

<h1 id="Logistic-regression"><a href="#Logistic-regression" class="headerlink" title="Logistic regression"></a>Logistic regression</h1><p>The logistic regression model in scikit-learn is <code>LogisticRegression</code>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># import module</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="comment"># Create and fit the model</span></span><br><span class="line">lr_model = LogisticRegression()</span><br><span class="line">lr_model.fit(X, y)</span><br><span class="line"><span class="comment"># Make predictions</span></span><br><span class="line">y_pred = lr_model.predict(X)</span><br><span class="line"><span class="comment"># Calculate accuracy</span></span><br><span class="line">lr_model.score(X, y) <span class="comment"># Return the percentage of correct predictions</span></span><br></pre></td></tr></table></figure>

<h1 id="Datasets-and-dataset-partition"><a href="#Datasets-and-dataset-partition" class="headerlink" title="Datasets and dataset partition"></a>Datasets and dataset partition</h1><p>The <code>sklearn.datasets</code> module includes utilities to load datasets. These datasets are useful for the training of model. See more information on <a href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets">sklearn.datasets</a>.</p>
<p>Module <code>train_test_split</code> in <code>sklearn.model_selection</code> can help us split training set into training set and test set, for examples:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_train, X_, y_train, y_ = train_test_split(X, y, test_size=<span class="number">0.4</span>, random_state=<span class="number">1</span>)</span><br><span class="line">X_cv, X_test, y_cv, y_test = train_test_split(X_, y_, test_size=<span class="number">0.5</span>, random_state=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<h1 id="More-information"><a href="#More-information" class="headerlink" title="More information"></a>More information</h1><ul>
<li><a href="https://scikit-learn.org/stable/">Scikit-learn.org</a></li>
<li><a href="https://scikit-learn.org/stable/modules/classes.html#">Scikit-learn API</a></li>
</ul>
]]></content>
      <categories>
        <category>Tool</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Tool</tag>
        <tag>Unsupervised Learning</tag>
        <tag>Machine Learning</tag>
        <tag>Supervised Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>NumPy</title>
    <url>/2023/04/22/NumPy/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="Basic-data-structure"><a href="#Basic-data-structure" class="headerlink" title="Basic data structure"></a>Basic data structure</h1><p><code>list</code> in python can support different data type so its elements are actually pointers, which waste a lot of memori and CPU time. The basic objects of NumPy are <code>ndarray</code> and <code>ufnc</code>. <code>ndarray</code> store data (bool, int, float and etc.). <code>ufunc</code> contains function to cope with <code>ndarray</code>. <code>ndarray</code>, an indexable, n-dimensional array containing elements of the same type (<code>dtype</code>), where dimension is the number of indices that we need to visit a scalar of the array, is the basic data struture of NumPy .Vectors are 1-D arrays and matrices are 2-D arrays.</p>
<blockquote>
<p>Use <code>.shape</code> and <code>.dtype</code> to get the dimension and element type of an array.</p>
<p>Instead of <code>float64</code>, we often use <code>float32</code> to accelerate computing&#x2F;</p>
</blockquote>
<h2 id="Vectors"><a href="#Vectors" class="headerlink" title="Vectors"></a>Vectors</h2><p>Vectors are 1-D arrays in NumPy. To create a vector, we can use:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a vector with 4 elements whose values are 0 and types are float64</span></span><br><span class="line">a = np.zeros(<span class="number">4</span>)</span><br><span class="line"><span class="comment"># The same as above. We use this mode to create n-D arrays</span></span><br><span class="line">a = np.zeros((<span class="number">4</span>,)) <span class="comment"># np.ones creates ndarray whose values are 1</span></span><br><span class="line"><span class="comment"># Create a vector with 4 elements whose values are random value in [0, 1) and types are float64</span></span><br><span class="line">a = np.random.random_sample((<span class="number">4</span>,))</span><br><span class="line"><span class="comment"># np.arange([start=0], stop, [step=1]). Create an arithmetic progression, [start, stop)</span></span><br><span class="line">a = np.arange(<span class="number">4.</span>)</span><br><span class="line"><span class="comment"># Create a vector with 4 elements whose values are in [0, 1) obeying uniform distribution</span></span><br><span class="line">a = np.random.rand((<span class="number">4</span>,))</span><br><span class="line"><span class="comment"># Specify values manually</span></span><br><span class="line">a = np.array([<span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>])</span><br><span class="line">a = np.array([<span class="number">5.0</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<h2 id="Matrices"><a href="#Matrices" class="headerlink" title="Matrices"></a>Matrices</h2><p>Matrices are 2-D arrays in NumPy. To create a matrix, the functions used are as those in creating vectors. For examples:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = np.zeros((<span class="number">4</span>, <span class="number">2</span>))</span><br></pre></td></tr></table></figure>
<p>However, when specifying values, numpy specifies rows first:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = np.array([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">              [<span class="number">3</span>, <span class="number">4</span>]])</span><br></pre></td></tr></table></figure>
<p>We can also create a matrix from a vector:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = np.zeros(<span class="number">6</span>).reshape(-<span class="number">1</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>which will create a 3x2 matrix. <code>-1</code> indicates that the number of row depends on the number of column. If <code>.reshape(-1, )</code>, we turn a matrix to a vector by concentrating the vector row by row, that is:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = np.array([[<span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">              [<span class="number">4</span>, <span class="number">5</span>]])</span><br><span class="line"><span class="built_in">print</span>(a.reshape(-<span class="number">1</span>, ))</span><br><span class="line"><span class="comment"># We get: [2 3 4 5]</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p><code>.reshape</code> regard a matrix as a vector with $mn$ elements. Therefore, if we want to transpose a matrix, we must use <code>a.T</code> rather than <code>.reshape</code>. <strong>The return object of <code>.reshape</code> shares memory with initial object but <code>.reshape</code> doesn&#39;t change the shape of initial object</strong>.</p>
</blockquote>
<h1 id="Operations"><a href="#Operations" class="headerlink" title="Operations"></a>Operations</h1><h2 id="Indexing-amp-slicing"><a href="#Indexing-amp-slicing" class="headerlink" title="Indexing &amp; slicing"></a>Indexing &amp; slicing</h2><p>Arrays in NumPy can be used as <code>list</code> in python, which means that the indexing and slicing in arrays are the same as those in <code>list</code>, though the data type is a built-in type of numpy (<code>float64</code>, <code>int32</code>, <code>ndarray</code>, etc.).</p>
<blockquote>
<p>When slicing a certain column, we should use <code>a[:, j]</code></p>
</blockquote>
<p>In general, there are 5 different ways to read <code>ndarray</code> using <code>[]</code>:</p>
<ol>
<li>Integer<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = np.arange(<span class="number">12</span>).reshape(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"> <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"> a = [[0 1 2 3]</span></span><br><span class="line"><span class="string">       [4 5 6 7]</span></span><br><span class="line"><span class="string">       [8 9 10 11]]</span></span><br><span class="line"><span class="string"> &#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(a[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># Get [0 1 2 3], shape(4,)</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(a[<span class="number">0</span>][<span class="number">0</span>])</span><br><span class="line"><span class="comment"># Get 0, shape(), scalar</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>The return object shares memory with initial object.</p>
</blockquote>
</li>
<li>Slicing<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(a[:, <span class="number">1</span>])</span><br><span class="line"><span class="comment"># Get [1 5 9], the 1 column, shape(3,)</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(a[<span class="number">0</span>:<span class="number">2</span>, :])</span><br><span class="line"><span class="comment"># Get [[1 2] [5 6]], shape(2, 2)</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(a[::<span class="number">2</span>, ::<span class="number">2</span>]) <span class="comment"># that is set steps to 2</span></span><br><span class="line"><span class="comment"># Get [[0 2] [8 10]], shape(2, 2)</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>The return object still shares memory with initial object.</p>
</blockquote>
</li>
<li>Integer list<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(a[[<span class="number">0</span>, <span class="number">1</span>]])</span><br><span class="line"><span class="comment"># Get [[0 1 2 3] [4 5 6 7]], shape(2, 3)</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>The return object still shares memory with initial object.</p>
</blockquote>
</li>
<li>Integer matrix<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = np.arange(<span class="number">12</span>)</span><br><span class="line">b = np.array[[<span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>, <span class="number">9</span>]]</span><br><span class="line"><span class="built_in">print</span>(a[b])</span><br><span class="line"><span class="comment"># Get [[1 2 4] [5 6 7]], shape(2, 3)</span></span><br><span class="line"></span><br><span class="line">c = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">d = np.array([[<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>]])</span><br><span class="line"><span class="built_in">print</span>(c[d])</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[[[3 4]</span></span><br><span class="line"><span class="string">  [1 2]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"> [[1 2]</span></span><br><span class="line"><span class="string">  [3 4]]]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># c[[1, 0]] is (2, 2) so c[d] is (2, 2, 2)</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>The return object is a new object. It <strong>doesn&#39;t share</strong> memory with initial object. The value of <code>b</code> is the index of element in specific dimension in <code>a</code>.</p>
<p>To understand the second one, you should focus on <code>d</code> rather than <code>c</code>. That is, the values of <code>d</code> are the indexex of <code>c</code>&#39;s first dimension (row). And NumPy just replaces them with value of the specified dimension.</p>
</blockquote>
</li>
<li>Bool array<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">b = np.array([[<span class="literal">True</span>, <span class="literal">False</span>], [<span class="literal">False</span>, <span class="literal">True</span>]])</span><br><span class="line"><span class="built_in">print</span>(a[b])</span><br><span class="line"><span class="comment"># Get [1 4], shape(2, )</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>The return object is not a new object. It <strong>shares</strong> memory with initial object. When using bool array, Numpy will only keep the <code>True</code> element and return a vector.</p>
</blockquote>
</li>
</ol>
<h2 id="Elementwise-computations"><a href="#Elementwise-computations" class="headerlink" title="Elementwise computations"></a>Elementwise computations</h2><p>In elementwise computations, NumPy apply the same operation to each elment of matrix. For example:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">b = np.array([[<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>]])</span><br><span class="line"></span><br><span class="line">a +-*/ b = [[<span class="number">1</span>+-*/<span class="number">5</span>, <span class="number">2</span>+-*/<span class="number">6</span>], [<span class="number">3</span>+-*/<span class="number">7</span>, <span class="number">4</span>+-*/<span class="number">8</span>]]</span><br><span class="line">a**b = [[<span class="number">1</span>**<span class="number">5</span>, <span class="number">2</span>**<span class="number">6</span>], [<span class="number">3</span>**<span class="number">7</span>, <span class="number">4</span>**<span class="number">8</span>]] <span class="comment"># equal to np.power(a, b)</span></span><br><span class="line"><span class="comment"># exp^a</span></span><br><span class="line">e = np.exp(a)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<blockquote>
<p>Because of broadcasting, <code>b = 5 * a</code> <code>b = a + 1</code> or <code>b = a**2</code> are also valid. Such guidelines also apply to boolen operations, that is:</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = np.array([[<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">              [<span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">              [<span class="number">0</span>, <span class="number">1</span>]])</span><br><span class="line">pos = a == <span class="number">1</span> <span class="comment"># a == 1 will be apply to each element of a and return matrix</span></span><br><span class="line"><span class="built_in">print</span>(pos)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">We get:</span><br><span class="line">[[False  True]</span><br><span class="line"> [ True  True]</span><br><span class="line"> [False  True]]</span><br></pre></td></tr></table></figure>

<h2 id="Broadcasting"><a href="#Broadcasting" class="headerlink" title="Broadcasting"></a>Broadcasting</h2><p>If <code>a</code> is a matrix, <code>b</code> is a vector with the same row or column number. Then <code>a+-*/b</code> will <code>+-*/</code> <code>b</code> to each row or column of <code>a</code>. This is the <strong>Broadcasting</strong> in numpy. See more about it on <a href="https://numpy.org/doc/stable/user/basics.broadcasting.html">Broadcasting</a>.</p>
<p>More generally, the mechanism of broadcasting is aligning the shape of each dimension of arrays to the largest one of both arrays. Broadcasting only works when two arrays have different dimensions or two arrays have the same dimensions but at least one dimension is <code>1</code>. For examples:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = np.arange(<span class="number">6</span>).reshape(-<span class="number">1</span>, <span class="number">1</span>) <span class="comment"># shape (6, 1)</span></span><br><span class="line">b = np.arange(<span class="number">5</span>) <span class="comment"># shape(5, )</span></span><br><span class="line">c = a + b <span class="comment"># shape(6, 5)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">a extends to (6, 5)</span></span><br><span class="line"><span class="string">b extends to (6, 5)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>The procedures of broadcasting:</p>
<ol>
<li>Compare the shape of each dimension of two arrays from the last dimension; [e.g. a(6, 1), b(5, ), compare 1 with 5]</li>
<li>Broadcasting the dimension: Broadcast the dimension of the array with smaller dimension from back to front; [e.g. a(6, 1), b(5, ), broadcast b(5, ) to b(1, 5)]</li>
<li>Broadcasting the shape of dimension: The array with shape 1 in one dimension will be stretched to match the corresponding dimension shape of another array. [e.g. a(6, 1)-&gt;a(6, 5); b(1, 5)-&gt;b(6, 5)]</li>
<li>Report error when one dimension can&#39;t be broadcast. Namely, the two arrays have different shapes in this dimension but neither of them have a shape <code>1</code>.</li>
</ol>
<h2 id="Dot-product"><a href="#Dot-product" class="headerlink" title="Dot product"></a>Dot product</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">c = np.dot(a, b)</span><br></pre></td></tr></table></figure>

<h2 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Add all the elements up, return a scalar</span></span><br><span class="line">b = np.<span class="built_in">sum</span>(a) <span class="comment"># use [axis] to determine row or column</span></span><br><span class="line"><span class="comment"># Avearge of a, return a scalar</span></span><br><span class="line">c = np.mean(a) <span class="comment"># use [axis] to determini row or column</span></span><br><span class="line"><span class="comment"># Concentrate vectors to form a matrix. Each item is a column.</span></span><br><span class="line">d = np.c_[a, a**<span class="number">2</span>] <span class="comment"># if a.shape=4, d.shape=(4,2)</span></span><br><span class="line"><span class="comment"># Returns the index of the maximum value of an array along a certain axis (0: column, 1: row).</span></span><br><span class="line">f = np.argmax(a, [axis], [out]) <span class="comment"># type: ndarray</span></span><br></pre></td></tr></table></figure>

<h2 id="Tile"><a href="#Tile" class="headerlink" title="Tile"></a>Tile</h2><p><code>numpy.tile(A, reps)</code>, where <code>A</code> is the input array and <code>reps</code> is the replication factor of <code>A</code> in each dimension, extends the dimension or shape of the original array.</p>
<h3 id="A-dim-gt-len-reps"><a href="#A-dim-gt-len-reps" class="headerlink" title="A.dim &gt; len(reps)"></a>A.dim &gt; len(reps)</h3><p>For examples:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = np.array([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]])</span><br><span class="line">b = np.tile(a, (<span class="number">2</span>))</span><br></pre></td></tr></table></figure>
<p>NumPy will extend the shape of <code>b</code> to <code>(1, 2)</code> from back to front. Therefore, it is equal to <code>np.tile(a, (1, 2))</code>:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">b = </span><br><span class="line">[[0 1 2 0 1 2]</span><br><span class="line"> [3 4 5 3 4 5]]</span><br></pre></td></tr></table></figure>

<h3 id="A-dim-lt-len-reps"><a href="#A-dim-lt-len-reps" class="headerlink" title="A.dim &lt; len(reps)"></a>A.dim &lt; len(reps)</h3><p>For examples:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = np.array([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]]) <span class="comment"># shape (2, 3)</span></span><br><span class="line">b = np.tile(a, (<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))</span><br></pre></td></tr></table></figure>
<p>NumPy will extend the shape of <code>a</code> to <code>(1, 2, 3)</code> from back to front. Therefore, the column of <code>a</code> will copy 3 times, the row of <code>a</code> will copy 2 times and the first dimension will copy 1 time.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">b =</span><br><span class="line">[[[0 1 2 0 1 2 0 1 2 0 1 2]</span><br><span class="line">  [3 4 5 3 4 5 3 4 5 3 4 5]</span><br><span class="line">  [0 1 2 0 1 2 0 1 2 0 1 2]</span><br><span class="line">  [3 4 5 3 4 5 3 4 5 3 4 5]</span><br><span class="line">  [0 1 2 0 1 2 0 1 2 0 1 2]</span><br><span class="line">  [3 4 5 3 4 5 3 4 5 3 4 5]]</span><br><span class="line"></span><br><span class="line"> [[0 1 2 0 1 2 0 1 2 0 1 2]</span><br><span class="line">  [3 4 5 3 4 5 3 4 5 3 4 5]</span><br><span class="line">  [0 1 2 0 1 2 0 1 2 0 1 2]</span><br><span class="line">  [3 4 5 3 4 5 3 4 5 3 4 5]</span><br><span class="line">  [0 1 2 0 1 2 0 1 2 0 1 2]</span><br><span class="line">  [3 4 5 3 4 5 3 4 5 3 4 5]]]</span><br></pre></td></tr></table></figure>

<blockquote>
<p>All vectorizable operations use SIMD, so they are much faster than <code>loop</code>.</p>
</blockquote>
<h2 id="Random-choice"><a href="#Random-choice" class="headerlink" title="Random.choice"></a>Random.choice</h2><p><code>numpy.random.choice(a, size=None, replace=True, p=None)</code>, which is an random sampling operation and will return an array whose elements are the result of random sampling.</p>
<ul>
<li><code>a</code>, an array or integer. If <code>a</code> is an array, the samples it chooses come from it, otherwise, the samples it chooses come from <code>np.arrange(a)</code>;</li>
<li><code>size</code>, an integer or a tuple. If <code>size</code> is an integer, it will choose <code>size</code> samples in total. If <code>size</code> is an tuple (e.g. <code>(m, n, k)</code>), it will produce <code>m x n x k</code> samples and arrange them in the shape of <code>(m, n, k)</code>;</li>
<li><code>replace</code>, <code>True</code> or <code>False</code>, where <code>True</code> means sampling with replacement (放回取样) and <code>False</code> means sampling without replacement (不放回取样);</li>
<li><code>p</code>, <code>None</code> or an array. If <code>None</code>, the probability of selecting each number is the same; if it is an array, the length of the array <code>p</code> should be the same as the length of <code>a</code>, and the elements in the array <code>p</code> correspond to the probability of choosing each element in <code>a</code>.</li>
</ul>
<h1 id="More-information"><a href="#More-information" class="headerlink" title="More information"></a>More information</h1><ul>
<li><a href="https://numpy.org/doc/stable/">NumPy.org</a></li>
<li><a href="https://numpy.org/doc/stable/user/basics.broadcasting.html">NumPy Broadcasting</a></li>
<li><a href="https://numpy.org/doc/stable/genindex.html">NumPy function</a></li>
</ul>
]]></content>
      <categories>
        <category>Tool</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Tool</tag>
        <tag>Data Analysis</tag>
      </tags>
  </entry>
  <entry>
    <title>Python String Formatting</title>
    <url>/2023/07/10/StringFormatting/</url>
    <content><![CDATA[<span id="more"></span>


<h1 id="格式化"><a href="#格式化" class="headerlink" title="%格式化"></a>%格式化</h1><p>运算符<code>%</code>可用于格式化字符串：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">&#x27;%s world %d&#x27;</span> % (<span class="string">&#x27;Hello&#x27;</span>, <span class="number">12</span>)</span><br><span class="line"><span class="string">&#x27;Hello world 12&#x27;</span></span><br></pre></td></tr></table></figure>

<p>其中前半段<code>&#39;&#39;</code>内的是原字符串，<code>%s</code>和<code>%d</code>是占位符，最后<code>()</code>内的是占位符的实际内容（若是变量则将输出其值，即若令<code>a=&#39;Hello&#39;</code>，并将<code>Hello</code>替换为<code>a</code>，则上面代码的输出结果不变）。占位符的用法与C语言的一致，如<code>%d</code>表<code>int</code>，<code>%f</code>表<code>float</code>，<code>%s</code>表<code>string</code>，<code>%5s</code>表右对齐占5位（不够则用空格补齐），<code>%.2s</code>表取2位（超出则截断，对于<code>%.f</code>是指保留2位小数），<code>%-s</code>表字符串左对齐，等等，此处不再赘述。</p>
<h1 id="format格式化"><a href="#format格式化" class="headerlink" title="format格式化"></a>format格式化</h1><p><code>format</code>是<code>string</code>对象的成员函数，功能是格式化原字符串，并返回格式化后的字符串：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">&#x27;&#123;&#125; &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(<span class="string">&#x27;hello&#x27;</span>,<span class="string">&#x27;world&#x27;</span>)  <span class="comment"># 不带字段</span></span><br><span class="line"><span class="string">&#x27;hello world&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">&#x27;&#123;0&#125; &#123;1&#125;&#x27;</span>.<span class="built_in">format</span>(<span class="string">&#x27;hello&#x27;</span>,<span class="string">&#x27;world&#x27;</span>)  <span class="comment"># 带位置编号</span></span><br><span class="line"><span class="string">&#x27;hello world&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">&#x27;&#123;0&#125; &#123;1&#125; &#123;0&#125;&#x27;</span>.<span class="built_in">format</span>(<span class="string">&#x27;hello&#x27;</span>,<span class="string">&#x27;world&#x27;</span>)  <span class="comment"># 打乱顺序</span></span><br><span class="line"><span class="string">&#x27;hello world hello&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">&#x27;&#123;a&#125; &#123;b&#125; &#123;a&#125;&#x27;</span>.<span class="built_in">format</span>(b=<span class="string">&#x27;hello&#x27;</span>,a=<span class="string">&#x27;world&#x27;</span>)  <span class="comment"># 带关键字</span></span><br><span class="line"><span class="string">&#x27;world hello world&#x27;</span></span><br></pre></td></tr></table></figure>

<p>原字符串中的<code>&#123;&#125;</code>起着占位的作用，占位的方式有三种：</p>
<ol>
<li>不带任何字段的占位，即第一种。这种情况下，传给<code>format</code>的参数将顺序替换掉占位的<code>&#123;&#125;</code>；</li>
<li>带位置编号的占位，即第二、三种。这种情况下，位置编号代表传给<code>format</code>的参数的先后次序（如<code>&#39;hello&#39;</code>为0），传入参数将按编号替换掉占位符；</li>
<li>带关键字的占位，即第四种。这种最直观，直接用变量名来占位。</li>
</ol>
<p><code>format</code>格式化的占位符也可以结合<code>%</code>格式化的占位符，不过要用<code>:</code>区分开，如：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">&#x27;&#123;a&#125; &#123;b&#125; &#123;a:.2s&#125;&#x27;</span>.<span class="built_in">format</span>(b=<span class="string">&#x27;hello&#x27;</span>,a=<span class="string">&#x27;world&#x27;</span>)</span><br><span class="line"><span class="string">&#x27;world hello wo&#x27;</span></span><br></pre></td></tr></table></figure>

<p><code>&#123;a:.2s&#125;</code>表示按字符串输出但是只保留两位。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://www.cnblogs.com/fat39/p/7159881.html">python基础_格式化输出（%用法和format用法）</a></li>
</ul>
]]></content>
      <categories>
        <category>Programming Language</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Copy and Deepcopy in Python</title>
    <url>/2023/07/07/CopyAndDeepcopy/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="浅拷贝（Copy）"><a href="#浅拷贝（Copy）" class="headerlink" title="浅拷贝（Copy）"></a>浅拷贝（Copy）</h1><p>无论浅拷贝还是深拷贝，在使用前都要先导入库<code>copy</code>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br></pre></td></tr></table></figure>

<h2 id="对不可变类型的浅拷贝"><a href="#对不可变类型的浅拷贝" class="headerlink" title="对不可变类型的浅拷贝"></a>对不可变类型的浅拷贝</h2><p>Python中的不可变数据类型一般指字符串string、数值number和布尔值bool。想要修改引用了不可变数据类型的变量，我们只能让其引用新的对象：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = <span class="number">3</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(a))</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">1944168497456</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">a += <span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(a))</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">1944168497520</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<p>当浅拷贝了引用不可变数据类型的对象时，浅拷贝相当于赋值操作，即两者会引用同一个不可变对象：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = <span class="number">5</span></span><br><span class="line">b = copy.copy(a)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(a), <span class="built_in">id</span>(b))</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">1944168497456 1944168497456</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<h2 id="对可变类型的浅拷贝"><a href="#对可变类型的浅拷贝" class="headerlink" title="对可变类型的浅拷贝"></a>对可变类型的浅拷贝</h2><p>Python中的可变类型指列表list、字典dictionary、集合set以及自定义的类。当变量引用可变类型时，该变量相当于拥有了一个<strong>袋子</strong>，这个袋子里面装的东西可以随便修改，但是只要变量不引用新的袋子，无论怎么修改原来的袋子，原来的袋子始终是原来的袋子。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line"><span class="built_in">print</span>(a, <span class="built_in">id</span>(a))</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">[1, 2, 3] 1944176728512</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">a[<span class="number">0</span>] = <span class="number">2</span></span><br><span class="line"><span class="built_in">print</span>(a, <span class="built_in">id</span>(a))</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">[2, 2, 3] 1944176728512</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<p>当浅拷贝了引用可变数据类型的对象时，Python会直接创造一个新的“袋子”给新对象，但是袋子里的内容与原来的一样：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">b = copy.copy(a)</span><br><span class="line"><span class="built_in">print</span>(a, <span class="built_in">id</span>(a), <span class="string">&#x27;\n&#x27;</span>, b, <span class="built_in">id</span>(b))</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">[1, 2, 3] 1944176728640</span></span><br><span class="line"><span class="string">[1, 2, 3] 1944176728512</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">a[<span class="number">0</span>] = <span class="number">2</span></span><br><span class="line"><span class="built_in">print</span>(a, <span class="string">&#x27;\n&#x27;</span>, b)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">[2, 2, 3]</span></span><br><span class="line"><span class="string">[1, 2, 3]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<p>这是对于“袋子”里装的是不可变数据类型的情况，若袋子里装的是可变数据类型，如嵌套列表：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = [[<span class="number">1</span>, <span class="number">2</span>], <span class="number">1</span>]</span><br><span class="line">b = copy.copy(a)</span><br><span class="line">a[<span class="number">0</span>][<span class="number">0</span>] = <span class="number">2</span></span><br><span class="line"><span class="built_in">print</span>(a, <span class="string">&#x27;\n&#x27;</span>, b)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">[[2, 2], 1]</span></span><br><span class="line"><span class="string">[[2, 2], 1]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<p>即，里面的袋子还是同一个。这就是浅拷贝（Copy），它只新创造最外层的“袋子”，但是里面的内容完全不动：引用不可变类型的变量仍引用同一个不可变对象，引用可变类型的变量仍引用同一个“袋子”。</p>
<h1 id="深拷贝（Deepcopy）"><a href="#深拷贝（Deepcopy）" class="headerlink" title="深拷贝（Deepcopy）"></a>深拷贝（Deepcopy）</h1><p>对于不可变类型的深浅拷贝，其效果相同，此处不再赘述。</p>
<h2 id="可变类型的深拷贝"><a href="#可变类型的深拷贝" class="headerlink" title="可变类型的深拷贝"></a>可变类型的深拷贝</h2><p>当可变类型“袋子”中只有不可变类型的数据时，深拷贝与浅拷贝的效果相同。但是当可变类型“袋子”中也有可变类型时，深拷贝会对“袋子”里面的所有可变类型变量递归地使用深拷贝。也就是说，深拷贝后的两个变量<code>a</code>和<code>b</code>尽管会引用相同的不可变类型对象，但是它们是完全不同的两个变量，修改<code>a</code>完全不会影响<code>b</code>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = [[<span class="number">1</span>, <span class="number">2</span>], <span class="number">1</span>]</span><br><span class="line">b = copy.deepcopy(a)</span><br><span class="line">a[<span class="number">0</span>][<span class="number">0</span>] = <span class="number">2</span></span><br><span class="line"><span class="built_in">print</span>(a, <span class="string">&#x27;\n&#x27;</span>, b)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">[[2, 2], 1]</span></span><br><span class="line"><span class="string">[[1, 2], 1]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<h1 id="元组的深浅拷贝"><a href="#元组的深浅拷贝" class="headerlink" title="元组的深浅拷贝"></a>元组的深浅拷贝</h1><p>元组本身是不可变的数据类型，因此，它的深浅拷贝应该是一样的。但是，当元组内含有可变类型的对象时，对元组的深拷贝会递归地对该可变类型的对象进行深拷贝，进而导致整个元组被放在新开辟的空间：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tup = (<span class="number">1</span>, [<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">tup1 = copy.copy(tup)</span><br><span class="line">tup2 = copy.deepcopy(tup)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(tup), <span class="string">&#x27;\n&#x27;</span>, <span class="built_in">id</span>(tup1), <span class="string">&#x27;\n&#x27;</span>, <span class="built_in">id</span>(tup2))</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">1944176723264</span></span><br><span class="line"><span class="string">1944176723264</span></span><br><span class="line"><span class="string">1944176723328</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzU0NjgzMDIxMQ==&mid=2247569389&idx=4&sn=7f93a257c597821bb55da3e2637a3f3d">【Python基础】Python的深浅拷贝讲解</a></li>
</ul>
]]></content>
      <categories>
        <category>Programming Language</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Zip: Make Aggregation Easier</title>
    <url>/2023/05/06/ZipMakeLifeEasier/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="zip"><a href="#zip" class="headerlink" title="zip"></a>zip</h1><p>It is easy to aggregates elements from iterables using <code>zip</code>:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">zip(*iterables)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">number = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">letters = [<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">list</span>(<span class="built_in">zip</span>(number, letters)))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">list</span>(<span class="built_in">zip</span>(<span class="built_in">range</span>(<span class="number">5</span>), <span class="built_in">range</span>(<span class="number">100</span>))))  <span class="comment"># the number of zip is equal to the shortest iterable</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[(1, &#x27;a&#x27;), (2, &#x27;b&#x27;), (3, &#x27;c&#x27;)]</span></span><br><span class="line"><span class="string">[(0, 0), (1, 1), (2, 2), (3, 3), (4, 4)]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>We can even use zip to generate a dictionary:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fields = [<span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;job&#x27;</span>]</span><br><span class="line">items = [<span class="string">&#x27;John&#x27;</span>, <span class="string">&#x27;worker&#x27;</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">dict</span>(<span class="built_in">zip</span>(fields, items)))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#123;&#x27;name&#x27;: &#x27;John&#x27;, &#x27;job&#x27;: &#x27;worker&#x27;&#125;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>If the iterables are ordered (e.g. <code>list</code>, <code>dict</code>), <code>zip</code> will output the tuples in order, otherwise the order is indeterminate.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">number = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>&#125;</span><br><span class="line">letters = &#123;<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>&#125;</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">list</span>(<span class="built_in">zip</span>(number, letters)))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[(1, &#x27;b&#x27;), (2, &#x27;c&#x27;), (3, &#x27;a&#x27;)]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<h1 id="unzip"><a href="#unzip" class="headerlink" title="unzip"></a>unzip</h1><p><code>zip</code> can also work as <code>unzip</code>, which returns tuples:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">zip(*pairs)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">pairs = [(<span class="number">1</span>, <span class="string">&#x27;a&#x27;</span>), (<span class="number">2</span>, <span class="string">&#x27;b&#x27;</span>), (<span class="number">3</span>, <span class="string">&#x27;c&#x27;</span>)]</span><br><span class="line">number, letters = <span class="built_in">zip</span>(*pairs)</span><br><span class="line"><span class="built_in">print</span>(number)</span><br><span class="line"><span class="built_in">print</span>(letters)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">(1, 2, 3)</span></span><br><span class="line"><span class="string">(&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Programming Language</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Gamma and Factorial</title>
    <url>/2023/05/05/GammaAndFactorial/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="Gamma-n-1-and-n"><a href="#Gamma-n-1-and-n" class="headerlink" title="$\Gamma(n+1)$ and $n!$"></a>$\Gamma(n+1)$ and $n!$</h1><p>Factorial is an operation unique to positive integers and zero. There is no direct definition of factorial for positive real numbers. But if we connect the discrete points in the factorial function into a smooth curve, we get the factorial of a positive real number. And that&#39;s gamma function:<br>$$<br>\Gamma(x)&#x3D;\int_0^{+\infty}t^{x-1}e^{-t}dt(x&gt;0)<br>$$<br>gamma function is the expansion of factorial on positive real numbers:<br>$$<br>\Gamma(n+1)&#x3D;n!<br>$$<br>Therefore:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line">math.gamma(<span class="number">5</span>)==<span class="number">24</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">True</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Programming Language</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Lambda: Anomymous Functions</title>
    <url>/2023/05/05/PythonLambda/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h1><p>Lambda functions are defined by <code>lambda</code> keyword, which could only have one expression but multiple arguments. We can assign the lambda function to any variable so that we can use it as a function. More generally, we use <code>lambda</code> together with <code>map</code>, <code>reduce</code> or <code>filter</code>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">lambda [arguments] : [expression]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># zero argument</span></span><br><span class="line">a = <span class="keyword">lambda</span> : <span class="built_in">print</span>(<span class="string">&#x27;hello&#x27;</span>)</span><br><span class="line"><span class="comment"># one argument</span></span><br><span class="line">b = <span class="keyword">lambda</span> x : x**<span class="number">2</span></span><br><span class="line"><span class="comment"># multiple arguments</span></span><br><span class="line">c = <span class="keyword">lambda</span> x, y : x + y</span><br><span class="line">a()</span><br><span class="line"><span class="built_in">print</span>(b(<span class="number">2</span>))</span><br><span class="line"><span class="built_in">print</span>(c(<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">hello</span></span><br><span class="line"><span class="string">4</span></span><br><span class="line"><span class="string">5</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<h1 id="Usage"><a href="#Usage" class="headerlink" title="Usage"></a>Usage</h1><p>In addition to the basic usage above, there are 4 ways to make use of <code>lambda</code> in general.</p>
<h2 id="Functions-nested-lambda"><a href="#Functions-nested-lambda" class="headerlink" title="Functions nested lambda"></a>Functions nested lambda</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">new_func</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">lambda</span> y : x + y</span><br><span class="line">t = new_func(<span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(t(<span class="number">3</span>))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">6</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>Anomymous functions are also functions and they have their own scope, because of which, anomymous function will use variables of enclosing function or global variables once they can&#39;t find the specific variable in the local scope.</p>
<h2 id="map-and-lambda"><a href="#map-and-lambda" class="headerlink" title="map and lambda"></a>map and lambda</h2><p><code>map</code> funtion makes use the idea of <strong>SIMD</strong>. It maps a certain function to all the elements of input list and return a iterator which can be turned to list using <code>list()</code>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">map(func_to_apply, iterable)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">a = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">list</span>(<span class="built_in">map</span>(<span class="keyword">lambda</span> x : x**<span class="number">2</span>, a)))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[1, 4, 9, 16, 25]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p><code>map</code> can also deal with two or more lists as long as the length of lists is the same and the function supports more than one arguments:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">map</span>(<span class="keyword">lambda</span> x, y : x + y, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]):</span><br><span class="line">    <span class="built_in">print</span>(i)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">5</span></span><br><span class="line"><span class="string">7</span></span><br><span class="line"><span class="string">9</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>What&#39;s more, the element of list could even be function:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sq</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> x**<span class="number">2</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">doub</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span> * x</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">list</span>(<span class="built_in">map</span>(<span class="keyword">lambda</span> x : x(i), [sq, doub])))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[0, 0]</span></span><br><span class="line"><span class="string">[1, 2]</span></span><br><span class="line"><span class="string">[4, 4]</span></span><br><span class="line"><span class="string">[9, 6]</span></span><br><span class="line"><span class="string">[16, 8]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<h2 id="reduce-and-lambda"><a href="#reduce-and-lambda" class="headerlink" title="reduce and lambda"></a>reduce and lambda</h2><p><code>reduce</code> firstly passes the first two elements of list to the function. Then, take the result of function as the new first argument and pass it to the function with the third element of list and so on until the last element:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">reduce(func, iterable)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> reduce</span><br><span class="line">a = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line"><span class="built_in">print</span>(reduce(<span class="keyword">lambda</span> x, y : x + y, a)) <span class="comment"># equal to 1 + 2 + 3 + 4 + 5</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">15</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>The element of list could also be function.</p>
</blockquote>
<h2 id="filter-and-lambda"><a href="#filter-and-lambda" class="headerlink" title="filter and lambda"></a>filter and lambda</h2><p><code>filter</code> returns a iterator that contains elements in the input_list that match the criteria of the function:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">filter(func, iterable)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">a = <span class="built_in">filter</span>(<span class="keyword">lambda</span> x: x % <span class="number">2</span> == <span class="number">0</span>, <span class="built_in">range</span>(<span class="number">10</span>)) <span class="comment"># get even number</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">list</span>(a))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[0, 2, 4, 6, 8]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p><code>filter</code> is easily replaced by <code>for</code> or <code>for if</code> comprehension:</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>)]</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line">a = [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>) <span class="keyword">if</span> i % <span class="number">2</span> == <span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</span></span><br><span class="line"><span class="string">[0, 2, 4, 6, 8]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Programming Language</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Python Package</title>
    <url>/2023/06/15/PythonPackage/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="Python包和Python版本"><a href="#Python包和Python版本" class="headerlink" title="Python包和Python版本"></a>Python包和Python版本</h1><p>在安装Python包之前，一定要先确认包和当下的Python版本是否兼容，很多安装问题都源于版本的不兼容。当使用远程镜像或Pycharm无法正确地安装包时，可以手动下载与当前Python版本相兼容的包的<code>.whl</code>文件。</p>
<blockquote>
<p><code>.whl</code>文件(WHL file)也称为轮子（wheel），这是用于Python分发（Distribution）的标准内置包格式（Standard built-package format）。它包含安装所需的所有文件和元数据（Metadata）。<code>.whl</code>文件使用zip进行压缩。<code>.whl</code>文件还包含有关此wheel文件支持的Python版本和平台的信息。<code>.whl</code>文件格式是一种即装即用格式（Ready-to-install format），允许在不构建源代码分发（Without building the source distribution）的情况下运行安装包。其本质上是一个安装包。</p>
</blockquote>
<p><a href="https://www.lfd.uci.edu/~gohlke/pythonlibs/#ta">Archived: Unofficial Windows Binaries for Python Extension Packages</a>该网站提供了一些常用的<code>.whl</code>安装包，<a href="https://download.pytorch.org/whl/torch_stable.html">Pytorch Stable</a>则提供了各个版本的torch和torchvision的安装包。</p>
<p>下载好<code>.whl</code>安装包后，在当前的Python环境（或虚拟环境）下运行命令行，切换到<code>.whl</code>安装包所在的目录，用<code>pip</code>安装即可：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pip install [Your .whl file]</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Programming Language</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Configuration</tag>
      </tags>
  </entry>
  <entry>
    <title>Splat</title>
    <url>/2023/05/02/Splat/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="What-is-splat"><a href="#What-is-splat" class="headerlink" title="What is splat"></a>What is splat</h1><p>Splat is <code>*</code> or <code>**</code>. And why do we use splat?</p>
<blockquote>
<p>Python supports functions with an uncertain number of parameters. The splat operator provides an artistic way of unpacking argument lists.</p>
</blockquote>
<h1 id="Positional-arguments-vs-keyword-arguments"><a href="#Positional-arguments-vs-keyword-arguments" class="headerlink" title="Positional arguments vs keyword arguments"></a>Positional arguments vs keyword arguments</h1><p>Positional arguments are arguments without default values while keyword arguments are arguments with default values. Both the number of positional arguments and the number of keyword arguments could be uncertain: </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">func</span>(<span class="params">*args, **kwargs</span>)</span><br></pre></td></tr></table></figure>

<h1 id="Argument-lists"><a href="#Argument-lists" class="headerlink" title="Argument lists"></a>Argument lists</h1><p>To call the function with uncertain arguments, we could pass the argument one by one:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = <span class="number">1</span></span><br><span class="line">y = <span class="number">2</span></span><br><span class="line">z = <span class="number">3</span></span><br><span class="line">func(x, y, z, kwarg1=<span class="string">&#x27;key1&#x27;</span>, kwarg2=<span class="string">&#x27;key2&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>However, we could also pass an argument list and use splat to unpack the list:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">test_list = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">test_dict = &#123;<span class="string">&#x27;kwarg1&#x27;</span>: <span class="string">&#x27;key1&#x27;</span>, <span class="string">&#x27;kwarg2&#x27;</span>: <span class="string">&#x27;key2&#x27;</span>&#125;</span><br><span class="line">func(*test_list, **test_dict)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Programming Language</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>With: Context Managers</title>
    <url>/2023/05/02/WithContextManagers/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="Basic-concept"><a href="#Basic-concept" class="headerlink" title="Basic concept"></a>Basic concept</h1><p><code>with</code> (or <code>with...as...</code>) is actually a special <strong>sign</strong> that tells the interpreter to call a certain function when entering <code>with</code> block and call another function when leaving <code>with</code> block.</p>
<blockquote>
<p>Since it calls a function, it will get return object. We can use <code>with...as...</code> to catch this object.</p>
</blockquote>
<h1 id="Context-manager-classes"><a href="#Context-manager-classes" class="headerlink" title="Context manager classes"></a>Context manager classes</h1><p>For each class that implements <code>__enter__</code> and <code>__exit__</code>, it is a context manager. </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CTManager</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;__init__ runs&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__enter__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;__enter__ runs&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__exit__</span>(<span class="params">self, exc_type, exc_val, exc_tb</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;__exit__ runs&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">print_message</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;this is a context manager&#x27;</span>)</span><br><span class="line"></span><br><span class="line">ct = CTManager()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;with block&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> ct <span class="keyword">as</span> c:</span><br><span class="line">    c.print_message()</span><br><span class="line"><span class="comment"># with CTManager() as c:</span></span><br><span class="line"><span class="comment">#   c.print_message()</span></span><br><span class="line"><span class="comment"># is also valid</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">We get:</span></span><br><span class="line"><span class="string">__init__ runs</span></span><br><span class="line"><span class="string">with block</span></span><br><span class="line"><span class="string">__enter__ runs</span></span><br><span class="line"><span class="string">this is a context manager</span></span><br><span class="line"><span class="string">__exit__ runs</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>For a context manager class, if it hasn&#39;t been initialized, <code>__init__</code> will be called. Then <code>__enter__</code> will be implemented automatically and <code>__exit__</code> will be implemented when leaving the block.</p>
<h1 id="Context-manager-functions"><a href="#Context-manager-functions" class="headerlink" title="Context manager functions"></a>Context manager functions</h1><p>The definition of context manager functions is much easier than context manager classes since it is almost the same as defining a generator:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> contextlib</span><br><span class="line"></span><br><span class="line"><span class="meta">@contextlib.contextmanager</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ctmanager</span>():</span><br><span class="line">    <span class="comment"># __enter__()</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;enter&quot;</span>)</span><br><span class="line">    <span class="keyword">yield</span> </span><br><span class="line">    <span class="comment"># __exit__()</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;exit&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> ctmanager():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;main&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">We get:</span></span><br><span class="line"><span class="string">enter</span></span><br><span class="line"><span class="string">main</span></span><br><span class="line"><span class="string">exit</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>Before creating a context manager function, we should <code>import contextlib</code> and pass the context manager function as a parameter to <code>contextlib.contextmanager</code>.</p>
<blockquote>
<p><code>@</code> has two meanings in python:</p>
<ul>
<li>Matrix multiplication <code>A @ B</code>;</li>
<li>Function modifier: Pass the function defined below it as a parameter of the function behind it.</li>
</ul>
</blockquote>
<p>When <code>with</code> a context manager function, the function will keep running to <code>yield</code>. Then, finish the code left when <code>with</code> block finishs.</p>
]]></content>
      <categories>
        <category>Programming Language</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Iterator &amp; Generator in Python</title>
    <url>/2023/05/02/IteratorAndGenerator/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="Iterator"><a href="#Iterator" class="headerlink" title="Iterator"></a>Iterator</h1><p>The iterator in python is slightly different from its counterpart in C++. Once a class define <code>__iter__(self)</code> member function and <code>__next__(self)</code> member function, we can use <code>iter()</code> to get its iterator and <code>next()</code> to get the next element of this iterator:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Number</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__iter__</span>(<span class="params">self</span>):</span><br><span class="line">        self.n = <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__next__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">if</span> self.n &lt;= <span class="number">20</span>:</span><br><span class="line">            x = self.n</span><br><span class="line">            self.n += <span class="number">1</span></span><br><span class="line">            <span class="keyword">return</span> x</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> StopIteration</span><br><span class="line"></span><br><span class="line">n = Number()</span><br><span class="line">_<span class="built_in">iter</span> = <span class="built_in">iter</span>(n)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(_<span class="built_in">iter</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">next</span>(_<span class="built_in">iter</span>))</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">We get:</span></span><br><span class="line"><span class="string">&lt;class &#x27;__main__.Number&#x27;&gt;</span></span><br><span class="line"><span class="string">1</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p><code>StopIteration</code> is the end of iterator for every iterator. It is an exception and <code>for</code> can catch and solve it. Therefore, we can also use <code>for</code> to traverse a iterator:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">_<span class="built_in">iter</span> = <span class="built_in">iter</span>(a)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> _<span class="built_in">iter</span>:</span><br><span class="line">    <span class="built_in">print</span>(i)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">We get:</span></span><br><span class="line"><span class="string">1</span></span><br><span class="line"><span class="string">2</span></span><br><span class="line"><span class="string">3</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p><code>list</code>, <code>tuple</code>, <code>string</code> have implemented iterators.</p>
</blockquote>
<h1 id="Generator"><a href="#Generator" class="headerlink" title="Generator"></a>Generator</h1><p>The generator is a special type of iterator, which makes a function iterable. Once a function uses <code>yield</code> to return object, this function is a generator:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">func</span>(<span class="params">n</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;a&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        <span class="keyword">yield</span> i</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;hello&#x27;</span>)</span><br><span class="line"></span><br><span class="line">_generator = func(<span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(_generator))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">next</span>(_generator))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">We get:</span></span><br><span class="line"><span class="string">&lt;class &#x27;generator&#x27;&gt;</span></span><br><span class="line"><span class="string">0</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">next</span>(_generator))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">We get:</span></span><br><span class="line"><span class="string">hello</span></span><br><span class="line"><span class="string">1</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>When a function becomes a generator, it will not run immediately when called, instead, it will return a generator. When we use <code>next()</code> to traverse the generator, the function will run to the place where <code>yield</code> is, record the address and return a value. Next time, it will run starting from <code>yield</code>. Similarly, it will return <code>StopIteration</code> when finish the function. Therefore, we can also use <code>for</code> to traverse a generator:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> func(<span class="number">10</span>):</span><br><span class="line">    <span class="built_in">print</span>(i + <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Programming Language</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Usage of Import Keyword in Python</title>
    <url>/2023/04/17/UsageOfImportInPython/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="Basic-concept"><a href="#Basic-concept" class="headerlink" title="Basic concept"></a>Basic concept</h1><h2 id="Namespace"><a href="#Namespace" class="headerlink" title="Namespace"></a>Namespace</h2><p>See <a href="/2023/04/17/NamespaceInPython/">Namespace</a>.</p>
<h2 id="Search-path"><a href="#Search-path" class="headerlink" title="Search path"></a>Search path</h2><p>When importing a module in a python file, the python interpreter will search for the module according to the following priorites:</p>
<ul>
<li>Built-in module: The modules in the same folder as our python file;</li>
<li>sys.path: <code>sys.path</code> includes some search paths. For examples, path of this python project. It is a list that we can modify. (When visiting it, we should <code>import sys</code>)</li>
<li>Environment variable path;</li>
<li>Std lib.</li>
</ul>
<h2 id="Module-amp-package"><a href="#Module-amp-package" class="headerlink" title="Module &amp; package"></a>Module &amp; package</h2><p>In general, a module is a file ended with <code>.py</code>, that is, it is a python file. When importing a module, python interpreter will run this module, therefore, if we write some test statements in the imported module, it will be run.</p>
<blockquote>
<p>Each module has a metadata called <code>__name__</code>. When a module is run as a usual module, the value of <code>__name__</code> is <code>__main__</code>. When it is run as an imported module, the value of <code>__name__</code> is the name of module. Therefore, if we want to test our module, we can put the test statement under <code>if __name__ == &#39;__main__&#39;</code>.</p>
</blockquote>
<p>A package is a folder contains modules and file <code>__init__.py</code>. When importing a package, python interpreter will run <code>__init__.py</code>. Therefore, we should put our import statement in <code>__init__.py</code>.</p>
<blockquote>
<p>We can create nested package, that is, ceate packages in a package.</p>
</blockquote>
<p><img src="/2023/04/17/UsageOfImportInPython/1.png" alt="1"></p>
<center style="font-size:12px; font-weight:bold">Fig. 1. Nested package</center>

<h1 id="Importing-modules"><a href="#Importing-modules" class="headerlink" title="Importing modules"></a>Importing modules</h1><p>When importing a module, the name of the module is its full path after one of the search path. For examples, you have a search path <code>C:\Users\xx</code> and a module to be imported <code>C:\Users\xx\yy\module_1.py</code>, then your code is:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> yy.module_1</span><br></pre></td></tr></table></figure>
<p>Python will do the following things when it interprets a import statement:</p>
<ul>
<li>Create a new and empty module object;</li>
<li>Load this object to <code>sys.modules</code> (<code>sys.modules</code> is a dictionary that stores all the modules that can be recogized in current file);</li>
<li>Find this module through search path and load, run this module.</li>
</ul>
<p>When using <code>import</code> to import modules that under other folder, python only puts the first file to namespace, that is, if you:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> yy.module_1</span><br></pre></td></tr></table></figure>
<p>only <code>yy</code> will be put into namespace. Therefore, if you want to visit functions in <code>module_1</code>, you must start from <code>yy</code>:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">yy.module_1.func()</span><br></pre></td></tr></table></figure>
<p>However, if using <code>from ... import ...</code>, python will put the function or class or module after <code>import</code> to namespace:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> yy.module_1 <span class="keyword">import</span> func</span><br><span class="line">func()  <span class="comment"># You can directly visit func()</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>If the module we import imports other modules, we must make sure that python can find these modules <strong>in the current file</strong> (by modifying <code>sys.path</code>).</p>
</blockquote>
<h1 id="Importing-packages"><a href="#Importing-packages" class="headerlink" title="Importing packages"></a>Importing packages</h1><p>Importing a package is almost the same as importing a module. However, since package can not be run, python will run <code>__init__.py</code> once we import something from the package. Therefore, we can put the statements about importing modules in <code>__init__.py</code>. Then, we can just import the package and all the modules will be imported. In contrast, if <code>__init__.py</code> is empty and we only import the package, we will get nothing. The follwing is the structure of a python project:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">|--&gt;pack1</span><br><span class="line">    |--&gt;__init_.py</span><br><span class="line">    |--&gt;mod1.py</span><br><span class="line">module_test.py</span><br></pre></td></tr></table></figure>
<p>If <code>__init__.py</code> is empty and we import pack1 in module_test.py:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pack1</span><br><span class="line"></span><br><span class="line">pack1.mod1.func()</span><br></pre></td></tr></table></figure>
<p>Then we will get:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">AttributeError: module &#x27;pack1&#x27; has no attribute &#x27;mod1&#x27;</span><br></pre></td></tr></table></figure>
<p>Similarly, we must make sure that python can find these modules <strong>in the current file</strong> when we import the package since the search path of <code>__init__.py</code> will become the search path of the current file if the package is imported:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Run __init__.py directly:</span></span><br><span class="line"><span class="built_in">print</span>(sys.path)</span><br><span class="line"><span class="comment"># We get: &#x27;C:\\Users\\xxx\\PycharmProjects\\Foundation\\module_myself\\pack1&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Run __init__.py when importing pack1 in module_test.py</span></span><br><span class="line"><span class="built_in">print</span>(sys.path)</span><br><span class="line"><span class="comment"># We get: &#x27;C:\\Users\\zcl\\PycharmProjects\\Foundation\\module_myself&#x27;</span></span><br></pre></td></tr></table></figure>

<p>Some correct ways to import mod1.py:</p>
<ol>
<li><p>Import module:</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># __init__.py</span></span><br><span class="line"><span class="keyword">from</span> . <span class="keyword">import</span> mod1 <span class="comment"># . is still the PWD of __init__.py, that is xxx\pack1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># module_test.py</span></span><br><span class="line"><span class="keyword">import</span> pack1</span><br><span class="line">pack1.mod1.func() <span class="comment"># namespace just has pack1</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>Import fucntion:</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># __init__.py</span></span><br><span class="line"><span class="keyword">from</span> .mod1 <span class="keyword">import</span> func</span><br><span class="line"></span><br><span class="line"><span class="comment"># module_test.py</span></span><br><span class="line"><span class="keyword">import</span> pack1</span><br><span class="line">pack1.func()</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Since <code>__init__.py</code> is also a python file, its behaviours are the same as common file. Therefore, it will add <code>mod1</code> or <code>func</code> to its namespace.</p>
</blockquote>
</li>
<li><p>Import:</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># __init__.py</span></span><br><span class="line"><span class="keyword">import</span> mod1</span><br><span class="line"></span><br><span class="line"><span class="comment"># module_test.py</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">&#x27;xxxxxxxx\\pack1&#x27;</span>) <span class="comment"># Add path before importing</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pack1</span><br><span class="line">pakc1.mod1.func()</span><br></pre></td></tr></table></figure></li>
</ol>
<p>The most important thing we must remember is that when importing a module or package that has imported other modules or packages, we must make sure that these modules or packages can be found in the current file.</p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul>
<li><a href="https://cloud.tencent.com/developer/article/2103543">Python 3.x | 史上最详解的 导入（import）</a></li>
</ul>
]]></content>
      <categories>
        <category>Programming Language</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Namespace in Python</title>
    <url>/2023/04/17/NamespaceInPython/</url>
    <content><![CDATA[<span id="more"></span>

<p>There are totally 4 types of namespace in python, that is LEGB. They are all independent from each other. However, there is priority among them. When compiler searches for a variable, it will search in the sequence of LEGB:</p>
<ul>
<li>Locals: The namespace inside a function, which including local variables and formal parameters:  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">func1</span>(<span class="params">x, y</span>):</span><br><span class="line">    a = <span class="number">1</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">locals</span>())</span><br><span class="line">func1(<span class="number">1</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Output:</span><br><span class="line">&#123;&#x27;x&#x27;: 1, &#x27;y&#x27;: 1, &#x27;a&#x27;: 1&#125;</span><br></pre></td></tr></table></figure></li>
<li>Enclosing function: The namespace of outer function in nested function:  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">func1</span>(<span class="params">x, y</span>):</span><br><span class="line">    a = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">func2</span>(<span class="params">z, k</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="built_in">locals</span>())</span><br><span class="line"></span><br><span class="line">    func2(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">func1(<span class="number">1</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">output:</span><br><span class="line">&#123;&#x27;z&#x27;: 1, &#x27;k&#x27;: 1&#125;</span><br></pre></td></tr></table></figure>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">func1</span>(<span class="params">x, y</span>):</span><br><span class="line">    a = <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">func2</span>(<span class="params">z, k</span>):</span><br><span class="line">        z += x</span><br><span class="line">        k += y</span><br><span class="line">        <span class="built_in">print</span>(<span class="built_in">locals</span>())</span><br><span class="line">    </span><br><span class="line">    func2(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">func1(<span class="number">1</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">output:</span><br><span class="line">&#123;&#x27;z&#x27;: 2, &#x27;k&#x27;: 2, &#x27;x&#x27;: 1, &#x27;y&#x27;: 1&#125;</span><br></pre></td></tr></table></figure>
  This shows that the inner function can access to the outer function (only read) and add variables in enclosing function to its local namespace.</li>
<li>Globals: The namespace of current module, that is <code>.py</code> file, including variables, functions, classes and so on.</li>
<li><code>__builtins__</code>: The namespace of built-in variables and built-in functions.</li>
</ul>
<p>Variables in higher-priority namespaces will override variables with the same name in lower-priority namespaces.</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference:"></a>Reference:</h3><ul>
<li><a href="https://blog.csdn.net/weixin_39781363/article/details/110279649">python中space_漫谈Python的Namespace
</a></li>
</ul>
]]></content>
      <categories>
        <category>Programming Language</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Analysis of Variables in Python</title>
    <url>/2023/04/17/AnalysisOfVariableInPython/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="Assignment"><a href="#Assignment" class="headerlink" title="Assignment"></a>Assignment</h1><p>In python, variables are just strings that refer to certain objects, which means that they are actually &quot;pointers&quot; in C&#x2F;C++. When assigning a value to a variable, we actually modifies the <strong>reference</strong> of the variable:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = <span class="number">10</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(<span class="number">10</span>))  <span class="comment"># id(ob) return the address of ob</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(a))</span><br></pre></td></tr></table></figure>
<p>The outputs are:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">140723224699976</span><br><span class="line">140723224699976</span><br></pre></td></tr></table></figure>
<p>This shows that when creating <code>a</code> in python, it will bind <code>a</code> to the address of <code>10</code>:</p>
<p><img src="/2023/04/17/AnalysisOfVariableInPython/1.png" alt="1"></p>
<center style="font-size:12px; font-weight:bold">Fig. 1. Relationship between a and 10</center><br>

<p>Similarly, when changing the value of <code>a</code>, we are actually make <code>a</code> refer to the new value. In addition, when assigning the value of <code>a</code> to a new variable <code>b</code>, python does not assign new memory, but just make <code>b</code> refer to the value of <code>a</code>:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = <span class="number">10</span></span><br><span class="line">b = a</span><br><span class="line">a = <span class="number">20</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(<span class="number">10</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(b))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(<span class="number">20</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(a))</span><br></pre></td></tr></table></figure>
<p>The outputs are:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">140722523399240</span><br><span class="line">140722523399240</span><br><span class="line">140722523399560</span><br><span class="line">140722523399560</span><br></pre></td></tr></table></figure>
<p><img src="/2023/04/17/AnalysisOfVariableInPython/2.png" alt="2"></p>
<center style="font-size:12px; font-weight:bold">Fig. 2. Reference of a and b</center>

<h1 id="Parameters-passing-and-return"><a href="#Parameters-passing-and-return" class="headerlink" title="Parameters passing and return"></a>Parameters passing and return</h1><p>The process of parameters passing is actually a process of assignment. Therefore, what we are passing to the formal parameter <code>a</code> is the reference of actual parameter <code>b</code>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">func</span>(<span class="params">a</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">id</span>(a))</span><br><span class="line"></span><br><span class="line">b = <span class="number">10</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(<span class="number">10</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(b))</span><br><span class="line">func(b)</span><br></pre></td></tr></table></figure>
<p>The outputs are:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">140723839624264</span><br><span class="line">140723839624264</span><br><span class="line">140723839624264</span><br></pre></td></tr></table></figure>
<p>Similarly, when returning a value, we are also returning its reference:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">func</span>():</span><br><span class="line">    a = <span class="number">2</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">id</span>(a))</span><br><span class="line">    <span class="keyword">return</span> a</span><br><span class="line">b = <span class="number">2</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(b))</span><br></pre></td></tr></table></figure>
<p>The outputs are:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">140723839624008</span><br><span class="line">140723839624008</span><br></pre></td></tr></table></figure>

<h1 id="Mutable-amp-immutable-objects"><a href="#Mutable-amp-immutable-objects" class="headerlink" title="Mutable &amp; immutable objects"></a>Mutable &amp; immutable objects</h1><p>In python, everything is an object.</p>
<p>For variables that refer to immutable objects (e.g. string, number, tuple), we can&#39;t change the value of objects. When we change the value of variable, we are actually make the variable refer to another object (e.g. fig. 2).</p>
<p>For variables that refer to mutable objects (e.g. list, dictionary), they actually refer to the beginning address of their inner objects, which means variables are &quot;secondary pointers&quot;. Therefore, we can change the value of the elements of these objects. Change the value of a variable will also make the variable refer to another object.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = [<span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line">b = a</span><br><span class="line">b[<span class="number">0</span>] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(a))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(b))</span><br><span class="line"></span><br><span class="line">a = [<span class="number">1</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(a))</span><br></pre></td></tr></table></figure>
<p>The outputs are:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[2, 2]</span><br><span class="line">[2, 2]</span><br><span class="line">1330208790976</span><br><span class="line">1330208790976</span><br><span class="line">1330210492608</span><br></pre></td></tr></table></figure>

<blockquote>
<p>For mutable objects, their member functions may have side effects while member functions of immutable objects will not have side effects but return a new object. The class defined by users is always mutable obejct.</p>
</blockquote>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul>
<li><a href="https://blog.csdn.net/Invokar/article/details/89138684">Python变量引用浅析</a></li>
</ul>
]]></content>
      <categories>
        <category>Programming Language</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Conda</title>
    <url>/2023/04/11/Conda/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="Installation"><a href="#Installation" class="headerlink" title="Installation"></a>Installation</h1><p>5 steps:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># create a directory to install minicaonda in</span><br><span class="line">mkdir -p ~/miniconda3</span><br><span class="line"></span><br><span class="line"># download latest miniconda version</span><br><span class="line">wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh</span><br><span class="line"></span><br><span class="line"># run the install script</span><br><span class="line">bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3</span><br><span class="line"></span><br><span class="line"># delete the intall script</span><br><span class="line">rm -rf ~/miniconda3/miniconda.sh</span><br><span class="line"></span><br><span class="line"># add a conda initialize to your bash</span><br><span class="line">~/miniconda3/bin/conda init bash</span><br><span class="line"></span><br><span class="line"># Verify the installaton </span><br><span class="line">conda list</span><br></pre></td></tr></table></figure>

<h1 id="Environment-amp-Library"><a href="#Environment-amp-Library" class="headerlink" title="Environment &amp; Library"></a>Environment &amp; Library</h1><p>Create a new environment:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">conda create -n [newevname] python=[YourVersion]</span><br></pre></td></tr></table></figure>

<p>Switch to another environment:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">conda activate [evname]</span><br></pre></td></tr></table></figure>

<blockquote>
<p><code>conda deactivate</code> will make you exit the current environment and turn back to <code>base</code> environment.</p>
</blockquote>
<p>Rename an environment:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">conda create -n [newevname] --<span class="built_in">clone</span> [oldenvname]</span><br><span class="line">conda remove -n [oldevname] --all</span><br></pre></td></tr></table></figure>
<blockquote>
<p>If there are some packages or programs related to the environment name (e.g. jupyter notebook), avoid renaming or copying. Actually, the commands above are <code>clone</code> and <code>remove</code> commands.</p>
</blockquote>
<p>Install a new library:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">conda install [libname]</span><br></pre></td></tr></table></figure>

<blockquote>
<p><code>pip</code> can also be used to install new libraries. Use <code>uninstall</code> to uninstall the library you don&#39;t need.</p>
</blockquote>
<h1 id="Information"><a href="#Information" class="headerlink" title="Information"></a>Information</h1><p>List all installed libraries in the current environment:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">conda list</span><br></pre></td></tr></table></figure>

<p>List all the virtual environments you have created:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">conda <span class="built_in">env</span> list</span><br></pre></td></tr></table></figure>

<h1 id="Sharing"><a href="#Sharing" class="headerlink" title="Sharing"></a>Sharing</h1><p>To share your environment with others, you should first export your configuration to a <code>yml</code> file:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">conda <span class="built_in">env</span> <span class="built_in">export</span> &gt; [filename].yml</span><br></pre></td></tr></table></figure>

<blockquote>
<p>The <code>yml</code> file will be created in your current folder.</p>
</blockquote>
<p>Then, the <code>yml</code> file can be used to create an environment that is the same as yours:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">conda <span class="built_in">env</span> create -f [filename].yml</span><br></pre></td></tr></table></figure>

<blockquote>
<p>The first line of the <code>yml</code> file is the name of the created environment. You should make sure that the <code>yml</code> file is in your current folder.</p>
</blockquote>
<h1 id="Configuration"><a href="#Configuration" class="headerlink" title="Configuration"></a>Configuration</h1><p>Create <code>.condarc</code>:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">conda config --add channels r</span><br></pre></td></tr></table></figure>

<p>Tsinghua mirror and env path:</p>
<figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="attr">channels:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/pro</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/mro</span></span><br><span class="line"><span class="attr">envs_dirs:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">C://[root</span> <span class="string">of</span> <span class="string">your</span> <span class="string">anaconda/miniconda]//envs</span></span><br></pre></td></tr></table></figure>

<h1 id="More-information"><a href="#More-information" class="headerlink" title="More information"></a>More information</h1><ul>
<li><a href="https://docs.conda.io/en/latest/">Conda docs</a></li>
<li><a href="https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/index.html">Basic conda commands</a></li>
</ul>
]]></content>
      <categories>
        <category>Tool</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Tool</tag>
      </tags>
  </entry>
  <entry>
    <title>Markdown</title>
    <url>/2023/04/05/Markdown/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Markdown is a lightweight <strong>markup language</strong> for creating formatted text using a plain-text-editor.</p>
<h1 id="Block-element"><a href="#Block-element" class="headerlink" title="Block element"></a>Block element</h1><p>For each block, you&#39;d better put a <strong>space</strong> behind it.</p>
<h2 id="Title"><a href="#Title" class="headerlink" title="Title"></a>Title</h2><p>One # indicates a level of the heading. There are six levels in total.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Example1</span><br></pre></td></tr></table></figure>

<p><strong>That is</strong>:</p>
<h1 id="Example1"><a href="#Example1" class="headerlink" title="Example1"></a>Example1</h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">## EXSubtitle1</span><br></pre></td></tr></table></figure>

<p><strong>That is</strong>:</p>
<h2 id="EXSubtitle1"><a href="#EXSubtitle1" class="headerlink" title="EXSubtitle1"></a>EXSubtitle1</h2><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>One &gt; indicates this block is a <strong>reference</strong>.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt; Reference example</span><br></pre></td></tr></table></figure>

<p><strong>That is</strong>:</p>
<blockquote>
<p>Reference example</p>
</blockquote>
<h2 id="Unordered-list"><a href="#Unordered-list" class="headerlink" title="Unordered list"></a>Unordered list</h2><p>One - indicates <strong>a line of lists</strong>.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">- saber</span><br><span class="line">- lancer</span><br><span class="line">- berserker</span><br></pre></td></tr></table></figure>

<p><strong>That is</strong></p>
<ul>
<li>saber</li>
<li>lancer</li>
<li>berserker</li>
</ul>
<h2 id="Ordered-list"><a href="#Ordered-list" class="headerlink" title="Ordered list"></a>Ordered list</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1. saber</span><br><span class="line">2. lancer</span><br><span class="line">3. berserker</span><br></pre></td></tr></table></figure>

<p><strong>That is</strong></p>
<ol>
<li>saber</li>
<li>lancer</li>
<li>berserker</li>
</ol>
<h2 id="Multilevel-list"><a href="#Multilevel-list" class="headerlink" title="Multilevel list"></a>Multilevel list</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1. saber</span><br><span class="line">  - 1.1 king</span><br><span class="line">  - 1.2 queen</span><br><span class="line">2. lancer</span><br><span class="line">  - 2.1 red</span><br><span class="line">  - 2.2 blue</span><br></pre></td></tr></table></figure>

<p><strong>That is</strong></p>
<ol>
<li>saber<ul>
<li>1.1 king</li>
<li>1.2 queen</li>
</ul>
</li>
<li>lancer<ul>
<li>2.1 red</li>
<li>2.2 blue</li>
</ul>
</li>
</ol>
<h2 id="Task-list"><a href="#Task-list" class="headerlink" title="Task list"></a>Task list</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">- [ ] lunch</span><br><span class="line">- [ ] play</span><br><span class="line">- [x] sleep</span><br></pre></td></tr></table></figure>

<p><strong>That is</strong></p>
<ul>
<li><input disabled type="checkbox"> lunch</li>
<li><input disabled type="checkbox"> play</li>
<li><input checked disabled type="checkbox"> sleep</li>
</ul>
<h2 id="Code-block"><a href="#Code-block" class="headerlink" title="Code block"></a>Code block</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">just occupy```language</span><br><span class="line">           Content</span><br><span class="line">just occupy```</span><br></pre></td></tr></table></figure>

<p><strong>That is</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Content</span><br></pre></td></tr></table></figure>

<p>Languages that code block supports:</p>
<ul>
<li>applescript</li>
<li>shell&#x2F;bash</li>
<li>cpp&#x2F;c</li>
<li>c#</li>
<li>java</li>
<li>javascript&#x2F;js</li>
<li>text</li>
<li>python&#x2F;py</li>
<li>sql</li>
<li>matlab</li>
<li>go</li>
<li>css</li>
<li>delphi</li>
<li>erlang</li>
<li>groovy</li>
<li>coldfusion</li>
<li>...</li>
</ul>
<h2 id="Arithmetic-formula"><a href="#Arithmetic-formula" class="headerlink" title="Arithmetic formula"></a>Arithmetic formula</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">\frac&#123;\partial f&#125;&#123;\partial x&#125; = 2\sqrt&#123;a&#125;x</span><br><span class="line">$$</span><br></pre></td></tr></table></figure>

<p><strong>That is</strong><br>$$<br>\frac{\partial f}{\partial x} &#x3D; 2\sqrt{a}x<br>$$</p>
<p><del>But NexT theme will not show the formula correctly.</del><br>Markdown uses LateX to display formula. You just need to put LateX formula between <code>$$</code> and <code>$$</code>. See more LateX syntax in <a href="/2023/04/12/LateX/">LateX</a>.</p>
<h2 id="Table"><a href="#Table" class="headerlink" title="Table"></a>Table</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">|name|age|grade|</span><br><span class="line">|:---|---:|:---:|  // Three -; left right mid</span><br><span class="line">|Jack|20|80|</span><br><span class="line">|Bob|20|90|</span><br></pre></td></tr></table></figure>
<p><strong>That is</strong></p>
<table>
<thead>
<tr>
<th align="left">name</th>
<th align="right">age</th>
<th align="center">grade</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Jack</td>
<td align="right">20</td>
<td align="center">80</td>
</tr>
<tr>
<td align="left">Bob</td>
<td align="right">20</td>
<td align="center">90</td>
</tr>
</tbody></table>
<h3 id="Changing-the-style-of-table"><a href="#Changing-the-style-of-table" class="headerlink" title="Changing the style of table"></a>Changing the style of table</h3><p>Put the following code before the table. It will change the size of cell.</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">style</span>&gt;</span><span class="language-css"> <span class="selector-tag">table</span> <span class="selector-tag">th</span> &#123;</span></span><br><span class="line"><span class="language-css">    <span class="attribute">width</span>: <span class="number">160px</span>;</span></span><br><span class="line"><span class="language-css">&#125;</span></span><br><span class="line"><span class="language-css"></span><span class="tag">&lt;/<span class="name">style</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h1 id="Inline-element"><a href="#Inline-element" class="headerlink" title="Inline element"></a>Inline element</h1><h2 id="Link"><a href="#Link" class="headerlink" title="Link"></a>Link</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[baidu](baidu.com &quot;A search engine&quot;)</span><br></pre></td></tr></table></figure>

<p><strong>That is</strong> <a href="baidu.com" title="A search engine">baidu</a></p>
<h2 id="Reference-link"><a href="#Reference-link" class="headerlink" title="Reference link"></a>Reference link</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[baidu][id]</span><br><span class="line"></span><br><span class="line">[baidu][id]</span><br><span class="line"></span><br><span class="line">[id]: baidu.com &quot;A search engine&quot;</span><br></pre></td></tr></table></figure>

<p><strong>That is</strong> <a href="baidu.com" title="A search engine">baidu</a> <a href="baidu.com" title="A search engine">baidu</a></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Please refer to [Link](#Link)</span><br></pre></td></tr></table></figure>

<p><strong>That is</strong><br>Please refer to <a href="#Link">Link</a></p>
<h2 id="URL"><a href="#URL" class="headerlink" title="URL"></a>URL</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">http://www.baidu.com</span><br></pre></td></tr></table></figure>

<p><strong>That is</strong> <a href="http://www.baidu.com/">http://www.baidu.com</a></p>
<h2 id="Picture-link"><a href="#Picture-link" class="headerlink" title="Picture link"></a>Picture link</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">![baidu](https://www.baidu.com/img/PCtm_d9c8750bed0b3c7d089fa7d55720d6cf.png &quot;baidu logo&quot;)</span><br></pre></td></tr></table></figure>

<p><strong>That is</strong> <img src="https://www.baidu.com/img/PCtm_d9c8750bed0b3c7d089fa7d55720d6cf.png" alt="baidu" title="baidu logo"></p>
<h2 id="Italic"><a href="#Italic" class="headerlink" title="Italic"></a>Italic</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">*italic*</span><br></pre></td></tr></table></figure>

<p><strong>That is</strong> <em>italic</em></p>
<h2 id="Bold"><a href="#Bold" class="headerlink" title="Bold"></a>Bold</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">**bold**</span><br></pre></td></tr></table></figure>

<p><strong>That is</strong> <strong>bold</strong></p>
<h2 id="Line-code"><a href="#Line-code" class="headerlink" title="Line code"></a>Line code</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">`printf()`</span><br></pre></td></tr></table></figure>

<p><strong>That is</strong> <code>printf()</code></p>
<h2 id="Underline"><a href="#Underline" class="headerlink" title="Underline"></a>Underline</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;u&gt;underline&lt;/u&gt;</span><br></pre></td></tr></table></figure>

<p><strong>That is</strong> <u>underline</u></p>
<h2 id="Superscript"><a href="#Superscript" class="headerlink" title="Superscript"></a>Superscript</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">saber&lt;sup&gt;1&lt;/sup&gt;</span><br></pre></td></tr></table></figure>

<p><strong>That is</strong> saber<sup>1</sup></p>
<h2 id="Subscript"><a href="#Subscript" class="headerlink" title="Subscript"></a>Subscript</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">saber&lt;sub&gt;1&lt;/sub&gt;</span><br></pre></td></tr></table></figure>

<p><strong>That is</strong> saber<sub>1</sub></p>
<h2 id="In-the-middle"><a href="#In-the-middle" class="headerlink" title="In the middle"></a>In the middle</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;center&gt; That is &lt;/center&gt;</span><br></pre></td></tr></table></figure>
<p><strong><center> That is </center></strong></p>
<h2 id="Line-Arithmetic-formula"><a href="#Line-Arithmetic-formula" class="headerlink" title="Line Arithmetic formula"></a>Line Arithmetic formula</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$x$</span><br></pre></td></tr></table></figure>

<p><strong>That is</strong> $x$</p>
<p>See more LateX syntax in <a href="/2023/04/12/LateX/">LateX</a>.</p>
<h2 id="Deletion-line"><a href="#Deletion-line" class="headerlink" title="Deletion line"></a>Deletion line</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">~~aaa~~</span><br></pre></td></tr></table></figure>

<p><strong>That is</strong> <del>aaa</del></p>
<h1 id="More-information"><a href="#More-information" class="headerlink" title="More information"></a>More information</h1><ul>
<li><a href="https://www.markdownguide.org/basic-syntax/">Markdown Guide</a></li>
</ul>
]]></content>
      <categories>
        <category>Tool</category>
      </categories>
      <tags>
        <tag>Tool</tag>
        <tag>Hexo</tag>
        <tag>Markup Language</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo Configuration</title>
    <url>/2023/04/10/HexoConfiguration/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="Image"><a href="#Image" class="headerlink" title="Image"></a>Image</h1><p>When <code>post_asset_folder</code> is <code>true</code>, each post created by bash will also create a folder with the same name. Then, we can put all the picture that will be used in the post in this folder and use <strong>relative path</strong> (just the picture&#39;s full name) to refer to it.</p>
<figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="attr">post_asset_folder:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure>

<p>However, because of the update of hexo, some bugs in the original plugin used to show images on website, like <code>hexo-simple-image</code> or <code>hexo-asset-image</code>, occur, which <strong>lead the path of images to be wrongly interpreted</strong>. For example:</p>
<ul>
<li>When using <code>hexo-simple-image</code>, the root path will be incorrectly interpreted as <code>/.io/</code> (this depends on the <strong>url</strong> you set);</li>
</ul>
<p><img src="/2023/04/10/HexoConfiguration/1.png" alt="1"></p>
<ul>
<li>When using <code>hexo-asset-image</code>, we can not show the picture on website either.</li>
</ul>
<p>To solve this problem, firstly, we need to uninstall <code>hexo-simple-image</code>.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm uninstall hexo-simple-image --save</span><br></pre></td></tr></table></figure>
<p>Secondly, we need to install <code>hexo-asset-image</code>.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm install hexo-asset-image --save</span><br></pre></td></tr></table></figure>
<p>Thirdly, we should change the content of <code>/node_modules/hexo-asset-image/index.js</code> to:</p>
<figure class="highlight js"><table><tr><td class="code"><pre><span class="line"><span class="meta">&#x27;use strict&#x27;</span>;</span><br><span class="line"><span class="keyword">var</span> cheerio = <span class="built_in">require</span>(<span class="string">&#x27;cheerio&#x27;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// http://stackoverflow.com/questions/14480345/how-to-get-the-nth-occurrence-in-a-string</span></span><br><span class="line"><span class="keyword">function</span> <span class="title function_">getPosition</span>(<span class="params">str, m, i</span>) &#123;</span><br><span class="line">  <span class="keyword">return</span> str.<span class="title function_">split</span>(m, i).<span class="title function_">join</span>(m).<span class="property">length</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> version = <span class="title class_">String</span>(hexo.<span class="property">version</span>).<span class="title function_">split</span>(<span class="string">&#x27;.&#x27;</span>);</span><br><span class="line">hexo.<span class="property">extend</span>.<span class="property">filter</span>.<span class="title function_">register</span>(<span class="string">&#x27;after_post_render&#x27;</span>, <span class="keyword">function</span>(<span class="params">data</span>)&#123;</span><br><span class="line">  <span class="keyword">var</span> config = hexo.<span class="property">config</span>;</span><br><span class="line">  <span class="keyword">if</span>(config.<span class="property">post_asset_folder</span>)&#123;</span><br><span class="line">    	<span class="keyword">var</span> link = data.<span class="property">permalink</span>;</span><br><span class="line">	<span class="keyword">if</span>(version.<span class="property">length</span> &gt; <span class="number">0</span> &amp;&amp; <span class="title class_">Number</span>(version[<span class="number">0</span>]) == <span class="number">3</span>)</span><br><span class="line">	   <span class="keyword">var</span> beginPos = <span class="title function_">getPosition</span>(link, <span class="string">&#x27;/&#x27;</span>, <span class="number">1</span>) + <span class="number">1</span>;</span><br><span class="line">	<span class="keyword">else</span></span><br><span class="line">	   <span class="keyword">var</span> beginPos = <span class="title function_">getPosition</span>(link, <span class="string">&#x27;/&#x27;</span>, <span class="number">3</span>) + <span class="number">1</span>;</span><br><span class="line">	<span class="comment">// In hexo 3.1.1, the permalink of &quot;about&quot; page is like &quot;.../about/index.html&quot;.</span></span><br><span class="line">	<span class="keyword">var</span> endPos = link.<span class="title function_">lastIndexOf</span>(<span class="string">&#x27;/&#x27;</span>) + <span class="number">1</span>;</span><br><span class="line">    link = link.<span class="title function_">substring</span>(beginPos, endPos);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">var</span> toprocess = [<span class="string">&#x27;excerpt&#x27;</span>, <span class="string">&#x27;more&#x27;</span>, <span class="string">&#x27;content&#x27;</span>];</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">var</span> i = <span class="number">0</span>; i &lt; toprocess.<span class="property">length</span>; i++)&#123;</span><br><span class="line">      <span class="keyword">var</span> key = toprocess[i];</span><br><span class="line"> </span><br><span class="line">      <span class="keyword">var</span> $ = cheerio.<span class="title function_">load</span>(data[key], &#123;</span><br><span class="line">        <span class="attr">ignoreWhitespace</span>: <span class="literal">false</span>,</span><br><span class="line">        <span class="attr">xmlMode</span>: <span class="literal">false</span>,</span><br><span class="line">        <span class="attr">lowerCaseTags</span>: <span class="literal">false</span>,</span><br><span class="line">        <span class="attr">decodeEntities</span>: <span class="literal">false</span></span><br><span class="line">      &#125;);</span><br><span class="line"></span><br><span class="line">      $(<span class="string">&#x27;img&#x27;</span>).<span class="title function_">each</span>(<span class="keyword">function</span>(<span class="params"></span>)&#123;</span><br><span class="line">		<span class="keyword">if</span> ($(<span class="variable language_">this</span>).<span class="title function_">attr</span>(<span class="string">&#x27;src&#x27;</span>))&#123;</span><br><span class="line">			<span class="comment">// For windows style path, we replace &#x27;\&#x27; to &#x27;/&#x27;.</span></span><br><span class="line">			<span class="keyword">var</span> src = $(<span class="variable language_">this</span>).<span class="title function_">attr</span>(<span class="string">&#x27;src&#x27;</span>).<span class="title function_">replace</span>(<span class="string">&#x27;\\&#x27;</span>, <span class="string">&#x27;/&#x27;</span>);</span><br><span class="line">			<span class="keyword">if</span>(!<span class="regexp">/http[s]*.*|\/\/.*/</span>.<span class="title function_">test</span>(src) &amp;&amp;</span><br><span class="line">			   !<span class="regexp">/^\s*\//</span>.<span class="title function_">test</span>(src)) &#123;</span><br><span class="line">			  <span class="comment">// For &quot;about&quot; page, the first part of &quot;src&quot; can&#x27;t be removed.</span></span><br><span class="line">			  <span class="comment">// In addition, to support multi-level local directory.</span></span><br><span class="line">			  <span class="keyword">var</span> linkArray = link.<span class="title function_">split</span>(<span class="string">&#x27;/&#x27;</span>).<span class="title function_">filter</span>(<span class="keyword">function</span>(<span class="params">elem</span>)&#123;</span><br><span class="line">				<span class="keyword">return</span> elem != <span class="string">&#x27;&#x27;</span>;</span><br><span class="line">			  &#125;);</span><br><span class="line">			  <span class="keyword">var</span> srcArray = src.<span class="title function_">split</span>(<span class="string">&#x27;/&#x27;</span>).<span class="title function_">filter</span>(<span class="keyword">function</span>(<span class="params">elem</span>)&#123;</span><br><span class="line">				<span class="keyword">return</span> elem != <span class="string">&#x27;&#x27;</span> &amp;&amp; elem != <span class="string">&#x27;.&#x27;</span>;</span><br><span class="line">			  &#125;);</span><br><span class="line">			  <span class="keyword">if</span>(srcArray.<span class="property">length</span> &gt; <span class="number">1</span>)</span><br><span class="line">				srcArray.<span class="title function_">shift</span>();</span><br><span class="line">			  src = srcArray.<span class="title function_">join</span>(<span class="string">&#x27;/&#x27;</span>);</span><br><span class="line">			  $(<span class="variable language_">this</span>).<span class="title function_">attr</span>(<span class="string">&#x27;src&#x27;</span>, config.<span class="property">root</span> + link + src);</span><br><span class="line">			  <span class="variable language_">console</span>.<span class="property">info</span>&amp;&amp;<span class="variable language_">console</span>.<span class="title function_">info</span>(<span class="string">&quot;update link as:--&gt;&quot;</span>+config.<span class="property">root</span> + link + src);</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;<span class="keyword">else</span>&#123;</span><br><span class="line">			<span class="variable language_">console</span>.<span class="property">info</span>&amp;&amp;<span class="variable language_">console</span>.<span class="title function_">info</span>(<span class="string">&quot;no src attr, skipped...&quot;</span>);</span><br><span class="line">			<span class="variable language_">console</span>.<span class="property">info</span>&amp;&amp;<span class="variable language_">console</span>.<span class="title function_">info</span>($(<span class="variable language_">this</span>));</span><br><span class="line">		&#125;</span><br><span class="line">      &#125;);</span><br><span class="line">      data[key] = $.<span class="title function_">html</span>();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>
<p>At last, add some configuration behind <code>post_asset_folder</code> which is <code>true</code> in <code>/_config.yml</code>.</p>
<figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="attr">marked:</span></span><br><span class="line">  <span class="attr">prependRoot:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">postAsset:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure>

<p><strong>This will make hexo automatically extend the path of image to its absolute path.</strong></p>
<h1 id="Math-formula"><a href="#Math-formula" class="headerlink" title="Math formula"></a>Math formula</h1><p>There are some bugs in plugin <code>hexo-renderer-marked</code> which lead the math formula to be wrongly interpreted.</p>
<p>The solution is to revise <code>/node_modules/marked/lib/marked.cjs</code>.</p>
<ul>
<li><p>Change all the lines starting with <code>escape:</code> to:</p>
<figure class="highlight cjs"><table><tr><td class="code"><pre><span class="line"><span class="attr">escape</span>: <span class="regexp">/^\\([`*\[\]()#$+\-.!_&gt;])/</span>,</span><br></pre></td></tr></table></figure>
</li>
<li><p>Change all the lines starting with <code>em:</code> to:</p>
<figure class="highlight cjs"><table><tr><td class="code"><pre><span class="line"><span class="attr">em</span>: <span class="regexp">/^\*((?:\*\*|[\s\S])+?)\*(?!\*)/</span>,</span><br></pre></td></tr></table></figure></li>
</ul>
<p>Then, enable <code>mathjax</code> in <code>/themes/next/_config.yml</code>, that is, set <code>enable</code> in <code>math</code> to <code>true</code>.</p>
<p>At last, add <code>mathjax: true</code> in <code>/scaffolds/post.md</code> (<strong>the posts created before need you to add this manually</strong>).</p>
<p>However, there are still some bugs on the website.</p>
<ol>
<li>The plugin may take <code>\\</code> as <code>\</code>, <code>\;</code> as <code>;</code>. To solve this, we just need to add one <code>\</code> before <code>\</code>;</li>
<li><del>Two <code>&#123;&#123;&#125;&#125;</code> need a <em>space</em> to distinguish.</del> (I do not know what this means...)</li>
</ol>
<h1 id="Tag-cloud"><a href="#Tag-cloud" class="headerlink" title="Tag cloud"></a>Tag cloud</h1><p><img src="/2023/04/10/HexoConfiguration/2.png" alt="2"></p>
<p><a href="https://github.com/D0n9X1n/hexo-tag-cloud">tag cloud</a></p>
<p>Some problems:</p>
<ol>
<li>When using NexT, we should insert the code before <code>back2top.sidebar</code>;</li>
<li>It is advisable to just download the package and put it in <code>/node_modules</code>. Then add <code>&quot;hexo-tag-cloud&quot;: &quot;^2.1.*&quot;</code> to package.json.</li>
</ol>
<h1 id="Colorful-tag"><a href="#Colorful-tag" class="headerlink" title="Colorful tag"></a>Colorful tag</h1><p><img src="/2023/04/10/HexoConfiguration/3.png" alt="3"></p>
<p>Create <code>tag-color.swig</code> in <code>/themes/next/layout</code>.</p>
<p>Add code:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;script type=&quot;text/javascript&quot;&gt;</span><br><span class="line">     var alltags = document.getElementsByClassName(&#x27;tag-cloud-tags&#x27;);</span><br><span class="line">     var tags = alltags[0].getElementsByTagName(&#x27;a&#x27;);</span><br><span class="line">     for (var i = tags.length - 1; i &gt;= 0; i--) &#123;</span><br><span class="line">       var r=Math.floor(Math.random()*75+130);</span><br><span class="line">       var g=Math.floor(Math.random()*75+100);</span><br><span class="line">       var b=Math.floor(Math.random()*75+80);</span><br><span class="line">       tags[i].style.background = &quot;rgb(&quot;+r+&quot;,&quot;+g+&quot;,&quot;+b+&quot;)&quot;;</span><br><span class="line">     &#125;</span><br><span class="line">&lt;/script&gt;</span><br><span class="line"></span><br><span class="line">&lt;style&gt;</span><br><span class="line">  .tag-cloud-tags&#123;</span><br><span class="line">    /*font-family: Helvetica, Tahoma, Arial;*/</span><br><span class="line">    font-family: Arial;</span><br><span class="line">    font-weight: 200;</span><br><span class="line">    text-align: center;</span><br><span class="line">    counter-reset: tags;</span><br><span class="line">  &#125;</span><br><span class="line">  .tag-cloud-tags a&#123;</span><br><span class="line">    box-shadow: 0 1px 3px #6f42c1, 0 1px 2px #d9534f; </span><br><span class="line">    padding: 2px 10px; </span><br><span class="line">    margin: 8px; </span><br><span class="line">    border-bottom: none; </span><br><span class="line">    border-radius: 20px;</span><br><span class="line">  &#125;</span><br><span class="line">  .tag-cloud-tags a:before&#123;</span><br><span class="line">    // content: &quot;?&quot;;  When enable this,there will be &#x27;content&#x27; before each tag</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  .tag-cloud-tags a:hover&#123;</span><br><span class="line">     box-shadow: 0px 5px 15px 0px rgba(0,0,0,.4);</span><br><span class="line">     transform: scale(1.1);</span><br><span class="line">     /*box-shadow: 10px 10px 15px 2px rgba(0,0,0,.12), 0 0 6px 0 rgba(104, 104, 105, 0.1);*/</span><br><span class="line">     transition-duration: 0.15s;</span><br><span class="line">  &#125;</span><br><span class="line">&lt;/style&gt;</span><br></pre></td></tr></table></figure>

<p>Call this script: add <code>&#123;% include 'tag-color.swig' %&#125;</code> to the bottom of <code>&#123;%- if page.type === 'tags' %&#125;</code> in <code>/themes/next/layout/page.swig</code>, or, just add the <strong>script code</strong> (if add code, we do not need the <code>tag-color.swig</code> file).</p>
<p>This is actually a <strong>script code</strong>, so we can change the configuration by ourselves.</p>
<h1 id="Colorful-bottom-tag"><a href="#Colorful-bottom-tag" class="headerlink" title="Colorful bottom tag"></a>Colorful bottom tag</h1><p><img src="/2023/04/10/HexoConfiguration/4.png" alt="4"></p>
<p>Add <em>script code</em></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;script type=&quot;text/javascript&quot;&gt;</span><br><span class="line">var tagsall=document.getElementsByClassName(&quot;post-tags&quot;)</span><br><span class="line">for (var i = tagsall.length - 1; i &gt;= 0; i--)&#123;</span><br><span class="line">  var tags=tagsall[i].getElementsByTagName(&quot;a&quot;);</span><br><span class="line">  for (var j = tags.length - 1; j &gt;= 0; j--) &#123;</span><br><span class="line">      var r=Math.floor(Math.random()*75+130);</span><br><span class="line">      var g=Math.floor(Math.random()*75+100);</span><br><span class="line">      var b=Math.floor(Math.random()*75+80);</span><br><span class="line">      tags[j].style.background = &quot;rgb(&quot;+r+&quot;,&quot;+g+&quot;,&quot;+b+&quot;)&quot;;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;                        </span><br><span class="line">&lt;/script&gt;</span><br></pre></td></tr></table></figure>
<p>to the bottom of <code>&lt;div class=&quot;post-tags&quot;&gt;</code> in <code>/themes/next/layout/post.swig</code> or <code>/themes/next/layout/_macro/post.swig</code>.</p>
<p>Then we get colorful bottom tags.</p>
<p>Add</p>
<figure class="highlight styl"><table><tr><td class="code"><pre><span class="line"><span class="selector-class">.posts-expand</span> <span class="selector-class">.post-tags</span> a&#123;</span><br><span class="line">  <span class="attribute">display</span>: inline-block;</span><br><span class="line">  <span class="attribute">font-size</span>: <span class="number">0.8em</span>;</span><br><span class="line">  <span class="attribute">padding</span>: <span class="number">0px</span> <span class="number">10px</span>;</span><br><span class="line">  <span class="attribute">border-radius</span>: <span class="number">8px</span>;</span><br><span class="line">  <span class="attribute">color</span>: <span class="number">#fff</span>;</span><br><span class="line">  <span class="attribute">border</span>: <span class="number">0px</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>in <code>/source/_data/styles.styl</code> (if you do not have this file, just create one and enable <code>style: source/_data/styles.styl</code> in <code>/themes/next/_config.yml</code>).</p>
<p>Then we get bottom tags with border so that we can add color.</p>
<h1 id="Read-more-and-description"><a href="#Read-more-and-description" class="headerlink" title="Read more and description"></a>Read more and description</h1><ol>
<li><p>Enable <code>read_more_btn</code> and <code>excerpt_description</code> in <code>/themes/next/_config.yml</code>;</p>
</li>
<li><p>Add <code>description:</code> in <code>/scaffolds/post.md</code>;</p>
</li>
<li><p>Add your excerpt behind <code>description</code> and <code>&lt;!-- more --&gt;</code> behind the metadata.</p>
</li>
</ol>
<h1 id="Blog-background"><a href="#Blog-background" class="headerlink" title="Blog background"></a>Blog background</h1><p>Add </p>
<figure class="highlight styl"><table><tr><td class="code"><pre><span class="line"><span class="selector-tag">body</span> &#123; </span><br><span class="line">  <span class="attribute">background-image</span>: <span class="built_in">url</span>(/images/bg.png);  <span class="comment">// the path of image in your blog</span></span><br><span class="line">  <span class="attribute">background-repeat</span>: no-repeat; </span><br><span class="line">  <span class="attribute">background-attachment</span>: fixed;  <span class="comment">// does not scroll with the page </span></span><br><span class="line">  <span class="attribute">background-position</span>: <span class="number">50%</span> <span class="number">50%</span>;  <span class="comment">// center </span></span><br><span class="line">  <span class="attribute">background-size</span>: <span class="number">100%</span> <span class="number">100%</span>;</span><br><span class="line">  <span class="attribute">color</span>: <span class="number">#000</span>; <span class="comment">// color of body font</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>in <code>styles.styl</code> (if you do not have this file, just create one and enable <code>style: source/_data/styles.styl</code> in <code>/themes/next/_config.yml</code>).</p>
<h1 id="Transparency-of-blog"><a href="#Transparency-of-blog" class="headerlink" title="Transparency of blog"></a>Transparency of blog</h1><p>Add</p>
<figure class="highlight styl"><table><tr><td class="code"><pre><span class="line"><span class="comment">// transparency of the content of your post</span></span><br><span class="line"><span class="selector-class">.content-wrap</span> &#123;</span><br><span class="line">   <span class="attribute">opacity</span>: <span class="number">0.95</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// transparency of search</span></span><br><span class="line"><span class="comment">//.popup &#123;</span></span><br><span class="line"><span class="comment">//  opacity: 0.8;</span></span><br><span class="line"><span class="comment">//&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// submenu,post,comments,pagination</span></span><br><span class="line"><span class="selector-class">.main-inner</span> &gt; <span class="selector-class">.sub-menu</span>, <span class="selector-class">.main-inner</span> &gt; <span class="selector-class">.post-block</span>, <span class="selector-class">.main-inner</span> &gt; <span class="selector-class">.tabs-comment</span>,</span><br><span class="line"><span class="selector-class">.main-inner</span> &gt; <span class="selector-class">.comments</span>, <span class="selector-class">.main-inner</span> &gt; <span class="selector-class">.pagination</span> &#123;</span><br><span class="line">	<span class="attribute">background</span>: <span class="built_in">rgba</span>(<span class="number">255</span>,<span class="number">255</span>,<span class="number">255</span>,<span class="number">0.85</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// side bar</span></span><br><span class="line"><span class="selector-class">.sidebar</span> &#123;</span><br><span class="line">  <span class="attribute">opacity</span>: <span class="number">0.85</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// color and transparency of background of menu</span></span><br><span class="line"><span class="selector-class">.header-inner</span> &#123;</span><br><span class="line">  <span class="attribute">background</span>: <span class="built_in">rgba</span>(<span class="number">255</span>,<span class="number">255</span>,<span class="number">255</span>,<span class="number">0.85</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// color of title (the default color is grey)</span></span><br><span class="line"><span class="selector-class">.posts-expand</span> <span class="selector-class">.post-title-link</span> &#123;</span><br><span class="line">  <span class="attribute">color</span>: <span class="number">#000</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// color of data under title</span></span><br><span class="line"><span class="selector-class">.posts-expand</span> <span class="selector-class">.post-meta-container</span> &#123;</span><br><span class="line">  <span class="attribute">color</span>: <span class="number">#000</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>in <code>styles.styl</code> (if you do not have this file, just create one and enable <code>style: source/_data/styles.styl</code> in <code>/themes/next/_config.yml</code>).</p>
<h1 id="Catalog"><a href="#Catalog" class="headerlink" title="Catalog"></a>Catalog</h1><p>All the configurations of <code>catalog </code> are in <code>toc:</code> of <code>/themes/next/_config.yml</code>.</p>
<figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="attr">toc:</span></span><br><span class="line"> <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line"> <span class="comment"># Automatically add list number to toc.</span></span><br><span class="line"> <span class="attr">number:</span> <span class="literal">true</span></span><br><span class="line"> <span class="comment"># If true, all words will placed on next lines if header width longer then sidebar width.</span></span><br><span class="line"> <span class="attr">wrap:</span> <span class="literal">true</span></span><br><span class="line"> <span class="comment"># If true, all level of TOC in a post will be displayed, rather than the activated part of it.</span></span><br><span class="line"> <span class="attr">expand_all:</span> <span class="literal">true</span></span><br><span class="line"> <span class="comment"># Maximum heading depth of generated toc.</span></span><br><span class="line"> <span class="attr">max_depth:</span> <span class="number">6</span></span><br></pre></td></tr></table></figure>

<p>If we want to display the catalog selectively, we just need to edit <code>/themes/next/layout/_macro/sidebar.swig</code>:</p>
<p>change :</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;%- set display_toc = page.toc.enable and display_toc %&#125;</span><br></pre></td></tr></table></figure>
<p>to:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;%- set display_toc = page.toc.enable and page.etoc and display_toc %&#125;</span><br></pre></td></tr></table></figure>
<p>Then, if we want to display calalog, we just need to add <code>etoc: true</code> in the metadata of the post (<code>false</code> if we do not want). It is recommended to add <code>etoc: true</code> in <code>/scaffolds/post.md</code>.</p>
<h1 id="Rounded-corners"><a href="#Rounded-corners" class="headerlink" title="Rounded corners"></a>Rounded corners</h1><p><img src="/2023/04/10/HexoConfiguration/5.png" alt="5"></p>
<p>Enable <code>variable: source/_data/variables.styl</code> in <code>/themes/next/_config.yml</code>. Create <code>/source/_data/variables.styl</code> and add code:</p>
<figure class="highlight styl"><table><tr><td class="code"><pre><span class="line"><span class="variable">$border</span>-radius-inner     = <span class="number">20px</span> <span class="number">20px</span> <span class="number">20px</span> <span class="number">20px</span>;</span><br><span class="line"><span class="variable">$border</span>-radius           = <span class="number">20px</span>;</span><br></pre></td></tr></table></figure>
<p>However, there are still some bugs on the sidebar:</p>
<p><img src="/2023/04/10/HexoConfiguration/6.png" alt="6"></p>
<p>This is caused by the default body background which covers the rounded corners. To solve this, we just need to set <code>$body-bg-color</code> in <code>/themes/next/source/css/_variables/Gemini.styl</code> to <code>transparent</code>:</p>
<figure class="highlight styl"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Variables of Gemini scheme</span></span><br><span class="line"><span class="comment">// ==================================================</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">@import</span> <span class="string">&quot;Pisces.styl&quot;</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Settings for some of the most global styles.</span></span><br><span class="line"><span class="comment">// --------------------------------------------------</span></span><br><span class="line"><span class="comment">// $body-bg-color           = #eee;</span></span><br><span class="line"><span class="variable">$body</span>-bg-<span class="attribute">color</span>           = transparent;</span><br></pre></td></tr></table></figure>
<h1 id="Background-of-menu"><a href="#Background-of-menu" class="headerlink" title="Background of menu"></a>Background of menu</h1><p>Configure <code>/themes/next/source/css/_schemes/Pisces/_header.styl</code>, add your favorite bg in <code>background</code>.</p>
<figure class="highlight styl"><table><tr><td class="code"><pre><span class="line"><span class="selector-class">.site-brand-container</span> &#123;</span><br><span class="line">  <span class="comment">// background: $black-deep;</span></span><br><span class="line">  <span class="attribute">background</span>: <span class="built_in">url</span>(/uploads/bluemoon.png)</span><br><span class="line">  <span class="comment">// background: url(/uploads/space.png)</span></span><br><span class="line">  <span class="comment">// background: url(/uploads/walkonmoon.png)</span></span><br><span class="line">  +<span class="built_in">tablet-mobile</span>() &#123;</span><br><span class="line">    <span class="attribute">box-shadow</span>: <span class="number">0</span> <span class="number">0</span> <span class="number">16px</span> <span class="built_in">rgba</span>(<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, .<span class="number">5</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="attribute">background-size</span>: cover;  <span class="comment">// it is advisable to enable this</span></span><br><span class="line">  <span class="comment">//background-size: 50% 100%;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>If you do not like your logo on the menu, you can just disable it in your theme configuration file:</p>
<figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="attr">custom_logo:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure>

<h1 id="Top-offset-of-post"><a href="#Top-offset-of-post" class="headerlink" title="Top offset of post"></a>Top offset of post</h1><p><img src="/2023/04/10/HexoConfiguration/7.png" alt="7"></p>
<p>Configure <code>index.styl</code> in the specific scheme folder you choose in <code>/themes/next/source/css/_schemes</code>:</p>
<figure class="highlight styl"><table><tr><td class="code"><pre><span class="line"><span class="selector-class">.content-wrap</span> &#123;</span><br><span class="line">  <span class="attribute">background</span>: initial;</span><br><span class="line">  <span class="attribute">box-shadow</span>: initial;</span><br><span class="line">  <span class="attribute">padding</span>: initial;</span><br><span class="line">  <span class="attribute">padding-top</span>: <span class="number">45px</span>;  <span class="comment">// top offset of your post</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="Fork-me-on-github"><a href="#Fork-me-on-github" class="headerlink" title="Fork me on github"></a>Fork me on github</h1><p>A github icon in the upper right corner:</p>
<p><img src="/2023/04/10/HexoConfiguration/8.png" alt="8"></p>
<ol>
<li>Find the icon you like on <a href="https://tholman.com/github-corners/">GitHub Corners</a> or <a href="https://github.blog/2008-12-19-github-ribbons/">GitHub Ribbons</a>;</li>
<li>Copy the code to <code>/themes/next/layout/_layout.swig</code> and put it behind <code>&lt;div class=&quot;headband&quot;&gt;&lt;/div&gt;</code>;</li>
<li>Change the value of <code>href</code> to your gihub homepage, like:</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;a href=&quot;https://github.com/zclzcl0223&quot; class=&quot;github-corner&quot; aria-label=&quot;View source on GitHub&quot;&gt;&lt;svg width=&quot;80&quot; height=&quot;80&quot; viewBox=&quot;0 0 250 250&quot; style=&quot;fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;&quot; aria-hidden=&quot;true&quot;&gt;&lt;path d=&quot;M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z&quot;&gt;&lt;/path&gt;&lt;path d=&quot;M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2&quot; fill=&quot;currentColor&quot; style=&quot;transform-origin: 130px 106px;&quot; class=&quot;octo-arm&quot;&gt;&lt;/path&gt;&lt;path d=&quot;M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z&quot; fill=&quot;currentColor&quot; class=&quot;octo-body&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;style&gt;.github-corner:hover .octo-arm&#123;animation:octocat-wave 560ms ease-in-out&#125;@keyframes octocat-wave&#123;0%,100%&#123;transform:rotate(0)&#125;20%,60%&#123;transform:rotate(-25deg)&#125;40%,80%&#123;transform:rotate(10deg)&#125;&#125;@media (max-width:500px)&#123;.github-corner:hover .octo-arm&#123;animation:none&#125;.github-corner .octo-arm&#123;animation:octocat-wave 560ms ease-in-out&#125;&#125;&lt;/style&gt;</span><br></pre></td></tr></table></figure>

<h1 id="Dynamic-background"><a href="#Dynamic-background" class="headerlink" title="Dynamic background"></a>Dynamic background</h1><p><img src="/2023/04/10/HexoConfiguration/9.png" alt="9"></p>
<p>Enable <code>footer: source/_data/footer.swig</code> in your theme configuration file. Create <code>footer.swig</code> under <code>_data</code> and add code:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;script color=&quot;0,0,255&quot; opacity=&quot;0.5&quot; zIndex=&quot;-1&quot; count=&quot;99&quot; src=&quot;https://cdn.jsdelivr.net/npm/canvas-nest.js@1/dist/canvas-nest.js&quot;&gt;&lt;/script&gt;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>You can modify <code>color</code>, <code>opacity</code>, <code>zIndex</code> and <code>count</code> as you like.</p>
</blockquote>
<h1 id="Picture-viewer"><a href="#Picture-viewer" class="headerlink" title="Picture viewer"></a>Picture viewer</h1><p>There are two types of picture viewers in <code>/themes/next/_config_yml</code>, fancybox and mediumzoom. Choose the one you like and enable it.</p>
<p>For mediumzoom, I don&#39;t like its overlay. I wanna make it slightly more transparent. Therefore, I download its package from:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">https://www.jsdelivr.com/package/npm/medium-zoom. </span><br></pre></td></tr></table></figure>
<p>Extract <code>/package/dist/medium-zoom.js</code> and modify all</p>
<figure class="highlight js"><table><tr><td class="code"><pre><span class="line">.<span class="property">medium</span>-zoom-overlay &#123;</span><br><span class="line">  <span class="attr">opacity</span>: <span class="number">0</span> <span class="comment">/* 0 means opaque; 1 means completely transparent*/</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Actually, just modify the last one is ok. Then push this file to one of your github repo and migrate its url from github to jsDelivr on <a href="https://www.jsdelivr.com/github">https://www.jsdelivr.com/github</a>, for example:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># github</span></span><br><span class="line">https://github.com/zclzcl0223/my_medium-zoom/blob/master/medium-zoom-v7.js</span><br><span class="line"><span class="comment"># jsDelivr</span></span><br><span class="line">https://cdn.jsdelivr.net/gh/zclzcl0223/my_medium-zoom@master/medium-zoom-v7.js</span><br></pre></td></tr></table></figure>
<p>Set your jsDelivr url as your vendor of mediumzoom in <code>/themes/next/_config.yml mediumzoom:</code>. The effect is as follows (<code>opacity: 0.5</code>):</p>
<p><img src="/2023/04/10/HexoConfiguration/10.png" alt="10"></p>
<h1 id="Related-posts"><a href="#Related-posts" class="headerlink" title="Related posts"></a>Related posts</h1><p>Add related posts at the bottom of each post based on tag relevance and creation time. First, download dependencies:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm install hexo-related-popular-posts --save</span><br></pre></td></tr></table></figure>
<p>Just ignoring the warnings is ok. Then, set <code>related_posts</code> to <code>true</code> in <code>/themes/next/_config.yml</code>. The effect is as follows:</p>
<p><img src="/2023/04/10/HexoConfiguration/11.png" alt="11"></p>
<h1 id="Running-time-of-site"><a href="#Running-time-of-site" class="headerlink" title="Running time of site"></a>Running time of site</h1><p>Add:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;!-- &lt;br /&gt; --&gt;</span><br><span class="line">&lt;!-- 网站运行时间的设置 --&gt;</span><br><span class="line">&lt;span id=&quot;timeDate&quot;&gt;载入天数...&lt;/span&gt;</span><br><span class="line">&lt;!-- &lt;span id=&quot;times&quot;&gt;载入时分秒...&lt;/span&gt; --&gt;</span><br><span class="line">&lt;script&gt;</span><br><span class="line">    var now = new Date();</span><br><span class="line">    function createtime() &#123;</span><br><span class="line">        var grt= new Date(&quot;11/17/2022 8:00:00&quot;);//此处修改你的建站时间或者网站上线时间</span><br><span class="line">        now.setTime(now.getTime()+250);</span><br><span class="line">        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);</span><br><span class="line">        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);</span><br><span class="line">        if(String(hnum).length ==1 )&#123;hnum = &quot;0&quot; + hnum;&#125; minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);</span><br><span class="line">        mnum = Math.floor(minutes); if(String(mnum).length ==1 )&#123;mnum = &quot;0&quot; + mnum;&#125;</span><br><span class="line">        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);</span><br><span class="line">        snum = Math.round(seconds); </span><br><span class="line">        if(String(snum).length ==1 )&#123;snum = &quot;0&quot; + snum;&#125;</span><br><span class="line">        // var times = document.getElementById(&quot;times&quot;).innerHTML = hnum + &quot; 小时 &quot; + mnum + &quot; 分 &quot; + snum + &quot; 秒&quot;;</span><br><span class="line">        document.getElementById(&quot;timeDate&quot;).innerHTML = &quot;本站已安全运行 &quot;+dnum+&quot; 天 &quot;+hnum + &quot; 小时 &quot; + mnum + &quot; 分 &quot; + snum + &quot; 秒&quot;;</span><br><span class="line">    &#125;</span><br><span class="line">setInterval(&quot;createtime()&quot;,250);</span><br><span class="line">&lt;/script&gt;</span><br></pre></td></tr></table></figure>
<p>to <code>/themes/next/layout/_partials/footer.swig</code>.</p>
<h1 id="Icon-on-footer"><a href="#Icon-on-footer" class="headerlink" title="Icon on footer"></a>Icon on footer</h1><p><img src="/2023/04/10/HexoConfiguration/12.png" alt="12"></p>
<p>Modify the following code in <code>/themes/next/_config.yml</code>:</p>
<figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="attr">footer:</span></span><br><span class="line">   <span class="attr">icon:</span></span><br><span class="line">     <span class="comment"># name: fa fa-heart</span></span><br><span class="line">     <span class="attr">name:</span> <span class="string">fas</span> <span class="string">fa-star-of-david</span></span><br><span class="line">     <span class="comment"># If you want to animate the icon, set it to true.</span></span><br><span class="line">     <span class="attr">animated:</span> <span class="literal">false</span></span><br><span class="line">     <span class="comment"># Change the color of icon, using Hex Code.</span></span><br><span class="line">     <span class="attr">color:</span> <span class="string">&quot;#B0E0E6&quot;</span></span><br></pre></td></tr></table></figure>
<p>Search for your favorite icon in <a href="https://fontawesome.com/v5/search">https://fontawesome.com/v5/search</a> and modify <code>name: xx</code> (remember to use v5.15.4).</p>
<h1 id="Font-size-line-amp-paragraph-spacing"><a href="#Font-size-line-amp-paragraph-spacing" class="headerlink" title="Font size, line &amp; paragraph spacing"></a>Font size, line &amp; paragraph spacing</h1><p>All these configurations are in <code>/themes/next/source/css/_variables/base.styl</code>:</p>
<figure class="highlight styl"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Font size</span></span><br><span class="line"><span class="variable">$font</span>-size-base           = (<span class="built_in">hexo-config</span>(<span class="string">&#x27;font.enable&#x27;</span>) and <span class="built_in">hexo-config</span>(<span class="string">&#x27;font.global.size&#x27;</span>) is <span class="selector-tag">a</span> <span class="string">&#x27;unit&#x27;</span>) ? <span class="built_in">unit</span>(<span class="built_in">hexo-config</span>(<span class="string">&#x27;font.global.size&#x27;</span>), em) : <span class="number">1em</span>;</span><br><span class="line"><span class="variable">$font</span>-size-smallest       = .<span class="number">75em</span>;</span><br><span class="line"><span class="variable">$font</span>-size-smaller        = .<span class="number">8125em</span>;</span><br><span class="line"><span class="variable">$font</span>-size-small          = .<span class="number">875em</span>;</span><br><span class="line"><span class="variable">$font</span>-size-medium         = <span class="number">1em</span>;</span><br><span class="line"><span class="variable">$font</span>-size-large          = <span class="number">1.125em</span>;</span><br><span class="line"><span class="variable">$font</span>-size-larger         = <span class="number">1.25em</span>;</span><br><span class="line"><span class="variable">$font</span>-size-largest        = <span class="number">1.5em</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// Headings font size</span></span><br><span class="line"><span class="variable">$font</span>-size-headings-step  = .<span class="number">15em</span>;  <span class="comment">// discounts for all levels</span></span><br><span class="line"><span class="variable">$font</span>-size-headings-base  = (<span class="built_in">hexo-config</span>(<span class="string">&#x27;font.enable&#x27;</span>) and <span class="built_in">hexo-config</span>(<span class="string">&#x27;font.headings.size&#x27;</span>) is <span class="selector-tag">a</span> <span class="string">&#x27;unit&#x27;</span>) ? <span class="built_in">unit</span>(<span class="built_in">hexo-config</span>(<span class="string">&#x27;font.headings.size&#x27;</span>), em) : <span class="number">1.75em</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// Global line height</span></span><br><span class="line"><span class="variable">$line</span>-<span class="attribute">height</span>-base         = <span class="number">1.5</span>;  <span class="comment">// line spacing</span></span><br><span class="line"><span class="variable">$line</span>-<span class="attribute">height</span>-code-block   = <span class="number">1.5</span>; <span class="comment">// Can&#x27;t be less than 1.3;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// paragrah spacing</span></span><br><span class="line"><span class="selector-class">.post-block</span> <span class="selector-tag">p</span> &#123;</span><br><span class="line">  <span class="attribute">margin-bottom</span>: <span class="number">5px</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="Local-search"><a href="#Local-search" class="headerlink" title="Local search"></a>Local search</h1><p>Install:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm install hexo-generator-searchdb --save</span><br></pre></td></tr></table></figure>

<p>In <code>/themes/next/_config.yml</code> enable <code>local_search</code>.</p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul>
<li><a href="https://github.com/theme-next/hexo-theme-next">NexT (old version)</a></li>
<li><a href="https://wangwei1237.github.io/2020/02/05/handle-the-bug-of-hexo-asset-image-plugin/">解决hexo-asset-image的图片地址错误问题</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/476601594">Hexo 发布博客的图片和公式问题</a></li>
<li><a href="https://juejin.cn/post/7169115268944560135">Hexo-Next主题搭建个人博客最新配置教程</a></li>
<li><a href="https://blog.csdn.net/mqdxiaoxiao/article/details/93796367">Hexo博客NexT主题右上角添加fork me on github入口</a></li>
<li><a href="https://github.com/theme-next/theme-next-canvas-nest">canvas_nest</a></li>
</ul>
]]></content>
      <categories>
        <category>Configuration</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Configuration</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo</title>
    <url>/2023/04/04/Hexo/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Hexo is a static website builder based on <strong>nodejs</strong> and markdown which supports remote deployment and theme customization.</p>
<h1 id="Hexo-Installation"><a href="#Hexo-Installation" class="headerlink" title="Hexo Installation"></a>Hexo Installation</h1><h2 id="Node-js"><a href="#Node-js" class="headerlink" title="Node.js"></a>Node.js</h2><p>The installation program,including <strong>node and npm</strong>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">$ node -v  <span class="comment"># check the version of node</span></span><br><span class="line">$ npm -v  <span class="comment"># check the version of npm</span></span><br></pre></td></tr></table></figure>
<p>The step of installation:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">$ npm install -g cnpm --registry=http://registry.npm.taobao.org  <span class="comment"># install the cnpm manager of taobao</span></span><br><span class="line">$ cnmp install -g hexo-cli  <span class="comment"># install the scheme of hexo</span></span><br></pre></td></tr></table></figure>

<h2 id="Git"><a href="#Git" class="headerlink" title="Git"></a>Git</h2><p>In windows, all the command line commands should be run in <strong>Git bash</strong>. Github can serve as the remoete blog of our local hexo blog:</p>
<ul>
<li><p>Create a new repositry in github,whose name is <u>YourGithubName.github.io</u>;</p>
</li>
<li><p>Install the git deployment plugin;</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">$ cnpm install --save hexo-deployer-git</span><br></pre></td></tr></table></figure>
</li>
<li><p>Configure <strong>_config.yml</strong>.</p>
<figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="attr">deploy:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">git</span></span><br><span class="line">  <span class="attr">repo:</span> <span class="string">https://github.com/YourGithubName/YourGithubName.github.io.git</span></span><br></pre></td></tr></table></figure></li>
<li><p>Visit remote blog through <a href="https://yourgithubname.github.io/">https://YourGithubName.github.io/</a>.</p>
</li>
</ul>
<h1 id="Hexo-theme"><a href="#Hexo-theme" class="headerlink" title="Hexo theme"></a>Hexo theme</h1><p>Download the specific theme from github:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">$ git clone https://github.com/themename.git themes/themename</span><br></pre></td></tr></table></figure>

<p>Configure <strong>_config.yml</strong>:</p>
<figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="attr">theme:</span> <span class="string">themename</span></span><br></pre></td></tr></table></figure>

<p>Then, configure the theme with the help of its manual.</p>
<h1 id="Hexo-Command"><a href="#Hexo-Command" class="headerlink" title="Hexo Command"></a>Hexo Command</h1><p>Create a folder in computer and use this folder as the <strong>root</strong> of our blog.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ hexo init [folder]</span><br></pre></td></tr></table></figure>

<p>Create a new blog:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ hexo new &lt;title&gt;</span><br></pre></td></tr></table></figure>

<p>Create categories or tags:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ hexo new page categories</span><br><span class="line">$ hexo new page tags</span><br></pre></td></tr></table></figure>

<p>Once we edit our blog,we should:</p>
<ul>
<li><p>Clean cache files.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ hexo clean</span><br></pre></td></tr></table></figure>
</li>
<li><p>Generate new public blog. That is the content we use to build our website.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ hexo g</span><br></pre></td></tr></table></figure></li>
</ul>
<p>Start our server:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ hexo s</span><br></pre></td></tr></table></figure>

<p>Deploy our blog to remote site:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ hexo d</span><br></pre></td></tr></table></figure>
<p>All the file created in command line will be placed in <strong>&#x2F;source</strong>.</p>
<h1 id="Local-article-link"><a href="#Local-article-link" class="headerlink" title="Local article link"></a>Local article link</h1><p>Jump to the specific local article:</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line">&#123;% post_link &#x27;YourPostName&#x27; LinkName %&#125;  // the default linkname is the name of article</span><br><span class="line">or</span><br><span class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;&#123;% post_path &#x27;YourPostName&#x27; %&#125;&quot;</span>&gt;</span>LinkName<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>Jump to the specific catalog of local article:</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line">// Use &#x27;#&#x27; for all levels of catalog</span><br><span class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;&#123;% post_path &#x27;YourPostName&#x27; %&#125;#CatalogName&quot;</span>&gt;</span>LinkName<span class="tag">&lt;/<span class="name">a</span>&gt;</span> </span><br></pre></td></tr></table></figure>
<p>You can also use:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[LinkName](&#123;% post_path &#x27;YourPostName&#x27; %&#125;#CatalogName)</span><br></pre></td></tr></table></figure>
<p>But when you configure the path of your local image like this: <a href="/2023/04/10/HexoConfiguration/#Image">Image</a>, this method will fail.</p>
<blockquote>
<p>If your <code>CatalogName</code> has space, you should replace the space to <code>-</code>. The same to <code>YourPostName</code> if you do not use <code>&#39;&#39;</code>.</p>
</blockquote>
<h1 id="Custom-article-priority"><a href="#Custom-article-priority" class="headerlink" title="Custom article priority"></a>Custom article priority</h1><p>See <a href="https://blog.csdn.net/jziwjxjd/article/details/107836486">Hexo设置文章的优先级别</a>.</p>
<h1 id="Some-bugs"><a href="#Some-bugs" class="headerlink" title="Some bugs"></a>Some bugs</h1><h2 id="Only-show-Chinese-39"><a href="#Only-show-Chinese-39" class="headerlink" title="Only show Chinese &#39;"></a>Only show Chinese &#39;</h2><p>This is a setting bug of marked renderer, turning off <code>smartypants</code> can solve it:</p>
<figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="attr">marked:</span></span><br><span class="line">  <span class="attr">smartypants:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure>

<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://hexo.io/zh-cn/docs/">HexoDocs</a></p>
]]></content>
      <categories>
        <category>Tool</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>LateX</title>
    <url>/2023/04/12/LateX/</url>
    <content><![CDATA[<span id="more"></span>

<h1 id="Superscript-x-1"><a href="#Superscript-x-1" class="headerlink" title="Superscript $x^1$"></a>Superscript $x^1$</h1><figure class="highlight latex"><table><tr><td class="code"><pre><span class="line">x<span class="built_in">^</span>1</span><br></pre></td></tr></table></figure>

<h1 id="Subscript-x-1"><a href="#Subscript-x-1" class="headerlink" title="Subscript $x_1$"></a>Subscript $x_1$</h1><figure class="highlight latex"><table><tr><td class="code"><pre><span class="line">x<span class="built_in">_</span>1</span><br></pre></td></tr></table></figure>

<h1 id="Hat-widehat-y"><a href="#Hat-widehat-y" class="headerlink" title="Hat $\widehat{y}$"></a>Hat $\widehat{y}$</h1><figure class="highlight latex"><table><tr><td class="code"><pre><span class="line"><span class="keyword">\widehat</span>&#123;y&#125;</span><br></pre></td></tr></table></figure>
<h1 id="Bar-bar-y"><a href="#Bar-bar-y" class="headerlink" title="Bar $\bar{y}$"></a>Bar $\bar{y}$</h1><figure class="highlight latex"><table><tr><td class="code"><pre><span class="line"><span class="keyword">\bar</span>&#123;y&#125;</span><br></pre></td></tr></table></figure>

<h1 id="Ensemble-delta-1"><a href="#Ensemble-delta-1" class="headerlink" title="Ensemble ${delta}^1$"></a>Ensemble ${delta}^1$</h1><figure class="highlight latex"><table><tr><td class="code"><pre><span class="line">&#123;delta&#125;<span class="built_in">^</span>1</span><br></pre></td></tr></table></figure>

<h1 id="Fraction-frac-1-n"><a href="#Fraction-frac-1-n" class="headerlink" title="Fraction $\frac{1}{n}$"></a>Fraction $\frac{1}{n}$</h1><figure class="highlight latex"><table><tr><td class="code"><pre><span class="line"><span class="keyword">\frac</span>&#123;1&#125;&#123;n&#125;</span><br></pre></td></tr></table></figure>

<h1 id="Aproximately-equal-approx"><a href="#Aproximately-equal-approx" class="headerlink" title="Aproximately equal $\approx$"></a>Aproximately equal $\approx$</h1><figure class="highlight latex"><table><tr><td class="code"><pre><span class="line"><span class="keyword">\approx</span></span><br></pre></td></tr></table></figure>

<h1 id="Time-times"><a href="#Time-times" class="headerlink" title="Time $\times$"></a>Time $\times$</h1><figure class="highlight latex"><table><tr><td class="code"><pre><span class="line"><span class="keyword">\times</span></span><br></pre></td></tr></table></figure>

<h1 id="Dot-product-cdot"><a href="#Dot-product-cdot" class="headerlink" title="Dot product $\cdot$"></a>Dot product $\cdot$</h1><figure class="highlight latex"><table><tr><td class="code"><pre><span class="line"><span class="keyword">\cdot</span></span><br></pre></td></tr></table></figure>

<h1 id="Belong-to-in"><a href="#Belong-to-in" class="headerlink" title="Belong to $\in$"></a>Belong to $\in$</h1><figure class="highlight latex"><table><tr><td class="code"><pre><span class="line"><span class="keyword">\in</span></span><br></pre></td></tr></table></figure>

<h1 id="Greek-alphabet-sigma-alpha"><a href="#Greek-alphabet-sigma-alpha" class="headerlink" title="Greek alphabet $\sigma$ $\alpha$"></a>Greek alphabet $\sigma$ $\alpha$</h1><figure class="highlight latex"><table><tr><td class="code"><pre><span class="line"><span class="keyword">\sigma</span> <span class="keyword">\alpha</span></span><br></pre></td></tr></table></figure>

<h1 id="Sum-sum-limits-i-x3D-1-n"><a href="#Sum-sum-limits-i-x3D-1-n" class="headerlink" title="Sum $\sum\limits_{i&#x3D;1}^{n}$"></a>Sum $\sum\limits_{i&#x3D;1}^{n}$</h1><figure class="highlight latex"><table><tr><td class="code"><pre><span class="line"><span class="keyword">\sum</span><span class="keyword">\limits</span><span class="built_in">_</span>&#123;i=1&#125;<span class="built_in">^</span>&#123;n&#125;</span><br></pre></td></tr></table></figure>

<h1 id="Multiplication-prod-limits-j-x3D-1-n"><a href="#Multiplication-prod-limits-j-x3D-1-n" class="headerlink" title="Multiplication $\prod\limits_{j&#x3D;1}^{n}$"></a>Multiplication $\prod\limits_{j&#x3D;1}^{n}$</h1><figure class="highlight latex"><table><tr><td class="code"><pre><span class="line"><span class="keyword">\prod</span><span class="keyword">\limits</span><span class="built_in">_</span>&#123;j=1&#125;<span class="built_in">^</span>&#123;n&#125;</span><br></pre></td></tr></table></figure>

<h1 id="Derivation-partial-x"><a href="#Derivation-partial-x" class="headerlink" title="Derivation $\partial{x}$"></a>Derivation $\partial{x}$</h1><figure class="highlight latex"><table><tr><td class="code"><pre><span class="line"><span class="keyword">\partial</span>&#123;x&#125;</span><br></pre></td></tr></table></figure>

<h1 id="Integral-int-x-infty"><a href="#Integral-int-x-infty" class="headerlink" title="Integral $\int_{x}^{+\infty}$"></a>Integral $\int_{x}^{+\infty}$</h1><figure class="highlight latex"><table><tr><td class="code"><pre><span class="line"><span class="keyword">\int</span><span class="built_in">_</span>&#123;x&#125;<span class="built_in">^</span>&#123;+<span class="keyword">\infty</span>&#125;</span><br></pre></td></tr></table></figure>

<h1 id="Piecewise-function-begin-cases-1-amp-y-x3D-0-0-amp-y-not-x3D-0-end-cases"><a href="#Piecewise-function-begin-cases-1-amp-y-x3D-0-0-amp-y-not-x3D-0-end-cases" class="headerlink" title="Piecewise function $\begin{cases} 1,&amp;y&#x3D;0 \\ 0,&amp;y\not &#x3D;{0} \end{cases}$"></a>Piecewise function $\begin{cases} 1,&amp;y&#x3D;0 \\ 0,&amp;y\not &#x3D;{0} \end{cases}$</h1><figure class="highlight latex"><table><tr><td class="code"><pre><span class="line"><span class="keyword">\begin</span>&#123;cases&#125;</span><br><span class="line">    1,<span class="built_in">&amp;</span>y=0 <span class="keyword">\\</span></span><br><span class="line">    0,<span class="built_in">&amp;</span>y<span class="keyword">\not</span> =&#123;0&#125;</span><br><span class="line"><span class="keyword">\end</span>&#123;cases&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p><code>&amp;</code> means align. </p>
</blockquote>
<h1 id="More-information"><a href="#More-information" class="headerlink" title="More information"></a>More information</h1><ul>
<li><a href="https://latex-project.org/">latex-project.org</a></li>
</ul>
]]></content>
      <categories>
        <category>Tool</category>
      </categories>
      <tags>
        <tag>Tool</tag>
        <tag>Markup Language</tag>
      </tags>
  </entry>
</search>
