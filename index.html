<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=[object Object]"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', '[object Object]');
</script>
<!-- End Google Analytics -->

  
  <title>JourneyToCoding</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="Start of Something New">
<meta property="og:type" content="website">
<meta property="og:title" content="JourneyToCoding">
<meta property="og:url" content="https://zclzcl0223.github.io/index.html">
<meta property="og:site_name" content="JourneyToCoding">
<meta property="og:description" content="Start of Something New">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="ChaosTsang">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="JourneyToCoding" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/%5Bobject%20Object%5D">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">JourneyToCoding</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">Code for fun</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/">home</a>
        
          <a class="main-nav-link" href="/about/">about</a>
        
          <a class="main-nav-link" href="/tags/">tags</a>
        
          <a class="main-nav-link" href="/categories/">categories</a>
        
          <a class="main-nav-link" href="/archives/">archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://zclzcl0223.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-Diffusion" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/06/17/Diffusion/" class="article-date">
  <time class="dt-published" datetime="2024-06-17T11:38:01.000Z" itemprop="datePublished">2024-06-17</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Advanced-Model/">Advanced Model</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/06/17/Diffusion/">Diffusion</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <span id="more"></span>

<h1 id="What-is-Diffusion-Model"><a href="#What-is-Diffusion-Model" class="headerlink" title="What is Diffusion Model"></a>What is Diffusion Model</h1><blockquote>
<p>Sculpture is born from stone, I just cut off the part which is unnecessary. --Michelangelo</p>
</blockquote>
<p>In diffusion models, while training, the &#39;stone&#39; is generated from the training data by adding random noise. The sculpture is revealed by iteratively removing the noise added.</p>
<p>Unlike other generative models, the diffusion model does not generate data but generates noise. That is, the generator of diffusion model outputs noise, which is then removed from the noisy data. The denoised data is used as the input for the next denoising process. Therefore, the ground truth of each denoising process is the noise added and the prediction is the noise generated.</p>
<p><img src="/2024/06/17/Diffusion/1.png" alt="1"></p>
<center style="font-size:12px; font-weight:bold">Fig. 1. Pipeline of Diffusion</center><br>

<h1 id="Maths-behind-Diffusion-x2F-Generation"><a href="#Maths-behind-Diffusion-x2F-Generation" class="headerlink" title="Maths behind Diffusion&#x2F;Generation"></a>Maths behind Diffusion&#x2F;Generation</h1><p>The goal of image generation: given $z$ from a normal distribution, output $G(z)&#x3D;x$, where $x$ is similar to the original image distribution.</p>
<p><img src="/2024/06/17/Diffusion/2.png" alt="2"></p>
<center style="font-size:12px; font-weight:bold">Fig. 2. Goal of Image Generation</center><br>

<blockquote>
<p>The same is true for conditional generation, except that we consider the distribution that meets this condition.</p>
</blockquote>
<p>Hence, given training set $\{x^1,...,x^m\}\in P_{data}(x)$, we aim to find $\theta$ that maximizes the probability of these observations. And this is maximum likelihood estimation:</p>
<p>$$<br>\theta ^*&#x3D; arg\max _\theta \prod _{i&#x3D;1} ^m P _{\theta}(x ^i)<br>$$</p>
<p><img src="/2024/06/17/Diffusion/3.png" alt="3"></p>
<center style="font-size:12px; font-weight:bold">Fig. 3. Maximum Likelihood Estimation (MLE)</center><br>

<p>Actually, MLE is equal to minimizing <em>KL Divergence</em> :</p>
<p>$$<br>\begin{align*}<br>    \theta ^*&#x3D;&amp; arg\max _\theta \prod _{i&#x3D;1} ^m P _{\theta}(x ^i)&#x3D;arg\max _\theta \sum _{i&#x3D;1} ^m \log P _{\theta}(x ^i)\\<br>    \approx &amp;arg\max _\theta E _{x\sim P _{data}} \log P _{\theta}(x)\\<br>    &#x3D;&amp;arg\max _\theta \int _x P _{data}(x) \log P _{\theta}(x)\\<br>    &#x3D;&amp;arg\max _\theta \int _x P _{data}(x) \log P _{\theta}(x) - \int _x P _{data}(x)\log P _{data}(x) dx\\<br>    &#x3D;&amp;arg\max _\theta \int _x P _{data}(x) \log \frac{P _{\theta}(x)}{P _{data}(x)}\\<br>    &#x3D;&amp;arg\min _\theta KL(P _{data}||P _{\theta})<br>\end{align*}<br>$$</p>
<h1 id="Diffusion-Models"><a href="#Diffusion-Models" class="headerlink" title="Diffusion Models"></a>Diffusion Models</h1><h2 id="Denoising-Diffusion-Probabilistic-Models-DDPM"><a href="#Denoising-Diffusion-Probabilistic-Models-DDPM" class="headerlink" title="Denoising Diffusion Probabilistic Models (DDPM)"></a>Denoising Diffusion Probabilistic Models (DDPM)</h2><p>In DDPM, </p>
<p>$$<br>P _\theta(x _0)&#x3D;\int _{x _1:x_T}P(x _T)P _{\theta}(x _{T-1}|x _T)...P _\theta(x _{t-1}|x _t)...P _\theta(x _{0}|x _1)dx _1:x _T<br>$$</p>
<p><img src="/2024/06/17/Diffusion/4.png" alt="4"></p>
<center style="font-size:12px; font-weight:bold">Fig. 4. Denoising</center><br>

<p>Similar to <a href="/2024/01/20/VAE/">VAE</a>, we actually only optimize the lower bound of $P _\theta(x)$, that is:</p>
<p>$$<br>arg\max _\theta L _b&#x3D;E _{q(x _1:x _T| x)}[\log (\frac{P(x: x _T)}{q (x _1: x _T|x)})]<br>$$</p>
<p>where $q(x _1:x _T| x)&#x3D;q(x _1|x)q(x _2| x _1)...q(x _T| x _{T-1})$ is the forward process, which is equal to the encoder of VAE.</p>
<h3 id="Forward-Process"><a href="#Forward-Process" class="headerlink" title="Forward Process"></a>Forward Process</h3><p>During forward process, multi-steps could be turned to single-step. As shown in the following figure:</p>
<p><img src="/2024/06/17/Diffusion/5.png" alt="5"></p>
<center style="font-size:12px; font-weight:bold">Fig. 5. One-step Forward</center><br>

<h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><p>Therefore, during training, we actually only add noise once and denoise once:</p>
<p><img src="/2024/06/17/Diffusion/6.png" alt="6"></p>
<center style="font-size:12px; font-weight:bold">Fig. 6. Training</center><br>

<h3 id="Generation"><a href="#Generation" class="headerlink" title="Generation"></a>Generation</h3><p>While generating, we randomly sample data from the normal distribution and generate the image after $T$ times of denoising.</p>
<p><img src="/2024/06/17/Diffusion/7.png" alt="7"></p>
<center style="font-size:12px; font-weight:bold">Fig. 7. Generation</center><br>

<p>where the first term is mean and the second term is variance.</p>
<h2 id="Stable-Diffusion"><a href="#Stable-Diffusion" class="headerlink" title="Stable Diffusion"></a>Stable Diffusion</h2><p>Common architecture of modern (Text2Image) generative networks:</p>
<p><img src="/2024/06/17/Diffusion/8.png" alt="8"></p>
<center style="font-size:12px; font-weight:bold">Fig. 8. Architecture of Modern Generative Networks</center><br>

<ul>
<li><strong>Text Encoder</strong>: Encode text to latent representation, which is essential for the quality of final output.</li>
<li><strong>Generation Model</strong>: Any generation model, e.g., VAE, Diffusion, and GAN.</li>
<li><strong>Decoder</strong>: Decode latent representation to image.</li>
</ul>
<p>These three parts could be trained independently. For the decoder, it can accept two different inputs, depending on how you train it:</p>
<p><img align="left" src="/2024/06/17/Diffusion/9.png" style=" width:380px; padding: 0px 0px; "> <img align="mid" src="/2024/06/17/Diffusion/10.png" style=" width:380px; padding: 0px 0px; "></p>
<ol>
<li>A smaller version of the original image (Imagen): we can train this decoder in a supervised learning manner.</li>
<li>Latent representation (Stable Diffusion, DALL-E): we can train this decoder via AE.</li>
</ol>
<p>The following is the pipeline of stable diffusion, which is similar to the common architecture:</p>
<p><img src="/2024/06/17/Diffusion/11.png" alt="11"></p>
<center style="font-size:12px; font-weight:bold">Fig. 9. Architecture of Stable Diffusion</center><br>

<ul>
<li>In addition to text, stable diffusion also supports other conditional generation.</li>
<li>Stable diffusion train decoder in latent space. Hence, while training, the training image is transformed to latent representation $z$ using the encoder of AE. </li>
<li>The forward and denoising processes are also finished in the latent space.</li>
</ul>
<p><img src="/2024/06/17/Diffusion/12.png" alt="12"></p>
<center style="font-size:12px; font-weight:bold">Fig. 10. Forward in Latent Space</center><br>

<h1 id="Why-Diffusion-Works"><a href="#Why-Diffusion-Works" class="headerlink" title="Why Diffusion Works"></a>Why Diffusion Works</h1><p>The diffusion model can actually be regarded as an autoregressive model with a global perspective. It ensures the integrity of the generated image while achieving self-correction through iterative denoising.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zclzcl0223.github.io/2024/06/17/Diffusion/" data-id="clzik0lc7001sv47k2hr9hs4f" data-title="Diffusion" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Deep-Learning/" rel="tag">Deep Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Generative-AI/" rel="tag">Generative AI</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-VAE" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/01/20/VAE/" class="article-date">
  <time class="dt-published" datetime="2024-01-20T09:10:10.000Z" itemprop="datePublished">2024-01-20</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Advanced-Model/">Advanced Model</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/01/20/VAE/">VAE</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <span id="more"></span>

<h1 id="Intuitive-AE和VAE"><a href="#Intuitive-AE和VAE" class="headerlink" title="Intuitive: AE和VAE"></a>Intuitive: AE和VAE</h1><p><img src="/2024/01/20/VAE/1.png" alt="1"></p>
<center style="font-size:12px; font-weight:bold">Fig. 1. AE, VAE</center><br>

<p>训练时，两者都是优化输入图片和输出图片的差距来更新参数。最后部署、生成图片时，只使用Decoder，接受某个向量输入，生成图片。</p>
<p>AE的Encoder本质上是将一个图片映射为高维向量空间的一个点，而VAE的Encoder通过加入随机噪声，使得图片被映射为高维空间的一段区间。这样的好处在于，当输入Decoder的向量介于某两个向量之间时，VAE更有可能输出一个结合了这两个向量对应图片特征的图片，因为训练的误差会诱导网络这么做。</p>
<p><img src="/2024/01/20/VAE/2.png" alt="2"></p>
<center style="font-size:12px; font-weight:bold">Fig. 2. Intuitive. AE, VAE</center>

<h1 id="原理-噪声角度"><a href="#原理-噪声角度" class="headerlink" title="原理: 噪声角度"></a>原理: 噪声角度</h1><p>VAE的Encoder输出两个向量，一个原始编码$m$ (均值)，一个方差$\sigma$。我们期望加了噪声后的分布能尽可能地趋近于一个正态分布，因此引入一个从正态分布中采样出的向量$e$引导噪声。$\sigma$和$e$结合生成最终的噪声。但是，极端情况为学到的$\sigma$趋近于负无穷，使得VAE退化为AE。因此，除了输入图片和输出图片产生的$\mathcal{L} _1$以外，引入$\mathcal{L} _2$:</p>
<p>$$<br>\mathcal{L} _2 &#x3D; \min\sum (\text{exp}(\sigma _i)-(1+\sigma _i) + m _i ^2)<br>$$</p>
<p>其中前两项在$\sigma _i$取0时取最小值，这将迫使$\sigma _i$能够产生有效的噪声。最后一项是L2正则。</p>
<h1 id="原理-高斯混合模型角度"><a href="#原理-高斯混合模型角度" class="headerlink" title="原理: 高斯混合模型角度"></a>原理: 高斯混合模型角度</h1><p>对于一个给定的图片集，其在高维空间服从一个高斯混合分布$P(x)$ ($x$是图的RGB矩阵)，$P(x)$的值表明某个$x$属于这个空间的概率 (如某张图片是宝可梦精灵的概率)。</p>
<p><img src="/2024/01/20/VAE/3.png" alt="3"></p>
<center style="font-size:12px; font-weight:bold">Fig. 3. GMM</center><br>

<p>高斯混合模型由多个高斯分布组成，因而可写成:</p>
<p>$$<br>P(x)&#x3D;\sum _m P(m)P(x | m)<br>$$</p>
<p>其中$P(m)$是$m$高斯分布在混合分布中的权重。$P(x|m)$即为$m$高斯分布，有均值$\mu _m$和方差$\sigma _m$。极限情况下，高斯混合模型由无数个高斯分布组成:</p>
<p>$$<br>P(x) &#x3D; \int _z P(z)P(x |z)dz<br>$$</p>
<p>假设$z$服从一个标准正态分布，要将$z$对应为一个高斯分布，只需要生成均值$\mu(z)$和方差$\sigma (z)$即可:</p>
<p><img src="/2024/01/20/VAE/4.png" alt="4"></p>
<center style="font-size:12px; font-weight:bold">Fig. 4. Decoder</center><br>

<p>生成这样映射的过程显然是一个参数估计的过程，可由最大似然估计 (MLE) 优化:</p>
<p>$$<br>L &#x3D; \max\sum _x \log P(x)<br>$$</p>
<p>这又可以转化为一个EM过程。$z$是一个隐变量，可由已知的$x$估计，对$\log P(x)$做如下修改:</p>
<p>$$<br>\log P(x) &#x3D; \int _z q(z|x)\log P(x)dz<br>$$</p>
<p>$q(z|x)$是给定$x$，$z$的概率密度，显然其积分为1，故上式是一个恒等式。进一步处理:</p>
<p>$$<br>\begin{align*}<br>    \log P(x) &#x3D;&amp;\int _z q(z|x)\log (\frac{P(z,x)}{P(z|x)})dz\\<br>    &#x3D;&amp;\int _z q(z|x)\log (\frac{P(z,x)}{P(z|x)}\frac{q(z|x)}{q(z|x)})dz\\<br>    &#x3D;&amp;\int _z q(z|x)\log (\frac{P(z,x)}{q(z|x)})dz+\int _z q(z|x)\log (\frac{q(z|x)}{P(z|x)})dz<br>\end{align*}<br>$$</p>
<p>第一步用的是条件概率密度，其他都是普通的变换。第二项被称为KL散度 ($\text{KL}(q(z|x)||P(z|x))$)，用于衡量两个分布的相似性，大于0。故</p>
<p>$$<br>\log P(x) \ge L _b&#x3D;\int _z q(z|x)\log (\frac{P(z,x)}{q(z|x)})dz<br>$$</p>
<p>这一步体现了引入$q(z|x)$的作用。$q(z|x)$的值不会影响$L$ (积分恒为1)，但是通过优化$q(z|x)$，使其趋近于$P(z|x)$，我们可以让KL散度项逐渐趋于0，使得:</p>
<p>$$<br>\log P(x) \approx L _b<br>$$</p>
<p>于是MLE的优化对象就变为$L _b$，而$L _b$可进一步转化为:</p>
<p>$$<br>\begin{align*}<br>    L _b&#x3D;&amp;\int _z q(z|x)\log (\frac{P(x|z)P(z)}{q(z|x)})dz\\<br>    &#x3D;&amp;\int _z q(z|x)\log (\frac{P(z)}{q(z|x)})dz +\int _z q(z|x)\log (P(x|z))dz<br>\end{align*}<br>$$</p>
<p>其中前项是$P(z)$和$q(z|x)$的KL散度的相反数。而$q(z|x)$由$x$经过映射生成:</p>
<p><img src="/2024/01/20/VAE/5.png" alt="5"></p>
<center style="font-size:12px; font-weight:bold">Fig. 5. Encoder</center><br>

<p>这项实际上就是VAE的$\mathcal{L} _2$，因为要$\max L _b$，则必须要$\min \text{KL}(P(z),q(z|x))$，也就是要让生成的编码尽可能地趋向正态分布。</p>
<p>而第二项实际上是在求$\log P(x|z)$在分布$q(z|x)$下的均值:</p>
<p>$$<br>\int _z q(z|x)\log (P(x|z))dz&#x3D;E _{q(z|x)}[\log P(x|z)]<br>$$</p>
<p>$P(x|z)$又由$z$经过映射生成。最终便可整合为VAE的过程:</p>
<p><img src="/2024/01/20/VAE/6.png" alt="6"></p>
<center style="font-size:12px; font-weight:bold">Fig. 6. VAE</center><br>

<p>即$x$生成$q(z|x)$高斯分布，然后从中采样出$z$，$z$生成高斯分布$P(x|z)$，由于均值$\mu (x)$处的概率最大，因此$x$越趋近$\mu (x)$，概率就越大，这也就是$\mathcal{L} _1$。</p>
<h1 id="Conditional-VAE"><a href="#Conditional-VAE" class="headerlink" title="Conditional VAE"></a>Conditional VAE</h1><p>在训练时给VAE一些引导，使其Encoder的生成有倾向性。可用于文本生成图片。</p>
<h1 id="Shortcoming-of-VAE"><a href="#Shortcoming-of-VAE" class="headerlink" title="Shortcoming of VAE"></a>Shortcoming of VAE</h1><p>只是对训练集的模仿。</p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p>台湾大学李宏毅课程。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zclzcl0223.github.io/2024/01/20/VAE/" data-id="clzik0ld1009cv47k9lj957ze" data-title="VAE" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Deep-Learning/" rel="tag">Deep Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Generative-AI/" rel="tag">Generative AI</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-CountingComplexity" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/12/02/CountingComplexity/" class="article-date">
  <time class="dt-published" datetime="2023-12-02T11:27:04.000Z" itemprop="datePublished">2023-12-02</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/CS7313-Computational-Complexity/">CS7313: Computational Complexity</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/12/02/CountingComplexity/">Computational Complexity: Complexity of Counting</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <span id="more"></span>

<h1 id="Counting-Problem"><a href="#Counting-Problem" class="headerlink" title="Counting Problem"></a>Counting Problem</h1><p>一些计数问题：</p>
<ul>
<li>$\sharp \text{CYCLE}$：计算有向图G中环的个数。</li>
<li>$\sharp \text{SAT}$：使得该公式为真的真值指派的个数。</li>
</ul>
<blockquote>
<p>计数问题要远难于判定问题。</p>
</blockquote>
<p>定理：如果$\sharp \text{CYCLE}$有多项式的算法，则$\textbf{P}&#x3D;\textbf{NP}$。</p>
<h1 id="sharp-textbf-P"><a href="#sharp-textbf-P" class="headerlink" title="$\sharp \textbf{P}$"></a>$\sharp \textbf{P}$</h1><p>对函数$f:\{0,1\} ^* \to \textbf{N}$，如果$\exists p : \textbf{N} \to \textbf{N}$和一个多项式图灵机$\mathbb{M}$使得$\forall x \in \{0,1\}  ^*$，有：</p>
<p>$$<br>f(x)&#x3D;|\{y \in \{0,1\} ^{p(|x|)}| \mathbb{M}(x,y)&#x3D;1\}|<br>$$</p>
<p>其中，右边表示使得括号成立的$y$的个数。这样的$f$是在$\sharp \textbf{P}$中的。</p>
<blockquote>
<p>上述定义可视为用“证据”定义的，因而也可用非确定图灵机定义$\sharp \textbf{P}$，此时$f$的值即为非确定图灵机回答YES的路径数。</p>
<p>$\textbf{PP}$可视为$\sharp \textbf{P}$的决策版本。</p>
</blockquote>
<h2 id="textbf-FP"><a href="#textbf-FP" class="headerlink" title="$\textbf{FP}$"></a>$\textbf{FP}$</h2><p>函数$(f:\{0,1\} ^*\to \textbf{N}) \in \textbf{FP}$，如果该函数可由多项式图灵机计算。</p>
<blockquote>
<p>$\textbf{FP} \subseteq \sharp\textbf{P}$</p>
</blockquote>
<h2 id="sharp-textbf-P-完全"><a href="#sharp-textbf-P-完全" class="headerlink" title="$\sharp \textbf{P}$完全"></a>$\sharp \textbf{P}$完全</h2><p>$f \in \sharp \textbf{P}\text{-complete}$如果$f \in \sharp \textbf{P}$且$\sharp \textbf{P} \in \textbf{FP} ^f$。</p>
<blockquote>
<p>即$\sharp \textbf{P}$中的问题都可以通过调用$f$解决。</p>
</blockquote>
<p>$\sharp \textbf{SAT}$是$\sharp \textbf{P}\text{-complete}$的。还是C-L归约。</p>
<blockquote>
<p>$\sharp \textbf{SAT}$：$&lt;\varphi, i&gt;$，其中$\varphi$是可取范式，$i$是使得$\varphi$为真的真值指派的个数。</p>
</blockquote>
<h1 id="Valiant-Theorem"><a href="#Valiant-Theorem" class="headerlink" title="Valiant Theorem"></a>Valiant Theorem</h1><p>Perm是$\sharp \textbf{P}$完全的。</p>
<p>Perm转换为有向图中包含所有结点的环。</p>
<p>把3CNF的变量和语句表示为图。</p>
<blockquote>
<p>最终Perm&#x3D;$4 ^{3m}\sharp \varphi$。</p>
</blockquote>
<p>还要证明Perm$\le _K$0-1Perm：把边转为$k$条权为1的平行边。</p>
<h1 id="Universal-Hash-Function"><a href="#Universal-Hash-Function" class="headerlink" title="Universal Hash Function"></a>Universal Hash Function</h1><p>$\mathcal{H} \subseteq B ^A$，其中$B ^A$表示从$A$到$B$的函数。$\mathcal{H}$是通用哈希函数族，如果：</p>
<p>$$\text{Pr} _{h \in _R \mathcal{H}}[h(x)&#x3D;h(x&#39;)]\le \frac{1}{|B|}$$</p>
<p>其中$x,x&#39; \in A,x \ne x&#39;$。</p>
<h2 id="m-wise-Independent-Hash-Function-Family"><a href="#m-wise-Independent-Hash-Function-Family" class="headerlink" title="m-wise Independent Hash Function Family"></a>m-wise Independent Hash Function Family</h2><p>$$<br>\text{Pr} _{h \in _R \mathcal{H} _{n,k}}[\land _{i&#x3D;1} ^m h(x _i)&#x3D; y _i]&#x3D;\frac{1}{2 ^{mk}}<br>$$</p>
<p>则称$\mathcal{H} _{n,k}$是m-independent的。其中，$x _i$互不相等。$\mathcal{H} _{n,k}$将长度为$n$的串映射为长度为$k$的串。换句话说，$\mathcal{H} _{n,k}$能保证对$m$个串，其对每一位的映射都是独立的。</p>
<blockquote>
<p>多项式线性方程的解，因为只有唯一解，所以能求出唯一的系数$a _0,...,a _{m-1}$。范德蒙德式。</p>
</blockquote>
<h1 id="Valiant-Vazirani-Theorem"><a href="#Valiant-Vazirani-Theorem" class="headerlink" title="Valiant-Vazirani Theorem"></a>Valiant-Vazirani Theorem</h1><p>$\textbf{UP}$，对其中的类，非确定图灵机的所有路径中，至多有一条路径接受它。$\textbf{P}\subseteq \textbf{UP} \subseteq \textbf{NP}$。</p>
<blockquote>
<p>$\textbf{USAT}$：只有唯一的真值指派使之为真。</p>
</blockquote>
<p>定理：$\forall n$变量的$\phi$，存在多项式概率图灵机$\mathbb{A}$：</p>
<p>$$<br>\begin{align*}<br>    &amp;\phi \in \textbf{SAT} \to \text{Pr}[\mathbb{A}(\phi)\in \textbf{USAT}] \ge 1&#x2F;(8n) \\<br>    &amp;\phi \notin \textbf{SAT} \to \text{Pr}[\mathbb{A}(\phi)\in \textbf{SAT}] &#x3D;0<br>\end{align*}<br>$$</p>
<p>证明：对$\phi(x,y...,z)$，定义归约$\phi \land x&#x3D;c _1 \land y&#x3D; c _2\land...\land z &#x3D; c _n$。由于$\phi$没有多项式算法，所以$c _1,...,c _n$是满足合取范式的猜测。</p>
<blockquote>
<p>详细证明：集合并公式的拆分。<br>不能通过重复实验来提高概率，因为每次随机算法$\mathbb{A}$生成的都是随机的公式。</p>
</blockquote>
<h2 id="oplus-textbf-P"><a href="#oplus-textbf-P" class="headerlink" title="$\oplus \textbf{P}$"></a>$\oplus \textbf{P}$</h2><p>$\oplus\textbf{P}$，对其中的类，非确定图灵机的所有路径中，只有奇数条路径接受它（或者0条则表拒绝）。$\textbf{UP} \subseteq \oplus \textbf{P}$。</p>
<blockquote>
<p>$\oplus \textbf{SAT}$是$\oplus \textbf{P}$完全的。</p>
</blockquote>
<p>将前面的$\textbf{USAT}$换成$\oplus \textbf{SAT}$也是对的，且能重复实验提高精度，这就是Toda Theorem。</p>
<p>Toda Theorem：任何$\textbf{PH}$问题都能通过调用多项式次$\sharp \textbf{P}$完成，即：</p>
<p>$$<br>\textbf{PH} \subseteq \textbf{P} ^{\sharp \textbf{P}}<br>$$</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zclzcl0223.github.io/2023/12/02/CountingComplexity/" data-id="clzik0lc6001lv47khyp70d0l" data-title="Computational Complexity: Complexity of Counting" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Math/" rel="tag">Math</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Theoretical-Computer-Science/" rel="tag">Theoretical Computer Science</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-MatrixTheory5" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/11/24/MatrixTheory5/" class="article-date">
  <time class="dt-published" datetime="2023-11-24T04:26:01.000Z" itemprop="datePublished">2023-11-24</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/MATH6005-Matrix-Theory/">MATH6005: Matrix Theory</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/11/24/MatrixTheory5/">MatrixTheory: 特殊矩阵与矩阵分解</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <span id="more"></span>

<h1 id="特殊矩阵"><a href="#特殊矩阵" class="headerlink" title="特殊矩阵"></a>特殊矩阵</h1><p>一些复数域的特殊矩阵。</p>
<h2 id="正规矩阵"><a href="#正规矩阵" class="headerlink" title="正规矩阵"></a>正规矩阵</h2><p>复数域可酉对角化（不一定正交对角化）的矩阵。$Q^* &#x3D; Q ^{-1}$。</p>
<p>$$<br>Q^* A Q &#x3D; \Lambda<br>$$</p>
<p>定义：复数域上的方阵，满足$AA^*&#x3D;A^*A$</p>
<h3 id="性质"><a href="#性质" class="headerlink" title="性质"></a>性质</h3><ol>
<li>与正规矩阵酉相似的矩阵还是正规矩阵，即$B &#x3D; Q^*A Q$，其中$A$是正规矩阵，则$B$也是正规矩阵。</li>
<li>$A$为正规三角矩阵，则$A$一定为对角矩阵。</li>
<li>$A$为正规矩阵$\iff$$A$可酉对角化。</li>
</ol>
<blockquote>
<p>左到右：舒尔定理——复数域的方阵一定可酉相似于三角矩阵。而正规三角矩阵一定为对角矩阵。</p>
</blockquote>
<p>实数域的正规矩阵：$A ^T &#x3D; A$，$A ^T &#x3D; -A$，$A ^T &#x3D; A ^{-1}$<br>复数域的正规矩阵：$A^* &#x3D; A$，$A^* &#x3D; -A$，$A^* &#x3D; A ^{-1}$</p>
<h4 id="Hermite矩阵：-A-x3D-A"><a href="#Hermite矩阵：-A-x3D-A" class="headerlink" title="Hermite矩阵：$A^* &#x3D; A$"></a>Hermite矩阵：$A^* &#x3D; A$</h4><p>实对称矩阵的推广。性质：</p>
<ol>
<li>特征值都是实数，类似地，反Hermite矩阵的特征值都是纯虚数或0。</li>
<li>复二次型：$x^* A x$（一定是实数），相应地，可定义正定、标准化。</li>
</ol>
<blockquote>
<p>等积变换：正交变换、酉变换$x &#x3D; Qy$</p>
</blockquote>
<h4 id="酉矩阵：-AA-x3D-A-A-x3D-E"><a href="#酉矩阵：-AA-x3D-A-A-x3D-E" class="headerlink" title="酉矩阵：$AA^*&#x3D;A^*A&#x3D;E$"></a>酉矩阵：$AA^*&#x3D;A^*A&#x3D;E$</h4><ol>
<li>特征值模为1。</li>
</ol>
<h1 id="矩阵分解（20-30分）"><a href="#矩阵分解（20-30分）" class="headerlink" title="矩阵分解（20~30分）"></a>矩阵分解（20~30分）</h1><h2 id="满秩分解"><a href="#满秩分解" class="headerlink" title="满秩分解"></a>满秩分解</h2><p>化埃尔米特标准型求。四个子空间。</p>
<blockquote>
<p>可能能用于求矩阵的次方。如$A ^2 &#x3D; PQPQ&#x3D;P(QP)Q$。特别是对于$A$的秩为1的时候，$P$和$Q$都是向量，此时$QP$是一个数。</p>
</blockquote>
<h2 id="正交三角分解"><a href="#正交三角分解" class="headerlink" title="正交三角分解"></a>正交三角分解</h2><p>$A _{m\times n}$：必须列满秩才有正交三角分解。</p>
<p>$$<br>A _{m\times n}&#x3D; U _{m \times n}R _{n \times n}<br>$$</p>
<p>其中$U$的列向量单位正交，$R$是主对角元大于0的上三角。</p>
<blockquote>
<p>$U$即为$A$的列向量的斯密特单位正交化。$R _{ij}&#x3D;(\alpha _i, \gamma _j)(i \ne j),R _{ii}&#x3D;||\beta _i||$且上三角。或者$R &#x3D; U^* A$。</p>
<p>正交三角分解是唯一的。</p>
<p>应用：解线性方程组。</p>
</blockquote>
<h2 id="谱分解"><a href="#谱分解" class="headerlink" title="谱分解"></a>谱分解</h2><p>$A _{n \times n}$：可对角化的方阵才有谱分解。</p>
<p>$P&#x3D;(\alpha _1, \alpha _2,..., \alpha _n)$，其中$\alpha _i$为特征向量。</p>
<p>最基本的谱分解：<br>$$<br>A &#x3D; (\alpha _1,...,\alpha _n)eye(\lambda _1,...,\lambda _n)(\beta _1 ^T,...,\beta _n ^T)<br>$$</p>
<p>其中$P ^{-1}&#x3D;(\beta _1 ^T,...,\beta _n ^T)$（竖着的）</p>
<p>于是：</p>
<p>$$<br>A &#x3D; \lambda _1 \alpha _1 \beta _1 ^T +...+ \lambda _n \alpha _n \beta _n ^T<br>$$</p>
<p>记$S _1,..., S _m$为不同的特征值，则：</p>
<p>$$<br>A &#x3D; S _1 G _1+...+ S _m G _m<br>$$</p>
<p>性质：</p>
<ol>
<li>$\sum _{i &#x3D; 1} ^m G _i &#x3D; E$</li>
<li>$\beta _i ^T \alpha _j &#x3D; 1 (i &#x3D; j);0 (i \ne j)$。$(\alpha _i \beta _i ^T) ^2 &#x3D; \alpha _i \beta _i ^T$，$(\alpha _i \beta _i ^T)(\alpha _j \beta _j ^T) &#x3D; 0 (i \ne j)$。故$G _i ^2 &#x3D; G _i$，$G _i G _j &#x3D; 0 (i \ne j)$。</li>
<li>$r (G _i) &#x3D; n _i (\text{特征值的代数重数})$。</li>
<li>谱分解唯一。用性质1、2证明。</li>
</ol>
<p>正规矩阵的谱分解：不需要专门求逆矩阵。</p>
<p>$$<br>A ^2 &#x3D; \sum \lambda _i ^2 G _i<br>$$</p>
<h2 id="三角分解（cholesky分解）"><a href="#三角分解（cholesky分解）" class="headerlink" title="三角分解（cholesky分解）"></a>三角分解（cholesky分解）</h2><p>$A _{n\times n}$：秩为$r$的方阵，$A&#x3D;LU$，其中$L$为单位下三角，$U$是上三角。</p>
<ul>
<li>条件：1~r阶顺序主子式非零。</li>
</ul>
<p>本质是高斯消元：即行变换相当于左乘矩阵，让行变换后的矩阵是个上三角，此时左乘的矩阵一定是一个单位下三角。由于左乘的单位下三角是可逆的，故左乘其逆矩阵即可得到$A&#x3D;LU$。</p>
<ul>
<li>注：此时的行变换只能用上面的行去减下面的行。</li>
</ul>
<h3 id="实正定矩阵"><a href="#实正定矩阵" class="headerlink" title="实正定矩阵"></a>实正定矩阵</h3><p>首先，一定有三角分解，但是更特殊，称Cholesky分解：$A&#x3D;R ^T R$。$R$为主对角元大于零的上三角。</p>
<p>因为正定矩阵一定是对称的，所以可以正交相似对角化：$Q ^TA Q&#x3D;\Lambda$，可得$A &#x3D; Q\Lambda Q ^T&#x3D;Q\sqrt{\Lambda}\sqrt{\Lambda}Q ^T&#x3D;(\sqrt{\Lambda}Q ^T) ^T (\sqrt{\Lambda}Q ^T)&#x3D;B ^T B&#x3D;(\sqrt{\Lambda}Q ^T) ^T Q ^T Q(\sqrt{\Lambda}Q ^T)&#x3D;(Q\sqrt{\Lambda}Q ^T) ^T(Q\sqrt{\Lambda}Q ^T) &#x3D;C ^TC$，显然，$C$是一个正定矩阵。</p>
<p>而$B$是满秩的，所以一定有正交三角分解，即$B &#x3D; UR$，$U$为正交矩阵，于是$B ^TB&#x3D;(UR) ^T(UR)&#x3D;R ^TR$，即为所求。</p>
<blockquote>
<p>这样的分解是唯一的。</p>
</blockquote>
<p>求法：同时行列变换（仍然只能上减下、左减右），将$A$化为对角矩阵，分别左右乘逆矩阵，然后对角矩阵开根号，前两个当一个矩阵，后两个当一个矩阵即可。</p>
<h2 id="奇异值分解（极分解）"><a href="#奇异值分解（极分解）" class="headerlink" title="奇异值分解（极分解）"></a>奇异值分解（极分解）</h2><p>$A _{m\times n}$，$r(A) &#x3D; r$，$A _{m\times n}&#x3D; U _{m\times m} D _{m\times n} V^* _{n\times n}$，其中$U,V$是酉矩阵，$D$的所有元素中只有主对角线上前面的元素$\delta _i$非零，$\delta _i &gt; 0,0\le i\le r$，$\delta _i$称为$A$的奇异值。</p>
<ul>
<li>奇异值：$A A^*$的特征值开根号。</li>
<li>$U$：$A A^*$特征值的单位正交向量组成的矩阵。</li>
<li>$V$：$A^* A$特征值的单位正交向量组成的矩阵。</li>
</ul>
<h3 id="一些性质"><a href="#一些性质" class="headerlink" title="一些性质"></a>一些性质</h3><ul>
<li>$r(A) &#x3D; r (A^*) &#x3D; r (AA^*) &#x3D; r (A^* A)$：即证$N(A)&#x3D;N(AA^*)$。</li>
<li>$AA^*$与$A^* A$的非零特征值完全一样：$A^* A$特征值为$\lambda$，特征向量为$\alpha$，则$AA^*$的特征值为$\lambda$，特征向量为$A \alpha$。</li>
<li>$A^* A$和$A A^*$都是半正定矩阵，特征值开根号即为奇异值。</li>
<li>设$\lambda$为$A^* A$的一个非零特征值，$\alpha _1,...,\alpha _k$为其单位正交特征向量，则$\lambda$为$A A^*$特征向量，$A\alpha _1,...,A\alpha _k$为其正交特征向量，但不是单位的，要除以$\sqrt{\lambda}$（即奇异值）才是单位化的。</li>
</ul>
<h3 id="奇异值分解求法"><a href="#奇异值分解求法" class="headerlink" title="奇异值分解求法"></a>奇异值分解求法</h3><ol>
<li>求$A^*A$<strong>大于0</strong>的特征值$\lambda _i$，及其对应的<strong>单位正交</strong>的特征向量$\alpha _i$；求0特征值对应的<strong>单位正交</strong>的特征向量$\alpha _{i+1}$。</li>
<li>求$AA^*$<strong>大于0</strong>的特征值（和$A^*A$）相同，及其对应的<strong>单位正交</strong>的特征向量$\beta _i$（$\beta _i &#x3D; (A \alpha _i) &#x2F; \sqrt{\lambda _i}$）；求0特征值对应的<strong>单位正交</strong>的特征向量$(\beta _{i+1})$。（无特殊求法）</li>
</ol>
<p>$A(\alpha _1,..., \alpha _i, \alpha _{i+1}, ...,\alpha _n)&#x3D;(\beta _1,...,\beta _i,\beta _{i+1},..,\beta _n)\Lambda(\sqrt{\lambda _1},..,\sqrt{\lambda _i},0,...,0)$，故$A&#x3D;UDV^*$。其中$U&#x3D;(\beta _1,...,\beta _i,\beta _{i+1},..,\beta _n)$，$V&#x3D;(\alpha _1,..., \alpha _i, \alpha _{i+1}, ...,\alpha _n)$。</p>
<blockquote>
<p>先求$AA^*$还是$A^*A$看哪个阶数小。</p>
</blockquote>
<p>$\alpha _1,..,\alpha _i$是$A$的行空间中的向量，$\beta _1,..,\beta _i$是$A$的列空间中的向量；$\alpha _{i+1},...,\alpha _n$是$N(A)$的向量，$\beta _{i+1},..,\beta _n$是$N ^T(A)$中的向量。</p>
<h3 id="极分解"><a href="#极分解" class="headerlink" title="极分解"></a>极分解</h3><p>若$A$是方阵，则$A&#x3D;UDV^*&#x3D;(UDU^*)(UV^*)$，其中，左边是一个半正定矩阵，右边是酉矩阵，这就是极分解。或者$A&#x3D;(UV^*)(VDV^*)$，其中左边是酉矩阵，右边是半正定矩阵。若$A$可逆，则为半正定变为正定。</p>
<h3 id="A是正规矩阵"><a href="#A是正规矩阵" class="headerlink" title="A是正规矩阵"></a>A是正规矩阵</h3><p>原始式很重要。牢记，有些就是对原始式做变形。</p>
<h1 id="范数"><a href="#范数" class="headerlink" title="范数"></a>范数</h1><h2 id="向量范数"><a href="#向量范数" class="headerlink" title="向量范数"></a>向量范数</h2><p>长度的推广。</p>
<p>满足性质：</p>
<ol>
<li>$||\alpha|| &gt; 0$，正定性</li>
<li>$||k \cdot \alpha|| &#x3D; |k| ||\alpha||$，齐次性</li>
<li>$||\alpha + \beta|| \le ||\alpha|| + ||\beta||$，三角不等式</li>
</ol>
<p>非负函数。</p>
<blockquote>
<p>定义了范数的线性空间称<em>赋范线性空间</em>。</p>
</blockquote>
<ul>
<li>$\mathcal{l} _1$：$\sum |x _i|$</li>
<li>$\mathcal{l} _2$：$(\sum |x _i| ^2) ^{1&#x2F;2}$</li>
<li>$\mathcal{l} _p$：$(\sum |x _i| ^p) ^{1&#x2F;p}(p \ge 1)$</li>
<li>$\mathcal{l} _\infty$：$\max\{|x _i|,1\le i\le n\}$</li>
</ul>
<p>以上统称$p$范数。向量范数是等价的。等价：</p>
<p>$$<br>c _1,c _2, c _2||\alpha|| _b \le ||\alpha|| _a\le c _1||\alpha|| _b<br>$$</p>
<h2 id="矩阵范数"><a href="#矩阵范数" class="headerlink" title="矩阵范数"></a>矩阵范数</h2><p>除前面三条性质外，还有：</p>
<ul>
<li>$|||AB||| \le |||A|||\space |||B|||$</li>
</ul>
<p>F范数：</p>
<ul>
<li>$|||A||| _F&#x3D;(\sum |a _{ij}| ^2) ^{1&#x2F;2}&#x3D;\sqrt{\text{tr}(AA^*)}&#x3D;\sqrt{\text{tr}(A^*A)}$</li>
</ul>
<p>Schur不等式，$A &#x3D; (a _{ij}) _{n\times n},\lambda _1,...,\lambda _n$，则：</p>
<p>$$<br>\sum |\lambda _i| ^2 \le \sum |a _{ij}| ^2\iff\text{A是正规矩阵取等号}<br>$$</p>
<p>Schur定理：$Q^* A Q &#x3D; \text{diag}(\lambda _1, ...,\lambda _n)+\text{上三角}$，其中$Q^*&#x3D;Q ^{-1}$。</p>
<h2 id="相容"><a href="#相容" class="headerlink" title="相容"></a>相容</h2><p>若</p>
<p>$$<br>||A \alpha|| \le |||A|||\cdot ||\alpha||<br>$$<br>其中$A$是矩阵，$\alpha$是向量，则称这两个范数是相容的。</p>
<h3 id="算子范数"><a href="#算子范数" class="headerlink" title="算子范数"></a>算子范数</h3><p>由向量范数$||\cdot|| _a$构造相容的矩阵范数：</p>
<p>$$<br>|||A|||&#x3D;\max _{x \ne 0,x \in C ^n}\frac{||Ax|| _a}{||x|| _a}&#x3D;\max _{||x|| _a&#x3D;1}||Ax|| _a<br>$$</p>
<p>如$||\alpha|| _2$导出的矩阵范数：</p>
<p>$$<br>|||A||| _2&#x3D;&#x3D;\max _{||x|| _2&#x3D;1}||Ax|| _2<br>$$</p>
<p>$$<br>||Ax|| _2 ^2&#x3D;(Ax,Ax)&#x3D;x^* A^* Ax<br>$$</p>
<p>设$A^*A$的$n$个特征值为$\lambda _1\ge ...\ge \lambda _n \ge 0$，对应的单位正交特征向量为$\alpha _1,...,\alpha _n$。显然，这是$C ^n$的一组基，故：</p>
<p>$$<br>x &#x3D; x _1\alpha _1+...+x _n \alpha _n<br>$$</p>
<p>$$<br>x^*&#x3D; \overline{x} _1\alpha _1^*+...+\overline{x} _n \alpha _n^*<br>$$</p>
<p>$$<br>x^* A^* A x&#x3D;(\overline{x} _1\alpha _1^*+...+\overline{x} _n \alpha _n^*)A^* A(x _1\alpha _1+...+x _n \alpha _n)<br>$$</p>
<p>即：</p>
<p>$$<br>\begin{align*}<br>x^* A^* A x<br>&amp;&#x3D;(\overline{x} _1\alpha _1^*+...+\overline{x} _n \alpha _n^*)(x _1\lambda _1\alpha _1+...+x _n\lambda _n \alpha _n)\\<br>&amp;&#x3D;\lambda _1 |x _1| ^2+...+\lambda _n |x _n| ^2\\<br>&amp;\le \lambda _1(|x _1| ^2+...+ |x _n| ^2)<br>\end{align*}<br>$$</p>
<p>而可以取等，所以$x^* A^* Ax &#x3D; \lambda _1$，$|||A||| _2 &#x3D;\sqrt{\lambda _1}$。其中$\lambda _1$为最大特征值。<em>称谱范数</em>。</p>
<blockquote>
<p>作业：求$||\alpha|| _1$和$||\alpha|| _\infty$的导出的矩阵算子范数。（提示：矩阵行取模最大、列取模最大）。</p>
</blockquote>
<h2 id="向量、矩阵序列的收敛性"><a href="#向量、矩阵序列的收敛性" class="headerlink" title="向量、矩阵序列的收敛性"></a>向量、矩阵序列的收敛性</h2><p>向量序列$x ^{(k)}&#x3D;(x _1 ^{(k)},...,x _n ^{(k)}) ^T$，收敛：$\lim _{k \to \infty} x ^{(k)}&#x3D;x\iff \lim _{k\to \infty}||x ^{(k)}-x||&#x3D;0$。</p>
<p>同理，可定义矩阵序列及其收敛。</p>
<blockquote>
<p>矩阵或向量的收敛是每个元素都收敛到一个值。</p>
</blockquote>
<p>矩阵的幂序列：$A ^{(k)}&#x3D; A ^k$。化为Jordan标准型，Jordan块的$k$次方，二项展开i即可，不用记公式。</p>
<ul>
<li>$|\lambda| _i &lt; 1$，收敛到0；</li>
<li>$|\lambda| _i&#x3D;1$且$\lambda _i &#x3D; 1$且$m &#x3D; 1$，收敛；</li>
<li>其他发散</li>
</ul>
<p>故，矩阵的谱半径小于$1$时幂矩阵序列幂收敛到0。</p>
<h2 id="谱半径范数是最小的矩阵范数"><a href="#谱半径范数是最小的矩阵范数" class="headerlink" title="谱半径范数是最小的矩阵范数"></a>谱半径范数是最小的矩阵范数</h2><p>$\rho (A) \le |||A|||$</p>
<blockquote>
<p>证明：令$B &#x3D; A &#x2F; (|||A|||+\epsilon)$，则$|||B|||&#x3D;|||A|||&#x2F;(|||A|||+\epsilon) &lt; 1$。</p>
</blockquote>
<h2 id="矩阵级数"><a href="#矩阵级数" class="headerlink" title="矩阵级数"></a>矩阵级数</h2><p>向量&#x2F;矩阵序列的项相加。$\sum _{k&#x3D;0} ^{\infty}A ^{(k)}$。收敛即$\sum a _{ij} ^{(k)}$都收敛。</p>
<h3 id="矩阵幂级数"><a href="#矩阵幂级数" class="headerlink" title="矩阵幂级数"></a>矩阵幂级数</h3><p>$\sum _{k&#x3D;0} ^\infty a _k A ^k$收敛性与$f(t)&#x3D;\sum _{k&#x3D;0} ^\infty a _k t ^k$有关。假设为Jordan块，则每一行的和就是$f(\lambda)$的泰勒级数。最终：</p>
<p>$$<br>\sum a _k A ^k &#x3D; P\text{diag}(f(J _1),...,f(J _s))p ^{-1}<br>$$</p>
<p>其中$\rho(A)&lt; r$，其中$r$为数项幂级数的收敛半径。</p>
<h3 id="矩阵函数"><a href="#矩阵函数" class="headerlink" title="矩阵函数"></a>矩阵函数</h3><p>$e ^A$，$\sin A$,$\cos A$：可泰勒展开化为矩阵多项式求。而矩阵多项式又可用哈密尔顿凯勒定理求：</p>
<ul>
<li>哈密尔顿凯勒定理求：$A ^n$可由$A ^{n-1},...,E$线性表示。即$f(A)&#x3D;\varPhi(\lambda)g+\text{余式}$。其中$g$是某个零化多项式，$\text{余式}$是比它低一次的多项式。只要待定系数求出余式的系数即可，带入的数是零化多项式的根（缺少约束就两边求导）。</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zclzcl0223.github.io/2023/11/24/MatrixTheory5/" data-id="clzik0lcm005cv47k9gtdeiiw" data-title="MatrixTheory: 特殊矩阵与矩阵分解" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Math/" rel="tag">Math</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-RandomizedComputation" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/11/13/RandomizedComputation/" class="article-date">
  <time class="dt-published" datetime="2023-11-13T00:30:46.000Z" itemprop="datePublished">2023-11-13</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/CS7313-Computational-Complexity/">CS7313: Computational Complexity</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/11/13/RandomizedComputation/">Computational Complexity: Randomized Computation</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <span id="more"></span>

<h1 id="Probabilistic-Turing-Machine"><a href="#Probabilistic-Turing-Machine" class="headerlink" title="Probabilistic Turing Machine"></a>Probabilistic Turing Machine</h1><p>$\textbf{PTM}$是$\textbf{NDTM}$的升级，其以确定的概率（一般均为0.5）选择$\delta _0$和$\delta _1$。故概率图灵机的输出$\mathbb{P}(x)$是一个随机分布（变量）。$<br>\text{Pr}[\mathbb{P}(x)&#x3D;y]$表示给定输入$x$，该概率图灵机输出$y$的概率。</p>
<p>一个函数$\phi$是概率可计算的（能被概率图灵机$\mathbb{P}$计算），若：</p>
<p>$$<br>\phi(x)&#x3D;<br>\begin{cases}<br>    y,\text{if}\space \text{Pr}[\mathbb{P}(x)&#x3D;y] &gt; 1&#x2F;2 \\<br>    \uparrow, \text{if no such y exists}<br>\end{cases}<br>$$</p>
<p>一个语言$L$能被$\textbf{PTM}$判定，当且仅当：</p>
<p>$$<br>\text{Pr}[\mathbb{P}(x)&#x3D;L(x)] &gt; 1&#x2F;2<br>$$</p>
<blockquote>
<p>概率图灵机可计算的函数就是可计算函数。</p>
</blockquote>
<h2 id="Average-Case-Time-Complexity"><a href="#Average-Case-Time-Complexity" class="headerlink" title="Average Case Time Complexity"></a>Average Case Time Complexity</h2><p>概率图灵机可以定义平均时间复杂性。</p>
<p>With bounded error:</p>
<p>$$<br>\phi(x)&#x3D;<br>\begin{cases}<br>    y,\text{if}\space \text{Pr}[\mathbb{P}(x)&#x3D;y] \ge 1&#x2F;2 + \epsilon \\<br>    \uparrow, \text{if no such y exists}<br>\end{cases}<br>$$</p>
<blockquote>
<p>带bounded error是为了避免出现任何问题都能平均两步解决的错误结论。</p>
</blockquote>
<h2 id="Polynomial-Identity-Testing"><a href="#Polynomial-Identity-Testing" class="headerlink" title="Polynomial Identity Testing"></a>Polynomial Identity Testing</h2><p>著名随机算法——等价性测试：测试两个电路是否是等价的。</p>
<blockquote>
<p>在有限域中随机取值，直接输入该电路。</p>
</blockquote>
<h1 id="textbf-PP"><a href="#textbf-PP" class="headerlink" title="$\textbf{PP}$"></a>$\textbf{PP}$</h1><p>若$\forall x \in \{0,1\} ^*$，$\text{Pr}[\mathbb{P}(x)&#x3D;L(x)] &gt; 1&#x2F;2$，且对任意的选择，$\mathbb{P}$在$T(|x|)$步内停机，则$\mathbb{P}$在$T(n)$时间内判定$L$。（$T$为最坏时间复杂性）</p>
<p>上面的定义即为$\textbf{PP}$类，其等价定义为：有个多项式时间的普通图灵机$\mathbb{M}$，使得：</p>
<p>$$<br>\text{Pr} _{r \in \{0,1\} ^{p(|x|)}}[\mathbb{M}(x,r)&#x3D;L(x)] &gt; 1&#x2F;2<br>$$</p>
<p>其中$r$为一个随机串。</p>
<p>$$<br>\textbf{NP},\text{co}\textbf{NP} \subseteq \textbf{PP} \subseteq \textbf{PSPACE}<br>$$</p>
<h2 id="PP完全"><a href="#PP完全" class="headerlink" title="PP完全"></a>PP完全</h2><p>都是$\textbf{SAT}$的变形。</p>
<p>$\phi \in \text{MajSAT}$如果有超过一半的真值指派，使得$\phi&#x3D;1$。</p>
<blockquote>
<p>C-L归约，$\delta$的选择转变为一个个真值指派。</p>
</blockquote>
<p>$\textbf{PP}$是封闭的，所以对$A \in \textbf{PP}$，$B \in \textbf{PP}$，有$A\cap B \in \textbf{PP}$，$A \cup B \in \textbf{PP}$。</p>
<h1 id="textbf-BPP"><a href="#textbf-BPP" class="headerlink" title="$\textbf{BPP}$"></a>$\textbf{BPP}$</h1><p>B：Bounded-Error</p>
<p>接受标准：With bounded-error，即要严格大于$1&#x2F;2$。时间复杂度：最坏时间复杂度与平均时间复杂度都行（马尔可夫不等式），两者定义出来的是等价的。$\textbf{BPP}$一般用的标准是$2&#x2F;3$。</p>
<p>$$<br>\textbf{P} \subseteq \textbf{BPP} \subseteq \textbf{PP}<br>$$</p>
<p>$$<br>\textbf{BPP}&#x3D; \textbf{coBPP}<br>$$</p>
<blockquote>
<p>$\textbf{BPP}$是很鲁棒的，出错概率大和出错概率小的$\textbf{BPP}$定义的是同一个类。（出错概率高，则多运行几次，取出现多的为最终值）。</p>
<p>错误压缩定理：可以把错误压缩到指数的倒数。</p>
</blockquote>
<h2 id="带神谕的BPP"><a href="#带神谕的BPP" class="headerlink" title="带神谕的BPP"></a>带神谕的BPP</h2><p>$$<br>\textbf{BPP} ^{\textbf{BPP}} &#x3D; \textbf{BPP}<br>$$</p>
<blockquote>
<p>错误压缩，让子程序的调用几乎不出错。</p>
</blockquote>
<h2 id="textbf-BPP-完全"><a href="#textbf-BPP-完全" class="headerlink" title="$\textbf{BPP}$完全"></a>$\textbf{BPP}$完全</h2><p>不存在。如果$\textbf{BPP}&#x3D;\textbf{P}$，则存在。</p>
<p>Adleman Theorem：</p>
<p>$$<br>\textbf{BPP} \subseteq \textbf{P} _{\text{poly}}<br>$$</p>
<p>定理：</p>
<p>$$<br>\textbf{BPP} \subseteq \Sigma _2 ^p \cap \Pi _2 ^p<br>$$</p>
<blockquote>
<p>用错误压缩定理证明。</p>
</blockquote>
<h1 id="textbf-ZPP"><a href="#textbf-ZPP" class="headerlink" title="$\textbf{ZPP}$"></a>$\textbf{ZPP}$</h1><p>平均运行时间为P且不出错的概率图灵机：</p>
<p>$$<br>\textbf{ZPP}&#x3D;\bigcup _{c \in \text{N}}\textbf{ZTIME}(n ^c)<br>$$</p>
<blockquote>
<p>如一些排序算法</p>
</blockquote>
<p>$L \in \textbf{ZPP}$等价于存在最坏时间为$\textbf{P}$的概率图灵机，其在多项式时间内输出$\{0,1,?\}$，其中$?$表示算不出来，要求算不出来的概率小于等于1&#x2F;3。</p>
<blockquote>
<p>单向出错算法：回答是一定对，回答否可能错。</p>
</blockquote>
<p>多项式时间的单向出错算法：<br>$$<br>\textbf{RP}&#x3D;\bigcup _{c \in \text{N}} \textbf{RTIME}(n ^c)<br>$$</p>
<p>其与$\textbf{ZPP}$存在关系：</p>
<p>$$<br>\textbf{ZPP}&#x3D;\textbf{RP}\cap \textbf{coRP}<br>$$</p>
<blockquote>
<p>左到右：若让$?$是始终回答NO，则ZPP就是RP；若始终回答YES，则ZPP就是coRP；<br>右到左：RP回答YES一定是对的，coRP回答NO一定是对的，因此交叉运行两个算法即可。</p>
</blockquote>
<p>RP也有错误压缩定理。类似地，ZPP也有错误压缩定理。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zclzcl0223.github.io/2023/11/13/RandomizedComputation/" data-id="clzik0lcv007lv47k6wwx0r2m" data-title="Computational Complexity: Randomized Computation" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Math/" rel="tag">Math</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Theoretical-Computer-Science/" rel="tag">Theoretical Computer Science</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-DifferentTypesofGraph" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/11/07/DifferentTypesofGraph/" class="article-date">
  <time class="dt-published" datetime="2023-11-07T05:11:04.000Z" itemprop="datePublished">2023-11-07</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/GNN/">GNN</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/11/07/DifferentTypesofGraph/">Graphs of  Different Types</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <span id="more"></span>


<h1 id="同配图-amp-异配图"><a href="#同配图-amp-异配图" class="headerlink" title="同配图 &amp; 异配图"></a>同配图 &amp; 异配图</h1><ul>
<li>同配图（Homophilic&#x2F;Homophilous Graph）：具有相似特征或相同标签的结点趋向于连接在一起，即相连的结点一般属于同一个类别；</li>
<li>异配图（Heterophilic&#x2F;Heterophilous Graph）：相连的结点的特征或标签不具有相似性。</li>
</ul>
<p><img src="/2023/11/07/DifferentTypesofGraph/1.png" alt="1"></p>
<center style="font-size:12px; font-weight:bold">Fig. 1. Homophilic Graph and Heterophilic Graph</center>

<h1 id="同构图-amp-异构图"><a href="#同构图-amp-异构图" class="headerlink" title="同构图 &amp; 异构图"></a>同构图 &amp; 异构图</h1><ul>
<li>同构图（Homogeneous Graph）：结点和边都只有一个类型，即结点只有一类，结点间若存在关系，则该关系是确定的。</li>
<li>异构图（Heterogeneous Graph）：结点或边有多种类型，即结点的类型不定，结点间若存在关系，关系的类型也是不定的。</li>
</ul>
<p><img src="/2023/11/07/DifferentTypesofGraph/2.jpg" alt="2"></p>
<center style="font-size:12px; font-weight:bold">Fig. 2. Homogeneous Graph and Heterogeneous Graph</center>

<h1 id="时序图、时空图"><a href="#时序图、时空图" class="headerlink" title="时序图、时空图"></a>时序图、时空图</h1><ul>
<li>时序图（Temporal&#x2F;Dynamic graph）：结点和边的信息随时间变化的图。</li>
<li>时空图（Spatio-temporal graph）：在时序图的基础上，图的结构信息还反映了空间信息。</li>
</ul>
<p><img src="/2023/11/07/DifferentTypesofGraph/3.jpg" alt="3"></p>
<center style="font-size:12px; font-weight:bold">Fig. 3. Dynamic graph</center><br>

<p><img src="/2023/11/07/DifferentTypesofGraph/4.png" alt="4"></p>
<center style="font-size:12px; font-weight:bold">Fig. 4. Spatio-temporal graph</center>

<h1 id="二部图"><a href="#二部图" class="headerlink" title="二部图"></a>二部图</h1><ul>
<li>二部图（Bipartite graph）：节点可以分为两个子集的图，每一个边都连接属于不同子集的节点。</li>
</ul>
<p><img src="/2023/11/07/DifferentTypesofGraph/5.png" alt="5"></p>
<center style="font-size:12px; font-weight:bold">Fig. 5. Bipartite graph</center>
      
    </div>
    <footer class="article-footer">
      <a data-url="https://zclzcl0223.github.io/2023/11/07/DifferentTypesofGraph/" data-id="clzik0lc8001wv47kfaguclnw" data-title="Graphs of  Different Types" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/GNN/" rel="tag">GNN</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-CircuitComplexity" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/10/31/CircuitComplexity/" class="article-date">
  <time class="dt-published" datetime="2023-10-31T06:08:15.000Z" itemprop="datePublished">2023-10-31</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/CS7313-Computational-Complexity/">CS7313: Computational Complexity</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/10/31/CircuitComplexity/">Computational Complexity: Circuit Complexity</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <span id="more"></span>

<h1 id="Boolean-Circuit-Model"><a href="#Boolean-Circuit-Model" class="headerlink" title="Boolean Circuit Model"></a>Boolean Circuit Model</h1><p>布尔电路（Boolean Circuit）是一种不一致模型，对于同一个算法，不同的输入大小对应不同的布尔电路。（每个结点都有输入，也有对应的布尔运算，与、或、非）</p>
<blockquote>
<p>单调电路：不包含非门的电路。</p>
</blockquote>
<h2 id="电路复杂性"><a href="#电路复杂性" class="headerlink" title="电路复杂性"></a>电路复杂性</h2><p>电路复杂性即电路$C$包含的<em>门</em>的个数$|C|$。</p>
<blockquote>
<p>布尔电路族：解决同一个算法的不同大小的输入的一群电路称布尔电路族（Boolean Circuit Family）$\{C _n\} _{n \in \text{N}}$，其复杂度$S(n)$定义为$\forall n, |C _n| \le S(n)$。</p>
<p>电路能算的图灵机不一定能算。</p>
</blockquote>
<h2 id="Nonuniform-Complexity-Class"><a href="#Nonuniform-Complexity-Class" class="headerlink" title="Nonuniform Complexity Class"></a>Nonuniform Complexity Class</h2><p>电路族判定问题：如果存在一个$S(n)$大小的电路族$\{C _n\} _{n\in \text{N}}$，使得对任意$x \in \{0,1\} ^n$，$x \in L$当且仅当$C _n(x)&#x3D;1$，则称该电路族判定了问题$L$，也即问题$L$是在$\textbf{SIZE}(S(n))$中的。</p>
<blockquote>
<p>对非一致模型，$\textbf{SIZE}(cS(n)) \ne \textbf{SIZE}(S(n)),c &gt; 2$。</p>
</blockquote>
<h2 id="香农定理"><a href="#香农定理" class="headerlink" title="香农定理"></a>香农定理</h2><p>大多数$n$个输入的布尔函数，其电路复杂性一般大于$\frac{2 ^n}{n}-O(\frac{2 ^n}{n})$。证明的说明：</p>
<ol>
<li>$S$为电路门的个数。$3 ^S$表门有与、或、非三种，$2S$表每个门有0或1两种输入，$S+n+2$表不同的输入来源（直接输入、别的门的输出），故大小为$S$的电路个数约为$(S+n+2) ^{2S} 3 ^S$；</li>
<li>$(S-1)!$表不同排列但功能相同的电路；</li>
<li>$2 ^{2 ^n}$表具有$n$个输入的布尔函数的个数。</li>
</ol>
<h2 id="Circuit-Hierachy-Theorem"><a href="#Circuit-Hierachy-Theorem" class="headerlink" title="Circuit Hierachy Theorem"></a>Circuit Hierachy Theorem</h2><p>如果对$\epsilon &gt; 0$，$n &lt; (2+\epsilon)S(n) &lt; S&#39;(n) &lt;&lt; 2 ^n &#x2F;n$，则$\textbf{SIZE}(S(n)) \subsetneq \textbf{SIZE}(S&#39;(n))$</p>
<h1 id="Uniform-Circuit"><a href="#Uniform-Circuit" class="headerlink" title="Uniform Circuit"></a>Uniform Circuit</h1><p>一致电路族：可在对数空间内由$1 ^n$生成$C _n$。</p>
<blockquote>
<p>一致电路族能判定的问题就是$\text{P}$。</p>
</blockquote>
<p>右往左：Cook-Levin归约的用条带表示的格局图可视为一个电路。</p>
<h2 id="Circuit-Satisfability"><a href="#Circuit-Satisfability" class="headerlink" title="Circuit Satisfability"></a>Circuit Satisfability</h2><p>电路可满足问题$\textbf{CKT}-\textbf{SAT}$：一个代表输入大小为$n$的电路$C$的字符串$s$，若$\exists u \in \{0,1\} ^n$，使得$C(u)&#x3D;1$，则该字符串是在$\textbf{CKT}-\textbf{SAT}$里面的。</p>
<blockquote>
<p>$\textbf{CKT}-\textbf{SAT}$是$\text{NP}$完全的。</p>
<p>引申开来，任何NP问题都可以对数空间归约到SAT，因为CKT-SAT可以对数空间归约到SAT。</p>
</blockquote>
<h1 id="P-x2F-poly"><a href="#P-x2F-poly" class="headerlink" title="P &#x2F;poly"></a>P &#x2F;poly</h1><p>带advice的图灵机：</p>
<ul>
<li>$\text{P} _{&#x2F;\text{poly}}$</li>
<li>$\text{NP} _{&#x2F;\text{poly}}$</li>
<li>...</li>
</ul>
<p>其中advice即$\alpha _n \in \{0,1\} ^{a(n)}$。</p>
<ul>
<li>对于$L \in \text{P} _{&#x2F;\text{poly}}$，$x \in L \iff \mathbb{M}(x,\alpha(n))&#x3D;1$，且判定时间为多项式时间，$a(n)$亦为多项式。</li>
<li>对于$L \in \text{NP} _{&#x2F;\text{poly}}$，$x \in L \iff \mathbb{N}(x,\alpha(n))&#x3D;1$，且判定时间为多项式时间，$a(n)$亦为多项式。</li>
</ul>
<p>定理：<br>$$<br>\text{P} _{&#x2F;\text{poly}}&#x3D;\bigcup _c \textbf{SIZE}(cn ^c)<br>$$</p>
<blockquote>
<p>证明：</p>
<p>右到左：用对$C _n$的描述作为advice，由于$C _n$是多项式的，对它的描述也是多项式的；</p>
<p>左到右：将格局图的计算过程翻译成电路图，并把advice烧进电路。</p>
</blockquote>
<h1 id="Karp-Lipton-Theorem"><a href="#Karp-Lipton-Theorem" class="headerlink" title="Karp-Lipton Theorem"></a>Karp-Lipton Theorem</h1><p>显然$\textbf{P} \subseteq \textbf{P} _{&#x2F;\text{poly}}$。</p>
<p>定理：若$\textbf{NP} \subseteq \textbf{P} _{&#x2F;\text{poly}}$，则$\textbf{PH}&#x3D; \Sigma _2 ^p$</p>
<p>只需要证明$\Pi _2 ^p\subseteq \Sigma _2 ^p$，即证明$\Pi _2 ^p-\text{SAT}\subseteq \Sigma _2 ^p-\text{SAT}$。</p>
<h1 id="NC-and-AC"><a href="#NC-and-AC" class="headerlink" title="NC and AC"></a>NC and AC</h1><p>电路模型研究并行运算--同一层的门可以并行运算。</p>
<ul>
<li><p>有效的并行算法（Efficient Parallel Algorithm）：用多项式个处理器可以加速到对数的多项式时间的算法。</p>
</li>
<li><p>$\text{NC} ^d$：可被大小为$p(n)$，深度为$\log ^d (n)$的一致电路族$\{C _n\} _{n \in N}$判定的问题类。</p>
</li>
</ul>
<p>$$<br>\textbf{NC} &#x3D; \bigcup _{d \in \text{N}} \textbf{NC} ^d<br>$$</p>
<blockquote>
<p>$\textbf{NC} \in \textbf{P}$</p>
</blockquote>
<p>$\textbf{AC}$是$\textbf{NC}$的扩展，它可以接受无数个输入，但事实上：</p>
<p>$$<br>\textbf{AC}&#x3D;\textbf{NC}<br>$$</p>
<p>可达性就是布尔矩阵连乘（$A ^n$），故$\text{Reachability} \in \textbf{NC} ^2$。</p>
<h1 id="textbf-P-完全"><a href="#textbf-P-完全" class="headerlink" title="$\textbf{P}$完全"></a>$\textbf{P}$完全</h1><p>对数空间归约保持<em>高效可并行性</em>，即若$L \le _L L&#39; \in \textbf{AC}$，则$L \in \textbf{AC}$。</p>
<ul>
<li>对数空间归约时，每一位的输出都可并行计算；</li>
<li>格局图矩阵（多项式大小）的每一位都可并行计算，可在对数空间输出；</li>
<li>$\text{Reachability} \in \textbf{NC} ^2$。</li>
</ul>
<h2 id="textbf-P-完全-1"><a href="#textbf-P-完全-1" class="headerlink" title="$\textbf{P}$完全"></a>$\textbf{P}$完全</h2><p>$L&#39; \in \textbf{PC}$，如果$\forall L \in \textbf{P}$，$L \le _L L&#39;$。</p>
<blockquote>
<p>$\text{Circuit-Eval}$ 是$\textbf{PC}$的。单调电路也是$\textbf{PC}$的。</p>
</blockquote>
<p>$$<br>\textbf{NC} ^1 \subseteq \textbf{L} \subseteq \textbf{NL} \subseteq \textbf{NC} ^2...\textbf{NC} ^i \subseteq \textbf{P}<br>$$</p>
<ul>
<li>$\textbf{NL} \in \textbf{NC} ^2$，因为$\text{Reachability} \in \textbf{NC} ^2$。</li>
</ul>
<h1 id="Hastad-Switching-Lemma"><a href="#Hastad-Switching-Lemma" class="headerlink" title="Hastad Switching Lemma"></a>Hastad Switching Lemma</h1><p>$$<br>\textbf{NC} ^0 \subsetneq \textbf{AC} ^0 \subsetneq \textbf{NC} ^1<br>$$</p>
<blockquote>
<p>有些函数无法用常数高度的电路计算。即常数高度的电路严格弱于对数高度的电路。</p>
</blockquote>
<p>该引理专门用于研究电路复杂性的下界。</p>
<p>Minterm：$x _1,...,x _n$组成的集合，可取非，可$\land$。</p>
<p>$f$的极小项：$f _\rho (x _1,..., x _n)$恒等于1且不存在$\text{sub}-\rho$。</p>
<blockquote>
<p>极小项就是合取项。</p>
</blockquote>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zclzcl0223.github.io/2023/10/31/CircuitComplexity/" data-id="clzik0lby000iv47k4hxudl0m" data-title="Computational Complexity: Circuit Complexity" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Math/" rel="tag">Math</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Theoretical-Computer-Science/" rel="tag">Theoretical Computer Science</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-MatrixTheory4" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/10/25/MatrixTheory4/" class="article-date">
  <time class="dt-published" datetime="2023-10-25T04:52:41.000Z" itemprop="datePublished">2023-10-25</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/MATH6005-Matrix-Theory/">MATH6005: Matrix Theory</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/10/25/MatrixTheory4/">MatrixTheory: 特征值、特征向量</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <span id="more"></span>

<h1 id="不变子空间"><a href="#不变子空间" class="headerlink" title="不变子空间"></a>不变子空间</h1><p>$V: \mathbb{F}上$线性空间，$T \in L(V)$，$U$是$V$的子空间。若$\forall \alpha \in U$，$T(\alpha) \in U$，即$T(U) \subseteq U$，则称$U$是$V$的关于$T$的不变子空间。</p>
<blockquote>
<p>平凡子空间$\{0\}$和$V$是不变子空间。线性变换的零空间和像空间也是不变子空间。</p>
</blockquote>
<h2 id="特征值、特征向量"><a href="#特征值、特征向量" class="headerlink" title="特征值、特征向量"></a>特征值、特征向量</h2><p>若$V &#x3D; U _1 \oplus U _2 \oplus U _3$，其中$U _1...U _3$均为不变子空间，则以不变子空间的基为基向量的线性变换矩阵为分块对角矩阵。进一步地，若存在$\text{dim}V$个不变子空间且这些子空间的直和为$V$，则线性变换矩阵就是对角矩阵，对角线的值为特征值，不变子空间的基为特征向量。</p>
<p>$T(\alpha _i) &#x3D; \lambda _i \alpha _i$，$\lambda _i$为特征值，$\alpha _i$为特征向量，其中$\lambda \in \mathbb{F}$，$\alpha \not &#x3D;{0}$。</p>
<blockquote>
<p>这和$Ax &#x3D; \lambda x$是等价的，只不过矩阵的只能是有限维的。</p>
<p>不同特征值对应的特征向量线性无关。</p>
<p>$T\in L(V)$，$\mathbb{F}&#x3D;C$，$\text{dim}V&#x3D;n$，则$T$在<em>复数域</em>内一定有特征值。</p>
</blockquote>
<p>$T \in L(V)$，$U$是不变子空间</p>
<ol>
<li>$T | _U \in L(U)$，缩小算子$T$的定义域。</li>
<li>$T &#x2F; U: V&#x2F;U \to V&#x2F;U$，商空间到商空间的映射，只有在不变子空间下才合理。如$(T&#x2F;U)(\alpha + U) &#x3D; T(\alpha) + U$</li>
</ol>
<h2 id="简单的基下矩阵"><a href="#简单的基下矩阵" class="headerlink" title="简单的基下矩阵"></a>简单的基下矩阵</h2><p>对$T \in L(V)$，$\mathbb{F}&#x3D;C$，$\text{dim}V &#x3D; n$，一定存在一组基，使得基下的矩阵是上三角。</p>
<blockquote>
<p>证明：即证$T(\alpha _i) \in \text{span}[\alpha _1, ...,\alpha _i]$。要用到商空间和商空间的线性变换。</p>
</blockquote>
<p>等价地，复数域上的任意一个方阵一定相似于一个上三角。</p>
<h2 id="特征子空间"><a href="#特征子空间" class="headerlink" title="特征子空间"></a>特征子空间</h2><p>特征子空间$E(T, \lambda) &#x3D; \text{null}(T - \lambda I)$，内部元素为所有的特征向量+零向量。不同特征值的特征子空间的和是直和。</p>
<p>$$<br>E(T,\lambda _1) \oplus E(T, \lambda _2) \oplus...\oplus E(T, \lambda _m) \le V<br>$$</p>
<blockquote>
<p>取等当且仅当存在一组基使得基下的矩阵为对角矩阵；<br>也当且仅当$V$可以分成$n$个一维不变子空间的直和。</p>
</blockquote>
<h1 id="内积空间"><a href="#内积空间" class="headerlink" title="内积空间"></a>内积空间</h1><p>内积空间：定义了内积的线性空间。其中，内积满足：</p>
<ol>
<li>$(\alpha, \beta)&#x3D;(\beta, \alpha)$</li>
<li>$(\alpha+\beta,\gamma)&#x3D;(\alpha, \gamma)+(\beta,\gamma)$</li>
<li>$(k\cdot\alpha, \beta)&#x3D;k(\beta, \alpha)$</li>
<li>$(\alpha,\alpha)\ge 0,\text{iff}\space \alpha&#x3D;0\space \text{取等}。$</li>
</ol>
<p>对复数域，坐标相乘相加的内积要对第二项取共轭，即：</p>
<p>$$<br>(\alpha, \beta)&#x3D;\alpha \overline{\beta}<br>$$</p>
<blockquote>
<p>实数域的内积空间称<em>欧式空间</em>，复数域上的称<em>酉空间</em>。</p>
</blockquote>
<h2 id="内积的性质"><a href="#内积的性质" class="headerlink" title="内积的性质"></a>内积的性质</h2><ol>
<li>设$\beta \in V$，$\beta$固定。定义$T: V \to \mathbb{F}$，$\forall \alpha \in V$，$T(\alpha) &#x3D; (\alpha, \beta)$，$T \in L(V,\mathbb{F})$</li>
<li>$(0,\beta) &#x3D; (\beta,0)&#x3D;0$</li>
<li>$(\alpha, \beta + \gamma)&#x3D;(\alpha, \beta)+(\alpha,\gamma)$</li>
<li>$(\alpha, k\cdot\beta)&#x3D;\overline{k}(\alpha, \beta)$</li>
<li>$(\sum _{i&#x3D;1} ^m x _i \alpha _i,\sum _{j&#x3D;1} ^n \gamma _j \beta _j )&#x3D;\sum\sum x _i \overline{y _j}(\alpha _i, \beta _j)$</li>
</ol>
<h2 id="向量的长度"><a href="#向量的长度" class="headerlink" title="向量的长度"></a>向量的长度</h2><p>$$<br>||\alpha|| &#x3D; \sqrt{(\alpha, \alpha)}<br>$$</p>
<ol>
<li>$||k\alpha|| &#x3D; |k|\sqrt{(\alpha, \alpha)}$</li>
<li>$|(\alpha, \beta)|\le ||\alpha||||\beta||$（柯西不等式）。当且仅当两者线性相关等号成立。</li>
<li>$||\alpha + \beta||\le ||\alpha||+||\beta||$（三角不等式）。当且仅当两者共线等号成立。</li>
</ol>
<h1 id="正交向量组、标准正交向量组、标准正交基"><a href="#正交向量组、标准正交向量组、标准正交基" class="headerlink" title="正交向量组、标准正交向量组、标准正交基"></a>正交向量组、标准正交向量组、标准正交基</h1><p>正交向量组：向量间两两正交。正交向量组一定线性无关。</p>
<p>校准正交组：单位化的正交向量组。</p>
<p>标准正交基：可由一组线性无关的基经过斯密特正交化得到。</p>
<blockquote>
<p>斯密特正交化前后的基张成的空间是相同的空间。所以$\exists$标准正交基，使得基下的矩阵为上三角。</p>
<p>实数域的转置等价于复数域的共轭转置。</p>
<p>向量$\alpha$在标准正交基下的线性表示系数为$(\alpha, \gamma _i)$。</p>
</blockquote>
<h2 id="正交补空间"><a href="#正交补空间" class="headerlink" title="正交补空间"></a>正交补空间</h2><p>$V$内积空间，$U$子空间，定义：</p>
<p>$$<br>U ^{\perp} &#x3D; \{\alpha \in V: \forall \beta \in U, (\alpha,\beta)&#x3D;0\}<br>$$</p>
<p>$U ^{\perp}$是$V$的子空间，是$U$的正交不空间，且$V&#x3D;U \oplus U ^{\perp}$。</p>
<h3 id="度量矩阵"><a href="#度量矩阵" class="headerlink" title="度量矩阵"></a>度量矩阵</h3><p>$$<br>G &#x3D;<br>\begin{bmatrix}<br>    (\alpha _1, \alpha _1)...(\alpha _n, \alpha _1)\\<br>    ... &amp;\\\<br>    (\alpha _1, \alpha _n)...(\alpha _n, \alpha _n)<br>\end{bmatrix}<br>$$</p>
<p>则任意两个向量在指定基$\alpha _1,...,\alpha _n$下的内积为：</p>
<p>$$<br>(\alpha, \beta)&#x3D;\overline{y}Gx<br>$$</p>
<p>其中$y$是$\beta$在基下的坐标，$x$是$\alpha$在基下的坐标。</p>
<blockquote>
<p>实数域的$G$是个对称的正定矩阵。<br>复数域的$G$是个共轭对称的正定矩阵。（共轭转置：$G ^*&#x3D;G$）<br>正定一定对称。</p>
</blockquote>
<p>一个正定矩阵定义一个内积。不同基下的$G$矩阵合同。</p>
<h3 id="投影变换"><a href="#投影变换" class="headerlink" title="投影变换"></a>投影变换</h3><p>$V &#x3D; U \oplus U ^\perp,\forall \alpha \in V,\alpha &#x3D; \beta + \gamma,\beta \in U, \gamma \in U ^\perp$。定义$P _U: V \to V,\forall \alpha \in V, \alpha &#x3D; \beta + \gamma,P _U(\alpha)&#x3D;\beta$。$P _U$即为$V$中的向量$\alpha$到$U$的投影变换。投影变换有性质：</p>
<ol>
<li>$\text{null}P _U &#x3D; U ^\perp$</li>
<li>$\text{range}P _U &#x3D; U$</li>
<li>$P ^2 _U &#x3D; P _U$</li>
<li>设$\epsilon _1,...,\epsilon _k$为$U$的一组标准正交基，则$P _U (\alpha) &#x3D; (\alpha, \epsilon _1) \epsilon _1+...+(\alpha, \epsilon _k) \epsilon _k$</li>
</ol>
<h3 id="最佳近似向量"><a href="#最佳近似向量" class="headerlink" title="最佳近似向量"></a>最佳近似向量</h3><p>内积空间$V$，子空间$U$，$\beta \in V$，$\beta \notin U$，若有$\alpha \in U$，使得$\forall \gamma \in U$，$||(\beta - \alpha)|| \le ||\beta -\gamma||$，则称$\alpha$为$\beta$在$U$的最佳近似向量。</p>
<blockquote>
<p>即距离最小的。实际上$\alpha$就是$\beta$的正交投影向量。</p>
<p>用途：求矛盾方程的最佳近似解（又称最小二乘解）。如$Ax&#x3D;b$没有解，则$A ^TAx&#x3D;A^Tb$求出来的$x$就是最佳近似解。因为没有解，所以$b$不在列空间$C$中，列空间的正交补空间为$N(A ^T)$，设$A x _0$为最佳近似向量，则b在正交补空间的投影可表示为$b - A x _0$，正交补空间为$N(A ^T)$，所以有$A ^T(b - Ax _0)&#x3D;0$。</p>
</blockquote>
<h2 id="内积空间的线性变换"><a href="#内积空间的线性变换" class="headerlink" title="内积空间的线性变换"></a>内积空间的线性变换</h2><p>内积空间$V$，算子$T \in L(V)$。若$\forall \alpha,\beta \in V,(\alpha,\beta)&#x3D;(T(\alpha),T(\beta))$，则称该变换为等积变换。对应地，有：</p>
<ul>
<li>$||\alpha||&#x3D;||T(\alpha)||$，等长变换；</li>
<li>$||\alpha-\beta||&#x3D;||T(\alpha)-T(\beta)||$，等距变换；</li>
<li>$\epsilon _1,...,\epsilon _k$为$V$的标准正交基，$T(\epsilon _1),...,T(\epsilon _k)$也是标准正交基；</li>
<li>$T(\epsilon _1,..,\epsilon _n)&#x3D;(\epsilon _1,..,\epsilon _n)A _{n\times n},A ^* A &#x3D; A A ^* &#x3D;E$，这种变换称正交变换，即保持内积的变换。</li>
</ul>
<p>这几个都是等价的，也称正交变换。</p>
<blockquote>
<p>伴随变换：$(T(\alpha),\beta)&#x3D;(\alpha,S(\beta))$，其基下矩阵满足$A &#x3D; B ^*$。</p>
<p>自伴随$T&#x3D;S$，此时$A ^*&#x3D;A$</p>
</blockquote>
<h1 id="广义特征向量"><a href="#广义特征向量" class="headerlink" title="广义特征向量"></a>广义特征向量</h1><p>对$T\in L(V)$，$\lambda$为特征值，$(T -\lambda I) ^j (\alpha) &#x3D; 0$，$j$为正整数，$\alpha \ne 0$，则称$\alpha$为特征值$\lambda$的广义特征向量。$G(T,\lambda)$为广义特征子空间。</p>
<ol>
<li>$\{0\}&#x3D;\text{null}T ^0 \subseteq \text{null}T _1\subseteq ... \subseteq \text{null}T ^k$</li>
<li>若$\text{null}T ^{k+1} &#x3D;\text{null} ^k$，则$\text{null} ^{n+k+1}&#x3D;\text{null} ^{n+k}$</li>
<li>$\text{dim}V&#x3D;n$，则$\text{null}T ^n&#x3D;\text{null} T ^{n+1}$</li>
<li>$\text{dim}V&#x3D;n$，则$V &#x3D; \text{null} T ^n \oplus \text{range}T ^n$</li>
</ol>
<ul>
<li>$G(T,\lambda)&#x3D;\text{null}(T-\lambda I) ^{\text{dim}V}$，$T\in L(V)$，$\lambda _1,...,\lambda _m$是不同特征值，其对应的不同广义特征向量$\alpha _1,...,\alpha _m$线性无关。</li>
</ul>
<blockquote>
<p>证明：分别作用线性变换使得只有一项留下。如，定义$k$为使得$(T-\lambda _1 I) ^k (\alpha _1)\ne 0$的最大整数。</p>
</blockquote>
<h2 id="幂零变换"><a href="#幂零变换" class="headerlink" title="幂零变换"></a>幂零变换</h2><p>$N \in L(V)$，若$N ^k &#x3D; 0$，则称$N$为幂零变换，类似于幂零矩阵。对幂零变换，一定有$N ^{\text{dim}V}&#x3D;0$。</p>
<p>存在某个基，使得幂零变换在基下的上三角矩阵的对角线元素全为0，即幂零变换的特征值都是0。（因为上三角矩阵的对角线元素就是特征值）</p>
<p>$T \in L(V)$，$\lambda _1,...,\lambda _m$为不同特征值，则：</p>
<ol>
<li>$V &#x3D; G(T,\lambda _1)\oplus...\oplus G(T,\lambda _m)$;</li>
<li>$G(T,\lambda _i)$是$T$的不变子空间；</li>
<li>$(T - \lambda _i I) | _{G(T,\lambda _i)}$是幂零变换。</li>
</ol>
<blockquote>
<p>对2：$\forall \alpha \in G(T, \lambda _i) &#x3D; \text{null}(T - \lambda _i I) ^k$，有$(T - \lambda _i I) ^k (\alpha) &#x3D; 0$，$(T - \lambda _i I) ^k (T(\alpha)) &#x3D; T((T - \lambda _i I) ^k(\alpha))&#x3D;0$<br>对3：广义特征子空间的定义；</p>
</blockquote>
<blockquote>
<p>不变子空间的基下矩阵为分块对角阵。<br>特征值数、特征子空间维数：几何重数<br>广义特征向量、广义特征子空间维数：代数重数</p>
</blockquote>
<p>$V$存在由广义特征向量构成的基，基下的矩阵为分块对角，块数为特征值数，块的维数该特征值下广义特征子空间的维数。进一步地，该分块对角矩阵可被优化为上三角矩阵。</p>
<p>$$<br>T | _{G(T,\lambda _i)} &#x3D; (T - \lambda _i I) | _{G (T, \lambda _i)} + \lambda _i I | _{G (T, \lambda _i)}<br>$$</p>
<p>即，拆分为一个幂零变换和恒等变换的和。</p>
<h1 id="Jordan标准型"><a href="#Jordan标准型" class="headerlink" title="Jordan标准型"></a>Jordan标准型</h1><p>基下矩阵的进一步简化：只有对角线有值，且次对角线全为1。</p>
<p>$$<br>\begin{bmatrix}<br>    \lambda _1, 1, ...,...\\<br>    0, \lambda _2, 1,...\\<br>    0, 0, \lambda _3, 1<br>\end{bmatrix}<br>$$</p>
<blockquote>
<p>同一个特征值的Jordan块的数目取决于线性无关的特征向量的个数。</p>
</blockquote>
<p>$V$存在由广义特征向量构成的基，基下的矩阵为Jordan标准型。</p>
<blockquote>
<p>先研究幂零变换的Jordan标准型，其他可由幂零变换+恒等变换得到。</p>
</blockquote>
<p>若$N \in L(V)$为幂零变换，则$V$中存在一组向量$\alpha _1, \alpha _2,...,\alpha _m$，及一组非负整数$k _1, k _2,...,$，使得：</p>
<p>$$<br>\begin{align*}<br>    &amp;N ^{k _1}(\alpha _1), ... , N(\alpha _1), \alpha _1 \\<br>    &amp;N ^{k _2}(\alpha _2), ... , N(\alpha _2), \alpha _2 \\<br>    &amp;...\\<br>    &amp;N ^{k _m}(\alpha _m), ... , N(\alpha _m), \alpha _m \\<br>    &amp;N ^{K _i + 1}(\alpha _i)&#x3D;0<br>\end{align*}<br>$$</p>
<p>化为$V$的一组基。其中$m$为Jordan块个数。</p>
<blockquote>
<p>同一个特征值的Jordan块个数等于几何重数。<br>相似于Jordan标准型的特征向量为广义特征向量。</p>
</blockquote>
<h2 id="Hamilton-Cayley-Them"><a href="#Hamilton-Cayley-Them" class="headerlink" title="Hamilton-Cayley Them"></a>Hamilton-Cayley Them</h2><p>线性变换的特征多项式：</p>
<p>$$<br>f _{\tau}(\lambda) &#x3D; (\lambda - \lambda _1) ^{n _1}...(\lambda - \lambda _m) ^{n _m}<br>$$</p>
<p>其中$\sum n _i &#x3D; n$，若将该线性变换带入，即将$\lambda$换为$T$，则$f _\tau (T)$是一个零变换。该多项式称为零化多项式。</p>
<p>由该定理，$A ^n$以及更高次的$A _{n+1}$都可由$A ^{n-1},...,E$线性表示。若$A$可逆，则逆也可以由$A ^{n-1},...,E$线性表示。</p>
<h3 id="最小多项式"><a href="#最小多项式" class="headerlink" title="最小多项式"></a>最小多项式</h3><p>$A$的零化多项式中，次数$n$最低的首一多项式（最高项系数为1），记作$m _A(\lambda)$。</p>
<ol>
<li>最小多项式是唯一的；</li>
<li>设$f(\lambda)$为$A$的任意零化多项式，则$m _A(\lambda) | f(\lambda)$，即$f(\lambda)$一定可以被$m _A (\lambda)$整除。特别的，的任意零化多项式，则$m _A(\lambda) | f _A(\lambda)$，因为特征多项式是零化多项式；</li>
<li>$P ^{-1}A P &#x3D; B$，则$m _A(\lambda)&#x3D;m _B(\lambda)$；</li>
<li>$A$的任一特征值为$m _A (\lambda)$（对任意零化多项式也成立）的根；</li>
<li>若$A$是分块对角，则每个块的最小多项式$m _{A _i}(\lambda) | f(\lambda)$，即$A$的最小多项式是每个块最小多项式的最小公倍式；</li>
<li>$A$可对角化当且仅当$m _A(\lambda)$无重根（复数域上）。</li>
</ol>
<blockquote>
<p>Jordan标准型的最小多项式就是最大次数的几个不同Jordan块的最小多项式的乘积。<br>$\lambda ^n &#x3D; 1$无重根。</p>
</blockquote>
<blockquote>
<p>Jordan型求法：a. 求特征值：若特征值均不同，则对角化；否则求几何重数。对三阶矩阵，一定能求；对四阶，若所有特征值相同，几何重数为2，则可能的Jordan块组合为2+2和3+1，用最小多项式检验哪个是对的即可。（更高阶的不需要掌握）</p>
</blockquote>
<h1 id="圆盘定理"><a href="#圆盘定理" class="headerlink" title="圆盘定理"></a>圆盘定理</h1><p>近似地计算特征值 &amp; 确定特征值的范围：</p>
<p>$$<br>|\lambda - a _{ii}| \le \sum\limits _{i \ne j} ^n |a _{ij}|<br>$$</p>
<h2 id="第一圆盘定理"><a href="#第一圆盘定理" class="headerlink" title="第一圆盘定理"></a>第一圆盘定理</h2><p>$A$的任意一个特征值一定会落在某个圆盘内。</p>
<h2 id="第二圆盘定理"><a href="#第二圆盘定理" class="headerlink" title="第二圆盘定理"></a>第二圆盘定理</h2><p>几个圆盘叠在一起，那么这个叠加后的区域就有几个特征值。由此推知，若圆盘互相分离，则$A$一定有$n$个不同的特征值，$A$一定可以对角化，这是对复数域，若是实数域，则还可以说明特征值都是实数（因为此时圆盘是关于$x$轴对称的，根都是成对出现的，除了$x$轴上的根）。</p>
<h1 id="谱-amp-谱半径"><a href="#谱-amp-谱半径" class="headerlink" title="谱 &amp; 谱半径"></a>谱 &amp; 谱半径</h1><p>谱：所有特征值构成的集合。</p>
<p>谱半径$\rho (A)$：特征值模的最大值。</p>
<p>$$<br>\begin{align*}<br>    \mu&#x3D;&amp;\max \{\sum \limits _{j&#x3D;1} ^n |a _{ij}|\} _{1\le i \le n} \\<br>    \mu &#39;&#x3D;&amp;\max \{\sum \limits _{i&#x3D;1} ^n |a _{ij}|\} _{1\le j \le n} \\<br>    \rho (A) &amp;\le \min \{\mu,\mu &#39;\}<br>\end{align*}<br>$$</p>
<p>事实上：$\rho (A) \le ||A||$。</p>
<blockquote>
<p>$||A||$是矩阵范数。</p>
</blockquote>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zclzcl0223.github.io/2023/10/25/MatrixTheory4/" data-id="clzik0lcl0055v47k3nuudquy" data-title="MatrixTheory: 特征值、特征向量" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Math/" rel="tag">Math</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-PolynomialHierarchy" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/10/25/PolynomialHierarchy/" class="article-date">
  <time class="dt-published" datetime="2023-10-25T01:28:49.000Z" itemprop="datePublished">2023-10-25</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/CS7313-Computational-Complexity/">CS7313: Computational Complexity</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/10/25/PolynomialHierarchy/">Computational Complexity: Polynomial Hierarchy</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <span id="more"></span>

<h1 id="Problem-Beyond-NP"><a href="#Problem-Beyond-NP" class="headerlink" title="Problem Beyond NP"></a>Problem Beyond NP</h1><p>$\textbf{MINIMAL}&#x3D;\{\phi | \phi \text{DNF} \land \forall \text{DNF}\psi. |\psi| &lt; |\phi| \to \exists u. !{(\psi(u) &lt;&#x3D;&gt;\phi(u))}\}$，即$\phi$是所有等价的析取范式中最短的。</p>
<h1 id="Meyer-Stockmeyer-39-s-Definition"><a href="#Meyer-Stockmeyer-39-s-Definition" class="headerlink" title="Meyer-Stockmeyer&#39;s Definition"></a>Meyer-Stockmeyer&#39;s Definition</h1><p>$$<br>\begin{align*}<br>    \Sigma _0 ^p &amp;&#x3D;\textbf{P}\\<br>    \Sigma _{i+1} ^p &amp;&#x3D;\textbf{NP} ^{\Sigma _{i} ^p}\\<br>    \Delta _{i+1} ^{p}&amp;&#x3D;\textbf{P} ^{\Sigma _{i} ^p} \\<br>    \Pi _i ^p &amp;&#x3D; \textbf{co-}\Sigma _{i} ^p<br>\end{align*}<br>$$</p>
<p>有：</p>
<ul>
<li>$\Sigma _{i} ^p \subseteq \Delta _{i+1} ^{p} \subseteq \Sigma _{i + 1} ^p$</li>
<li>$\Pi _i ^p \subseteq \Delta _{i+1} ^{p} \subseteq \Pi _{i + 1} ^p$</li>
</ul>
<p>其中第二项是对第一项的取补。</p>
<p>定义多项式谱系为：$\textbf{PH}&#x3D;\bigcup _{i \ge 0}\Sigma _i ^p&#x3D;\bigcup _{i \ge 0}\Pi _i ^p&#x3D;\bigcup _{i \ge 0}\Delta _{i+1} ^p$。</p>
<p>某个语言$L \in \Sigma _i ^p$或$L \in \Pi _i ^p$的证据：</p>
<ul>
<li>对$\Sigma _i ^p$，证据为$\exists u _1 \in \{0,1\} ^{q(|x|)}.\forall u _2 \in \{0,1\} ^{q (|x|)}...Q _i u _i \in \{0,1\} ^{q (|x|)}$</li>
<li>对$\Pi _i ^p$，证据为$\forall u _1 \in \{0,1\} ^{q(|x|)}.\exists u _2 \in \{0,1\} ^{q (|x|)}...Q _i u _i \in \{0,1\} ^{q (|x|)}$</li>
</ul>
<p>这两个是对偶问题，只需证第一个即可。</p>
<ul>
<li>从右到左：猜测一个$u _1$，对每个猜测，计算$\Pi _{i-1} ^P$，而于是所有的猜测就是$\textbf{NP} ^{\Pi _{i-1} ^P}&#x3D;\textbf{NP} ^{\Sigma _{i-1} ^P}&#x3D;\Sigma _{i} ^P$。</li>
<li>从左到右：Cook-Levin定理，转化为$\textbf{SAT}$，再把$\textbf{SAT}$转化为量词的交替。</li>
</ul>
<blockquote>
<p>$\Sigma _i \textbf{SAT}$是$\Sigma _i ^p$中最难的问题，即它是$\Sigma _i ^p$完全的。证明：将$\mathbb{M}(x,u _1,...,u _{i+1})$Cook-Levin归约到$\Sigma _i \textbf{SAT}$。</p>
</blockquote>
<p>因为$\Sigma _i \textbf{SAT} \subseteq \textbf{PSPACE}$，所以$\textbf{PH} \subseteq \textbf{PSPACE}$</p>
<h2 id="Alternating-Turing-Machine"><a href="#Alternating-Turing-Machine" class="headerlink" title="Alternating Turing Machine"></a>Alternating Turing Machine</h2><p>交替图灵机：非确定图灵机的推广，每个状态都被标注为$\{\exists,\forall,\text{halt}\}$之一。交替图灵机$\text{ATM}$接受$x$当且仅当交替图灵机的执行序列中存在一个<em>接受子树</em>。</p>
<blockquote>
<p>接受子树：所有叶子结点标记$\text{halt}$且均为$1$，若某个非叶子结点被标记为$\exists$则它只有一个孩子在接受子树中，若被标记为$\forall$则有两个孩子接受子树中。</p>
</blockquote>
<p>$\text{TQBF}$可用$\text{ATM}$在平方时间和线性空间判定。</p>
<ul>
<li>$\textbf{NSPACE}(S(n)) \subseteq \textbf{ATIME}(S ^2 (n))$：格局图、路径，$\exists$则猜测，$forall$则并行计算，共猜测$S$次，每次耗时$S$。</li>
<li>$\textbf{ATIME}(T(n)) \subseteq \textbf{SPACE}(T (n))$：DFS。</li>
<li>$\textbf{ASPACE}(S(n)) \subseteq \bigcup _ {c \ge 0}\textbf{TIME}(c ^{S(n)})$：格局图，枚举。</li>
<li>$\textbf{TIME}(T(n)) \subseteq \textbf{ASPACE}(\log T(n))$：Cook-Levin证明一的图从下往上，猜测$\exist$，并行地验证$\forall$。每条路径空间为对数空间。</li>
</ul>
<p><img src="/2023/10/25/PolynomialHierarchy/1.png" alt="1"></p>
<center style="font-size:12px;font-weight:bold">Fig. 1.</center>

<blockquote>
<p>无穷谱系猜测是比$\textbf{NP}&#x3D;\textbf{P}$更强的假设，它假设多项式谱系间存在严格的包含关系。</p>
</blockquote>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zclzcl0223.github.io/2023/10/25/PolynomialHierarchy/" data-id="clzik0lcs006uv47kcxqo9ev8" data-title="Computational Complexity: Polynomial Hierarchy" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Math/" rel="tag">Math</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Theoretical-Computer-Science/" rel="tag">Theoretical Computer Science</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-MatrixTheory3" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/10/18/MatrixTheory3/" class="article-date">
  <time class="dt-published" datetime="2023-10-18T04:54:58.000Z" itemprop="datePublished">2023-10-18</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/MATH6005-Matrix-Theory/">MATH6005: Matrix Theory</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/10/18/MatrixTheory3/">MatrixTheory: Linear Transformation Matrix</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <span id="more"></span>

<h1 id="线性变换与矩阵"><a href="#线性变换与矩阵" class="headerlink" title="线性变换与矩阵"></a>线性变换与矩阵</h1><p>$T:U \to V$</p>
<p>零空间：$\text{null}\space T&#x3D;\{\alpha \in U:T(\alpha)&#x3D;0\}$</p>
<p>单射$\quad\text{iff}\quad\text{null}\space T &#x3D; {0}$。</p>
<p>像空间：$\text{range}\space T&#x3D;\{\beta \in V:\exists \alpha \in U,T(\alpha)&#x3D;\beta\}$</p>
<p>满射$\quad\text{iff}\quad\text{range}\space T &#x3D; V$。</p>
<p>维数公式：$\text{dim}(\text{range})+\text{dim}(\text{null})&#x3D;\text{dim}(U)$</p>
<blockquote>
<p>前提：有限维、线性变换。</p>
</blockquote>
<h2 id="线性变换矩阵"><a href="#线性变换矩阵" class="headerlink" title="线性变换矩阵"></a>线性变换矩阵</h2><p>对$\text{dim}U&#x3D;n,\text{dim}V&#x3D;m$，$T(\alpha _1,..., \alpha _n)&#x3D;(\beta _1, ...,\beta _m)C _{m\times n}$，其中$\alpha _1,...,\alpha _n$和$\beta _1,...,\beta _m$分别为$U$和$V$的基，称$C _{m\times n}$为线性变换$T$在给定基$\alpha _1,...,\alpha _n:\beta _1,...,\beta _m$下的矩阵。</p>
<blockquote>
<p>线性变换由一组基的变换唯一确定。</p>
<p>自身到自身的线性变换称为“算子”：$T \in L(U)$。此时，线性变换前后的基可以取一样的。</p>
<p>矩阵就是线性变换，线性变换就是矩阵·。</p>
</blockquote>
<h3 id="线性变换矩阵求零、像空间"><a href="#线性变换矩阵求零、像空间" class="headerlink" title="线性变换矩阵求零、像空间"></a>线性变换矩阵求零、像空间</h3><p>$T(\alpha) &#x3D; T((\alpha _1,...,\alpha _n)X)&#x3D; T(\alpha _1,...,\alpha _n)X&#x3D;(\beta _1,...,\beta _m)AX$</p>
<ul>
<li><p>求零空间<br>$T(\alpha)&#x3D;0\to \text{null}T\to N(A)$，此时得出的是能让$T(\alpha) &#x3D; 0$的$X$的基，记为$X&#39;$，再用$(\alpha _1,...,\alpha _n)X&#39;$即可得到零空间的基。</p>
</li>
<li><p>求像空间<br>此时要求的是像空间的基，而$AX$相当于对$A$进行列变换，因而$A$的秩就是像空间的维数，列变换得到列空间$C&#39;$，计算$(\beta _1,...,\beta _m)C&#39;$即可得到像空间的基。故$\text{range}T\to C(A)$</p>
</li>
</ul>
<blockquote>
<p>核心：什么样的基能够使得线性变化矩阵最简单。</p>
</blockquote>
<h3 id="线性变换与相似"><a href="#线性变换与相似" class="headerlink" title="线性变换与相似"></a>线性变换与相似</h3><p>同一个线性变换（算子）在不同基下的矩阵相似：</p>
<blockquote>
<p>对$T: U\to U$，$T (\alpha _1,...,\alpha _n) &#x3D; (\alpha _1,...,\alpha _n)A$；对$T: V \to V$，$T (\beta _1,...,\beta _m) &#x3D; (\beta _1,...,\beta _m)B$；对两个基，有过渡矩阵$P$，使得$(\beta _1,...,\beta _m)&#x3D;(\alpha _1,...,\alpha _n)P$，则：<br>$$<br>\begin{align*}<br>T (\beta _1,...,\beta _m)<br>&amp;&#x3D; (\beta _1,...,\beta _m)B \\<br>&amp;&#x3D; (\alpha _1,...,\alpha _n)PB \\<br>&amp;&#x3D; T((\alpha _1,...,\alpha _n)P) \\<br>&amp;&#x3D; (\alpha _1,...,\alpha _n)AP<br>\end{align*}<br>$$<br>故有：$PB&#x3D;AP$，若$P$可逆，则易知$A$和$B$是相似的。</p>
</blockquote>
<h2 id="逆变换"><a href="#逆变换" class="headerlink" title="逆变换"></a>逆变换</h2><p>若存在线性变换$T: U \to V$和$S: V\to U$，使得$ST &#x3D; I$，$TS&#x3D;I$，则称$S$为$T$的逆变换，其中$ST$和$TS$是之前定义的线性变换的乘法运算，$I$为恒等变换。逆变换若存在则是唯一的。</p>
<p>线性变换$T$可逆的充要条件：$T$单射且满射。</p>
<h2 id="同构"><a href="#同构" class="headerlink" title="同构"></a>同构</h2><p>若存在一个$T \in L(U,V)$，且$T$可逆，则称线性空间$U$和$V$是同构的。</p>
<p>两线性空间$U$、$V$同构的充要条件：$\text{dim}U&#x3D;\text{dim}V$。</p>
<ul>
<li>找同构映射：</li>
</ul>
<ol>
<li>定义一个映射$T$；</li>
<li>证明映射$T$是线性变换；</li>
<li>证明映射$T$单射且满射。</li>
</ol>
<ul>
<li>$V$为$\mathbb{F}$上的$n$维线性空间，则$V \simeq \mathbb{F} ^n$</li>
<li>$L(U,V)\simeq \mathbb{F} ^{n \times n}$</li>
</ul>
<p>线性变换就是矩阵。两个线性空间的线性变换空间的维数是者两个线性空间维数的乘积：</p>
<p>$$<br>\text{dim}L(U,V)&#x3D;\text{dim}U\cdot \text{dim}V<br>$$</p>
<h2 id="乘积空间"><a href="#乘积空间" class="headerlink" title="乘积空间"></a>乘积空间</h2><p>$V _1, V _2,..., V _k$为$\mathbb{F}$上的线性空间，则$V _1\times V _2\times ...\times V _k&#x3D;\{(\alpha _1,\alpha _2,...,\alpha _k):\alpha _i\in V _i \}$就称为$V _1, V _2,..., V _k$的乘积空间。乘积空间还是线性空间。</p>
<blockquote>
<p>$V _1, V _2,..., V _k$可以是元素类型不同的空间。乘积空间的加法、数乘仍是各元素在原空间的加法和数乘。<br>乘积空间的维数为各成员空间的维数和。</p>
</blockquote>
<h2 id="商空间"><a href="#商空间" class="headerlink" title="商空间"></a>商空间</h2><p>$V$是$\mathbb{F}$上的线性空间，$U$是$V$的子空间，$\alpha \in V$，定义$\alpha + U &#x3D; \{\alpha + \beta:\beta \in U\}$，则商空间为$V&#x2F;U&#x3D;{\alpha + U:\alpha \in V}$。</p>
<ul>
<li>商空间的加法：$(\alpha + U) + (\beta + U) &#x3D; (\alpha + \beta) + U$</li>
<li>商空间的数乘：$k\cdot (\beta + U) &#x3D; k\cdot \beta + U$</li>
</ul>
<p>下列三个公式等价：</p>
<ol>
<li>$\alpha - \beta \in U$</li>
<li>$\alpha + U &#x3D; \beta + \ U$</li>
<li>$(\alpha + U) \cap (\beta + U) \ne \emptyset$</li>
</ol>
<blockquote>
<p>平行线</p>
</blockquote>
<p>商空间也是线性空间，零向量是$\{\alpha + U,\alpha \in U\}$。</p>
<p>$\text{dim}(V&#x2F;U)&#x3D;n-k$，其中$\text{dim}V &#x3D; N$，$\text{dim}U&#x3D;k$</p>
<blockquote>
<p>证明：定义$T: V \to V &#x2F;U$，则对$\alpha \in V$，$T(\alpha) &#x3D; \alpha + U$（证明线性变换略）。</p>
<p>易知这个线性变换是满射的，即$\text{dim}\text{range}T&#x3D;\text{dim}(V&#x2F;U)$。而零空间，$T(\alpha) &#x3D; 0$，满足该条件的$\alpha$即$U$中的所有元素，故$\text{dim}\text{null}T &#x3D; \text{dim}U$。又$\text{dim}V &#x3D; \text{dim}\text{range}T + \text{dim}\text{null}T$，故$\text{dim}(V&#x2F;U) &#x3D; \text{dim}V - \text{dim}U$。</p>
</blockquote>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zclzcl0223.github.io/2023/10/18/MatrixTheory3/" data-id="clzik0lcl0059v47k9v291m4v" data-title="MatrixTheory: Linear Transformation Matrix" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Math/" rel="tag">Math</a></li></ul>

    </footer>
  </div>
  
</article>



  


  <nav id="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/10/">10</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Advanced-Model/">Advanced Model</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/CS7313-Computational-Complexity/">CS7313: Computational Complexity</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Configuration/">Configuration</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Dive-Into-Deep-Learning/">Dive Into Deep Learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/GNN/">GNN</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Kernel-Method/">Kernel Method</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/MATH6005-Matrix-Theory/">MATH6005: Matrix Theory</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning-by-AndrewNg/">Machine Learning by AndrewNg</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Math/">Math</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Paper/">Paper</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Programming-Language/">Programming Language</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Tool/">Tool</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Attention-Mechanism/" rel="tag">Attention Mechanism</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CNN/" rel="tag">CNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Configuration/" rel="tag">Configuration</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Data-Analysis/" rel="tag">Data Analysis</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Dataset-Distillation/" rel="tag">Dataset Distillation</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Decision-Trees/" rel="tag">Decision Trees</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Deep-Learning/" rel="tag">Deep Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GCN/" rel="tag">GCN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GNN/" rel="tag">GNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Generative-AI/" rel="tag">Generative AI</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hexo/" rel="tag">Hexo</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Lab/" rel="tag">Lab</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux/" rel="tag">Linux</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-Learning/" rel="tag">Machine Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Markup-Language/" rel="tag">Markup Language</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Math/" rel="tag">Math</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Paper/" rel="tag">Paper</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/" rel="tag">Python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RNN/" rel="tag">RNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Recommender-System/" rel="tag">Recommender System</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Reinforcement-Learning/" rel="tag">Reinforcement Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SVM/" rel="tag">SVM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Supervised-Learning/" rel="tag">Supervised Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Theoretical-Computer-Science/" rel="tag">Theoretical Computer Science</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Tool/" rel="tag">Tool</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Unsupervised-Learning/" rel="tag">Unsupervised Learning</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Attention-Mechanism/" style="font-size: 11.82px;">Attention Mechanism</a> <a href="/tags/CNN/" style="font-size: 12.73px;">CNN</a> <a href="/tags/Configuration/" style="font-size: 10px;">Configuration</a> <a href="/tags/Data-Analysis/" style="font-size: 10px;">Data Analysis</a> <a href="/tags/Dataset-Distillation/" style="font-size: 13.64px;">Dataset Distillation</a> <a href="/tags/Decision-Trees/" style="font-size: 10px;">Decision Trees</a> <a href="/tags/Deep-Learning/" style="font-size: 20px;">Deep Learning</a> <a href="/tags/GCN/" style="font-size: 11.82px;">GCN</a> <a href="/tags/GNN/" style="font-size: 15.45px;">GNN</a> <a href="/tags/Generative-AI/" style="font-size: 10.91px;">Generative AI</a> <a href="/tags/Hexo/" style="font-size: 10.91px;">Hexo</a> <a href="/tags/Lab/" style="font-size: 10.91px;">Lab</a> <a href="/tags/Linux/" style="font-size: 16.36px;">Linux</a> <a href="/tags/Machine-Learning/" style="font-size: 17.27px;">Machine Learning</a> <a href="/tags/Markup-Language/" style="font-size: 10px;">Markup Language</a> <a href="/tags/Math/" style="font-size: 19.09px;">Math</a> <a href="/tags/Paper/" style="font-size: 14.55px;">Paper</a> <a href="/tags/Python/" style="font-size: 18.18px;">Python</a> <a href="/tags/RNN/" style="font-size: 10.91px;">RNN</a> <a href="/tags/Recommender-System/" style="font-size: 10.91px;">Recommender System</a> <a href="/tags/Reinforcement-Learning/" style="font-size: 10.91px;">Reinforcement Learning</a> <a href="/tags/SVM/" style="font-size: 10px;">SVM</a> <a href="/tags/Supervised-Learning/" style="font-size: 11.82px;">Supervised Learning</a> <a href="/tags/Theoretical-Computer-Science/" style="font-size: 15.45px;">Theoretical Computer Science</a> <a href="/tags/Tool/" style="font-size: 18.18px;">Tool</a> <a href="/tags/Unsupervised-Learning/" style="font-size: 11.82px;">Unsupervised Learning</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/06/">June 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/01/">January 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/12/">December 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/06/">June 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/05/">May 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/04/">April 2023</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2024/06/17/Diffusion/">Diffusion</a>
          </li>
        
          <li>
            <a href="/2024/01/20/VAE/">VAE</a>
          </li>
        
          <li>
            <a href="/2023/12/02/CountingComplexity/">Computational Complexity: Complexity of Counting</a>
          </li>
        
          <li>
            <a href="/2023/11/24/MatrixTheory5/">MatrixTheory: 特殊矩阵与矩阵分解</a>
          </li>
        
          <li>
            <a href="/2023/11/13/RandomizedComputation/">Computational Complexity: Randomized Computation</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 ChaosTsang<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/" class="mobile-nav-link">home</a>
  
    <a href="/about/" class="mobile-nav-link">about</a>
  
    <a href="/tags/" class="mobile-nav-link">tags</a>
  
    <a href="/categories/" class="mobile-nav-link">categories</a>
  
    <a href="/archives/" class="mobile-nav-link">archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>





<script src="/js/script.js"></script>





  </div>
</body>
</html>