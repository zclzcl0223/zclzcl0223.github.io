<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=[object Object]"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', '[object Object]');
</script>
<!-- End Google Analytics -->

  
  <title>JourneyToCoding</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="Start of Something New">
<meta property="og:type" content="website">
<meta property="og:title" content="JourneyToCoding">
<meta property="og:url" content="https://zclzcl0223.github.io/page/5/index.html">
<meta property="og:site_name" content="JourneyToCoding">
<meta property="og:description" content="Start of Something New">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="ChaosTsang">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="JourneyToCoding" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/%5Bobject%20Object%5D">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">JourneyToCoding</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">Code for fun</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/">home</a>
        
          <a class="main-nav-link" href="/about/">about</a>
        
          <a class="main-nav-link" href="/tags/">tags</a>
        
          <a class="main-nav-link" href="/categories/">categories</a>
        
          <a class="main-nav-link" href="/archives/">archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://zclzcl0223.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-Attention" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/06/06/Attention/" class="article-date">
  <time class="dt-published" datetime="2023-06-05T21:29:23.000Z" itemprop="datePublished">2023-06-06</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Dive-Into-Deep-Learning/">Dive Into Deep Learning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/06/06/Attention/">Attention Mechanisms: More Targeted Information Extraction</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <span id="more"></span>

<h1 id="Attention-in-Biology"><a href="#Attention-in-Biology" class="headerlink" title="Attention in Biology"></a>Attention in Biology</h1><p>In biology, there are two kinds of cues that affect our attention: volitional cue (自主性提示) and nonvolitional cue (非自主性提示).</p>
<p><img align="left" src="/2023/06/06/Attention/1.png" style=" width:380px; padding: 0px 0px; "> <img align="mid" src="/2023/06/06/Attention/2.png" style=" width:380px; padding: 0px 0px; "></p>
<center style="font-size:12px; font-weight:bold">Fig. 1. Nonvolitional cue (left) and Volitional cue (right)</center><br>

<p>The nonvolitional cue is based on the saliency of objects in the environment. It&#39;s unconscous. As the above picture shows, when there is a red cup among several white objects, we will unconsciously choose the red cup as its color is more prominent. The volitional cue is controlled by our awareness and our decisions are led by our consciousness. For example, after drinking a cup of coffee, we may want to relax, because of which we choose a book to read.</p>
<h1 id="Attention-Mechanisms"><a href="#Attention-Mechanisms" class="headerlink" title="Attention Mechanisms"></a>Attention Mechanisms</h1><p>The nonvolitional cue and volitional cue inspire us to take the environment as input and use volitional cue to focus the attention to a certain object or feature.</p>
<h2 id="Queries-Keys-and-Values"><a href="#Queries-Keys-and-Values" class="headerlink" title="Queries, Keys and Values"></a>Queries, Keys and Values</h2><p>In attention mechanisms, the volitional cue is also called <strong>query</strong>. It reflects the tendency of feature extraction. The nonvolitional cue is actually our perception of the environment. The characteristics of the environment can be represented by several <strong>key-value pairs</strong>. These three things consist of the input of the attention mechanism. Giving queries and keys, the attention mechanism will aggregate values and choose the most prominent and suitable value as output. Such a process is also called <em>attention pooling</em>. It is queries that distinguish attention pooling from dense layers.</p>
<p><img src="/2023/06/06/Attention/3.png" alt="3"></p>
<center style="font-size:12px; font-weight:bold">Fig. 2. Attention pooling</center>

<h2 id="Mathematical-Representation-of-Attention"><a href="#Mathematical-Representation-of-Attention" class="headerlink" title="Mathematical Representation of Attention"></a>Mathematical Representation of Attention</h2><p>Our attention to a certain object reflects how much we care about it, or, mathematically, the weight we give it. When a certain query is offered to us, it is natural for us to think of finding something similar to this query to get the answer. That is, for a key-value pair, the more similar the key is to the query, the higher proportion its value should take up in the answer. This is how <em><strong>Nadaraya-Watson kernel regression</strong></em> works:</p>
<p>$$<br>f(\mathbf{q})&#x3D;\sum\limits _{i&#x3D;1} ^n \frac{K(\mathbf{q}-\mathbf{k}_i)}{\sum\limits _{j&#x3D;1} ^n K(\mathbf{q}-\mathbf{k}_j)}\mathbf{v}_i<br>$$</p>
<p>where $K$ is a kernel. Such an estimator assigns different weight to different keys for a certain query. More generally, attention is defined as:</p>
<p>$$<br>\text{Attention}(\mathbf{q},\mathbf{k},\mathbf{v})&#x3D;\sum\limits _{i&#x3D;1} ^n\alpha(\mathbf{q},\mathbf{k}_i)\mathbf{v}_i<br>$$</p>
<p>where $\alpha(\mathbf{q},\mathbf{k}_i)$ is <em>attention weight</em>. It is non-negative and its sum is $1$. The output is a weighted sum of the values for each key-value pair.</p>
<p><img src="/2023/06/06/Attention/4.png" alt="4"></p>
<center style="font-size:12px; font-weight:bold">Fig. 3. Mathematical representation of attention pooling</center> <br>

<p>It is noteworthy that Nadaraya-Watson kernel regression is a nonparametric model (<em>nonparametric attention pooling</em>). Therefore, it is consistent. With enough data, it will converge to the optimum result. However, in machine learning, the agent needs parameters to learn so that it could improve its performance. For instance, if we set $K$ as a gaussian kernel and add learned parameter $\mathbf{w}$, we get a formula that looks like softmax:</p>
<p>$$<br>\begin{align*}<br>K(\mathbf{u})&amp;&#x3D;\frac{1}{\sqrt{2\pi}}\exp(-\frac{\mathbf{u}^2}{2})\\<br>f(\mathbf{q})<br>    &amp;&#x3D;\sum\limits _{i&#x3D;1} ^n\alpha(\mathbf{q},\mathbf{k}_i)\mathbf{v}_i\\<br>    &amp;&#x3D;\sum\limits _{i&#x3D;1} ^n\frac{\exp[-\frac{1}{2}(\mathbf{q}-\mathbf{k}_i)^2\mathbf{w}^2]}{\sum\limits _{j&#x3D;1} ^n\exp[-\frac{1}{2}(\mathbf{q}-\mathbf{k}_i)^2\mathbf{w}^2]}\mathbf{v}_i\\<br>    &amp;&#x3D;\sum _{i&#x3D;1} ^n\text{softmax}[-\frac{1}{2}(\mathbf{q}-\mathbf{k}_i)^2\mathbf{w}^2]\mathbf{v}_i<br>\end{align*}<br>$$</p>
<h2 id="Attention-Scoring-Function"><a href="#Attention-Scoring-Function" class="headerlink" title="Attention Scoring Function"></a>Attention Scoring Function</h2><p>The exponent of the gaussian kernel above ($-{\mathbf{u}^2}&#x2F;2$) is actually an attention scoring function ($a$) which evaluates the similarity between $\mathbf{q}$ and $\mathbf{k}_i$. Hence, the attention pooling can be also represented as:</p>
<p>$$<br>\begin{align*}<br>    \text{Attention}(\mathbf{q},\mathbf{k},\mathbf{v})&amp;&#x3D;\sum\limits _{i&#x3D;1} ^n\alpha(\mathbf{q},\mathbf{k}_i)\mathbf{v}_i\\<br>    &amp;&#x3D;\sum\limits _{i&#x3D;1} ^n\text{softmax}[a(\mathbf{q},\mathbf{k}_i)]\mathbf{v}_i<br>\end{align*}<br>$$</p>
<p><img src="/2023/06/06/Attention/5.png" alt="5"></p>
<center style="font-size:12px; font-weight:bold">Fig. 4. Computing the output of attention pooling</center> <br>

<p>where $\text{softmax}$ generates attention weights. The more similar the key $\mathbf{k}_i$ is to the query $\mathbf{q}$, the higher weight its value $\mathbf{v}_i$ gets. The output is the weighted sum of values.</p>
<p>More generally, query $\mathbf{q}$ and key $\mathbf{k}_i$ are both <strong>vectors</strong> of different length. The attention scoring function maps these two vectors to a scalar</p>
<h3 id="Additive-Attention"><a href="#Additive-Attention" class="headerlink" title="Additive Attention"></a>Additive Attention</h3><p>Additive attention uses MLP to evaluate the similarity between $\mathbf{q}$ and $\mathbf{k}$, where $\mathbf{q}\in\mathbf{R}^q$ and $\mathbf{k}\in\mathbf{R}^k$:</p>
<p>$$<br>a(\mathbf{q,k})&#x3D;[\tanh(\mathbf{q}\mathbf{W}_q+\mathbf{k}\mathbf{W}_k)]\mathbf{w}_v\in\mathbf{R}<br>$$</p>
<p>where $\mathbf{W}_q\in\mathbf{R} ^{q\times h}$, $\mathbf{W}_k\in\mathbf{R} ^{k\times h}$ and $\mathbf{w}_v\in\mathbf{R} ^{h}$. All of them are learned parameters (here we ignore the bias) in one dense layer.</p>
<blockquote>
<p>In practice, each batch may have several queries and key-value pairs. And there are always more than one batches. In this case, the shape of $\mathbf{q}$ is <code>(batch_size, num_queries, q)</code> and the shape of $\mathbf{k}$ is <code>(batch_size, num_kv_pairs, k)</code>. The shape of $\mathbf{q}\mathbf{W}_q$ is <code>(batch_size, num_queries, h)</code> and the shape of $\mathbf{k}\mathbf{W}_k$ is <code>(batch_size, num_kv_pairs, h)</code>. <code>num_queries</code> and <code>num_kv_pairs</code> are always different, which means that we can&#39;t add $\mathbf{q}\mathbf{W}_q$ and $\mathbf{k}\mathbf{W}_k$ directly. </p>
<p>To solve this, we always use <code>queries.unsqueeze(2)</code> and <code>keys.unsqueeze(1)</code> (<code>queries</code> and <code>keys</code> are both tensor) to expand their dimensions. After <code>unsqueeze</code>, the shape of <code>queries</code> is <code>(batch_size, num_queries, 1, h)</code> and the shape of <code>keys</code> is <code>(batch_size, 1, num_kv_pairs, h)</code>. Now broadcasting can make $\mathbf{q}\mathbf{W}_q+\mathbf{k}\mathbf{W}_k$ legal. The nature of this operation is that each query should take all the keys into account so the $2$-th dimension of <code>queries</code> is expanded to <code>num_kv_pairs</code>, so is <code>keys</code>. </p>
</blockquote>
<h3 id="Scaled-Dot-Product-Attention"><a href="#Scaled-Dot-Product-Attention" class="headerlink" title="Scaled Dot-Product Attention"></a>Scaled Dot-Product Attention</h3><p>If the shape of $\mathbf{q}$ and $\mathbf{k}$ are the same (both are $d$), we could just use dot-product to evaluate their similarity:</p>
<p>$$<br>a(\mathbf{q,k})&#x3D;\mathbf{q}^\text{T}\mathbf{k}&#x2F;\sqrt{d}<br>$$</p>
<p>where $\sqrt{d}$ is to make sure that the value of $a(\mathbf{q,k})$ will not change significantly when the shape changes.</p>
<p>If considering SGD, the output of attention pooling is:</p>
<p>$$<br>\text{softmax}(\mathbf{Q}\mathbf{K}^\text{T}&#x2F;\sqrt{d})\mathbf{V}<br>$$</p>
<p>where $d$ is still the shape of each query or each key.</p>
<h1 id="Different-Types-of-Attention"><a href="#Different-Types-of-Attention" class="headerlink" title="Different Types of Attention"></a>Different Types of Attention</h1><p>Depending on the number of attention pooling and the relationship among queries, keys and values, there are various forms of attention mechanisms.</p>
<h2 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h2><p>Just as different convolutional output channels can extract different types of features, different attention pooling will also learn various behaviors. Hence, we can use multi-head attention to generate multiple attention pooling outputs $\mathbf{h}_1...\mathbf{h}_i$ in parallel and concatenate them together to form $(\mathbf{h}_1,...,\mathbf{h}_i)$.</p>
<p><img src="/2023/06/06/Attention/6.png" alt="6"></p>
<center style="font-size:12px; font-weight:bold">Fig. 5. Multi-head attention</center>

<h2 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h2><p>Self-attention (intra-attention) is a special kind of attention, whose queries, keys and values are all the same things.</p>
<h1 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h1><p>When dealing with sequence information, RNNs operate each token in sequence. Therefore, the location information of the sequence is used. However, the attention pooling operates all the inputs in parallel, because of which, the location information is wasted.</p>
<p>Positional encoding is the location information added to the keys, values and queries. In computer science, we use binary string to represent position (e.g. 0: 000, 1: 001, ..., 7: 111). As can be seen, the bit on each position alternates at diferent frequencies respectively. Higher bits alternate less frequently than lower bits. Such alternating changes can represent discrete positional information.</p>
<h2 id="Absolute-Positional-Information"><a href="#Absolute-Positional-Information" class="headerlink" title="Absolute Positional Information"></a>Absolute Positional Information</h2><p>For the input $\mathbf{X}\in\mathbf{R} ^{n\times d}$ which contains $n$ tokens and $d$ features for each token, we can use positional embedding matrix $\mathbf{P}\in\mathbf{R} ^{n\times d}$ to offer it positional information:</p>
<p>$$<br>\begin{align*}<br>    \mathbf{X&#39;}&amp;&#x3D;\mathbf{X}+\mathbf{P}\\<br>    \mathbf{P} _{i,2j}&amp;&#x3D;\sin(\frac{i}{10000 ^{2j&#x2F;d}})\\<br>    \mathbf{P} _{i,2j+1}&amp;&#x3D;\cos(\frac{i}{10000 ^{2j&#x2F;d}})<br>\end{align*}<br>$$</p>
<p>It works because different columns, which are similar to the bit position in binary string, oscillate at different frequencies. It is just like the binary string but the value is continuous.</p>
<p><img src="/2023/06/06/Attention/7.png" alt="7"></p>
<center style="font-size:12px; font-weight:bold">Fig. 6. Oscillating frequency of different columns</center>

<h2 id="Relative-Positional-Information"><a href="#Relative-Positional-Information" class="headerlink" title="Relative Positional Information"></a>Relative Positional Information</h2><p>The positional information we add to $\mathbf{X}$ is called absolute positional information. What&#39;s more, we can also get the relative positional information for different rows in the same column:</p>
<p>$$<br>\begin{align*}<br>    (\mathbf{P} _{i,2j}, \mathbf{P} _{i,2j+1})&amp;&#x3D;(\sin\frac{i}{10000 ^{2j&#x2F;d}},\cos\frac{i}{10000 ^{2j&#x2F;d}})\\<br>    (\mathbf{P} _{i+\delta,2j}, \mathbf{P} _{i+\delta,2j+1})&amp;&#x3D;(\sin\frac{i+\delta}{10000 ^{2j&#x2F;d}},\cos\frac{i+\delta}{10000 ^{2j&#x2F;d}})\\<br>    &amp;&#x3D;({\begin{bmatrix}<br>           \cos\delta\omega_j\space\space \sin\delta\omega_j\\<br>           -\sin\delta\omega_j\space\space \cos\delta\omega_j<br>       \end{bmatrix}}<br>      {\begin{bmatrix}<br>           \mathbf{P} _{i,2j}\\<br>           \mathbf{P} _{i,2j+1}<br>       \end{bmatrix}})^\text{T}\\<br>    &amp;&#x3D;[\sin(i+\delta)\omega_j,\cos(i+\delta)\omega_j]\\<br>\end{align*}<br>$$</p>
<p>where $\omega_j&#x3D;1&#x2F;10000^{2j&#x2F;d}$.</p>
<blockquote>
<p>Positional information is information added to the input $\mathbf{X}$, including keys, values and queries, it could be fixed positional encoding like what we do above or learned one.</p>
</blockquote>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zclzcl0223.github.io/2023/06/06/Attention/" data-id="clzik0lbw0009v47k49efh3tk" data-title="Attention Mechanisms: More Targeted Information Extraction" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Attention-Mechanism/" rel="tag">Attention Mechanism</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Deep-Learning/" rel="tag">Deep Learning</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-OptimizationAlgorithms" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/06/02/OptimizationAlgorithms/" class="article-date">
  <time class="dt-published" datetime="2023-06-02T07:39:10.000Z" itemprop="datePublished">2023-06-02</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Dive-Into-Deep-Learning/">Dive Into Deep Learning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/06/02/OptimizationAlgorithms/">Optimization Algorithms</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <span id="more"></span>

<h1 id="Momentum-method"><a href="#Momentum-method" class="headerlink" title="Momentum method"></a>Momentum method</h1><p>SGD introduces randomness so that gradient descent may jump out of local minimum points and saddle points. What&#39;s more, the computational complexity of SGD is lower than the regular gradient descent.</p>
<p>However, because of the randomness, SGD may oscillate around the optimum point. Besides, different model parameters may require different learning rate. When all model parameters share the same learning rate, it is likely that some model parameters with large gradient diverge and others with small gradient converge slowly. A smaller learning rate may solve this problem but it will slow the progress of the training of the whole model.</p>
<p>The momentum method helps us solve the problem above. It introduces a new parameter $\mathbf{v}_t$ (<em>speed</em>) to provide inertia for the gradient:</p>
<p>$$<br>\begin{cases}<br>    \mathbf{v}_t&#x3D;\beta\mathbf{v} _{t-1}+\mathbf{g} _t\\<br>    \mathbf{w}_t &#x3D;\mathbf{w} _{t-1}-\alpha\mathbf{v} _t<br>\end{cases}<br>$$</p>
<p>where $\beta$ is a hyperparameter that determines the magnitude of inertia. When $\beta$ is $0$, it reverts to the regular gradient descent. Compared to the regular gradient descent, the momentum method gives a certain weight to the gradient of the old moment $t_1$ at the new moment $t_2$. Under extreme conditions:</p>
<p>$$<br>\begin{cases}<br>    \tau&#x3D;t_2-t_1&#x3D;\infty\\<br>    \sum\limits _{\tau&#x3D;0} ^\infty\beta ^\tau\mathbf{g} _{t_1}&#x3D;\frac{1}{1-\beta}\mathbf{g} _{t_1}<br>\end{cases}<br>$$</p>
<p>that is, the weight of gradient is $1&#x2F;(1-\beta)$. By doing so, the size of the gradient is guaranteed and gradents are more likely to descend in a better direction.</p>
<blockquote>
<p>When using the momentum method in SGD, we should maintain a auxiliary variable $\mathbf{v}$. It is a vector and is initialized to $0$. It should record the history of all batches and iteration rounds. Namely, it is initialized only once.</p>
<p>In PyTorch, it has been included in <code>torch.optim.SGD</code>, we only need to pass the hyperparameter $\beta$ (<code>momentum</code>) to PyTorch.</p>
</blockquote>
<h1 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h1><p>Adam combines many techniques (momentum, adagrad, etc.) into one efficient learning algorithm. The mechanism of adam is quite simple, it uses the second moment to adjust the size of the gradient based on the momentum method so as to achieve the purpose of dynamically adjusting the learning rate:</p>
<p>$$<br>\begin{cases}<br>    \mathbf{v}_t&#x3D;\beta _1 \mathbf{v} _{t-1} + (1-\beta _1) \mathbf{g}_t \\<br>    \mathbf{s}_t&#x3D;\beta _2 \mathbf{s} _{t-1}+ (1-\beta _2) \mathbf{g}_t ^2<br>\end{cases}<br>$$</p>
<p>Similarly, under extreme conditions:</p>
<p>$$<br>\begin{cases}<br>    \sum\limits _{\tau&#x3D;0} ^\infty\beta _1 ^\tau(1-\beta _1)\mathbf{g} _{t_1}&#x3D;\mathbf{g} _{t_1}\\<br>    \sum\limits _{\tau&#x3D;0} ^\infty\beta _2 ^\tau(1-\beta _2) \mathbf{g} _{t_1} ^2&#x3D;\mathbf{g} _{t_1} ^2<br>\end{cases}<br>$$</p>
<p>However, when $\tau$ is not large enough, we can&#39;t get the formula above. Hence, we usually standardize $\mathbf{v}$ and $\mathbf{s}$:</p>
<p>$$<br>\begin{cases}<br>    \hat{\mathbf{v}}_t&#x3D;\frac{\mathbf{v}_t}{1-\beta_1^t}\\<br>    \hat{\mathbf{s}}_t&#x3D;\frac{\mathbf{s}_t}{1-\beta_2^t}<br>\end{cases}<br>$$</p>
<p>Now, the gradient is:</p>
<p>$$<br>\mathbf{g}_t&#39;&#x3D;\alpha\frac{\hat{\mathbf{v}}_t}{\sqrt{\hat{\mathbf{s}}_t}+\epsilon}<br>$$</p>
<p>where $\epsilon$ is to make sure that division by $0$ errors will not occur. Its value is usually $10^{-6}$.</p>
<p>And the gradient descent is:</p>
<p>$$<br>\mathbf{w}_t&#x3D;\mathbf{w} _{t-1}-\mathbf{g}_t&#39;\space\text{or}\space\mathbf{w}_t&#x3D;\mathbf{w} _{t-1}-\frac{\alpha}{\sqrt{\hat{\mathbf{s}}_t}+\epsilon}\hat{\mathbf{v}}_t<br>$$</p>
<p>where the latter reflects the fact that the adam algorithm dynamically adjusts the learning rate $\alpha$ and is not sensitive to the learning rate, just like what we talk about in <a href="/2023/04/10/NeuralNetwork/#Adam-algorithm">Adam</a>.</p>
<blockquote>
<p>In PyTorch, we can use <code>torch.optim.Adam</code> to call adam algorithm. The only hyperparameter we need to pass to it is $\alpha$.</p>
</blockquote>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zclzcl0223.github.io/2023/06/02/OptimizationAlgorithms/" data-id="clzik0lcq006cv47k1hpea11w" data-title="Optimization Algorithms" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Deep-Learning/" rel="tag">Deep Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-Learning/" rel="tag">Machine Learning</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-EncoderDecoder" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/05/31/EncoderDecoder/" class="article-date">
  <time class="dt-published" datetime="2023-05-31T05:14:00.000Z" itemprop="datePublished">2023-05-31</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Dive-Into-Deep-Learning/">Dive Into Deep Learning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/05/31/EncoderDecoder/">The Encoder-Decoder Architecture</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <span id="more"></span>

<h1 id="Another-view-to-ANNs"><a href="#Another-view-to-ANNs" class="headerlink" title="Another view to ANNs"></a>Another view to ANNs</h1><p>Firstly, let&#39;s explain CNNs from the perspective of encoder-decoder:</p>
<p><img src="/2023/05/31/EncoderDecoder/1.png" alt="1"></p>
<center style="font-size:12px; font-weight:bold">Fig. 1. View CNNs in a new perspective</center><br>

<p>When the agent extracts features from the input using convolutional layers and pooling layer, it encodes the image as a vector that better reflects some certain characteristics of the image. After that, the agent decodes these features and generates output.</p>
<p>Similarly, for CNNs, the embedding layer and LSTM (GRU) layer encode texts as vectors. Then, The agent decodes these message and generates output.</p>
<p><img src="/2023/05/31/EncoderDecoder/2.png" alt="2"></p>
<center style="font-size:12px; font-weight:bold">Fig. 2. View RNNs in a new perspective</center><br>

<p>This is the encoder-decoder architecture. In this architecture, the encoder deals with input , passes information to the decoder and the decoder generates output. More generally, the information the encoder passes to the decoder is called <em>state</em> which contains features of the input. The decoder can also receive new input so that it will get more information about the thing it copes with.</p>
<p><img src="/2023/05/31/EncoderDecoder/3.png" alt="3"></p>
<center style="font-size:12px; font-weight:bold">Fig. 3. Encoder-Decoder</center><br>

<h1 id="Seq2seq"><a href="#Seq2seq" class="headerlink" title="Seq2seq"></a>Seq2seq</h1><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1409.3215.pdf">Sequence to sequence</a> (<a href="(https://arxiv.org/pdf/1406.1078.pdf)">Seq2seq</a>) is a kind of neural network using the encoder-decoder architecture to complete the task of <em>sequence to sequence learning</em>. It is applied mainly in machine translation.</p>
<p><img src="/2023/05/31/EncoderDecoder/4.png" alt="4"></p>
<center style="font-size:12px; font-weight:bold">Fig. 4. Machine translation and Seq2seq (English to French)</center><br>

<p><img src="/2023/05/31/EncoderDecoder/5.png" alt="5"></p>
<center style="font-size:12px; font-weight:bold">Fig. 5. Structure of Seq2seq</center><br>

<h2 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h2><p>The encoder is a DRNN without regression layers. It works like the other DRNNs: take the source sequence (English) as input and output vectors. What is worth noticing is the <em>embedding layer</em>. In fact, it is an improvement on one-hot encoding. When using one-hot encoding, we have to use a $t\times m$ tensor for a sentence with $t$ tokens and a vocabulary with $m$ tokens. This is space-consuming. However, the embedding layer can map $m$-dimensional vectors to $k$-dimensional vectors ($k&lt;m$). This cuts the size of space we have to spend to store a sentence.</p>
<h2 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h2><p>The decoder takes the target sequence (French) $\mathbf{X}$ as a part of input and also uses the embedding layer. In addition, it also takes the last hidden state of the last RNN layer of the encoder $\mathbf{C}$ as a part of input. $\mathbf{C}$ contains the context about the source sequence. $[\mathbf{X}, \mathbf{C}]$ form the complete input and are fed to the decoder. The last hidden states of all the RNN layers of the encoder are also used to initialize the hidden states of all the RNN layers of the decoder. Hence, the depth of RNN layer in encoder and decoder should be the same (that is $n$).</p>
<blockquote>
<p>For convenience, the length of all the sequences ($t$) is fixed. Hence, for the sentences that are longer than $t$, they are cut to $t$; for the sentences that are shorter than $t$, they are filled up to $t$. When computing the loss using softmax, we should only consider the valid length and set the filled part to $0$.</p>
</blockquote>
<h2 id="Training-and-predicting"><a href="#Training-and-predicting" class="headerlink" title="Training and predicting"></a>Training and predicting</h2><p>When training, what we feed to the decoder are the correct translation sequence, like Fig. 4. However, when predicting, we only feed the token that indicates the start of a sentence (<code>&lt;bos&gt;</code>) to the decoder and the output is fed as input to the decoder until the decoder generates the token that indicates the end of a sentence (<code>&lt;eos&gt;</code>).</p>
<p><img src="/2023/05/31/EncoderDecoder/6.png" alt="6"></p>
<center style="font-size:12px; font-weight:bold">Fig. 6. Making prediction (translation)</center>

<h2 id="Assessment"><a href="#Assessment" class="headerlink" title="Assessment"></a>Assessment</h2><p><a target="_blank" rel="noopener" href="https://aclanthology.org/P02-1040.pdf">Bilingual evaluation understudy</a> (BLEU) is a method to evaluate the accuracy of machine translation or other sequence to sequence application. BLEU is always range between $0$ and $1$. The larger BLEU is, the more accurate the prediction is. It is defined as:</p>
<p>$$<br>\exp(\min(0,1-\frac{\text{len} _{label}}{\text{len} _{pred}}))\prod\limits _{n&#x3D;1}^k p _n ^{1&#x2F;2^n}<br>$$</p>
<p>where $\text{len} _{label}$ is the number of tokens in target sequence $y$ while $\text{len} _{pred}$ is the number of tokens in prediction sequence $\hat{y}$. Since shorter predictions contain less tokens, which makes $\prod\limits _{n&#x3D;1}^k p _n ^{1&#x2F;2^n}$ bigger, there will be penalty for short predictions.</p>
<blockquote>
<p>The smaller $\text{len} _{pred}$ is, the larger $\text{len} _{label}&#x2F;{\text{len} _{pred}}$ is, and thus the smaller $\exp(\min(...))$ is.</p>
</blockquote>
<p>$p_n$ is the accuracy of $n$-grams, namely, the accuracy of prediction of $n$ tokens in sequence. Due to the difficulty to predict long sequence, long sequence will get higher weight than short sequence. $1&#x2F;2^n$ help us do this. Because $p_n$ is often smaller than $1$ and the bigger $n$ is, the smaller $1&#x2F;2^n$ is, long sequence will always get larger value even though $p_n$ keeps the same.</p>
<p><img src="/2023/05/31/EncoderDecoder/7.png" alt="7"></p>
<center style="font-size:12px; font-weight:bold">Fig. 7. Curve of p<sub>n</sub><sup>1/2<sup>n</sup></sup>(p<sub>n</sub>=0.5)</center>

<h1 id="Beam-search"><a href="#Beam-search" class="headerlink" title="Beam search"></a>Beam search</h1><p>When predicting a token using softmax, we are actually using <em>Greedy search</em>. That is, for each timestep, the token we choose is the one with the highest probability.</p>
<p><img src="/2023/05/31/EncoderDecoder/8.png" alt="8"></p>
<center style="font-size:12px; font-weight:bold">Fig. 8. Greedy search</center> <br>

<p>However, local optimum does not necessarily lead to global optimum. If we choose B in timestep1 and feed it to decoder in timestep2, maybe we will get higher accuracy for the prediction in timestep2. <em>Exhaustive search</em> can solver this problem but it is time-consuming.</p>
<p>Beam search is a compromise between greedy search and Exhaustive search. In the first timestep, beam search will choose <em>k</em> tokens with high probability and finally generate <em>k</em> condidate sequence. Then, sequence with the highest probability is chosen. <em>k</em> is a hyperparameter called <em>beam size</em>.</p>
<p><img src="/2023/05/31/EncoderDecoder/9.png" alt="9"></p>
<center style="font-size:12px; font-weight:bold">Fig. 9. Beam search</center>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zclzcl0223.github.io/2023/05/31/EncoderDecoder/" data-id="clzik0lc90024v47k69bibu6b" data-title="The Encoder-Decoder Architecture" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/CNN/" rel="tag">CNN</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Deep-Learning/" rel="tag">Deep Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/RNN/" rel="tag">RNN</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-CommonRNNModels" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/05/29/CommonRNNModels/" class="article-date">
  <time class="dt-published" datetime="2023-05-29T11:53:15.000Z" itemprop="datePublished">2023-05-29</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Dive-Into-Deep-Learning/">Dive Into Deep Learning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/05/29/CommonRNNModels/">Common RNN Models</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>RNN is a special kind of MLP that takes its output as input to remember the previous information. It is such an simple struture that it can&#39;t remember as much information as we want. Besides, since RNN updates hidden state recurrently, the cumulative chain of gradients will become rather long, which may result in gradient explosion or gradient vanishing.</p>
<h1 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h1><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1406.1078.pdf">Gated Recurrent Unit</a> (GRU) is a method to solve gradient anomaly in RNN. Its idea is based on three facts:</p>
<ul>
<li>Some tokens have nothing to do with previous tokens.</li>
<li>Some tokens are closely related to previous tokens.</li>
<li>Current token may make no sense. It is better to just ignore its influence.</li>
</ul>
<p>Hence, GRU defines two kinds of gate to catch long-term and short-term relationship:</p>
<ul>
<li>Reset Gate $\mathbf{R}_t$, catches short-term relationship between current token and previous tokens:<br>$$\mathbf{R}_t&#x3D;\sigma(\mathbf{X}_t\mathbf{W} _{xr}+\mathbf{H} _{t-1}\mathbf{W} _{hr}+\mathbf{b}_r)$$</li>
<li>Update Gate $\mathbf{Z}_t$, catches long-term relationship between current token and previous tokens:<br>$$\mathbf{Z}_t&#x3D;\sigma(\mathbf{X}_t\mathbf{W} _{xz}+\mathbf{H} _{t-1}\mathbf{W} _{hz}+\mathbf{b}_z)$$</li>
</ul>
<p>Both values of $\mathbf{R}_t$ and $\mathbf{Z}_t$ ($\sigma$ means sigmoid) are between $0$ and $1$. $\mathbf{R}_t$ is used to generate <em>candidate hidden state</em>:</p>
<p>$$\tilde{\mathbf{H}}_t&#x3D;\tanh(\mathbf{X}_t\mathbf{W} _{xh}+(\mathbf{R}_t\odot\mathbf{H} _{t-1})\mathbf{W} _{hh}+\mathbf{b}_h)$$</p>
<p>As its name suggests, reset gate $\mathbf{R}_t$ will weigh the influence of previous tokens $\mathbf{H} _{t-1}$ on current sequence and gather the information of current token and previous tokens. Since $\mathbf{R}_t$ is always smaller than $1$, it pays more attention to current token $\mathbf{X}_t$ while weakens $\mathbf{H} _{t-1}$. That&#39;s why we say that $\mathbf{R}_t$ catches short-term relationship.</p>
<p>After generating the <em>candidate hidden state</em>, update gate $\mathbf{Z}_t$ generates real $\mathbf{H}_t$ from previous sequence $\mathbf{H} _{t-1}$ and current candidate sequence $\tilde{\mathbf{H}}_t$:</p>
<p>$$\mathbf{H}_t&#x3D;\mathbf{Z}_t\odot\mathbf{H} _{t-1}+(1-\mathbf{Z}_t)\odot\tilde{\mathbf{H}}_t$$</p>
<p>When $\mathbf{Z}_t$ is close to $0$, it means that the current token make sense and it may have something to do with previous tokens (based on $\mathbf{R}_t$). On the contrary, when $\mathbf{Z}_t$ is close to $1$, it means that the current token makes no sense. $\mathbf{Z}_t$ weighs $\mathbf{H} _{t-1}$ and $\tilde{\mathbf{H}}_t$. That&#39;s why we say that $\mathbf{Z}_t$ catches long-term relationship. </p>
<p><img src="/2023/05/29/CommonRNNModels/1.png" alt="1"></p>
<center style="font-size:12px; font-weight:bold">Fig. 1. Computing process in GRU</center><br>

<p>Compared with RNN, GRU requires more model parameters ($\mathbf{W} _{xr}$, $\mathbf{W} _{hr}$, $\mathbf{W} _{xz}$, $\mathbf{W} _{hz}$, $\mathbf{W} _{xh}$, $\mathbf{W} _{hh}$ and corresponding bias $\mathbf{b}$). All of them are learnable parameters in one GRU layer. The two gates help the agent properly forget current or previous information. This will shorten the gradient cumulative chain to a certain degree.</p>
<blockquote>
<p><code>nn.GRU</code> is the class of GRU in PyTorch. It is almost the same as <code>nn.RNN</code> though it has more model parameters.</p>
</blockquote>
<h1 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h1><p><a target="_blank" rel="noopener" href="https://papers.baulab.info/Hochreiter-1997.pdf">Long-Short-Term Memory</a> (LSTM) is almost the same as GRU but it is more complicated.</p>
<p><img src="/2023/05/29/CommonRNNModels/2.png" alt="2"></p>
<center style="font-size:12px; font-weight:bold">Fig. 2. Computing process in LSTM</center><br>

<p>Compared with GRU, LSTM use memory cell $\mathbf{C}_t$ to record information. The function of $\mathbf{C}_t$ in LSTM is similar as $\mathbf{H}_t$&#39;s in GRU. But $\mathbf{C}_t$ only propagates inside LSTM and the output of LSTM is still $\mathbf{H}_t$. In addition, LSTM takse $\mathbf{Z}_t$ and $(1-\mathbf{Z}_t)$ apart to form forget gate $\mathbf{F}_t$ and input gate $\mathbf{I}_t$:</p>
<p>$$<br>\begin{cases}<br>    \mathbf{F}_t&#x3D;\sigma(\mathbf{X}_t\mathbf{W} _{xf}+\mathbf{H} _{t-1}\mathbf{W} _{hf}+\mathbf{b}_f) \\<br>    \mathbf{I}_t&#x3D;\sigma(\mathbf{X}_t\mathbf{W} _{xi}+\mathbf{H} _{t-1}\mathbf{W} _{hi}+\mathbf{b}_i)<br>\end{cases}<br>$$</p>
<p>Similarly, LSTM use $\tilde{\mathbf{C}}_t$ to record the relationship between current token and previous tokens but it doesn&#39;t reset previous tokens:</p>
<p>$$\tilde{\mathbf{C}}_t&#x3D;\tanh(\mathbf{X}_t\mathbf{W} _{xc}+\mathbf{H} _{t-1}\mathbf{W} _{hc}+\mathbf{b}_c)<br>$$</p>
<p>Then, LSTM generate memory cell $\mathbf{C}_t$ by weighing previous information $\mathbf{C} _{t-1}$ and current information $\tilde{\mathbf{C}}_t$:</p>
<p>$$\mathbf{C}_t&#x3D;\mathbf{F}_t\odot\mathbf{C} _{t-1}+\mathbf{I}_t\odot\tilde{\mathbf{C}}_t$$</p>
<p>Since $\mathbf{F}_t$ and $\mathbf{I}_t$ are no longer relevant. LSTM can combine previous information and current information more flexibly. $\mathbf{C}_t$ only flows inside LSTM. To generate the output, LSTM use output gate $\mathbf{O}_t$ for reset gate $\mathbf{R}_t$:</p>
<p>$$<br>\begin{cases}<br>    \mathbf{O}_t&#x3D;\sigma(\mathbf{X}_t\mathbf{W} _{xo}+\mathbf{H} _{t-1}\mathbf{W} _{ho}+\mathbf{b}_o)\\<br>    \mathbf{H}_t&#x3D;\mathbf{O}_t\odot\tanh(\mathbf{C}_t)<br>\end{cases}<br>$$</p>
<p>where $\tanh$ is to make sure that $\mathbf{H}_t$ is range from $-1$ to $1$ so that the agent can control gradients better.</p>
<blockquote>
<p><code>nn.LSTM</code> is the class of LSTM in PyTorch. It is almost the same as <code>nn.GRU</code>.</p>
</blockquote>
<p>Though LSTM is more complex, GRU performs as well as LSTM.</p>
<h1 id="DRNN"><a href="#DRNN" class="headerlink" title="DRNN"></a>DRNN</h1><p>Deep Recurrent Neural Networks (DRNNs) are neural networks with multiple hidden layers. Their structures are the same as MLP&#39;s.</p>
<h1 id="BRNN"><a href="#BRNN" class="headerlink" title="BRNN"></a>BRNN</h1><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2210.08402.pdf">Bidirectional Recurrent Neural Networks</a> (BRNNs) are neural networks that consider not only the leftward context but also the rightward context. In some scenarios, a token is not only associated with its previous tokens but also its future tokens, for example, contextual analysis and cloze. In this case, it is better for the agent to consider the future tokens in addition to the previous tokens. And that&#39;s what BRNNs does:</p>
<p><img src="/2023/05/29/CommonRNNModels/3.png" alt="3"></p>
<center style="font-size:12px; font-weight:bold">Fig. 3. BRNNs</center><br>

<p>BRNNs assign two independent RNN layers with the same structure to each hidden layer, where one deals with the leftward context and another one deals with the rightward context (just takse the antisequence as input):</p>
<p>$$<br>\begin{cases}<br>    \overrightarrow{\mathbf{H}} _t&#x3D;\text{activation}(\mathbf{X}_t\mathbf{W} _{xh} ^{(f)}+\overrightarrow{\mathbf{H}} _{t-1}\mathbf{W} _{hh} ^{(f)}+\mathbf{b} _h ^{(f)})\\<br>    \overleftarrow{\mathbf{H}} _t&#x3D;\text{activation}(\mathbf{X}_t^r\mathbf{W} _{xh} ^{(b)}+\overleftarrow{\mathbf{H}} _{t-1}\mathbf{W} _{hh} ^{(b)}+\mathbf{b} _h ^{(b)})<br>\end{cases}<br>$$</p>
<p>Then, the concatenation of $\overrightarrow{\mathbf{H}} _t$ and $\overleftarrow{\mathbf{H}} _t$ is passed as output $\mathbf{H} _t&#x3D;(\overrightarrow{\mathbf{H}} _t,\overleftarrow{\mathbf{H}} _t)$ to the next layer.</p>
<p>BRNNs only work when the sequence contains information about future. Hence, BRNNs can&#39;t predict the future. Instead, BRNNs are usually used to <strong>extract features</strong> from a sequence, just like what convolutional layers do.</p>
<blockquote>
<p>There is keyword <code>bidirectional</code> in <code>nn.RNN</code>, <code>nn.GRU</code> and <code>nn.LSTM</code>. To enable BRNN, we can just set <code>bidirectional=True</code>.</p>
</blockquote>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zclzcl0223.github.io/2023/05/29/CommonRNNModels/" data-id="clzik0lc0000qv47kbqfr6abw" data-title="Common RNN Models" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Deep-Learning/" rel="tag">Deep Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/RNN/" rel="tag">RNN</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-RNN" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/05/25/RNN/" class="article-date">
  <time class="dt-published" datetime="2023-05-25T09:27:35.000Z" itemprop="datePublished">2023-05-25</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Dive-Into-Deep-Learning/">Dive Into Deep Learning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/05/25/RNN/">RNN: a Special Kind of MLP</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <span id="more"></span>

<h1 id="Sequence-model-amp-Language-model"><a href="#Sequence-model-amp-Language-model" class="headerlink" title="Sequence model &amp; Language model"></a>Sequence model &amp; Language model</h1><p>Sequence information is the data arranged in a certain order, whose biggest feature is contexutal correlation. Language, or text, is typical sequence information. When dealing with sequence information, what the neural network does is to predict the future based on the history, that is:</p>
<p>$$x_t\sim P(x_t|x_1,...,x _{t-1})$$</p>
<p>where $x_t$ is correlated with its history $(x_1,...,x _{t-1})$. Similarly, if we wanna predict a new sequence, we deal with:</p>
<p>$$(x_1,...,x_T)\sim \prod\limits_{t&#x3D;1}^TP(x_t|x_1,...,x _{t-1})$$</p>
<p>Such a model or such a relationship is called sequence model. In sequence model, the data is not independent but sequential. We use previous data to predict future data. However, when the sequence is extremely long, the quantity of previous data will be rather large. In general, there are two methods to cope with it.</p>
<h2 id="Markov-assumption"><a href="#Markov-assumption" class="headerlink" title="Markov assumption"></a>Markov assumption</h2><p>Markov assumption (or Markov model) assumes that the future $x_t$ is only correlated with a small span of the past $(x _{t-1},...,x _{t-\tau})$ where $\tau$ is the span. In this case:</p>
<p>$$x_t\sim P(x_t|x_{t-\tau},...,x _{t-1})$$</p>
<p>$\tau$ is an import hyperparameter that determines the complexity of prediction. When $\tau&#x3D;m$, the model is called <em>mth-order Markov model</em>:</p>
<p>$$(x_1,...,x_T)\sim \prod\limits_{t&#x3D;1}^TP(x_t|x _{\max (t-\tau,0)},...,x _{t-1})$$</p>
<p>where $x_0$ has nothing to do with $x_i$, that is, it is independent from others. Such a model is also called <em>Autoregressive model</em>.</p>
<h2 id="Latent-autoregressive-models"><a href="#Latent-autoregressive-models" class="headerlink" title="Latent autoregressive models"></a>Latent autoregressive models</h2><p>In this model, we use a new parameter $h_t$ to summarize the past information:</p>
<p>$$h_t&#x3D;g(h_{t-1},x_{t-1})$$</p>
<p>$$\hat{x}_t&#x3D;P(x_t|h_t)$$</p>
<p>where $h_t$ is the latent variable.</p>
<p><img src="/2023/05/25/RNN/1.png" alt="1"></p>
<center style="font-size:12px; font-weight:bold">Fig. 1. Latent autoregressive models</center><br>

<blockquote>
<p>In a word, $(x _{t-\tau},...,x _{t-1})$ is the feature of $x_t$. The aim of RNN is to obtain the mapping relation $x_t&#x3D;f(x _{t-\tau},...,x _{t-1})$ for markov models or $h_t&#x3D;g(h _{t-1},x _{t-1})$ and $\hat{x}_t&#x3D;f(h_t)$ for latent autoregressive models.</p>
</blockquote>
<h2 id="Language-models"><a href="#Language-models" class="headerlink" title="Language models"></a>Language models</h2><p>The language model is a typical sequence model. However, because languages are in the form of <em>string</em>, it is extremely hard for computers to cope with them. Hence, we always divide a text into several <em>tokens</em>. A token is a string and a word in the original text. Then, we count the probablity of occurrence of all tokens or token sequences and use markov models to model language models. The value of $\tau$ decides the number of tokens we take into account to predict $x_t$. For instance, one token (tokens are independent from each other) is <em>Unigram</em>, two tokens are <em>Bigram</em> and three tokens are <em>Trigram</em>.</p>
<blockquote>
<p>One-hot encoding is used to turn a token into a vector so that the neural network can work.</p>
</blockquote>
<h1 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h1><p>RNNs (recurrent neural networks) are neural networks with <em>hidden state</em> which is another <em>input</em> of the hidden layer and is updated by calling the hidden layer recurrrently. </p>
<p><img src="/2023/05/25/RNN/2.png" alt="2"></p>
<center style="font-size:12px; font-weight:bold">Fig. 2. Hidden state</center><br>

<p>The hidden state <em>H</em> is actually the same as input <em>X</em>, that is, they are both inputs of hidden layer:</p>
<p>$$\text{Output}_t&#x3D;H_t&#x3D;\phi(X _tW _{xh}+H _{t-1}W _{hh}+b_h)$$</p>
<p>The following picture shows the nature of RNN better:</p>
<p><img src="/2023/05/25/RNN/3.png" alt="3"></p>
<center style="font-size:12px; font-weight:bold">Fig. 3. CNN</center><br>

<p>In each timing, a token or other sequential unit $X_t$ enters the RNN as an input. $H_{t-1}$ records the information of previous tokens. They together produce $H_t$. $H_t$ records the information of current token $X_t$ and previous tokens. Hence, it is only the output of this hidden layer at this timing but also the input of the hidden layer at the next timing. That&#39;s why such a neural network is called RNN: for a sequence with several tokens, all the tokens will be fed to RNN in sequence. Their relationship is recorded by $H$. The updated $H$ is fed to RNN recurrently. And finally, after dealing with the last token, the prediction is made.</p>
<p><img src="/2023/05/25/RNN/4.png" alt="4"></p>
<center style="font-size:12px; font-weight:bold">Fig. 4. Process of RNN</center><br>

<blockquote>
<p>Though we use the RNN layer several times (depends on the length of sequence) to generate $H_1,...,H_t$ in a single iteration, it is still a layer. Namely, during one iteration, $W_{xh}$, $W_{hh}$ and $b_h$ keep the same. What changes is $H$.</p>
</blockquote>
<p>When the batch is generated by random sampling, that is, the sequence of different batches is not continuous (e.g. batch1: [1, 2], batch2: [8, 9]), $H$ must be initialized to 0 in each iteration. Otherwise (e.g. batch1: [1, 2], batch2: [3, 4]), $H$ should be kept as the last result of the former batch. By doing so, the current batch and the former batch form a longer sequence. In practice, we use random sampling more as the text we cope with is often too long to remember all of it only using $H$.</p>
<blockquote>
<p>The api of RNN in PyTorch is <code>nn.RNN</code> .</p>
</blockquote>
<h2 id="Gradient-clipping"><a href="#Gradient-clipping" class="headerlink" title="Gradient clipping"></a>Gradient clipping</h2><p>Timesteps represent the length of a sequence or $\tau$ in markov assumption. The larger the timestep is, the more information the RNN need to remember. In addition, because the operation of a RNN hidden layer of k timesteps is equivalent to the operation of k dense layers, it is more likely for RNN to have gradient explosion.</p>
<p>To solve this, what we use is gradient clipping. For all the parameters that require gradient, we put their gradient in $\mathbf{g}$ and project $\mathbf{g}$ back to a sphere of given radius $\theta$:</p>
<p>$$\mathbf{g}\leftarrow\min(1,\frac{\theta}{\left|\mathbf{g}\right|})\mathbf{g}$$</p>
<p>Usually, $\theta$ is 5.</p>
<h2 id="Perplexity"><a href="#Perplexity" class="headerlink" title="Perplexity"></a>Perplexity</h2><p>When coping with language model using RNN, what we actually do is to make the agent choose the best token from vocabulary. This is actually a softmax regression problem. However, in NLP, we don&#39;t use the average of crossentropy loss to measure its precision. Instead, we use its exponent, <em>perplexity</em>:</p>
<p>$$\exp(-\frac{1}{n}\sum\limits_{t&#x3D;1}^n\log P(x_t|x_{t-1},...,x_1))$$</p>
<p>They are actually the same but <em>perplexity</em> shows the range of tokens that the agent can choose. Namely, when <em>perplexity</em> is 1, which means that the agent can choose a specific token without hesitation, this is the ideal result. When <em>perlexity</em> is bigger than 1, this means that there are still some tokens confusing the agent.</p>
<h1 id="Structure-of-RNN"><a href="#Structure-of-RNN" class="headerlink" title="Structure of RNN"></a>Structure of RNN</h1><p>There are many types of RNN structure. Different structures are suitable for different functions. For instance, in NLP, one to many structure is suitable for text generation, that is, predicting the subsequent text using current text. Many to one structure is suitable for text categorization. It tries to remember the whole text and only output one value. Many to many structure is fit for machine translation and question answering. Seq2seq is a typical many to many model.</p>
<p><img src="/2023/05/25/RNN/5.png" alt="5"></p>
<center style="font-size:12px; font-weight:bold">Fig. 5. Different structures of RNN</center><br>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zclzcl0223.github.io/2023/05/25/RNN/" data-id="clzik0lcv007pv47k3u603yve" data-title="RNN: a Special Kind of MLP" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Deep-Learning/" rel="tag">Deep Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/RNN/" rel="tag">RNN</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-MultipleGPUsAndParallelism" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/05/19/MultipleGPUsAndParallelism/" class="article-date">
  <time class="dt-published" datetime="2023-05-19T10:54:24.000Z" itemprop="datePublished">2023-05-19</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Dive-Into-Deep-Learning/">Dive Into Deep Learning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/05/19/MultipleGPUsAndParallelism/">Multiple GPUs and Parallelism</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <span id="more"></span>

<h1 id="Batch-size"><a href="#Batch-size" class="headerlink" title="Batch size"></a>Batch size</h1><p>Batch size is related to sample diversity. When computing the loss of a batch, we actually take the average of the total loss. Therefore, no matter how large the batch size is, we only update the model parameters with one sample on average in each iteration. In this case, the diversity of samples is rather important. The more diverse the sample is, the more efficient updates we can get in each iteration. For example, if all the samples in a batch are the same, it works as if each batch has only one sample.</p>
<p>When the total number of samples is constant, the larger the batch size is, the fewer times the model parameter is updated in one iteration. Hence, to achieve the same performance, we have to enlarge learning rate when we enlarge the batch size.</p>
<blockquote>
<p>In theory, one sample in a batch will produce the best model performance but it is time-consuming.</p>
</blockquote>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zclzcl0223.github.io/2023/05/19/MultipleGPUsAndParallelism/" data-id="clzik0lco005vv47kdukq1d3g" data-title="Multiple GPUs and Parallelism" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Deep-Learning/" rel="tag">Deep Learning</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-CommonCNNModels" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/05/14/CommonCNNModels/" class="article-date">
  <time class="dt-published" datetime="2023-05-14T04:33:39.000Z" itemprop="datePublished">2023-05-14</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Dive-Into-Deep-Learning/">Dive Into Deep Learning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/05/14/CommonCNNModels/">Common CNN Models</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <span id="more"></span>

<h1 id="LeNet-5"><a href="#LeNet-5" class="headerlink" title="LeNet-5"></a>LeNet-5</h1><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/726791">LeNet-5</a> which was published in 1998, is one of the simplest CNN models. It was originally used to recognize handwritten numbers between 0 and 9.</p>
<p><img src="/2023/05/14/CommonCNNModels/1.png" alt="1"></p>
<center style="font-size:12px;font-weight:bold">Fig. 1. LeNet-5</center><br>

<p>Its structure is quite simple from today&#39;s perspective: start with two convolutional and pooling layers and end with three dense layers (the last layer is softmax). However, it pioneered the template of CNN, that is:</p>
<ol>
<li>Start with convolutional and pooling layers to extract features and reduce data dimensionality;</li>
<li>End with dense layers to solve the problems.</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># structure of LeNet-5</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Reshape</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> x.view((-<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>))  <span class="comment"># reshape the dimension of input data</span></span><br><span class="line"></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    Reshape(),</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>),  <span class="comment"># add padding to get 32*32</span></span><br><span class="line">    nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(<span class="number">2</span>),  <span class="comment"># windows of the pooling layer in pytorch do not overlap by default</span></span><br><span class="line">    nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>),</span><br><span class="line">    nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(<span class="number">2</span>),</span><br><span class="line">    nn.Flatten(),</span><br><span class="line">    nn.Linear(<span class="number">400</span>, <span class="number">120</span>),</span><br><span class="line">    nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">120</span>, <span class="number">84</span>),</span><br><span class="line">    nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">84</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure>

<h1 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h1><p><a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">AlexNet</a> started the craze for neural networks in 2012, before which, the kernel function and SVM were more powerful in machine learning. The key of kernel function is manual feature extraction. With features, it can calculate their correlation using the kernel function and transform it into a convex optimization problem. After that, SVM will work well by feeding these features to it. It has strong beautiful theorems and can be explained mathematically.</p>
<p>AlexNet is actually a larger LeNet. It improves on LeNet and increases the depth of the neural network:</p>
<ul>
<li>Dropout: control the overfitting of deeper neural networks;</li>
<li>ReLu: make gradients larger;</li>
<li>MaxPooling: make gradients larger and the model easier to train;</li>
<li>ImageNet: larger dataset than Mnist;</li>
<li>Data augmentation: create more samples and reduce the sensitivity of convolutional layers to position.</li>
</ul>
<p>AlexNet changed the way people think about machine learning:</p>
<ul>
<li>Learning features through CNN instead of extracting manually;</li>
<li>Training classifiers (traditional ML methods) together with feature extrators (CNN).</li>
</ul>
<p><img src="/2023/05/14/CommonCNNModels/2.png" alt="2"></p>
<center style="font-size:12px;font-weight:bold">Fig. 2. AlexNet</center><br>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># structure of AlexNet</span></span><br><span class="line"><span class="comment"># size of each image: 224*224</span></span><br><span class="line">Net = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">96</span>, kernel_size=<span class="number">11</span>, stride=<span class="number">4</span>, padding=<span class="number">1</span>),  <span class="comment"># the dataset is Mnist, if ImageNet, 1 should be 3</span></span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Conv2d(<span class="number">96</span>, <span class="number">256</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Conv2d(<span class="number">256</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">384</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">384</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Flatten(),</span><br><span class="line">    nn.Linear(<span class="number">6400</span>, <span class="number">4096</span>), nn.ReLU(), nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">    nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>), nn.ReLU(), nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">    nn.Linear(<span class="number">4096</span>, <span class="number">10</span>)  <span class="comment"># dataset is Mnist</span></span><br><span class="line">    )</span><br></pre></td></tr></table></figure>

<h1 id="VGG"><a href="#VGG" class="headerlink" title="VGG"></a>VGG</h1><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1409.1556.pdf">VGG</a> (Visual Geometry Group) encapsulates the convolutional and pooling layers in AlexNet into blocks so that they can be reused and the network will be more standardized.</p>
<p><img src="/2023/05/14/CommonCNNModels/3.png" alt="3"></p>
<center style="font-size:12px;font-weight:bold">Fig. 3. VGG</center><br>

<p>Usually, for each block, the height and width of matrices are halved, and the number of channels is doubled.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># structure of VGG</span></span><br><span class="line"><span class="comment"># size of each image: 224*224</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">vgg_block</span>(<span class="params">num_convs, in_channels, out_channels</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    definition of block.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        num_convs: the number of convolutional layer in this block</span></span><br><span class="line"><span class="string">        in_channels: the number of input channels in the first convolutional layer</span></span><br><span class="line"><span class="string">        out_channels: the number of output channels of the block</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">        nn.Sequential</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    layers = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_convs):</span><br><span class="line">        layers.append(nn.Conv2d(</span><br><span class="line">            in_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>))</span><br><span class="line">        layers.append(nn.ReLU())</span><br><span class="line">        in_channels = out_channels</span><br><span class="line">    layers.append(nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">conv_arch = ((<span class="number">1</span>, <span class="number">64</span>), (<span class="number">1</span>, <span class="number">128</span>), (<span class="number">2</span>, <span class="number">256</span>), (<span class="number">2</span>, <span class="number">512</span>), (<span class="number">2</span>, <span class="number">512</span>))</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">vgg</span>(<span class="params">conv_arch</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    definition of vgg.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        conv_arch: structure of all the blocks, including number of convolutional </span></span><br><span class="line"><span class="string">                   layer of each block and the number of output channels of each block</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">        the whole VGG network</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    conv_blks = []</span><br><span class="line">    in_channels = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> (num_convs, out_channels) <span class="keyword">in</span> conv_arch:</span><br><span class="line">        conv_blks.append(vgg_block(</span><br><span class="line">            num_convs, in_channels, out_channels))</span><br><span class="line">        in_channels = out_channels</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        *conv_blks, nn.Flatten(),</span><br><span class="line">        nn.Linear(out_channels * <span class="number">7</span> * <span class="number">7</span>, <span class="number">4096</span>), nn.ReLU(), nn.Dropout(p=<span class="number">0.5</span>),  <span class="comment"># 224 / 2^5 = 7</span></span><br><span class="line">        nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>), nn.ReLU(), nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">        nn.Linear(<span class="number">4096</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="NiN"><a href="#NiN" class="headerlink" title="NiN"></a>NiN</h1><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1312.4400.pdf">NiN</a> (Network in Network) follows the block idea of VGG. However, it removes the dense layer completely as the dense layer need too much memory. As an alternative to the dense layer, NiN utilizes the $1\times1$ kernel and ReLU to add nonlinearity to each pixel and uses global average pooling layer to replace the last dense layer, which compute the mean of each input channel. </p>
<blockquote>
<p>The function of global average pooling layer is the same as the last dense layer but it requires less computation. Softmax is realized by loss function CrossEntropy.</p>
</blockquote>
<p><img src="/2023/05/14/CommonCNNModels/4.png" alt="4"></p>
<center style="font-size:12px;font-weight:bold">Fig. 4. NiN</center><br>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># structure of NiN</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">nin_block</span>(<span class="params">in_channels, out_channels, kernel_size, strides, padding</span>):</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.Conv2d(in_channels, out_channels, kernel_size, strides, padding), nn.ReLU(),</span><br><span class="line">        nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">        nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">1</span>), nn.ReLU())</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nin_block(<span class="number">1</span>, <span class="number">96</span>, kernel_size=<span class="number">11</span>, strides=<span class="number">4</span>, padding=<span class="number">0</span>),</span><br><span class="line">    nn.MaxPool2d(<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nin_block(<span class="number">96</span>, <span class="number">256</span>, kernel_size=<span class="number">5</span>, strides=<span class="number">1</span>, padding=<span class="number">2</span>),</span><br><span class="line">    nn.MaxPool2d(<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nin_block(<span class="number">256</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, strides=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.MaxPool2d(<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    <span class="comment"># just normalization, we can remove it</span></span><br><span class="line">    nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">    <span class="comment"># the number of labels is 10</span></span><br><span class="line">    nin_block(<span class="number">384</span>, <span class="number">10</span>, kernel_size=<span class="number">3</span>, strides=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">    <span class="comment"># the shape of output is 1x1</span></span><br><span class="line">    nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>)),  <span class="comment"># the number of input channels must be equal to the number of labels</span></span><br><span class="line">    <span class="comment"># turn the 10 labels to a vector</span></span><br><span class="line">    nn.Flatten())</span><br></pre></td></tr></table></figure>

<blockquote>
<p>The global average pooling reduces the complexity of computation but also slows convergence.</p>
</blockquote>
<h1 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h1><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1409.4842.pdf">GoogLeNet</a> inherits the idea of block network, 1x1 convolutional layer, and global average pooling layer in NiN. The most important block in GoogLeNet is Inception block.</p>
<p><img src="/2023/05/14/CommonCNNModels/5.png" alt="5"></p>
<center style="font-size:12px;font-weight:bold">Fig. 5. Inception block(V1)</center><br>

<p>The inception block consists of multiple parallel pathways which <strong>extract different types of features</strong> from the input. Each pathway generates new output channels using all the channels of input data. Different pathways output different number of output channels but keep the shape of each channel the same as the input channel. Usually, the number of output channels is larger than the number of input channels. The output channels of different pathways are concatenated together to form the new input data.</p>
<blockquote>
<p>In the inception block, the <strong>1x1 kernel</strong> is used to merge channels so that the computaion complexity (it is equal to the number of parameters that the neural network can learn) can be reduced while <strong>the blue layer</strong> extract features from the input.</p>
<p>The output size of each pathway is the hyperparameter that can be modified in inception block.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="comment"># Inception block</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Inception</span>(nn.Module):</span><br><span class="line">    <span class="comment"># c1--c4 which may be a list, are the number of output channels of each pathway</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, c1, c2, c3, c4, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(Inception, self).__init__(**kwargs)</span><br><span class="line">        <span class="comment"># pathway1, single 1x1 convolutional layer</span></span><br><span class="line">        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># pathway2，1x1 convolutional layer + 3x3 convolutional layer</span></span><br><span class="line">        self.p2_1 = nn.Conv2d(in_channels, c2[<span class="number">0</span>], kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.p2_2 = nn.Conv2d(c2[<span class="number">0</span>], c2[<span class="number">1</span>], kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># pathway3，1x1 convolutional layer + 5x5 convolutional layer</span></span><br><span class="line">        self.p3_1 = nn.Conv2d(in_channels, c3[<span class="number">0</span>], kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.p3_2 = nn.Conv2d(c3[<span class="number">0</span>], c3[<span class="number">1</span>], kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># pathway4，3x3 maxpooling layer + 1x1 convolutional layer</span></span><br><span class="line">        self.p4_1 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># ReLU add nonlinearity for each layer</span></span><br><span class="line">        p1 = F.relu(self.p1_1(x))</span><br><span class="line">        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))</span><br><span class="line">        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))</span><br><span class="line">        p4 = F.relu(self.p4_2(self.p4_1(x)))</span><br><span class="line">        <span class="comment"># concatenate the output channels (dim 0 is batch size)</span></span><br><span class="line">        <span class="keyword">return</span> torch.cat((p1, p2, p3, p4), dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>The entire GoogLeNet uses 9 inception blocks and one global average pooling layer to generate the prediction. However, the global average pooling layer in GoogLeNet doesn&#39;t requires its output size to be equivalent with the number of labels in softmax because there is a dense layer behind it.</p>
<p><img src="/2023/05/14/CommonCNNModels/6.png" alt="6"></p>
<center style="font-size:12px;font-weight:bold">Fig. 6. GoogleNet</center><br>

<p>The whole GoogLeNet can be divided into 5 stages. After each stage, the length and width of the image are halved and the number of channels increases (often doubles). This is a idea followed by many subsequent neural networks. For instance, in the neural network we build below, the output shape of each stage is:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">64</span>, <span class="number">24</span>, <span class="number">24</span>])  <span class="comment"># the first stage makes the output a quarter</span></span><br><span class="line">Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">192</span>, <span class="number">12</span>, <span class="number">12</span>])</span><br><span class="line">Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">480</span>, <span class="number">6</span>, <span class="number">6</span>])</span><br><span class="line">Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">832</span>, <span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line">Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">1024</span>])</span><br><span class="line">Linear output shape:         torch.Size([<span class="number">1</span>, <span class="number">10</span>])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># structure of GoogLeNet</span></span><br><span class="line">b1 = nn.Sequential(nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">                   nn.ReLU(),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">b2 = nn.Sequential(nn.Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=<span class="number">1</span>),</span><br><span class="line">                   nn.ReLU(),</span><br><span class="line">                   nn.Conv2d(<span class="number">64</span>, <span class="number">192</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">                   nn.ReLU(),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">b3 = nn.Sequential(Inception(<span class="number">192</span>, <span class="number">64</span>, (<span class="number">96</span>, <span class="number">128</span>), (<span class="number">16</span>, <span class="number">32</span>), <span class="number">32</span>),</span><br><span class="line">                   Inception(<span class="number">256</span>, <span class="number">128</span>, (<span class="number">128</span>, <span class="number">192</span>), (<span class="number">32</span>, <span class="number">96</span>), <span class="number">64</span>),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">b4 = nn.Sequential(Inception(<span class="number">480</span>, <span class="number">192</span>, (<span class="number">96</span>, <span class="number">208</span>), (<span class="number">16</span>, <span class="number">48</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">160</span>, (<span class="number">112</span>, <span class="number">224</span>), (<span class="number">24</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">128</span>, (<span class="number">128</span>, <span class="number">256</span>), (<span class="number">24</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">112</span>, (<span class="number">144</span>, <span class="number">288</span>), (<span class="number">32</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">528</span>, <span class="number">256</span>, (<span class="number">160</span>, <span class="number">320</span>), (<span class="number">32</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">b5 = nn.Sequential(Inception(<span class="number">832</span>, <span class="number">256</span>, (<span class="number">160</span>, <span class="number">320</span>), (<span class="number">32</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   Inception(<span class="number">832</span>, <span class="number">384</span>, (<span class="number">192</span>, <span class="number">384</span>), (<span class="number">48</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   nn.AdaptiveAvgPool2d((<span class="number">1</span>,<span class="number">1</span>)),</span><br><span class="line">                   nn.Flatten())</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(<span class="number">1024</span>, <span class="number">10</span>))</span><br><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">96</span>, <span class="number">96</span>))</span><br></pre></td></tr></table></figure>
<p>The above is the first version of GoogLeNet. In the subsequent versions, Google made different improvements to the inception block:</p>
<ul>
<li><p>Inception-BN(V2): batch normalization;</p>
</li>
<li><p>Inception-V3: modify the size of kernel;</p>
</li>
<li><p>Inception-V4: use residual connections</p>
</li>
</ul>
<p>Among them, V3 is used more.</p>
<blockquote>
<p>Advantages of GoogLeNet: fewer model parameters and relatively low computational complexity.</p>
</blockquote>
<h2 id="Batch-normalization"><a href="#Batch-normalization" class="headerlink" title="Batch normalization"></a>Batch normalization</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1502.03167.pdf">Batch normalization</a> is a layer that will make the neural network converge faster as it keeps the input of hidden layer more stable. This is how it works:</p>
<p>$$\mathrm{BN}(\mathbf{x}) &#x3D; \boldsymbol{\gamma} \odot \frac{\mathbf{x} - \hat{\boldsymbol{\mu}}_\mathcal{B}}{\hat{\boldsymbol{\sigma}}_\mathcal{B}} + \boldsymbol{\beta}$$</p>
<p>where $\hat{\boldsymbol{\mu}}_\mathcal{B}$ is the mean of small batch $\mathcal{B}$ and $\hat{\boldsymbol{\sigma}}_\mathcal{B}$ is the standard deviation of small batch $\mathcal{B}$. However, we just artificially make the distribution of $\mathbf{x}$ into a standard normal distribution. Therefore, we reserve two learnable parameters for the machine so that it can automatically adjust the distribution of $\mathbf{x}$. And that is scale ($\boldsymbol{\gamma}$) and shift ($\boldsymbol{\beta}$), which adjust variance and mean respectively. They have the same shape as $\mathbf{x}$.</p>
<p>For $\hat{\boldsymbol{\mu}}_\mathcal{B}$, it is easy to compute for each batch:</p>
<p>$$\hat{\boldsymbol{\mu}} _\mathcal{B} &#x3D; \frac{1} {|\mathcal{B} |} \sum _{\mathbf{x} \in \mathcal{B}} \mathbf{ x }$$</p>
<p>But for $\hat{\boldsymbol{\sigma}}_\mathcal{B}$, we should add $\epsilon$ so that the formula don&#39;t divide by 0.</p>
<p>$$\hat{\boldsymbol {\sigma}} _\mathcal{B}^2 &#x3D; \frac{1} {|\mathcal{B} |} \sum _{\mathbf{ x } \in \mathcal{ B }} (\mathbf{x} - \hat{\boldsymbol{\mu}} _{\mathcal{B}})^2 + \epsilon$$</p>
<blockquote>
<p>Usually, we make $\epsilon$ 1e-5 or 1e-6.</p>
</blockquote>
<p>Similar to dropout, we only make the batch normalization layer work when training. When making prediction, we use <strong>global mean</strong> and <strong>global variance</strong> to normalize the input of hiddden layers. The method to get them is slightly similar to that we use to get RTT in computer network:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">moving_mean = momentum * moving_mean + (<span class="number">1.0</span> - momentum) * mean</span><br><span class="line">moving_var = momentum * moving_var + (<span class="number">1.0</span> - momentum) * var</span><br></pre></td></tr></table></figure>
<p>where <code>mean</code> and <code>var</code> are mean and variance the agent get in this small batch, <code>momentum</code> is a weight factor (range from 0 to 1).</p>
<blockquote>
<p>$\boldsymbol{\gamma}$ and $\boldsymbol{\beta}$ are still needed.</p>
</blockquote>
<p>It is important to understand that all the mean and variance the agent computes are mean and variance of each <strong>feature</strong> or <strong>channel</strong> (for the convolutional layer, a channel represents a feature). That&#39;s why the shape of the second dimension of $\boldsymbol{\gamma}$ and $\boldsymbol{\beta}$ is the same as the number of features for dense layers or the number of input channels for convolutional layers.</p>
<blockquote>
<p>We put the batch normalization layer before the activation layer. Its function is a bit like <em>dropout</em> so we often don&#39;t use both layers at the same time.</p>
</blockquote>
<p>It is not clear that why batch normalization works. Some researchers guess that it works as it add noise to each small batch. Those noise make the output of different layers more stable and more realistic. Hence, the gradients of bottom layers will larger and the model converge faster. But it doesn&#39;t change the accuracy of models.</p>
<blockquote>
<p>Xavier makes the initial value of the model parameters more stable. Batch normalization makes the output of hidden layers (or the input of hidden layers) stable. It&#39;s actually a linear layer that modifies the data as we wish. Namely, <strong>we give the agent some human guidance</strong>.</p>
</blockquote>
<h1 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h1><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1512.03385.pdf">ResNet</a> combined with batch normalization enables us to train a deeper neural network. Before ResNet, the model we train is based on non-nested function classes like the figure on the left. Namely, when we add a new block to complicate the model, we establish a new mapping relationship $y&#x3D;g(f(x))$. However, we can&#39;t make sure that $g(f(x))$ is better than $f(x)$ since $g(f(x))$ can&#39;t cover all the zone that $f(x)$ cover. What&#39;s more, in a deep neural network, the gradients of the bottom layers are usually very small, which makes the bottom layers converge slowly.</p>
<p>The core idea of ResNet is to compute $y&#x3D;g(f(x))+f(x)$ instead of $y&#x3D;g(f(x))$ (Fig. 8). This small change makes a more complex network contain a simpler network (like the picture on the right). Therefore, when using ResNet, a deeper network always performs better than a simple network. In addition, ResNet provides a fast track for the bottom layer to get larger gradients (through $f(x)$). What&#39;s more, when the simple network $f(x)$ has already performed well, its gradients will be rather small, that is, its parameters will not change significantly and it will keep performing well in a deeper network.</p>
<p><img src="/2023/05/14/CommonCNNModels/7.png" alt="7"></p>
<center style="font-size:12px;font-weight:bold">Fig. 7. Non-nested and nested function classes</center><br>

<p><img src="/2023/05/14/CommonCNNModels/8.png" alt="8"></p>
<center style="font-size:12px;font-weight:bold">Fig. 8. Residual block, the weight layer could be dense or convolutional layer</center><br>

<p>The structure of ResNet is similar to GoogLeNet but it replace the inception block with residual block.</p>
<p><img src="/2023/05/14/CommonCNNModels/9.png" alt="9"></p>
<center style="font-size:12px;font-weight:bold">Fig. 9. ResNet-18</center><br>

<blockquote>
<p>When counting the number of layer, we only count the convolutional layer and the dense layer.</p>
</blockquote>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zclzcl0223.github.io/2023/05/14/CommonCNNModels/" data-id="clzik0lbz000jv47k4izh1mm2" data-title="Common CNN Models" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/CNN/" rel="tag">CNN</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Deep-Learning/" rel="tag">Deep Learning</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-CNN" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/05/10/CNN/" class="article-date">
  <time class="dt-published" datetime="2023-05-10T15:45:10.000Z" itemprop="datePublished">2023-05-10</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Dive-Into-Deep-Learning/">Dive Into Deep Learning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/05/10/CNN/">CNN: Feature Extraction</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <span id="more"></span>

<h1 id="From-Dense-to-Convolution"><a href="#From-Dense-to-Convolution" class="headerlink" title="From Dense to Convolution"></a>From Dense to Convolution</h1><p>The convolutional layer is the key layer of CNN. It is a special kind of dense layer. CNN is used to deal with images. In this section, we take the grayscale image as example, that is, the feature of an input image is a 2-D matrix. When processing images, firstly, it is reasonable that agents&#39;s awareness of a certain object should not be overly concerned with the precise location of the object in the image because what the object looks like has nothing to do with its position. Besides, the features of one object should only be related to the pixels around it as each pixel only determines the depth of the color and we distinguish objects by their color boundaries. These two priciples are called <em>translation invariance</em> and <em>locality</em> respectively. If fulfilling both priciples, dense layers become convolutional layers.</p>
<h2 id="Constructing-a-dense-layer"><a href="#Constructing-a-dense-layer" class="headerlink" title="Constructing a dense layer"></a>Constructing a dense layer</h2><p>For convinience, we keep the dimension of a input image $X$ 2-D and make the model parameters of one neuron $w$ have the same dimension as $X$. Under such setting, the output of one neuron become:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(X*w).<span class="built_in">sum</span>() <span class="comment"># w is a matrix</span></span><br></pre></td></tr></table></figure>

<p>Now that the output of a hidden layer $H$ should be a image that has the same shape as $X$, we have to place $m*n$ neurons in the hidden layer, where $m$ and $n$ are the dimension of $X$. Hence, for a hidden layer, its model parameter $W$ is:</p>
<p>$$<br>{\begin{bmatrix}<br>    {\begin{bmatrix}<br>        w_{0,0,0,0} &amp; w_{0,0,0,1}\\<br>        w_{0,0,1,0} &amp; w_{0,0,1,1}<br>    \end{bmatrix}<br>    \begin{bmatrix}<br>        w_{0,1,0,0} &amp; w_{0,1,0,1}\\<br>        w_{0,1,1,0} &amp; w_{0,1,1,1}<br>    \end{bmatrix}}\\<br>    {\begin{bmatrix}<br>        w_{1,0,0,0} &amp; w_{1,0,0,1}\\<br>        w_{1,0,1,0} &amp; w_{1,0,1,1}<br>    \end{bmatrix}<br>    \begin{bmatrix}<br>        w_{1,1,0,0} &amp; w_{1,1,0,1}\\<br>        w_{1,1,1,0} &amp; w_{1,1,1,1}<br>    \end{bmatrix}}<br>\end{bmatrix}} _{m\times n\times m\times n&#x3D;2\times 2\times 2\times 2}<br>$$</p>
<p>And an element of output $H$ is:</p>
<p>$$<br>[H]_{i,j}&#x3D;\sum_k \sum_l {[W]} _{i,j,k,l} {[X]} _{k,l} + {[U]} _{i,j}<br>$$</p>
<p>where $U$ contains bias of each output pixel.</p>
<h2 id="Translation-invariance"><a href="#Translation-invariance" class="headerlink" title="Translation invariance"></a>Translation invariance</h2><p>Now we can clearly see that in order to process an image with $m * n$ data, we almost use $(m * n)^2$ model parameters. This is extremely space-consuming. However, if we apply translation invariance priciple to the dense layer, we can cut it to $m * n$.</p>
<p>As mentioned above, the activation of a pixel should have nothing to do with its position. This is only possible when ${[W]} _{i,j}$ and ${[U]} _{i,j}$ are both the same for any $(i,j)$. Hence, for a certain hidden layer, we actually just need a 2-D $W$ and a scalar $u$:</p>
<p>$$<br>{[H]} _{i,j}&#x3D;\sum_k\sum_l{[W]} _{k,l} {[X]} _{k,l}+u<br>$$</p>
<h2 id="Locality"><a href="#Locality" class="headerlink" title="Locality"></a>Locality</h2><p>So far, we have significantly cut the space of model parameters but it is still too large. Now it is time for <em>locality</em>. As motivated above, there is no need to look so far away from the location of input pixel ${[X]}_ {i,j}$. We just need to consider a small window around ${[X]} _{i,j}$:</p>
<p>$${[H]}_ {i, j} &#x3D; u + \sum_{a &#x3D; -\Delta}^{\Delta} \sum_{b &#x3D; -\Delta}^{\Delta} {[V]}_ {a, b}  {[X]}_ {i+a, j+b}.$$</p>
<p>where $\Delta\times\Delta$ is the size of the window and $V$ (size $\Delta\times\Delta$) is the model parameter inside the window, a filter that generates the feature of a zone in an image or a <strong>convolution kernel</strong>.</p>
<p>Now we construct a convolutional layer (<code>nn.Conv2d</code>) with hyperparameters <em>kernel size</em> and <em>stride</em> (see below).</p>
<p>$$Y &#x3D; X \star W+b$$</p>
<p>$W$ and $b$ are model parameters that the agent can learn. $\star$ is cross-correlation operator which is slightly different from convolution but their behaviors are the same in the convolutional layer (see below). The dimensions of $Y$ are smaller than the original dimensions, which is:</p>
<p>$$(n_h-k_h+1)\times(n_w-k_w+1)$$</p>
<p>where $n_h$ and $n_w$ are the dimensions of $X$, $k_h$ and $k_w$ are the dimensions of $W$. It doesn&#39;t matter and we can solve it easily (see below).</p>
<p><img src="/2023/05/10/CNN/1.png" alt="1"></p>
<center style="font-size:12px;font-weight:bold">Fig. 1. Convolutional layer</center>

<h1 id="Convolution-Fourier-and-Neural-Networks"><a href="#Convolution-Fourier-and-Neural-Networks" class="headerlink" title="Convolution, Fourier and Neural Networks"></a>Convolution, Fourier and Neural Networks</h1><p>Convolution is an operation on two functions ($f$ and $g$) that produces a third function ($f*g$) that express how the shape of one is modified by the other:</p>
<p>$$(f*g)(t)&#x3D;\int_{-\infty} ^{+\infty}f(\tau)g(t-\tau)d\tau$$</p>
<p>Physically, we can explain it from two perspectives.</p>
<h2 id="Stock"><a href="#Stock" class="headerlink" title="Stock"></a>Stock</h2><p>Suppose there is a system with input $f(t)$ at each moment. The input starts decaying with $g(t)$ by the time it enters the system, where $g(t)$ is its remaining percentage.</p>
<p><img src="/2023/05/10/CNN/2.png" alt="2"></p>
<center style="font-size:12px;font-weight:bold">Fig. 2. f(t) and g(t)</center><br>

<p>For items that enter the system between $(t,t+\Delta t)$, their margin at $x$ is:</p>
<p>$$f(t)g(x-t)\Delta t$$</p>
<p>Therefore, for items that enter the system between $(t_1,t_2)$, their margin at $x$ is:</p>
<p>$$\int_{t_1}^{t_2}f(t)g(x-t)dt$$</p>
<p>In this case, we take $f(t)$ as the unstable input and $g(t)$ as the stable output to compute the stock of a system after some time.</p>
<h2 id="Influence"><a href="#Influence" class="headerlink" title="Influence"></a>Influence</h2><p>In the explaination above, we actually make $dt$ relate to $f(t)$. Namely, $f(t)dt$ is the input of the system during $t$ and $t+\Delta t$. $g(x-t)$ has nothing to do with $dt$. It is just a decaying factor.</p>
<p>However, we can also take $g(x-t)dt$ as a whole. This will make the $t$ and  $f * g$ more meaningful. For example, we can regard $f(t)$ as a thing happens at $t$ and $g(t)$ as its influence over time. Then $f*g$ can be regarded as the totally influence at $x$ caused by things that happen during $t_1$ and $t_2$:</p>
<p>$$\int_{t_1}^{t_2}f(t)g(x-t)dt$$</p>
<p>To go a step further, we can take $t$ as position, $f(t)$ as a thing happens at $t$ and keep moving from $t$ to $x$, $g(t)$ as its award or loss per distance. For example, some people whose weight $f(t)$ starts running as $t$ and they will loss $g(t)$ kg per meter. Then $f*g$ is the weight loss of all. (the fatter a person is, the more contribution he&#x2F;she makes).</p>
<p><img src="/2023/05/10/CNN/3.png" alt="3"></p>
<center style="font-size:12px;font-weight:bold">Fig. 3. weight and weight loss</center>

<h2 id="Convolution-in-CNN"><a href="#Convolution-in-CNN" class="headerlink" title="Convolution in CNN"></a>Convolution in CNN</h2><p>For convolution in CNN, we&#39;d better expand its dimension to 2-D and take $t$ as a discrete number which represents position $(x,y)$. Then, we can take $X$ as function $f$ and $W$ as $g$. Now the meaning of convolution in CNN can be interpreted as the influence that pixels around $f(x,y)$ exert on pixel $f(x,y)$ or the certain average feature of the pixels around $f(x,y)$. </p>
<p>$$f * g&#x3D;\sum _{i,j}f(i,j)g(x-i,y-j)$$</p>
<p><img src="/2023/05/10/CNN/4.png" alt="4"></p>
<center style="font-size:12px;font-weight:bold">Fig. 4. Convolution in CNN</center><br>

<p>The phrase certain average feature may be a little confusing. However, if we regard the convolution kernel $W$ as a feature filter that extract specific features of a region of an image, it makes sense. Different filters extract different features and their parameters are totally learnt by the agent itself.</p>
<p><img src="/2023/05/10/CNN/5.png" alt="5"></p>
<center style="font-size:12px;font-weight:bold">Fig. 5. The feature filter</center><br>

<p>There are still some differences between convolution and the convolutional layer, that is, $g$ is not the kernel we actually use. For example, convolution computes $f(x-1,y-1)g(1,1)$ rather than $f(x-1,y-1)g(-1,-1)$. However, in CNN, we actually compute $f(x-1,y-1)g(-1,-1)$. In fact, it doesn&#39;t matter. We just need to turn $g$ around and we get the $W$ we use in CNN. Hence, what the agent computes is <strong>cross-correlation</strong> rather than convolution but the results are the same so we just call it convolution.</p>
<h1 id="Hyperparameters-of-convolutional-layer"><a href="#Hyperparameters-of-convolutional-layer" class="headerlink" title="Hyperparameters of convolutional layer"></a>Hyperparameters of convolutional layer</h1><p>Kernel size, padding, stride, output (input) channels are the new hyperparameters that we can adjust in the convolutional layer. </p>
<ol>
<li>Kernel size determines the size of window. It is the simplest parameter among them. We usually set it to $3\times3$ or $5\times5$ for 2-D input.</li>
<li>Padding is used to solve the dimensionality reduction problem of the output image.</li>
<li>Stride determines the movement of window. The stride we use above is $1\times1$.</li>
<li>The number of input channels is not a hyperparameter of the convolutional layer but it determines the size of input.</li>
<li>The number of output channels determines the number of images the convolutional layer outputs.</li>
</ol>
<h2 id="Kernel-size"><a href="#Kernel-size" class="headerlink" title="Kernel size"></a>Kernel size</h2><p>It is recommended to adopt a small kernel size and deep neural networks instead of a large kernel size and shallow neural networks. Their results are almost equivalent but the speed of former is faster as the speed is inversely related to the number of data (For kernel, this is generally on the order of the square). That&#39;s why we always make its size 3, 5 instead of 11, 13.</p>
<blockquote>
<p>The size here refers to the length of every dimension.</p>
</blockquote>
<p>Such a structure is very reasonable as what the kernel does is to gather the features of the edge points together. Therefore, in the final layer, data that the kernel see is the linear combination of all pixels in the image. Namely, its kernel size is actually the whole image:</p>
<p><img src="/2023/05/10/CNN/5_1.png" alt="5_1"></p>
<center style="font-size:12px;font-weight:bold">Fig. 6. Gathering of data</center>

<h2 id="Padding"><a href="#Padding" class="headerlink" title="Padding"></a>Padding</h2><p>Padding adds extra rows and columns around the input images:</p>
<p><img src="/2023/05/10/CNN/6.png" alt="6"></p>
<center style="font-size:12px;font-weight:bold">Fig. 7. Padding</center><br>

<p>After padding, the shape of output becomes:</p>
<p>$$(n_h-k_h+1+p_h)\times(n_w-k_w+1+p_w)$$</p>
<p>Generally, we let:</p>
<p>$$<br>p_h&#x3D;k_h-1,\space p_w&#x3D;k_w-1<br>$$</p>
<p>so that the shape of output is the same as the shape of input. For convinience, we often take $k_h\And k_w$ to be odd number. Therefore, we can pad $p_h&#x2F;2$ ($p_w&#x2F;2$)<br> on both sides. If $k_h\And k_w$ are even, $\lceil p_h&#x2F;2\rceil$ on the upper side and $\lfloor p_h&#x2F;2\rfloor$ on the lower side or vice versa.</p>
<h2 id="Stride"><a href="#Stride" class="headerlink" title="Stride"></a>Stride</h2><p>Stirde refers to the step size of the kernel window on the row and column. When stride is too small, the agent will need a amount of computation to get a small ouput.</p>
<p><img src="/2023/05/10/CNN/7.png" alt="7"></p>
<center style="font-size:12px;font-weight:bold">Fig. 8. Stride (3*2)</center><br>

<p>For a certain stride $s_h\times s_w$, the shape of output becomes:</p>
<p>$$\lfloor(n_h-k_h+p_h)&#x2F;s_h+1\rfloor\times\lfloor(n_w-k_w+p_w)&#x2F;s_w+1\rfloor<br>$$</p>
<p>If:</p>
<p>$$<br>p_h&#x3D;k_h-1,\space p_w&#x3D;k_w-1<br>$$</p>
<p>The shape becomes:</p>
<p>$$\lfloor(n_h-1)&#x2F;s_h+1\rfloor\times\lfloor(n_w-1)&#x2F;s_w+1\rfloor<br>$$</p>
<blockquote>
<p>We can roughly regard that the stride will cut the shape of output by a factor of $s_h\And s_w$.</p>
</blockquote>
<h2 id="Multiple-input-channels"><a href="#Multiple-input-channels" class="headerlink" title="Multiple input channels"></a>Multiple input channels</h2><p>The role of multiple input channels is to combine different features together.</p>
<p><img src="/2023/05/10/CNN/8.png" alt="8"></p>
<center style="font-size:12px;font-weight:bold">Fig. 9. Multiple input channels</center>

<ul>
<li>$X$: $c_i\times n_h\times n_W$</li>
<li>$W$: $c_i\times k_h\times k_w$</li>
<li>$Y$: $m_h\times m_w$</li>
</ul>
<p>For each input channel, there is a kernel. The output of the multiple input channels is the <strong>weighted</strong> sum of the output of each channel. Namely, even though there are several input channels, they just generate one output. To a certain extent, a kernel can be regarded as one of the generalized $w$ of a neuron and the multiple input channels are the model parameters of a neuron.</p>
<h2 id="Multiple-output-channels"><a href="#Multiple-output-channels" class="headerlink" title="Multiple output channels"></a>Multiple output channels</h2><p>An output channel generate an output. It contains an independent batch of kernels which form a 3-D kernel. In other words, the multiple input channels are parts of an output channel. That&#39;s why the input channel is not a hyperparameter. Its size totally depends on the data. For examples, the number of input channels should 3 for RGB images and 1 for grayscale images.</p>
<p><img src="/2023/05/10/CNN/9.png" alt="9"></p>
<center style="font-size:12px;font-weight:bold">Fig. 10. Multiple input and output channels</center>

<ul>
<li>$X$: $c_i\times n_h\times n_W$</li>
<li>$W$: $c_o\times c_i\times k_h\times k_w$</li>
<li>$Y$: $c_o\times m_h\times m_w$</li>
<li>$B$ (bias): $c_o\times c_i$, one kernel one bias.</li>
</ul>
<blockquote>
<p>The computational complexity is roughly $O(c_ik_hk_wc_om_hm_w)$, where $c_ik_hk_w$ is the complexity of computing an output pixel and $c_om_hm_w$ is the number of output pixels.</p>
</blockquote>
<h3 id="1-times-1-kernel"><a href="#1-times-1-kernel" class="headerlink" title="$1\times 1$ kernel"></a>$1\times 1$ kernel</h3><p>The $1\times 1$ kernel is a special kernel which doesn&#39;t recognize spatial patterns but just fuses channels. It can be regarded as a dense layer with input $n_hn_w\times c_i$ ($c_i$ is the number of features) and weight $c_ic_o$.</p>
<p><img src="/2023/05/10/CNN/10.png" alt="10"></p>
<center style="font-size:12px;font-weight:bold">Fig. 11. 1x1 kernel</center>

<h1 id="Pooling-layer"><a href="#Pooling-layer" class="headerlink" title="Pooling layer"></a>Pooling layer</h1><p>The pooling layer is another type of layer in CNN. It has two functions:</p>
<ol>
<li>Reduce the sensitivity of convolutional layers to position;</li>
<li>Reduce the computational complexity.</li>
</ol>
<h2 id="Sensitivity-to-position"><a href="#Sensitivity-to-position" class="headerlink" title="Sensitivity to position"></a>Sensitivity to position</h2><p>The convolutional layer is highly sensitive to the position of pixels. This makes a pixel offset will cause a large change in the output image. However, we may not want such a significant change. Therefore, we need pooling layer to reduce these effects.</p>
<p>The hyperparameters of the pooling layer are almost the same as the convolutional layer. But, the pooling layer is just an operator, that is, it doesn&#39;t have any model parameters to learn. Besides, it doesn&#39;t fuse channels. Therefore, the number of input channels is equal to output channels in the pooling layer.</p>
<p>There are generally two types of pooling layer:</p>
<ol>
<li>Maximum pooling <code>nn.MaxPool2d</code>, computes the maximum value of all elements in the pooling window;</li>
<li>Average pooling <code>nn.AvgPool2d</code>, computes the average value of all elements in the pooling window.</li>
</ol>
<h2 id="Computational-complexity"><a href="#Computational-complexity" class="headerlink" title="Computational complexity"></a>Computational complexity</h2><p>Since the pooling layer windows are the same as kernels in convolutional layer, the pooling layer can reduce the dimension of data. </p>
<p>However, because of the increase in data diversities and its consistency with the convolutional layer in dimensionality reduction, the pooling layer are no longer so important.</p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul>
<li><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1VV411478E/?spm_id_from=333.788&vd_source=2d980a0365f3ebea674b32924d8a4ce8">从“卷积”、到“图像卷积操作”、再到“卷积神经网络”，“卷积”意义的3次改变</a></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zclzcl0223.github.io/2023/05/10/CNN/" data-id="clzik0lbv0008v47kejfwedp5" data-title="CNN: Feature Extraction" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/CNN/" rel="tag">CNN</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Deep-Learning/" rel="tag">Deep Learning</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-MaximumMeanDiscrepancy" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/06/13/MaximumMeanDiscrepancy/" class="article-date">
  <time class="dt-published" datetime="2023-06-13T05:40:37.000Z" itemprop="datePublished">2023-06-13</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Math/">Math</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/06/13/MaximumMeanDiscrepancy/">Maximum Mean Discrepancy</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <span id="more"></span>

<h1 id="衡量两个随机变量的差异"><a href="#衡量两个随机变量的差异" class="headerlink" title="衡量两个随机变量的差异"></a>衡量两个随机变量的差异</h1><p>对于两个不同的随机变量$x$和$y$，其概率分布函数分别为$p(x)$和$p(y)$，我们常用<em>矩</em>来衡量其相似性：</p>
<p>$$<br>\mu_n&#x3D; \int x^np(x)dx\tag{1}<br>$$</p>
<p>上述公式为$x$的$n$阶零点矩，其中一阶原点矩是均值、二阶中心（中心即均值）矩是方差。<em>矩</em>可以理解为某一分布的特征。如果两个随机变量的任意阶矩都相同，那么说明这两个分布具有相同的特征，我们就可以说这两个分布是一致的。而当两个分布不相同时，那么使得两个分布差异最大的矩就应该作为衡量两个分布差异性的标准，这也正是<em>最大均值差异</em>（Maximum mean discrepancy, MMD）的基本思想。</p>
<blockquote>
<p><em>矩</em>与傅里叶变换密不可分，它其实就是抽象的频谱。不同的矩组成了希尔伯特空间的一组正交基。这些无穷维的正交基的线性组合组成了希尔伯特空间的一个无穷维向量：<br>$$<br>\vec{v}&#x3D;v_1\vec{e_1}+v_2\vec{e_2}+...<br>$$<br>其系数$(v_1,v_2,...)$在概率论中就是矩。</p>
</blockquote>
<h1 id="随机变量的任意阶矩"><a href="#随机变量的任意阶矩" class="headerlink" title="随机变量的任意阶矩"></a>随机变量的任意阶矩</h1><p>此处的任意阶矩包括原点矩、中心矩、标准矩等。观察式$(1)$，不难看出，一个随机变量的任意阶矩，实际上就是该随机变量经过某个函数$f(x)$映射后的期望：</p>
<p>$$<br>\mu_{\text{random}}&#x3D;\int f(x)p(x)dx<br>$$</p>
<h1 id="最大均值差异"><a href="#最大均值差异" class="headerlink" title="最大均值差异"></a>最大均值差异</h1><p>于是，对于服从分布$p$的$x$，服从分布$q$的$y$，最大均值差异将两个分布的差异定义为：</p>
<p>$$<br>\text{MMD}(q,p,\mathcal{H})&#x3D;\sup _{f\in\mathcal{H},||f||_\mathcal{H}\le1}(\text{E}_q[f(x)]-\text{E}_p[f(y)])\tag{2}<br>$$</p>
<p>式中，$\sup$表求上界，$\mathcal{H}$表再生希尔伯特空间，$f$为再生希尔伯特空间的任意映射函数，且范数小于等于1（若不加限制，则总能找到一个$f$使得MMD无限大）。</p>
<p>在再生希尔伯特空间中，函数值$f(x)$可以表示为希尔伯特空间中的函数向量$f$和该空间中的向量$\phi(x)$的点积：</p>
<p>$$<br>f(x)&#x3D;&lt;f,\phi(x)&gt;_\mathcal{H}<br>$$</p>
<p>所以，期望也可以写成点积的形式：</p>
<p>$$<br>\text{E}_q[f(x)]&#x3D;&lt;f,\text{E}_q[\phi(x)]&gt;_\mathcal{H}&#x3D;&lt;f,\mu _q&gt;_\mathcal{H}<br>$$</p>
<p>此时，式$(2)$可转变为：</p>
<p>$$<br>\begin{align*}<br>    \text{MMD}(q,p,\mathcal{H})<br>    &amp;&#x3D;\sup _{f\in\mathcal{H},||f||_\mathcal{H}\le1}(\text{E}_q[f(x)]-\text{E}_p[f(y)])\\<br>    &amp;&#x3D;\sup _{f\in\mathcal{H},||f||_\mathcal{H}\le1}&lt;f,\mu _q-\mu _p&gt;\\<br>    &amp;&#x3D;\sup _{f\in\mathcal{H},||f||_\mathcal{H}\le1}||f||\space||\mu _q-\mu _p||\space \cos&lt;f,\mu _q-\mu _p&gt;\\<br>    &amp;&#x3D;||\mu _q-\mu _p||<br>\end{align*}<br>$$</p>
<p>最后一个等号成立因为$||f||\le1$且$\cos x\le1$。$\mu _q$和$\mu _p$是期望，可以用均值估计：</p>
<p>$$<br>\begin{align*}<br>    \mu _q&amp;&#x3D;\frac{1}{m}\sum\limits _{i&#x3D;1} ^m\phi(x_i)\\<br>    \mu _p&amp;&#x3D;\frac{1}{n}\sum\limits _{j&#x3D;1} ^n\phi(y_j)<br>\end{align*}<br>$$</p>
<blockquote>
<p>一般地，我们会取MMD的平方，以保证值为非负值。</p>
</blockquote>
<p>也可以用核方法计算。</p>
<h1 id="核方法"><a href="#核方法" class="headerlink" title="核方法"></a>核方法</h1><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/163839117">统计知识（一）MMD Maximum Mean Discrepancy 最大均值差异</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/471732960">Maximum Mean Discrepancy（MMD）最大均值差异</a></li>
<li><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/288185961">怎样理解MMD（Maximum Mean Discrepancy），它的平方怎样定义？</a></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zclzcl0223.github.io/2023/06/13/MaximumMeanDiscrepancy/" data-id="clzik0lcm005hv47k0epsfru5" data-title="Maximum Mean Discrepancy" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Math/" rel="tag">Math</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-FourierTransformAndCNN" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/05/10/FourierTransformAndCNN/" class="article-date">
  <time class="dt-published" datetime="2023-05-10T15:46:45.000Z" itemprop="datePublished">2023-05-10</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Math/">Math</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/05/10/FourierTransformAndCNN/">Fourier Transform and CNN</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <span id="more"></span>

<h1 id="傅里叶变换与坐标变换"><a href="#傅里叶变换与坐标变换" class="headerlink" title="傅里叶变换与坐标变换"></a>傅里叶变换与坐标变换</h1><p>二维坐标系下的一条曲线$f(t)$，可以用无穷个坐标$(t_i,y_i)$表示。若将无穷个纵坐标融合为一个无穷维的坐标对$(y_1,...,y_n)$，那么二维下的一条曲线，便可以表示为无穷维度下的一个点$(y_1,...,y_n)$。更进一步地，可以将这个点视为一个向量$\vec{f}$。</p>
<blockquote>
<p>横坐标$t$为基向量。</p>
</blockquote>
<p>向量各分量的值是与空间的基向量直接相关的，其在基向量$\vec{d} _n$上投影的值实际上就是其在相应基向量上的坐标值：</p>
<p>$$<br>u_n&#x3D;|\vec{f}|\cos\theta_n<br>$$</p>
<p>不过，更为有用的是投影向量，因此，一般再将上式乘以基向量得到投影向量：</p>
<p>$$<br>\begin{align*}<br>    \vec{u} _n<br>    &amp;&#x3D;|\vec{f}|\vec{d}_n\cos\theta_n\\<br>    &amp;&#x3D;|\vec{f}||\vec{d}_n|\cos\theta _n\frac{\vec{d}_n}{|\vec{d}_n|}\\<br>    &amp;&#x3D;&lt;\vec{f},\vec{d}_n&gt;\cdot\vec{d}_n<br>\end{align*}<br>$$</p>
<blockquote>
<p>$&lt;\vec{f},\vec{d}_n&gt;$是两向量的内积，向量间存在内积的无穷维空间称希尔伯特空间，它是欧几里得空间的拓展。</p>
</blockquote>
<p>向量的内积可以表示为向量坐标的乘积和，于是，上式可变为：</p>
<p>$$<br>\vec{u}_n&#x3D;\int _{-\infty} ^{+\infty}f(t)d_n(t)\text{d}t\cdot\vec{d}_n<br>$$<br>式中，$\int _{-\infty} ^{+\infty}f(t)d_n(t)\text{d}t$是投影长，即特征值；$\vec{d}_n$是一组基向量中的任意一个，代表一种模式或特征。</p>
<p>不同的$(d_1,...,d_n)$代表一组不同的基向量，而上述操作则是对原基向量下的$f(t)$进行了一次坐标变换。若取基向量为$e^{i\omega t}$，则上式的特征值便可转变为一次傅里叶变换：</p>
<p>$$<br>F(\omega)&#x3D;\int _{-\infty} ^{+\infty}f(t)e ^{-i\omega t}\text{d}t&#x3D;&lt;\vec{f},\vec{d}_\omega&gt;<br>$$</p>
<blockquote>
<p>$e ^{i\omega t}$取共轭是因为它是复数。</p>
</blockquote>
<p>将投影向量相加可重新得到原向量，也即傅里叶逆变换：</p>
<p>$$<br>\begin{align*}<br>    \vec{f}<br>    &amp;&#x3D;\sum F(w)\cdot\frac{\vec{d}_{\omega}}{|\vec{d} _{\omega}|}\\<br>    &amp;&#x3D;\frac{1}{2\pi}\int _{-\infty} ^{+\infty}F(\omega)e ^{i\omega t}\text{d}\omega<br>\end{align*}<br>$$</p>
<blockquote>
<p>$|\vec{d} _\omega|&#x3D;|e ^{i\omega t}|&#x3D;\frac{2\pi}{\Delta\omega}$</p>
</blockquote>
<p>不难看出，傅里叶变换实际上是将希尔伯特空间上的向量$f$投影到了新的一组基向量$e ^{i\omega t}$上。其在原基向量上对应的二维曲线即为原曲线，如Fig 1所示。在新的基向量上也存在对应的二维曲线，如Fig 2所示。从Fig 1到Fig 2，相当于将原向量做了如Fig 3所示的旋转。</p>
<p><img src="/2023/05/10/FourierTransformAndCNN/1.png" alt="1"></p>
<center style="font-size:12px; font-weight:bold">Fig. 1. 时域</center><br>

<p><img src="/2023/05/10/FourierTransformAndCNN/2.png" alt="2"></p>
<center style="font-size:12px; font-weight:bold">Fig. 2. 频域</center><br>

<p><img src="/2023/05/10/FourierTransformAndCNN/3.png" alt="3"></p>
<center style="font-size:12px; font-weight:bold">Fig. 3. 坐标变换</center>

<h1 id="加窗傅里叶变换"><a href="#加窗傅里叶变换" class="headerlink" title="加窗傅里叶变换"></a>加窗傅里叶变换</h1><p>傅里叶变换会考虑曲线全局的信息，但是对于卷积神经网络，只考虑局部信息反而能达到更好的效果。傅里叶变换的基向量$e ^{i\omega t}$是希尔伯特空间里的一个向量，因此它也有对应的二维曲线，这实际体现为一个正弦波形：</p>
<p><img src="/2023/05/10/FourierTransformAndCNN/4.png" alt="4"></p>
<center style="font-size:12px; font-weight:bold">Fig. 4. 傅里叶变换基向量</center><br>

<p>加窗傅里叶变换通过抹平某个区域以外的基向量曲线来达到只考虑局部特征的目的：</p>
<p><img src="/2023/05/10/FourierTransformAndCNN/5.png" alt="5"></p>
<center style="font-size:12px; font-weight:bold">Fig. 5. 加窗傅里叶变换</center><br>

<p>$$<br>F(\omega)&#x3D;\int _{-\infty} ^{+\infty}f(t)g(t-s)e ^{-i\omega t}\text{d}t<br>$$</p>
<p>式中$g(t-s)$称窗函数，不同的窗函数可以得到不同的变换、提取不同的特征，$s$表当前窗口的位置，是个变量。</p>
<p>有了窗函数，变换域（频域）的曲线就必须以三维的方式表示。此时，重复的局部信息不会再影响频域轴的曲线，而是在窗口位置轴$s$中体现，如下图Fig 6和Fig 7所示。</p>
<p><img src="/2023/05/10/FourierTransformAndCNN/6.png" alt="6"></p>
<center style="font-size:12px; font-weight:bold">Fig. 6. 时域</center><br>

<p><img src="/2023/05/10/FourierTransformAndCNN/7.png" alt="7"></p>
<center style="font-size:12px; font-weight:bold">Fig. 7. 频域</center>

<h1 id="CNN与加窗傅里叶"><a href="#CNN与加窗傅里叶" class="headerlink" title="CNN与加窗傅里叶"></a>CNN与加窗傅里叶</h1><p>卷积神经网络的卷积实际上就是一种加窗傅里叶变换。原图片相当于$\vec{f}$，卷积过程中像素点的移动相当于窗口$s$的移动，而卷积核的大小、值则共同充当着窗函数和基向量。卷积（实际是加窗傅里叶变换）将像素点周边的局部特征通过像素值的线性组合标定为一种模式。后续的全连接神经网络为不同卷积核识别出的不同模式赋予不同的权重，最终组合出最有可能的模式。</p>
<p><img src="/2023/05/10/FourierTransformAndCNN/8.png" alt="8"></p>
<center style="font-size:12px; font-weight:bold">Fig. 8. CNN</center>
      
    </div>
    <footer class="article-footer">
      <a data-url="https://zclzcl0223.github.io/2023/05/10/FourierTransformAndCNN/" data-id="clzik0lcb002sv47k3pelg173" data-title="Fourier Transform and CNN" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/CNN/" rel="tag">CNN</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Math/" rel="tag">Math</a></li></ul>

    </footer>
  </div>
  
</article>



  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/page/4/">&laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/">6</a><a class="page-number" href="/page/7/">7</a><span class="space">&hellip;</span><a class="page-number" href="/page/10/">10</a><a class="extend next" rel="next" href="/page/6/">Next &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Advanced-Model/">Advanced Model</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/CS7313-Computational-Complexity/">CS7313: Computational Complexity</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Configuration/">Configuration</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Dive-Into-Deep-Learning/">Dive Into Deep Learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/GNN/">GNN</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Kernel-Method/">Kernel Method</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/MATH6005-Matrix-Theory/">MATH6005: Matrix Theory</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning-by-AndrewNg/">Machine Learning by AndrewNg</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Math/">Math</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Paper/">Paper</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Programming-Language/">Programming Language</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Tool/">Tool</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Attention-Mechanism/" rel="tag">Attention Mechanism</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CNN/" rel="tag">CNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Configuration/" rel="tag">Configuration</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Data-Analysis/" rel="tag">Data Analysis</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Dataset-Distillation/" rel="tag">Dataset Distillation</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Decision-Trees/" rel="tag">Decision Trees</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Deep-Learning/" rel="tag">Deep Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GCN/" rel="tag">GCN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GNN/" rel="tag">GNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Generative-AI/" rel="tag">Generative AI</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hexo/" rel="tag">Hexo</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Lab/" rel="tag">Lab</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux/" rel="tag">Linux</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-Learning/" rel="tag">Machine Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Markup-Language/" rel="tag">Markup Language</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Math/" rel="tag">Math</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Paper/" rel="tag">Paper</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/" rel="tag">Python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RNN/" rel="tag">RNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Recommender-System/" rel="tag">Recommender System</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Reinforcement-Learning/" rel="tag">Reinforcement Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SVM/" rel="tag">SVM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Supervised-Learning/" rel="tag">Supervised Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Theoretical-Computer-Science/" rel="tag">Theoretical Computer Science</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Tool/" rel="tag">Tool</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Unsupervised-Learning/" rel="tag">Unsupervised Learning</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Attention-Mechanism/" style="font-size: 11.82px;">Attention Mechanism</a> <a href="/tags/CNN/" style="font-size: 12.73px;">CNN</a> <a href="/tags/Configuration/" style="font-size: 10px;">Configuration</a> <a href="/tags/Data-Analysis/" style="font-size: 10px;">Data Analysis</a> <a href="/tags/Dataset-Distillation/" style="font-size: 13.64px;">Dataset Distillation</a> <a href="/tags/Decision-Trees/" style="font-size: 10px;">Decision Trees</a> <a href="/tags/Deep-Learning/" style="font-size: 20px;">Deep Learning</a> <a href="/tags/GCN/" style="font-size: 11.82px;">GCN</a> <a href="/tags/GNN/" style="font-size: 15.45px;">GNN</a> <a href="/tags/Generative-AI/" style="font-size: 10.91px;">Generative AI</a> <a href="/tags/Hexo/" style="font-size: 10.91px;">Hexo</a> <a href="/tags/Lab/" style="font-size: 10.91px;">Lab</a> <a href="/tags/Linux/" style="font-size: 16.36px;">Linux</a> <a href="/tags/Machine-Learning/" style="font-size: 17.27px;">Machine Learning</a> <a href="/tags/Markup-Language/" style="font-size: 10px;">Markup Language</a> <a href="/tags/Math/" style="font-size: 19.09px;">Math</a> <a href="/tags/Paper/" style="font-size: 14.55px;">Paper</a> <a href="/tags/Python/" style="font-size: 18.18px;">Python</a> <a href="/tags/RNN/" style="font-size: 10.91px;">RNN</a> <a href="/tags/Recommender-System/" style="font-size: 10.91px;">Recommender System</a> <a href="/tags/Reinforcement-Learning/" style="font-size: 10.91px;">Reinforcement Learning</a> <a href="/tags/SVM/" style="font-size: 10px;">SVM</a> <a href="/tags/Supervised-Learning/" style="font-size: 11.82px;">Supervised Learning</a> <a href="/tags/Theoretical-Computer-Science/" style="font-size: 15.45px;">Theoretical Computer Science</a> <a href="/tags/Tool/" style="font-size: 18.18px;">Tool</a> <a href="/tags/Unsupervised-Learning/" style="font-size: 11.82px;">Unsupervised Learning</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/06/">June 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/01/">January 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/12/">December 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/06/">June 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/05/">May 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/04/">April 2023</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2024/06/17/Diffusion/">Diffusion</a>
          </li>
        
          <li>
            <a href="/2024/01/20/VAE/">VAE</a>
          </li>
        
          <li>
            <a href="/2023/12/02/CountingComplexity/">Computational Complexity: Complexity of Counting</a>
          </li>
        
          <li>
            <a href="/2023/11/24/MatrixTheory5/">MatrixTheory: 特殊矩阵与矩阵分解</a>
          </li>
        
          <li>
            <a href="/2023/11/13/RandomizedComputation/">Computational Complexity: Randomized Computation</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 ChaosTsang<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/" class="mobile-nav-link">home</a>
  
    <a href="/about/" class="mobile-nav-link">about</a>
  
    <a href="/tags/" class="mobile-nav-link">tags</a>
  
    <a href="/categories/" class="mobile-nav-link">categories</a>
  
    <a href="/archives/" class="mobile-nav-link">archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>





<script src="/js/script.js"></script>





  </div>
</body>
</html>