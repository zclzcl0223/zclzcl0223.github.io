<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=[object Object]"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', '[object Object]');
</script>
<!-- End Google Analytics -->

  
  <title>JourneyToCoding</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="Start of Something New">
<meta property="og:type" content="website">
<meta property="og:title" content="JourneyToCoding">
<meta property="og:url" content="https://zclzcl0223.github.io/page/7/index.html">
<meta property="og:site_name" content="JourneyToCoding">
<meta property="og:description" content="Start of Something New">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="ChaosTsang">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="JourneyToCoding" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/%5Bobject%20Object%5D">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">JourneyToCoding</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">Code for fun</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/">home</a>
        
          <a class="main-nav-link" href="/about/">about</a>
        
          <a class="main-nav-link" href="/tags/">tags</a>
        
          <a class="main-nav-link" href="/categories/">categories</a>
        
          <a class="main-nav-link" href="/archives/">archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://zclzcl0223.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-DecisionTrees" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/04/16/DecisionTrees/" class="article-date">
  <time class="dt-published" datetime="2023-04-16T04:48:16.000Z" itemprop="datePublished">2023-04-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Machine-Learning-by-AndrewNg/">Machine Learning by AndrewNg</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/04/16/DecisionTrees/">Decision Trees</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <span id="more"></span>

<h1 id="Structure-of-decision-trees"><a href="#Structure-of-decision-trees" class="headerlink" title="Structure of decision trees"></a>Structure of decision trees</h1><p>Decision trees are a tree model, where each internal node (decision node) is a feature that has several possibile values. The number of values defines the degree of nodes.</p>
<p><img src="/2023/04/16/DecisionTrees/1.png" alt="1"></p>
<center style="font-size:12px;font-weight:bold">Fig. 1. Decision trees</center>

<h1 id="Decision-tree-learning"><a href="#Decision-tree-learning" class="headerlink" title="Decision tree learning"></a>Decision tree learning</h1><p>First decision: How to choose features to maximize purity? Second decision: When can the tree stop splitting</p>
<ul>
<li>When a node is 100% one class.</li>
<li>When splitting a node will result in the tree exceeding a <strong>maximum depth</strong> (defined by us).</li>
<li>When improvements in purity score are below a threshold.</li>
<li>When number of examples in a node is below a threshold.</li>
</ul>
<h2 id="Measuring-purity"><a href="#Measuring-purity" class="headerlink" title="Measuring purity"></a>Measuring purity</h2><p>p$_1$: Fraction of examples that are our target.</p>
<p>H(p$_1$): Degree of impurity of one set.</p>
<p>$$p_0&#x3D;1-p_1$$<br>$$H(p_1)&#x3D;-p_1\log_2(p_1)-p_0\log_2(p_0)$$</p>
<p><img src="/2023/04/16/DecisionTrees/2.png" alt="2"></p>
<center style="font-size:12px;font-weight:bold">Fig. 2. Entropy function</center>

<h2 id="Choosing-a-split"><a href="#Choosing-a-split" class="headerlink" title="Choosing a split"></a>Choosing a split</h2><h3 id="Information-gain"><a href="#Information-gain" class="headerlink" title="Information gain"></a>Information gain</h3><p>Information gain is the redunction of entropy after one split:</p>
<p>$$IG&#x3D;H(p_1^{root})-(w^{left}H(p_1^{left})+w^{right}H(p_1^{right}))$$</p>
<p>where $w^{left}H(p_1^{left})+w^{right}H(p_1^{right})$ is the average weighted entropy of two child nodes. $w$ is the propotion of examples in left&#x2F;right node that come from root node.</p>
<h2 id="Building-a-decision-tree"><a href="#Building-a-decision-tree" class="headerlink" title="Building a decision tree"></a>Building a decision tree</h2><p>We always construct decision trees in a preorder traversal. That is:</p>
<ul>
<li>Start with all examples at root node;</li>
<li>Calculate information gain for all possible features and pick the one with the highest information gain;</li>
<li>Split dataset and build subtrees recursively until stopping criteria is met.</li>
</ul>
<blockquote>
<p>For features with more than two discrete values, we can use <strong>one-hot encoding</strong>.  That is, decompose the features with $k$ values into $k$ bool features so that the values of each new feature will only be yes (1) or no (0). One-hot encoding can also be applied to neural networks to turn multiclass classification to binary classification.</p>
<p>For features with continuous values, we try splitting values using different thresholds and choose the threshold that gives the highest information gain.</p>
<p>It is reasonable that the same features appears several times in one decision trees.</p>
</blockquote>
<h2 id="Regression-trees"><a href="#Regression-trees" class="headerlink" title="Regression trees"></a>Regression trees</h2><p>Regression trees are the generalization of decision trees, which predict a number. The differences between building a regression tree and a decision tree are:</p>
<ul>
<li>The prediction of regression trees is the average value of examples in leaf node;</li>
<li>We choose the features to splite dataset by the reduction of variance.</li>
</ul>
<p>For the second difference:</p>
<ul>
<li>Calculate the average weighted variance of one split:<br>$$w^{left}v^{left}+w^{right}v^{right}$$</li>
<li>Calcuate the reduction of variance after splitting:<br>$$v^{root}-(w^{left}v^{left}+w^{right}v^{right})$$</li>
<li>Choose the feature that gives the highest reduction and continue until meet stopping criteria.</li>
</ul>
<h1 id="Ensemble-of-decision-trees"><a href="#Ensemble-of-decision-trees" class="headerlink" title="Ensemble of decision trees"></a>Ensemble of decision trees</h1><p>A single decision tree is highly sentitive to small changes of the data. We may get two totally different decision trees even though only one example changes in the training set. An ensemble of decision trees will make the overall algorithm more robust. An ensemble of decision trees makes decision by voting, that is, the prediction of the ensemble is the value most decision trees give.</p>
<p><img src="/2023/04/16/DecisionTrees/3.png" alt="3"></p>
<center style="font-size:12px;font-weight:bold">Fig. 3. Ensemble of decision trees</center>

<h2 id="Random-forest-algorithm"><a href="#Random-forest-algorithm" class="headerlink" title="Random forest algorithm"></a>Random forest algorithm</h2><p>One algorithm to create an ensemble of decision trees is random forest algorithm. The algorithm can automatically explore a lot of small changes to the data set, which makes it more robust.</p>
<p>Random forests are generated by sampling with replacement and randomizing feature choice:</p>
<ul>
<li>Determine the number of trees $B$ we need and the size of subset $m$;</li>
<li>Use sampling with replacement to create a new training set of size $m$;</li>
<li>Randomly pick a subset of $k$ (usually $\sqrt{n}$) features from $n$ features and train a decision tree only use these features and the data set generated above;</li>
<li>Repeatedly generate $B$ trees.</li>
</ul>
<p>Sampling with replacement and randomizing feature choice allow us generate different trees as much as possible.</p>
<h2 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h2><p>XGBoost is an open-source library that makes ensemble of decision trees perform better. The core idea of XGBoost is that it is more worthwhile to train decision trees using the examples that the previously trained decision trees have failed to predict rather than using totally random examples. Trees generated like this is called <strong>boosted trees</strong>. In XGBoost, instead of picking subsets from all samples with equal probability, it makes it <strong>more likely</strong> to pick examples that the previously trained trees have misclassfied. The details of XGBoost are very complicated, but the using of it is quite simple:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> XGBClassifier <span class="comment"># or XGBRegressor for regression</span></span><br><span class="line"></span><br><span class="line">model = XGBClassifier()</span><br><span class="line"></span><br><span class="line">model.fit(X_train, Y_train)</span><br><span class="line">y_pred = model.predict(X_test)</span><br></pre></td></tr></table></figure>

<h1 id="Decision-trees-amp-Neural-Networks"><a href="#Decision-trees-amp-Neural-Networks" class="headerlink" title="Decision trees &amp; Neural Networks"></a>Decision trees &amp; Neural Networks</h1><p>For decision trees and tree ensembles:</p>
<ul>
<li>Work well on structured data; [advantage]</li>
<li>Fast to train; [advantage]</li>
<li>Small decision tress may be human interpretable; [advantage]</li>
<li>Not recommended for unstructured data (images, audio, text) [disadvantage];</li>
<li>Can&#39;t train multiple trees at a time. [disadvantage]</li>
</ul>
<p>Neural Networks:</p>
<ul>
<li>Works well on all types of data; [advantage]</li>
<li>Works with transfer learning; [advantage]</li>
<li>Multiple neural networks can be easier to string together when building a system of multiple models working together; [advantage]</li>
<li>Take long time to train. [disadvantage]</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zclzcl0223.github.io/2023/04/16/DecisionTrees/" data-id="clzik1qtd001xm07kg0ch9572" data-title="Decision Trees" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Decision-Trees/" rel="tag">Decision Trees</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-Learning/" rel="tag">Machine Learning</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-MachineLearningDevelopmentProcess" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/04/15/MachineLearningDevelopmentProcess/" class="article-date">
  <time class="dt-published" datetime="2023-04-15T08:33:36.000Z" itemprop="datePublished">2023-04-15</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Machine-Learning-by-AndrewNg/">Machine Learning by AndrewNg</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/04/15/MachineLearningDevelopmentProcess/">Machine Learning Development Process</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <span id="more"></span>

<h1 id="Full-cycle-of-ML"><a href="#Full-cycle-of-ML" class="headerlink" title="Full cycle of ML"></a>Full cycle of ML</h1><p><img src="/2023/04/15/MachineLearningDevelopmentProcess/1.png" alt="1"></p>
<center style="font-size:12px;font-weight:bold">Fig. 1. Full cycle of ML</center><br>

<p>When developing a machine learning project, these are the four assignments we need to conduct.</p>
<h2 id="Scope-project"><a href="#Scope-project" class="headerlink" title="Scope project"></a>Scope project</h2><p>The step defines the purposes of the machine learning project and the functions it should implement.</p>
<h2 id="Collect-data"><a href="#Collect-data" class="headerlink" title="Collect data"></a>Collect data</h2><p>In this step, we define the data we need and try to collect as much data as we can. However, instead of adding data of all types, it is advisable to focus on adding more data of the types that error analysis has indicated it migh help.</p>
<h3 id="Data-augmentation"><a href="#Data-augmentation" class="headerlink" title="Data augmentation"></a>Data augmentation</h3><p>Data augmentation is a useful technique used especially for image and audio data.It creates new training examples by modifying existing training examples.For image data, we can distort the image and for audio data, we can add some noise.However, the type of noise or distortions we add to the set must be meaningful, that is, they should not be purely random but should be pertinent.</p>
<h3 id="Data-synthesis"><a href="#Data-synthesis" class="headerlink" title="Data synthesis"></a>Data synthesis</h3><p>Unlike data augmentation, data synthesis creates brand new examples. This method is used most for CV.</p>
<h3 id="Transfer-learning"><a href="#Transfer-learning" class="headerlink" title="Transfer learning"></a>Transfer learning</h3><p>Transfer learning is a research problem in machine learning that focuses on applying knowledge gained while solving one task to a related task. For example, knowledge gained while learning to recognize cars could be applied when trying to recognize trucks. This method is especially useful when we do not have enough data or computing power to train a large neural network.</p>
<p>When using transfer learning:</p>
<ol>
<li>Download neural network parameters <strong>pretrained</strong> on a large dataset with <strong>the same input type</strong> (e.g., images, audio, text) as our application;</li>
<li>Change the output layer and further train (<strong>fine tune</strong>)the network on our own data.</li>
</ol>
<p>When fine tuning, we can only train output layers parameters when our training set is small or we can train all parameters when our training set is large enough.</p>
<p>The reason why tranfer learning works is that in the hidden layers of a pretrained model, they have learnt something (maybe a subset of our target). Therefore, using the parameters of a pretrained model will make our model start from a better place.</p>
<blockquote>
<p>AI &#x3D; Code + Data. When models or algorithms are good enough, focusing on data can be an efficient way to help learning algorithms improve their performance.</p>
</blockquote>
<h2 id="Train-model"><a href="#Train-model" class="headerlink" title="Train model"></a>Train model</h2><p><img src="/2023/04/15/MachineLearningDevelopmentProcess/2.png" alt="2"></p>
<center style="font-size:12px;font-weight:bold">Fig. 2. Iterative loop of ML</center><br>

<p>Depending on bias, variance and error analysis, we may need to modify the model or collect more data.</p>
<h2 id="Deploy-in-production"><a href="#Deploy-in-production" class="headerlink" title="Deploy in production"></a>Deploy in production</h2><p>A common structure of an application that uses machine learning models is as follows:</p>
<p><img src="/2023/04/15/MachineLearningDevelopmentProcess/3.png" alt="3"></p>
<center style="font-size:12px;font-weight:bold">Fig. 3. ML APP</center><br>

<p>The job of inference server is to call the trained model in order to make predictions.Inference server expose its API to mobile app. Mobile app just need to call API and wait for the prediction from inference server.</p>
<p>In this step, if the app does not work well, we may need to retrain the model or collect more data.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zclzcl0223.github.io/2023/04/15/MachineLearningDevelopmentProcess/" data-id="clzik1qtn004cm07kfykr4ncn" data-title="Machine Learning Development Process" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Deep-Learning/" rel="tag">Deep Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-Learning/" rel="tag">Machine Learning</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-MachineLearningDiagnostics" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/04/13/MachineLearningDiagnostics/" class="article-date">
  <time class="dt-published" datetime="2023-04-13T13:25:19.000Z" itemprop="datePublished">2023-04-13</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Machine-Learning-by-AndrewNg/">Machine Learning by AndrewNg</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/04/13/MachineLearningDiagnostics/">Machine Learning Diagnostics</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <span id="more"></span>

<h1 id="Evaluating-a-model"><a href="#Evaluating-a-model" class="headerlink" title="Evaluating a model"></a>Evaluating a model</h1><p>To evaluate a model is to measure the accuracy of the model&#39;s predictions. There are many ways to do so.</p>
<h2 id="Two-sets"><a href="#Two-sets" class="headerlink" title="Two sets"></a>Two sets</h2><p>One useful method is to splite our training set into <strong>training set</strong> and <strong>test set</strong>. Training set is used to train and evaluate the model, but test set is only applied to evaluate the model. To evaluate a trained model, we should calculate both the $J_{test}$ and $J_{train}$, both of which are without regularization term. But the model is regularized.</p>
<blockquote>
<p>For classification model, we can also count the number of misclassified examples in training set and test set respectively.</p>
</blockquote>
<h1 id="Choosing-a-model"><a href="#Choosing-a-model" class="headerlink" title="Choosing a model"></a>Choosing a model</h1><p>To choose a model or to determine the architecture of a model, using two datasets suffers from the same problems as evaluating models on one dataset.</p>
<p>When evaluating a model on one dataset, we actually get an optimistic estimate of generalization error. That is, the model only fits the training set well but can not be generalized. The same problem will happen when choosing a model with two datasets. When we use the test set to choose a model, it is likely that the model may just fit this test set well as the structure of the model is also a parameter like $W$ and $B$. In conclusion, we can not use datasets that determine the model to evaluate or choose a model.</p>
<p><img src="/2023/04/13/MachineLearningDiagnostics/1.png" alt="1"></p>
<center style="font-size:12px;font-weight:bold">Fig. 1. Model choosing</center>

<h2 id="Three-sets"><a href="#Three-sets" class="headerlink" title="Three sets"></a>Three sets</h2><p>The solution is to divide the dataset into three parts: <strong>training set</strong>, <strong>cross validation set</strong> and <strong>test set</strong>.</p>
<blockquote>
<p>Cross validation set is also called development set or dev set.</p>
</blockquote>
<p>The procedure of choosing a model is similar to evaluating a model:</p>
<ul>
<li>Train the model using training set, get $W$ and $B$;</li>
<li>Choose a model using development set, calculate $J_{cv}$ and get d (degree of polynomial);</li>
<li>Verify the model using test set, calculate $J_{test}$.</li>
</ul>
<style> table th {
    width: 160px;
}
</style>

<table>
<thead>
<tr>
<th align="center">type</th>
<th align="center">training set</th>
<th align="center">dev set</th>
<th align="center">test set</th>
</tr>
</thead>
<tbody><tr>
<td align="center">trained</td>
<td align="center">yes</td>
<td align="center">no</td>
<td align="center">no</td>
</tr>
<tr>
<td align="center">function</td>
<td align="center">get $W$ and $B$</td>
<td align="center">determine model structure</td>
<td align="center">evaluate generalization ability of model</td>
</tr>
<tr>
<td align="center">usage count</td>
<td align="center">multiple times</td>
<td align="center">multiple times</td>
<td align="center">one time</td>
</tr>
</tbody></table>
<blockquote>
<p>A vivid metaphor about these sets is: Training set is students&#39;textbook, dev set is students&#39;homework and test set is the final exam.</p>
</blockquote>
<h1 id="Diagnostics"><a href="#Diagnostics" class="headerlink" title="Diagnostics"></a>Diagnostics</h1><h2 id="Bias-and-variance"><a href="#Bias-and-variance" class="headerlink" title="Bias and variance"></a>Bias and variance</h2><p>Instead of plotting the model to judge underfitting or overfitting, a more common method is to calculate and compare $J_{train}$ and $J_{cv}$.</p>
<ul>
<li>If $J_{train}$ is high and $J_{train}\approx J_{cv}$, the algorithm(model) may have high bias;</li>
<li>If $J_{train}$ is low and $J_{train}&lt;&lt;J_{cv}$, the algorithm(model) may have high variance;</li>
<li>If $J_{train}$ is high and $J_{train}&lt;&lt;J_{cv}$, the algorithm(model) may have high bias and high variance.<blockquote>
<p>An algorithm having both high bias and high variance fits some training examples well but fits others badly.</p>
</blockquote>
</li>
</ul>
<p><img src="/2023/04/13/MachineLearningDiagnostics/2.png" alt="2"></p>
<center style="font-size:12px;font-weight:bold">Fig. 2. Relationship between D and error</center>

<h3 id="Choosing-a-good-lambda"><a href="#Choosing-a-good-lambda" class="headerlink" title="Choosing a good $\lambda$"></a>Choosing a good $\lambda$</h3><p>We can also use the dev set to choose a better regularization parameter $\lambda$.</p>
<ul>
<li>If $\lambda$ is rather small, we value fitting the data. Therefore, $J_{cv}$ may be rather high while $J_{train}$ may be rather low;</li>
<li>If $\lambda$ is rather big, we value scaling $\vec{w}$. Therefore, $\vec{w}$ is aproximate 0, $J_{cv}$ may be rather high and $J_{train}$ may also be rather high.</li>
</ul>
<p><img src="/2023/04/13/MachineLearningDiagnostics/3.png" alt="3"></p>
<center style="font-size:12px;font-weight:bold">Fig. 3. Relationship between lambda and error</center>

<h3 id="Quantitative-indicators"><a href="#Quantitative-indicators" class="headerlink" title="Quantitative indicators"></a>Quantitative indicators</h3><p>In order to judge bias or variance quantitatively, a baseline level of performance is required. It can be human level performance, competing algorithms performance or just guess based on experience.</p>
<ul>
<li>When gap between baseline and $J_{train}$ is rather high, the model may have high bias;</li>
<li>When gap between $J_{train}$ and $J_{cv}$ is rather high, the model may have high variance.</li>
</ul>
<h3 id="Learning-curve"><a href="#Learning-curve" class="headerlink" title="Learning curve"></a>Learning curve</h3><p>Learning curve is a function curve of training set size and error of training set and dev set. We can use learning curve to judge whether a model has high bias or high variance. A regular learning curve looks like:</p>
<p><img src="/2023/04/13/MachineLearningDiagnostics/4.png" alt="4"></p>
<center style="font-size:12px;font-weight:bold">Fig. 4. Regular learning curve</center><br>

<p>When a model has high bias, its learning curve looks like:</p>
<p><img src="/2023/04/13/MachineLearningDiagnostics/5.png" alt="5"></p>
<center style="font-size:12px;font-weight:bold">Fig. 5. High bias learning curve</center><br>

<p>It indicates that when a model has high bias, we can not train a good model by using more data.</p>
<p>When a model has high variance, its learning curve looks like:</p>
<p><img src="/2023/04/13/MachineLearningDiagnostics/6.png" alt="6"></p>
<center style="font-size:12px;font-weight:bold">Fig. 6. High variance learning curve</center><br>

<p>It indicates that when a model has high variance, we can train a better model by using more data because your model is complicated enough. Though learning curve gives us an intuitive visual experience of the performance of our model, it is seldom used as plotting it wastes too much time.</p>
<h3 id="Fixing-bias-variance"><a href="#Fixing-bias-variance" class="headerlink" title="Fixing bias variance"></a>Fixing bias variance</h3><p>High variance: </p>
<ul>
<li>Get more training examples;</li>
<li>Try smaller sets of features;</li>
<li>Try increasing $\lambda$.</li>
</ul>
<p>High bias:</p>
<ul>
<li>Try getting additional features;</li>
<li>Try adding polynomial features;</li>
<li>Try decreasing $\lambda$.</li>
</ul>
<h3 id="DL-and-bias-variance"><a href="#DL-and-bias-variance" class="headerlink" title="DL and bias variance"></a>DL and bias variance</h3><p>In deep learning, the contradiction between high bias and variance can be easily solved. In general, large neural networks are low bias machines as we can reduce bias by adding more layers or neurons. For variance, we can solve it by adding more data. As long as regularization is chosen appropriately, a large neural network will usually do as well or better than a smaller one.</p>
<p><img src="/2023/04/13/MachineLearningDiagnostics/7.png" alt="7"></p>
<center style="font-size:12px;font-weight:bold">Fig. 7. Training of neural networks</center><br>

<blockquote>
<p>A bigger network needs powerful computing power to train. Therefore, the development of hardwares, especially GPU, and big data contribute to the thriving of deep learning.</p>
</blockquote>
<h2 id="Error-analysis"><a href="#Error-analysis" class="headerlink" title="Error analysis"></a>Error analysis</h2><p>Error analysis is another useful diagnostics method when training a model. In error analysis, we take the examples that the model has wrongly predicted or inferred into account and group them into common themes or common properties. These categories can be overlapping. Sometimes, the error set may be too large for us to deal with. In this case, it is advisable to randomly sample a subset (usually 100 examples).</p>
<p>In the next step, we can modify the model according to these categories. It is advisable to process the categories that are large enough but just ignore those small categories.</p>
<h2 id="Skewed-datasets"><a href="#Skewed-datasets" class="headerlink" title="Skewed datasets"></a>Skewed datasets</h2><p>Skewed datasets are datasets that have an uneven subset distribution. That is, the number of one output is much more than the others. In this case, we can&#39;t simply judge the performance of model using $J$ as we may get a good result even if the model just predicts this label all the time. </p>
<p>The solution is to count <em>precision</em> and <em>recall</em> of the model. In binary regression (for multiclass classfication, we set the real label <code>1</code> and the others <code>0</code>), we can display the true value and predict value in the form of 2*2 matrix: </p>
<p><img src="/2023/04/13/MachineLearningDiagnostics/8.png" alt="8"></p>
<center style="font-size:12px;font-weight:bold">Fig. 8. Output distribution</center><br>

<p>Then <em>precision</em> is defined as the fraction that are actually 1 among all examples where we predicted $y&#x3D;1$:<br>$${Precision}&#x3D;\frac{TruePos}{TruePos+FalsePos}$$</p>
<p><em>Recall</em> is defined as the fraction that are correctly predicted among all examples that are actually 1:<br>$${Recall}&#x3D;\frac{TruePos}{TruePos+FalseNeg}$$</p>
<p>A good model should have both high <em>precision</em> and <em>recall</em>. Once <em>precision</em> or <em>recall</em> is 0, chances are that the model <code>print(&quot;y=0&quot;)</code> all the time.</p>
<h3 id="F-1-score-Trading-off-precision-and-recall"><a href="#F-1-score-Trading-off-precision-and-recall" class="headerlink" title="$F_1 score$: Trading off precision and recall"></a>$F_1 score$: Trading off precision and recall</h3><p>For logistic regression, threshold is the boundary value used to separate 0 and 1. If raising the threshold, we will get higher <em>precision</em> but lower <em>recall</em>. If decreasing the threshold, we will get higher <em>recall</em> but lower <em>precision</em>. We can&#39;t keep both <em>precision</em> and <em>recall</em> high. What&#39;s more, we can&#39;t solve this problem using dev set as threshold is defined by us.</p>
<p><img src="/2023/04/13/MachineLearningDiagnostics/9.png" alt="9"></p>
<center style="font-size:12px;font-weight:bold">Fig. 9. Precision and recall</center><br>

<p>The method to compare <em>precision</em> and <em>recall</em> is to calculate $F_1 \space score$:<br>$$F_1\space{score}&#x3D;\frac{1}{\frac{1}{2}(\frac{1}{P}+\frac{1}{R})}&#x3D;\frac{2PR}{P+R}$$</p>
<p>The larger the $F_1 \space score$ is , the better the model is.</p>
<h3 id="ROC-amp-AUC"><a href="#ROC-amp-AUC" class="headerlink" title="ROC &amp; AUC"></a>ROC &amp; AUC</h3><p>$F_1 \space score$ requires us to choose a threshold to judge the performance of the classifier while <em>ROC</em> and <em>AUC</em> are more intelligent.</p>
<p>Receiver operating characteristic curve (ROC) is a curve whose $x$ axis is False positive rate (FPR) and $y$ axis is True positive rate (TPR)ï¼š</p>
<p>$$<br>\begin{align*}<br>    TPR&#x3D;Recall&amp;&#x3D;\frac{TruePos}{TruePos+FalseNeg}\\<br>    FPR&#x3D;&amp;\frac{FalsePos}{FalsePos+TrueNeg}<br>\end{align*}<br>$$</p>
<p><img src="/2023/04/13/MachineLearningDiagnostics/10.png" alt="10"></p>
<center style="font-size:12px;font-weight:bold">Fig. 10. ROC</center><br>

<p>Each point on ROC represent a $(FPR, TPR)$ pair under a certain threshold. For example:</p>
<ul>
<li>If threshold is 0, all the samples will be predicted as 1. In this case, both FPR and TPR is 1;</li>
<li>If threshold is 1, all the samples will be predicted as 0. In this case, both FPR and TPR is 0;</li>
<li>With threshold decreasing, points on ROC moves to the top right (or right&#x2F;or up), or remains stationary;</li>
<li>When points on ROC are on $y&#x3D;x$, the classifier has no difference with random guesses. The closer the points are to the upper left corner, the better the classifier is;</li>
<li>Points on ROC should always be above $y&#x3D;x$ as when points on ROC are below $y&#x3D;x$, just letting the classifier make the opposite conclusion will make these points on top of $y&#x3D;x$.</li>
</ul>
<p>Area under curve (AUC) is the area below ROC. If we randomly pick a positive sample and a negative sample, AUC represents the probability that the classifier predicts the value of the positive sample is bigger than the value of the negative sample. Taking AUC as an indicator to measure the performance of a classifier helps us dismiss the influence of skewed datasets and threshold. In other words, AUC measures the ability of the classifier to correctly sort samples. That&#39;s why AUC is a more commonly used indicator.</p>
<p>Since we can&#39;t take all the values of threshold into account, we can only use the approximate method to calculate AUC, that is, computing the rate that the value of the positive sample is bigger than the value of the negative sample among all the postive-negative pairs. For a dataset with $M$ positive sample and $N$ negative samples, the number of positive-negative pairs is $M\times N$:</p>
<ol>
<li>Sort the values of all the positive and negative samples from largest to smallest;</li>
<li>Score them from $N+M$ to $1$;</li>
<li>Sum the score of positive samples;</li>
<li>Subtract $M(M+1)&#x2F;2$;</li>
<li>Divide by $M\times N$:<br> $$AUC&#x3D;\frac{\sum _{i\in\text{positive}}\text{score}_i-M(M+1)&#x2F;2}{MN}\tag{1}$$</li>
</ol>
<p>Such a process works as $\text{score} _i$ is the score of the $i$-th biggest positive sample ($i&#x3D;1,...,M$) and</p>
<p>$$<br>N+M-\text{score}_i-(i-1)<br>$$</p>
<p>is the number of negative samples whose score is higher than it. Then</p>
<p>$$<br>\begin{align*}<br>    N-[N+M-\text{score}_i-(i-1)]<br>    &amp;&#x3D;\text{score}_i+(i-1)-M\\<br>    &amp;&#x3D;\text{score}_i-(M+1-i)<br>\end{align*}<br>$$</p>
<p>is the number of positive-negative pairs that consist of the $i$-th positive sample and negative samples whose values are smaller than the positive sample. As a result, the number of positive-negative pairs that meet our requirement is</p>
<p>$$<br>\sum\limits _{i&#x3D;1} ^{M}[\text{score}_i-(M+1-i)]&#x3D;\sum _{i\in\text{positive}}\text{score}_i-M(M+1)&#x2F;2<br>$$</p>
<blockquote>
<p>In sklearn, function <code>sklearn.metrics.roc_auc_score</code> could calculate the AUC of a classifier.</p>
<p>From the relationship between AUC and threshold, it can be seen that: if a large threshold is used in actual deployment, a little FP can bring us large TP. As a result, the precision of the classifier will be very high. Since it is not necessary to classify all positive classes, using different threshold in training and deployment is quite reasonable.</p>
</blockquote>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zclzcl0223.github.io/2023/04/13/MachineLearningDiagnostics/" data-id="clzik1qto004jm07k8e77gjy1" data-title="Machine Learning Diagnostics" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Deep-Learning/" rel="tag">Deep Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-Learning/" rel="tag">Machine Learning</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-NeuralNetwork" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/04/10/NeuralNetwork/" class="article-date">
  <time class="dt-published" datetime="2023-04-10T13:30:44.000Z" itemprop="datePublished">2023-04-10</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Machine-Learning-by-AndrewNg/">Machine Learning by AndrewNg</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/04/10/NeuralNetwork/">Neural Network</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Neural network, which can also be named <em><strong>deep learning</strong></em>, is an advanced machine learning model.</p>
<blockquote>
<p>Neural network is an algorithm suitable for nearly all kinds of machine learning. Compared to traditional models, neural network performs better when the training set is large.</p>
</blockquote>
<h1 id="Component"><a href="#Component" class="headerlink" title="Component"></a>Component</h1><h2 id="Layer"><a href="#Layer" class="headerlink" title="Layer"></a>Layer</h2><p>Neural network is consisted of different layers. A layer is a grouping of neurons which takes the same or similar features as input and in turn outputs a few numbers together. The <em><strong>first layer</strong></em> (layer 0) is called <em><strong>input layer</strong></em> where input and output are the same. The <em><strong>last layer</strong></em> is called <em><strong>output layer</strong></em> which outputs the value of the neural network. Input and output layer are the only two layers that are visible to us, therefore, the other layers are called <em><strong>hidden layer</strong></em>.</p>
<blockquote>
<p>There are different types of hidden layer:</p>
<ul>
<li>Dense layer: Each neuron output is a function of all the activation outputs of the previous layer;</li>
<li>Convolutional layer: Each neuron only looks at part of the previous layer&#39;s outputs. Different neurons may look at the same outputs.</li>
<li>...</li>
</ul>
</blockquote>
<p><img src="/2023/04/10/NeuralNetwork/1.png" alt="1"></p>
<center style="font-size:12px;font-weight:bold">Fig. 1. Multilayer perceptron</center>

<h2 id="Neuron"><a href="#Neuron" class="headerlink" title="Neuron"></a>Neuron</h2><p>Each layer is made up of several (including one) neurons. Each neuron is a traditional machine learning model, like linear regression, logistic regression and so on. The output of one neuron is called <em><strong>activation</strong></em> and the function of this neuron is called <em><strong>activation function</strong></em>, which means it activate the next neuron.</p>
<p>The magic of neural network is that it can learn new features by itself. So, we do not need to define who is the father of one neuron. Actually, each neuron will take the activations as its input, but the parameter of some activations may be zero. We just need to input the training set and define the structure of neural network. Then, the neural network will produce the most suitable new features. That is, a neuron (traditional model) is actually a new feature.</p>
<blockquote>
<p>The structure of neural network is called <em><strong>neural network architecture</strong></em>. It defines the number of layers and the number of neurons in each layer.</p>
</blockquote>
<p><img src="/2023/04/10/NeuralNetwork/2.png" alt="2"></p>
<center style="font-size:12px;font-weight:bold">Fig. 2.</center>

<h2 id="Notation"><a href="#Notation" class="headerlink" title="Notation"></a>Notation</h2><ul>
<li>$a^{[i]}$ &#x3D; output of layer i;</li>
<li>$\vec{w}^{[i]}, b^{[i]}$&#x3D;parameters of layer i.</li>
</ul>
<h1 id="Forward-propagation-algorithm"><a href="#Forward-propagation-algorithm" class="headerlink" title="Forward propagation algorithm"></a>Forward propagation algorithm</h1><p>Forward propagation is a series of steps to count $f$. It is an inference or prediction of $y$. So, it is similar to $\widehat{y}$ in traditional model.</p>
<p><img src="/2023/04/10/NeuralNetwork/3.png" alt="3"></p>
<center style="font-size:12px;font-weight:bold">Fig. 3. Handwritten digit recognition</center>

<h2 id="Numpy-and-Tensorflow"><a href="#Numpy-and-Tensorflow" class="headerlink" title="Numpy and Tensorflow"></a>Numpy and Tensorflow</h2><p>The data representation in numpy is slightly different from tensorflow. In numpy, we can represent data either in the form of matrix or in the form of vector:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">x = np.array([[<span class="number">200</span>, <span class="number">17</span>]]) <span class="comment"># array 1*2</span></span><br><span class="line">x = np.array([[<span class="number">200</span>],[<span class="number">17</span>]]) <span class="comment"># array 2*1</span></span><br><span class="line">x = np.array([<span class="number">200</span>, <span class="number">17</span>]) <span class="comment"># just a row vector</span></span><br></pre></td></tr></table></figure>
<p>But we can only represent data in the form of matrix in tensorflow. Therefore, when using numpy and tensorflow together, it is advisable to store the data in the form of matrix.</p>
<p>The followings are the implementation of a neuron network about coffee roasting using numpy and tensorflow. (Assuming the neural network has been trained)</p>
<p><img src="/2023/04/10/NeuralNetwork/4.png" alt="4"></p>
<center style="font-size:12px;font-weight:bold">Fig. 4. Coffee roasting (two inputs)</center><br>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Dense</span><br><span class="line"></span><br><span class="line">x = np.array([[<span class="number">200.0</span>, <span class="number">17.0</span>]])</span><br><span class="line">layer_1 = Dense(units=<span class="number">3</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>)</span><br><span class="line">a1 = layer_1(x)</span><br><span class="line"></span><br><span class="line">layer_2 = Dense(units=<span class="number">1</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>)</span><br><span class="line">a2 = layer_2(a1)</span><br></pre></td></tr></table></figure>
<p>The data type of <code>a1</code> and <code>a2</code> are tensor,which is a built-in type in tensorflow :</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">When print a1: </span><br><span class="line">tf.Tensor([[0.2 0.7 0.3]], shape=(1, 3), dtype=float32)</span><br></pre></td></tr></table></figure>
<p>We can also print it in the form of numpy:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">When print a1.numpy():</span><br><span class="line">array([0.2, 0.7, 0.3], dtype=float32)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>The difference between tensor and array is that tensor has shape and data while array just has data. Therefore, a tensor variable can actually be treated as an <em>image</em>. That is why tensor data can be processed in GPU.</p>
</blockquote>
<p>Instead of building a neural network layer by layer, we can directly concatenate the layers to form the neural network. That is what <code>Sequential</code> do:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> Sequential</span><br><span class="line"></span><br><span class="line">model = Sequential([Dense(units=<span class="number">3</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>), Dense(units=<span class="number">1</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>)])</span><br><span class="line">...</span><br><span class="line">model.predict(x)</span><br></pre></td></tr></table></figure>

<h1 id="Vectorization"><a href="#Vectorization" class="headerlink" title="Vectorization"></a>Vectorization</h1><p>GPU and some CPU functions are very good at doing large matrix multiplications. Neural network can be vectorized, because of which neural network can be processed rapidly.</p>
<p>For layer 1 in the neural network of fig.3, the vectorized version is:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">A = np.array([[<span class="number">200</span>, <span class="number">17</span>]])</span><br><span class="line">W = np.array([[<span class="number">1</span>, -<span class="number">3</span>, <span class="number">5</span>], [-<span class="number">2</span>, <span class="number">4</span>, -<span class="number">6</span>]])</span><br><span class="line">B = np.array([[-<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>]])</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dense</span>(<span class="params">A_in, W, B</span>) :</span><br><span class="line">    Z = np.matmul(A_in, W) + B</span><br><span class="line">    A_out = g(Z)  <span class="comment"># A_out is a row vector</span></span><br><span class="line">    <span class="keyword">return</span> A_out</span><br></pre></td></tr></table></figure>

<h1 id="Activation-function"><a href="#Activation-function" class="headerlink" title="Activation function"></a>Activation function</h1><p>Activation function is actually a reprocessing of the model $f$ and creates <strong>a new model</strong>. By using activation function, we can divide our model into two parts. The first part is <strong>uniform</strong> for all models:<br>$$z&#x3D;\vec{w}\cdot\vec{x}+b$$<br>To generate different models, we only need to select the most suitable activation function $g(z)$. And that is the second part. There are three commonly used activation functions: linear function (identity), Sigmoid (soft step) and ReLU (rectified linear unit).</p>
<h2 id="Linear-function"><a href="#Linear-function" class="headerlink" title="Linear function"></a>Linear function</h2><p><img src="/2023/04/10/NeuralNetwork/5.png" alt="5"></p>
<center style="font-size:12px;font-weight:bold">Fig. 5. Linear function</center><br>

<p>In linear function, we do not do anything to the first part of the model. Therefore, our model is just a linear regression model:<br>$$f&#x3D;g(z)&#x3D;\vec{w}\cdot\vec{x}+b$$<br>Since the linear function of a linear function is still a linear function, we actually do not use linear function in the hidden layer, otherwise, the hidden layer will be useless.</p>
<h2 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a>Sigmoid</h2><p><img src="/2023/04/10/NeuralNetwork/6.png" alt="6"></p>
<center style="font-size:12px;font-weight:bold">Fig. 6. Sigmoid</center><br>

<p>Sigmoid is useful when we the output just has two possible value. So it often be used in the output layer of binary classification.</p>
<h2 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h2><p><img src="/2023/04/10/NeuralNetwork/7.png" alt="7"></p>
<center style="font-size:12px;font-weight:bold">Fig. 7. ReLU</center><br>

<p>ReLU is one of the most commonly used activation function in <strong>hidden layer</strong>. As the slope of it does not change on the negative or positive semi-axis, the convergence speed of ReLU is much faster than Sigmoid. In addition, ReLU makes sense because it has a &quot;off&quot; point which enables neurons to stitch together to form complex non-linear functions:</p>
<p><img src="/2023/04/10/NeuralNetwork/8.png" alt="8"></p>
<center style="font-size:12px;font-weight:bold">Fig. 8. Unit0+Unit1+Unit2</center><br>

<blockquote>
<p>Andrew Ng suggests that for the output layer, we should select the activation function that produces the exact result we need, but for the hidden layer, it is advisable to choose ReLU as out default activation function.</p>
</blockquote>
<h1 id="Softmax-regression"><a href="#Softmax-regression" class="headerlink" title="Softmax regression"></a>Softmax regression</h1><p>Softmax regression or softmax activation function is used to deal with multiclass classification. Multiclass classification is an extension of binary classification. In multiclass classification, the number of output is more than two.</p>
<p><img src="/2023/04/10/NeuralNetwork/9.png" alt="9"></p>
<center style="font-size:12px;font-weight:bold">Fig. 9. Multiclass classification</center><br>

<p>In binary regression, $g(z)$ is actually the possibility of $a&#x3D;&#x3D;1$. We can also get the possibility of $a&#x3D;&#x3D;0$ which is $1-g(z)$. But in multiclass classification, we can not do that. To solve this, softmax calculates the probability of all possible values. We use $z_i$ to represent a possible value and $a_i$ to represent its possibility:<br>$$z_1&#x3D;\vec{w_1}\cdot\vec{x}+b_1;a_1&#x3D;\frac{e^{z_1}}{e^{z_1}+...+e^{z_n}}&#x3D;P(y&#x3D;1)|\vec{x})$$<br>$$...$$<br>$$z_n&#x3D;\vec{w_n}\cdot\vec{x}+b_n;a_n&#x3D;\frac{e^{z_n}}{e^<br>{z_1}+...+e^{z_n}}&#x3D;P(y&#x3D;n|\vec{x})$$<br>And the loss function is:<br>$$L(a_1,...,a_n,y)&#x3D;\begin{cases}<br>-\log{a_1},&amp;y&#x3D;1 \\<br>...&amp; \\<br>-log{a_n},&amp;y&#x3D;n<br>\end{cases}$$</p>
<p>The loss function will make $a_i$ tends to 1 when $y&#x3D;i$.Binary classfication is a special case where n&#x3D;2.</p>
<p>Softmax is a special activation in neural network as it is actually a layer. Its output is a vector whose elements are the possibility of values.</p>
<p><img src="/2023/04/10/NeuralNetwork/10.png" alt="10"></p>
<center style="font-size:12px;font-weight:bold">Fig. 10. Neural network with softmax</center>

<h1 id="Multi-label-classification"><a href="#Multi-label-classification" class="headerlink" title="Multi-label classification"></a>Multi-label classification</h1><p>Multi-label classification is another type of classification. In multi-label classification, we are required to classify a thing into as many labels as we want. To realize this, we just need to use several sigmoid functions in our output layer.</p>
<p><img src="/2023/04/10/NeuralNetwork/11.png" alt="11"></p>
<center style="font-size:12px;font-weight:bold">Fig. 11. Multi-label classification</center>

<h1 id="Adam-algorithm"><a href="#Adam-algorithm" class="headerlink" title="Adam algorithm"></a>Adam algorithm</h1><p>Adam algorithm is optimization of gradient descent, which will automatically modify $\alpha$. In adam algorithm, each neuron of the same layer has different $\alpha$ (the initial value is the same):</p>
<ul>
<li>If $w_j$ or $b$ keeps moving in the same direction, it increases $\alpha_j$;</li>
<li>If $w_j$ or $b$ keeps oscillating, it reduces $\alpha_j$.</li>
</ul>
<p><img src="/2023/04/10/NeuralNetwork/12.png" alt="12"></p>
<center style="font-size:12px;font-weight:bold">Fig. 12. Adam algorithm</center>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zclzcl0223.github.io/2023/04/10/NeuralNetwork/" data-id="clzik1qty006wm07kdtpa9n3d" data-title="Neural Network" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Deep-Learning/" rel="tag">Deep Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-Learning/" rel="tag">Machine Learning</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-SupervisedLearning" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/04/05/SupervisedLearning/" class="article-date">
  <time class="dt-published" datetime="2023-04-05T15:34:48.000Z" itemprop="datePublished">2023-04-05</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Machine-Learning-by-AndrewNg/">Machine Learning by AndrewNg</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/04/05/SupervisedLearning/">Supervised Learning</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <span id="more"></span>

<h1 id="Regression-model"><a href="#Regression-model" class="headerlink" title="Regression model"></a>Regression model</h1><p>The steps in regression model are as follow:</p>
<p><img src="/2023/04/05/SupervisedLearning/1.png" alt="1"></p>
<center style="font-size:12px;font-weight:bold">Fig. 1. Steps of regression model</center><br>

<p>$f$ is the <strong>function</strong> or <strong>model</strong> getting from the learning algorithm, which can be used to predict the output. $\widehat{y}$, the value of $f$, is the prediction of $y$.</p>
<h2 id="Linear-regression-model"><a href="#Linear-regression-model" class="headerlink" title="Linear regression model"></a>Linear regression model</h2><p>Linear regression model is the most simple model in regression model, in which, $f$ is just a linear function:</p>
<p>$${f}_{w,b}(x)&#x3D;wx+b$$</p>
<center>or</center>

<p>$$f(x)&#x3D;wx+b$$</p>
<p>$w$ and $b$ are the <em>parameters</em> that we (the learning algorithms) can adjust to make $f$ more accurate.</p>
<blockquote>
<p>Linear regression with single input is also called <em>univariate linear regression</em>.</p>
</blockquote>
<h3 id="Cost-function"><a href="#Cost-function" class="headerlink" title="Cost function"></a>Cost function</h3><p>Cost function compares $\widehat{y}$ to $y$. <strong>The better the model is, the smaller the value of the cost function is</strong>. The simplest and most commonly used cost function in linear regression is:</p>
<p>$$J_{(w,b)}&#x3D;\frac{1}{2m}\sum\limits_{i&#x3D;1}^{m}(\widehat{y}^{(i)}-y^{(i)})^2$$</p>
<p>which is called <em><strong>mean squared error cost function</strong></em>.</p>
<blockquote>
<p>Why $2m$?</p>
<p><em>This is for the convenience of later calculations. When deriving $J$ using the gradient descent method, if it is $2m$, there will not be any constant in the derivative function</em>:</p>
<p>$$\frac{\partial{J_{(w,b)}}}{\partial{w}}&#x3D;\frac{1}{m}\sum\limits_{i&#x3D;1}^{m}(\widehat{y}^{(i)}-y^{(i)})\frac{\partial{\widehat{y}^{(i)}}}{\partial{w}}$$</p>
</blockquote>
<p>Since the training set is constant, $J$ is just the function of $w$ and $b$. Therefore, <strong>our goal is to find $w$ and $b$ that minimize $J_{(w,b)}$</strong>.</p>
<blockquote>
<p>The 3D bowl-shaped surface plot, which is the plot of $J$, can also be visualized as a contour plot. In the contour plot, each oval contains the choices of $w$ and $b$ that result in the same value of $J$.</p>
</blockquote>
<p><img src="/2023/04/05/SupervisedLearning/2.png" alt="2"></p>
<center style="font-size:12px;font-weight:bold">Fig. 2. 3D plot and contour plot</center>

<h3 id="Gradient-descent"><a href="#Gradient-descent" class="headerlink" title="Gradient descent"></a>Gradient descent</h3><p>Gradient descent is an algorithm that can be used to minimize any function. The steps of gradient descent are:</p>
<ol>
<li><p>Start with some parameters (just set them to 0 is ok);</p>
</li>
<li><p>Keep changing the parameters to reduce $J$:<br>$$w&#x3D;w-\alpha\frac{\partial{J_{(w,b)}}}{\partial{w}}$$<br>$\alpha$: learning rate, (0,1], which is used to control the speed of gradient descent.</p>
<p>$w$: any parameter.</p>
<p>All the parameters should be updated <strong>simultaneously</strong>, which means that when updating one parameter, the value of other parameters should be their original values.</p>
<blockquote>
<p>Principle: The ${grad(f)}$ at a certain point is the direction in which the function changes the fastest.</p>
</blockquote>
</li>
<li><p>Get a minimum or near minimum $J$.</p>
</li>
</ol>
<p>To achieve this, $J$ has to be a <strong>bowl shape function (convex function)</strong> or <strong>a function with local minima</strong>.</p>
<p><img src="/2023/04/05/SupervisedLearning/3.png" alt="3"></p>
<center style="font-size:12px;font-weight:bold">Fig. 3. Function with locak minima</center><br>

<p>In linear regression, <em>mean squared error cost function</em> is always a bowl shape function because it squares the loss. If $\alpha$ is too small, the gradient descent will work but may be very slow.</p>
<p><img src="/2023/04/05/SupervisedLearning/4.png" alt="4"></p>
<center style="font-size:12px;font-weight:bold">Fig. 4. Model with small learning rate</center><br>

<p>If $\alpha$ is too large, the gradient descent may not work, which means it may fail to converge but diverge.</p>
<p><img src="/2023/04/05/SupervisedLearning/5.png" alt="5"></p>
<center style="font-size:12px;font-weight:bold">Fig. 5. Model with large learning rate</center><br>

<blockquote>
<p><em>Batch gradient descent</em>: Each step of gradient descent uses all the training examples.</p>
</blockquote>
<h2 id="Multiple-linear-regression-model"><a href="#Multiple-linear-regression-model" class="headerlink" title="Multiple linear regression model"></a>Multiple linear regression model</h2><p>When there are more than one features determining the output, it is advisable for us to use <em>vector</em>.</p>
<blockquote>
<p>Feature engineering: Using intuition to design new features by <strong>transforming</strong> or <strong>combining</strong> original features. Good features will make the model better.</p>
</blockquote>
<h3 id="Notation"><a href="#Notation" class="headerlink" title="Notation"></a>Notation</h3><ul>
<li>$x_j$ &#x3D; $j^{th}$ feature;</li>
<li>$n$ &#x3D; number of features;</li>
<li>${\vec{x}^{(i)}}$ &#x3D; features of $i^{th}$ training example.</li>
<li>$x_j^{(i)}$ &#x3D; value of feature $j$ in $i^{th}$ training example ($x$ can also be $\vec{x}$).</li>
</ul>
<h3 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h3><p>$$f_{w,b}(x_1,x_2,...,x_n)&#x3D;w_1x_1+w_2x_2+...+w_nx_n+b$$</p>
<p>is equal to:</p>
<p>$$f_{\vec{w},b}(\vec{x})&#x3D;\vec{w}\cdot\vec{x}+b$$</p>
<p>where</p>
<p>$$\vec{w}&#x3D;[w_1,w_2,...,w_n],\vec{x}&#x3D;[x_1,x_2,...,x_n]$$</p>
<h3 id="Vectorization"><a href="#Vectorization" class="headerlink" title="Vectorization"></a>Vectorization</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">w = np.array([<span class="number">1.0</span>, <span class="number">2.5</span>, -<span class="number">3.3</span>])</span><br><span class="line">b = <span class="number">4</span></span><br><span class="line">x = np.array([<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>])</span><br><span class="line"><span class="comment"># vectorization</span></span><br><span class="line">f = np.dot(w, x) + b</span><br></pre></td></tr></table></figure>

<p><code>np.dot()</code> can make use of parallel hardwares, so it is much faster than <code>for loop</code>. (<em>SIMD</em>)</p>
<blockquote>
<p>GPU will help to deal with <strong>vectorized code</strong>.</p>
</blockquote>
<h3 id="Cost-function-1"><a href="#Cost-function-1" class="headerlink" title="Cost function"></a>Cost function</h3><p>The cost function can also be represented as $J(\vec{w},b)$. All the parameters $\vec{w}$ and $b$ should also be updated simultaneously.</p>
<blockquote>
<p>Normal equation: This method only works for linear regression. It sovles for $w,b$ without iterations. However, it does not generalize to other learning algorithms and it is slow when the number of features is too large (&gt;10,000). It may be useful on the backend.</p>
</blockquote>
<h2 id="More-about-gradient-descent"><a href="#More-about-gradient-descent" class="headerlink" title="More about gradient descent"></a>More about gradient descent</h2><h3 id="Feature-scaling"><a href="#Feature-scaling" class="headerlink" title="Feature scaling"></a>Feature scaling</h3><p>Feature scaling is a mothod to accelerate gradient descent. When the value of some features is too large, what may happen is that even though $\alpha$ is small, the changes of some parameters are still <strong>too significant</strong>, which slows down the convergence speed of gradient descent.</p>
<p><img src="/2023/04/05/SupervisedLearning/6.png" alt="6"></p>
<center style="font-size:12px;font-weight:bold">Fig. 6. Unscaled features</center><br>

<p>One useful method is <em>feature scaling</em>. By scaling down some features, we can make the convergence speed of different parameters basically the same.</p>
<p><img src="/2023/04/05/SupervisedLearning/7.png" alt="7"></p>
<center style="font-size:12px;font-weight:bold">Fig. 7. Scaled features</center><br>

<p>Ways to scale features:</p>
<ul>
<li>Dividing by the maximum of the feature;</li>
<li>Mean normalization. This method may produce negative value, but the range of feature is always 1.To realize this:<ol>
<li>Get the mean value of this feature in the training set $\mu$;</li>
<li>Subtract $\mu$ from each example of this feature and divide the result by the difference between the maximum and minimum values of this feature.<br>$$x&#x3D;\frac{x-\mu}{max-min}$$</li>
</ol>
</li>
<li>Z-score normalization. $\sigma$ is the <strong>standard deviation</strong> of this feature.<br>$$x&#x3D;\frac{x-\mu}{\sigma}$$<blockquote>
<p>When predicting, the new $x$ should also be scaled using the same parameters as before.</p>
</blockquote>
</li>
</ul>
<h3 id="Ways-to-check-convergence"><a href="#Ways-to-check-convergence" class="headerlink" title="Ways to check convergence"></a>Ways to check convergence</h3><ol>
<li>Draw $J-iterations$ curve or learning curve.</li>
</ol>
<p><img src="/2023/04/05/SupervisedLearning/8.png" alt="8"></p>
<center style="font-size:12px;font-weight:bold">Fig. 8. Iterations curve</center>

<ol start="2">
<li>Automatic convergence test: Let $\epsilon$ be a small value. Once $J$ decreases by $\leqslant$ $\epsilon$ in one iteration, it converges.</li>
</ol>
<h3 id="Choose-a-better-alpha"><a href="#Choose-a-better-alpha" class="headerlink" title="Choose a better $\alpha$"></a>Choose a better $\alpha$</h3><p>Once the learning curve looks like these:</p>
<p><img src="/2023/04/05/SupervisedLearning/9.png" alt="9"></p>
<center style="font-size:12px;font-weight:bold">Fig. 9. Model with large learning rate</center><br>

<p>It indicates that $\alpha$ is too large. The way to choose a good $\alpha$ is starting with a relative <strong>small value</strong> (like 0.001). And check the astringency of $J$. If $J$ still does not converge, there may be <strong>some bugs</strong> in the code. Then, we can try to increase $\alpha$ by <strong>three times</strong>, until we find a relative large $\alpha$.</p>
<h2 id="Polynomial-regression"><a href="#Polynomial-regression" class="headerlink" title="Polynomial regression"></a>Polynomial regression</h2><p>In polynomial regression, the model $f$ is a polynomial, which means that one feature may occur several times with different powers. For example:</p>
<p>$$f(x)&#x3D;w_1x+w_2\sqrt{x}+b$$</p>
<p>In fact, by changing the power, one feature can produce infinite features. That is, $x$ and $\sqrt{x}$ are two different features.</p>
<blockquote>
<p>Since they are actually different features, we can use another <em>variable</em> to represent them. Then, the function may become: $f(x,z)&#x3D;w_1x+w_2z+b$. It is a linear function formally.</p>
</blockquote>
<p>Compared to linear regression, in polynomial regression, feature scaling and the selection of feature are more important.</p>
<h1 id="Classification-model"><a href="#Classification-model" class="headerlink" title="Classification model"></a>Classification model</h1><h2 id="Binary-classification-model"><a href="#Binary-classification-model" class="headerlink" title="Binary classification model"></a>Binary classification model</h2><p>When the output of classification model only has two possible values, such classification model can also be named <em><strong>binary classification</strong></em>. In binary classification, we always use 0 (false) and 1 (true) to represent the output. 0 is also called <em><strong>negative class</strong></em> and 1 is also called <em><strong>positive class</strong></em>.</p>
<p><strong>Logistic regression</strong> is the most commonly used model in binary classification:</p>
<p>$$f_{(\vec{w},b)}{(\vec{x})}&#x3D;g(\vec{w}\cdot\vec{x}+b)&#x3D;g(z)&#x3D;\frac{1}{1+e^{-(\vec{w}\cdot\vec{x}+b)}}$$</p>
<p>where </p>
<p>$$g(z)&#x3D;\frac{1}{1+e^{-z}},0&lt;g(z)&lt;1$$</p>
<p>is called <em><strong>sigmoid function</strong></em> or <em><strong>logistic function</strong></em>.</p>
<p><img src="/2023/04/05/SupervisedLearning/10.png" alt="10"></p>
<center style="font-size:12px;font-weight:bold">Fig. 10. Sigmoid function</center><br>

<p>Actually, the output of logistic regression can also be regarded as the possibility of <code>y==1</code>, so $f$ can also be represented as:<br>$$f_{(\vec{w},b)}{(\vec{x})}&#x3D;P(y&#x3D;1|\vec{x};\vec{w},b)$$</p>
<h3 id="Decision-boundary"><a href="#Decision-boundary" class="headerlink" title="Decision boundary"></a>Decision boundary</h3><p>Since the output of logistic regression should be either 0 or 1, we must turn the value between 0 and 1 to 0 or 1. That is, we should find a threshold. When $f \ge threshold$,$f&#x3D;1$, otherwise, $f&#x3D;0$. The threshold we choose is often 0.5. When $f \ge 0.5$ ($\widehat{y}&#x3D;1$):<br>$$g(z) \ge 0.5$$<br>$$\downarrow$$<br>$$z \ge 0$$<br>$$\downarrow$$<br>$$\vec{w}\cdot\vec{x}+b \ge 0$$<br>The curve $\vec{w}\cdot\vec{x}+b &#x3D; 0$ is called <em><strong>decision boundary</strong></em>, where $\widehat{y}$ could be 0 or 1. For example:</p>
<p><img src="/2023/04/05/SupervisedLearning/11.png" alt="11"></p>
<center style="font-size:12px;font-weight:bold">Fig. 11. Decision boundary-line</center>

<p><img src="/2023/04/05/SupervisedLearning/12.png" alt="12"></p>
<center style="font-size:12px;font-weight:bold">Fig. 12. Decision boundary-circle</center>

<h3 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h3><p>In linear regression, the cost function:</p>
<p>$$J_{(w,b)}&#x3D;\frac{1}{2m}\sum\limits_{i&#x3D;1}^{m}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})^2$$</p>
<p>can also be represented as:</p>
<p>$$J_{(w,b)}&#x3D;\frac{1}{m}\sum\limits_{i&#x3D;1}^{m}\frac{1}{2}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})^2$$</p>
<p>where:</p>
<p>$$L(f_{\vec{w},b}(\vec{x}^{(i)}),y^{(i)})&#x3D;\frac{1}{2}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})^2$$</p>
<p>is called <em><strong>loss function</strong></em>.</p>
<p>Loss function indicates the error between the prediction value and real value of one example in training set. The convexity of cost function is actually determined by loss function. In logistic regression, <em>squared error loss function</em> is a non-convex function.</p>
<p><img src="/2023/04/05/SupervisedLearning/13.png" alt="13"></p>
<center style="font-size:12px;font-weight:bold">Fig. 13. Non-convex loss function</center><br>

<p>Therefore, a new convex function is needed in logistic regression, that is:<br>$$L(f_{\vec{w},b}(\vec{x}^{(i)}),y^{(i)})&#x3D;-y^{(i)}\log(f_{\vec{w},b}(\vec{x}^{(i)}))-(1-y^{(i)})\log(1-f_{\vec{w},b}(\vec{x}^{(i)}))$$<br>where $y^{(i)}&#x3D;0,1$; $f_{\vec{w},b}(\vec{x}^{(i)})\in(0,1)$; $\log$ uses $\ln$.</p>
<p>When $y^{(i)}&#x3D;1$, the curve of $L$ is:</p>
<p><img src="/2023/04/05/SupervisedLearning/14.png" alt="14"></p>
<center style="font-size:12px;font-weight:bold">Fig. 14.</center><br>

<p>Using gradient descent will make the loss close to 0.</p>
<p>When $y^{(i)}&#x3D;0$,the curve of $L$ is:</p>
<p><img src="/2023/04/05/SupervisedLearning/15.png" alt="15"></p>
<center style="font-size:12px;font-weight:bold">Fig. 15.</center><br>

<p>Using gradient descent will also make the loss close to 0.</p>
<h3 id="Cost-function-2"><a href="#Cost-function-2" class="headerlink" title="Cost function"></a>Cost function</h3><p>$$J_{(w,b)}&#x3D;-\frac{1}{m}\sum\limits_{i&#x3D;1}^{m}[y^{(i)}\log(f_{\vec{w},b}(\vec{x}^{(i)}))+(1-y^{(i)})\log(1-f_{\vec{w},b}(\vec{x}^{(i)}))]$$<br>The derivative of $J$ in logistic regression is actually the same as that in linear regression:<br>$$w_j&#x3D;w_j-\alpha[\frac{1}{m}\sum\limits_{i&#x3D;1}^{m}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})x_j^{(i)}]$$<br>$$b&#x3D;b-\alpha[\frac{1}{m}\sum\limits_{i&#x3D;1}^{m}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})]$$</p>
<p>However, the model $f$ is different.</p>
<blockquote>
<p>Vectorization and feature scaling can also be used in losgistic regression.</p>
</blockquote>
<h2 id="Multiclass-classification-model"><a href="#Multiclass-classification-model" class="headerlink" title="Multiclass classification model"></a>Multiclass classification model</h2><p>See <a href="/2023/04/10/NeuralNetwork/#Softmax-regression">Softmax regression</a>.</p>
<h2 id="Multi-layer-classification-model"><a href="#Multi-layer-classification-model" class="headerlink" title="Multi-layer classification model"></a>Multi-layer classification model</h2><p>See <a href="/2023/04/10/NeuralNetwork/#Multi-label-classification">Multi-label classification</a>.</p>
<h1 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h1><h2 id="Underfitting"><a href="#Underfitting" class="headerlink" title="Underfitting"></a>Underfitting</h2><p>When a model does not fit the training set well, the model is underfitting,</p>
<p><img src="/2023/04/05/SupervisedLearning/16.png" alt="16"></p>
<center style="font-size:12px;font-weight:bold">Fig. 16. Underfitting model</center><br>

<p>An underfitting model is an algorithm having <strong>high bias</strong>. In this case, we need to change a model.</p>
<h2 id="Overfitting"><a href="#Overfitting" class="headerlink" title="Overfitting"></a>Overfitting</h2><p>When a model fits the training set extremely well, that is, the errors are approximate 0, the model is overfitting.</p>
<p><img src="/2023/04/05/SupervisedLearning/17.png" alt="17"></p>
<center style="font-size:12px;font-weight:bold">Fig. 17. Overfitting model</center><br>

<p>An overfitting model is an algorithm having <strong>high variance</strong>. In this case, the model just fits the training set well, but it can not be generalized. This problem often occurs when the training set is too small. So, the first method to solve this problem is to <strong>collect more training examples</strong>. Besides, <strong>selecting more suitable features</strong> is also feasible (<em><strong>Feature Selection</strong></em>). However, this method may cause our model to lose some useful features. The most feasible and commonly used method is <strong>regularization</strong>.</p>
<h2 id="Weight-decay"><a href="#Weight-decay" class="headerlink" title="Weight decay"></a>Weight decay</h2><p>Overfitting may occurs when some features have an overly large effect, that is, a little change of these features may cause large changes to the model, which can make the model unexpandable.</p>
<p>The core idea of regularization is to <strong>reduce the weight of features with large value</strong>. When regularizing, the cost function $J$ becomes:</p>
<p>$$J(\vec{w},b)&#x3D;\frac{1}{m}\sum\limits_{i&#x3D;1}^{m}L(f_{\vec{w},b}(\vec{x}^{(i)}),y^{(i)})+\frac{\lambda}{2m}\sum\limits_{j&#x3D;1}^{n}w_j^2$$</p>
<p>We just need to regularize $w_j$ as $b$ just makes $f$ move up or down.</p>
<blockquote>
<p>$\lambda$ (&gt;0) is an important parameter that balance fitting data and keeping $w_j$ small. In fact, this is <strong>Lagrange multiplier</strong>, a method to find local extremea of a multivariate function when its variables are constrained by one or more conditions. The restriction here is $\frac{1}{2m}\sum\limits_{j&#x3D;1}^{n}w_j^2&#x3D;0$. Since we have limited the value of $\lambda$, our function doesn&#39;t obey the constraint strictly but the constraint does make $w$ smaller and the curve more smooth.</p>
</blockquote>
<p>Since $J$ changes, the update of $w_j$ will also change:</p>
<p>$$w_j&#x3D;w_j-\alpha[\frac{1}{m}\sum\limits_{i&#x3D;1}^{m}\frac{\partial{L(f_{\vec{w},b}(\vec{x}^{(i)}),y^{(i)})}}{\partial{w_j}}+\frac{\lambda}{m}w_j]$$<br>or<br>$$w_j&#x3D;(1-\alpha\frac{\lambda}{m})w_j-\alpha\frac{1}{m}\sum\limits_{i&#x3D;1}^{m}\frac{\partial{L(f_{\vec{w},b}(\vec{x}^{(i)}),y^{(i)})}}{\partial{w_j}}$$</p>
<p>The second formula indicates that in each iteration, $w_j$ will be smaller than before. Therefore, this kind of normalization is also called <strong>weight decay</strong>.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zclzcl0223.github.io/2023/04/05/SupervisedLearning/" data-id="clzik1qu3008fm07k28jzc3lw" data-title="Supervised Learning" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-Learning/" rel="tag">Machine Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Supervised-Learning/" rel="tag">Supervised Learning</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-MachineLearning" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/04/05/MachineLearning/" class="article-date">
  <time class="dt-published" datetime="2023-04-05T11:07:52.000Z" itemprop="datePublished">2023-04-05</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Machine-Learning-by-AndrewNg/">Machine Learning by AndrewNg</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/04/05/MachineLearning/">Machine Learning</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <span id="more"></span>

<h1 id="Machine-Learning"><a href="#Machine-Learning" class="headerlink" title="Machine Learning"></a>Machine Learning</h1><blockquote>
<p>Definition: Field of study that gives computers the ability to learn without being explicitly programmed.</p>
</blockquote>
<p>In general, there are three fields in machine learning, that is, supervised learning, unsupervised learning and reinforcement learning. Their relationships are as follow:</p>
<p><img src="/2023/04/05/MachineLearning/1.png" alt="1"></p>
<center style="font-size:12px;font-weight:bold">Fig. 1. Fields of machine learning</center>

<h2 id="Supervised-learning"><a href="#Supervised-learning" class="headerlink" title="Supervised learning"></a>Supervised learning</h2><p>Supervised learning is the most commonly used machine learning. It&#39;s definition is:</p>
<blockquote>
<p>Supervised learning refers to algorithms that learn input to output mappings. The key of it is that you give the <strong>input(x)</strong> and the <strong>correct output(y)</strong> for the algorithms to learn. Then, the learning algorithms eventually learn to get the input alone and give a reasonably accurate prediction or guess of the output. <strong>In short, supervised learning learns from data labeled with the &quot;right answers&quot;</strong>.</p>
</blockquote>
<p>There are generally two types of supervised learning, <strong>regression</strong> and <strong>classification</strong>.</p>
<h3 id="Regression"><a href="#Regression" class="headerlink" title="Regression"></a>Regression</h3><p>Regression is a kind of supervised learning that predicts <strong>a number</strong>, so its output contains <strong>infinitely</strong> many possible results. For example, housing price prediction:</p>
<p><img src="/2023/04/05/MachineLearning/2.png" alt="2"></p>
<center style="font-size:12px;font-weight:bold">Fig. 2. House price prediction</center>

<h3 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h3><p>Classification is a kind of supervised learning that predicts <strong>categories</strong> which can be <strong>number or non numeric</strong>, so its output just contains a small number of results. For example, breast cancer detection:</p>
<p><img src="/2023/04/05/MachineLearning/3.png" alt="3"></p>
<center style="font-size:12px;font-weight:bold">Fig. 3. Breast cancer detection</center><br>

<p>Both regression and classification can have <strong>more than one</strong> inputs.</p>
<h2 id="Unsupervised-learning"><a href="#Unsupervised-learning" class="headerlink" title="Unsupervised learning"></a>Unsupervised learning</h2><p>Unsupervised learning&#39;s definition:</p>
<blockquote>
<p>Unsupervised learning refers to algorithm that find something interesting in unlabed data. In unsupervised learning,the algorithms figure out all by themselves what&#39;s interesting or what patterns or structures might be in the dataset. <strong>Data only comes with input(x), but not output labels(y)</strong>. Algorithm has to find structure in the data.</p>
</blockquote>
<p>There are generally three types of unsupervised learning, <strong>clustering</strong>, <strong>anomaly detection</strong> and <strong>dimensionality reduction</strong>.</p>
<h3 id="Clustering"><a href="#Clustering" class="headerlink" title="Clustering"></a>Clustering</h3><p>Clustering is a kind of unsupervised learning that <strong>groups</strong> data into different categories by any standard. For example, grouping customers:</p>
<p><img src="/2023/04/05/MachineLearning/4.png" alt="4"></p>
<center style="font-size:12px;font-weight:bold">Fig. 4. Grouping customers</center>

<h3 id="Anomaly-detection"><a href="#Anomaly-detection" class="headerlink" title="Anomaly detection"></a>Anomaly detection</h3><p>Anomaly detection is a kind of unsupervised learning that finds unusual data points. It is very useful in financial system.</p>
<h3 id="Dimensionality-reduction"><a href="#Dimensionality-reduction" class="headerlink" title="Dimensionality reduction"></a>Dimensionality reduction</h3><p>This kind of unsupervised learning compresses data using fewer numbers.</p>
<h2 id="Reinforcement-learning"><a href="#Reinforcement-learning" class="headerlink" title="Reinforcement learning"></a>Reinforcement learning</h2><p>Reinforcement learning is an new field in machine learning that has not yet been well applied but has a promising future.</p>
<h1 id="Tool"><a href="#Tool" class="headerlink" title="Tool"></a>Tool</h1><h2 id="Jupyter-notebook"><a href="#Jupyter-notebook" class="headerlink" title="Jupyter notebook"></a>Jupyter notebook</h2><p>Jupyter notebook is the default <strong>environment</strong> that most researchers use to code up and experiment.</p>
<h2 id="PyTorch"><a href="#PyTorch" class="headerlink" title="PyTorch"></a>PyTorch</h2><p>see <a href="/2023/04/28/PyTorch/">PyTorch</a>.</p>
<h2 id="Tensorflow"><a href="#Tensorflow" class="headerlink" title="Tensorflow"></a>Tensorflow</h2><p>See <a href="/2023/04/12/Tensorflow/">Tensorflow</a>.</p>
<h2 id="NumOy"><a href="#NumOy" class="headerlink" title="NumOy"></a>NumOy</h2><p>See <a href="/2023/04/22/NumPy/">NumPy</a>.</p>
<h2 id="Scikit-learn"><a href="#Scikit-learn" class="headerlink" title="Scikit-learn"></a>Scikit-learn</h2><p>See <a href="/2023/04/22/Scikit-learn/">Scikit-learn</a>.</p>
<h1 id="Terminology"><a href="#Terminology" class="headerlink" title="Terminology"></a>Terminology</h1><ul>
<li>Training set: Data used to train a model.</li>
<li>Test set: Data used to evaluate a model.</li>
</ul>
<h1 id="Notation"><a href="#Notation" class="headerlink" title="Notation"></a>Notation</h1><ul>
<li>${x}$ &#x3D; &quot;input&quot; variable or feature;</li>
<li>${y}$ &#x3D; &quot;output&quot; variable or &quot;target&quot; variable;</li>
<li>${m}$ &#x3D; number of training examples;</li>
<li>$(x, y)$ &#x3D; a single training example;</li>
<li>$(x^i,y^i)$ &#x3D; $i^{th}$ training example;</li>
<li>$x_j$ &#x3D; $j^{th}$ feature;</li>
<li>$n$ &#x3D; number of features;</li>
<li>${\vec{x}^{(i)}}$ &#x3D; features of $i^{th}$ training example.</li>
<li>$x_j^{(i)}$ &#x3D; value of feature $j$ in $i^{th}$ training example ($x$ can also be $\vec{x}$).</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zclzcl0223.github.io/2023/04/05/MachineLearning/" data-id="clzik1qtq004wm07k56fp2nu2" data-title="Machine Learning" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-Learning/" rel="tag">Machine Learning</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-BashScript" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/09/02/BashScript/" class="article-date">
  <time class="dt-published" datetime="2023-09-02T07:45:55.000Z" itemprop="datePublished">2023-09-02</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Programming-Language/">Programming Language</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/09/02/BashScript/">Bash Script</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <span id="more"></span>

<h1 id="Basic-Concept"><a href="#Basic-Concept" class="headerlink" title="Basic Concept"></a>Basic Concept</h1><p>ä¸€äº›å…³äºŽBashå’ŒShellçš„åŸºæœ¬æ¦‚å¿µã€‚</p>
<h2 id="Bash-amp-Shell"><a href="#Bash-amp-Shell" class="headerlink" title="Bash &amp; Shell"></a>Bash &amp; Shell</h2><p>Shellæ˜¯ç”¨æˆ·å’Œæ“ä½œç³»ç»Ÿçš„æŽ¥å£ã€‚å®ƒæ˜¯ä¸€ä¸ªå‘½ä»¤è§£é‡Šå™¨ã€‚Bashï¼ˆBourne Again SHellï¼‰æ˜¯Shellçš„ä¸€ä¸ªç±»åž‹ï¼Œç”±GNUå¼€å‘ï¼Œæ˜¯å¯¹Bourne Shellï¼ˆshï¼‰çš„æ”¹è¿›å’Œæ‰©å±•ï¼ŒåŒæ—¶ä¹Ÿæ˜¯å½“ä»ŠLinuxå’ŒMacOSçš„é»˜è®¤Shellã€‚</p>
<blockquote>
<p>æ›´é€šä¿—åœ°æ¥è¯´ï¼ŒShellç›¸å½“äºŽLinuxæ“ä½œç³»ç»Ÿï¼Œè€ŒBashåˆ™æ˜¯Linuxæ“ä½œç³»ç»Ÿçš„ä¸€ä¸ªå‘è¡Œç‰ˆï¼Œå¦‚Ubuntuã€‚</p>
</blockquote>
<h2 id="Bash-Script-amp-Shell-Script"><a href="#Bash-Script-amp-Shell-Script" class="headerlink" title="Bash Script &amp; Shell Script"></a>Bash Script &amp; Shell Script</h2><p>è„šæœ¬ï¼ˆScriptï¼‰ï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬å¸¸è¯´çš„è§£é‡Šæ€§ç¼–ç¨‹è¯­è¨€ï¼Œå¸¸è§çš„Pythonã€JavaScriptéƒ½æ˜¯è„šæœ¬è¯­è¨€ã€‚Bashè„šæœ¬å’ŒShellè„šæœ¬éƒ½æ˜¯è®©Shellèƒ½å¤Ÿæ‰§è¡Œä¸€è¿žä¸²å‘½ä»¤è¡ŒæŒ‡ä»¤çš„ç¨‹åºï¼Œå®ƒä»¬çš„å…³ç³»å°±ç±»ä¼¼äºŽBashå’ŒShellçš„å…³ç³»ï¼š</p>
<ul>
<li>Bashè„šæœ¬åªèƒ½ç”¨Bashè¯­è¨€ç¼–å†™ï¼Œå…¶è¯­æ³•ä¹Ÿæ˜¯Bashçš„è¯­æ³•ï¼Œè€ŒShellè„šæœ¬åˆ™èƒ½ç”¨ä»»ä½•çš„Shellè¯­æ³•ç¼–å†™ï¼›</li>
<li>Bashè„šæœ¬çš„æ–‡ä»¶åŽç¼€ä¸º<code>.sh</code>ï¼Œè€ŒShellè„šæœ¬å°±æ˜¯ä¸€ä¸ªæ™®é€šçš„æ–‡æœ¬æ–‡ä»¶ï¼Œä¸éœ€è¦ä»»ä½•åŽç¼€ï¼Œåªè¦å¯æ‰§è¡Œå³å¯ã€‚</li>
</ul>
<blockquote>
<p>æ— è®ºæ˜¯Bashè„šæœ¬è¿˜æ˜¯Shellè„šæœ¬ï¼Œéƒ½éœ€è¦ç›¸åº”çš„Shellè§£é‡Šå™¨æ¥æ‰§è¡Œã€‚ä½¿ç”¨<code>cat /etc/shells</code>èƒ½å¤ŸæŸ¥çœ‹æœ¬æœºæ‰€æ‹¥æœ‰çš„Shellè§£é‡Šå™¨ã€‚</p>
</blockquote>
<h1 id="Execution-amp-Comment"><a href="#Execution-amp-Comment" class="headerlink" title="Execution &amp; Comment"></a>Execution &amp; Comment</h1><p>å¯¹äºŽä»»ä½•ä¸€ä¸ªBashè„šæœ¬ï¼Œç¼–å†™æ—¶é¦–å…ˆè¦æŒ‡å®šå…¶è¦ä½¿ç”¨çš„è§£é‡Šå™¨ï¼š</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#! /bin/bash</span></span><br></pre></td></tr></table></figure>

<p>ä¸Šè¿°è¡ŒæŒ‡å®šè¯¥Bashè„šæœ¬å°†ä½¿ç”¨è§£é‡Šå™¨<code>/bin/bash</code>è¿è¡Œã€‚è¿è¡ŒBashè„šæœ¬æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥ä»¥æ–‡ä»¶åè¿è¡Œï¼Œä¹Ÿå¯ä»¥ä»¥<code>bash</code>åŠ æ–‡ä»¶åè¿è¡Œï¼Œå¦‚ä¸€ä¸ªæœ€ç®€å•çš„Bashè„šæœ¬ï¼š</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[zclzcl@localhost Playground]$ <span class="built_in">touch</span> helloScript.sh</span><br><span class="line">[zclzcl@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">helloScript.sh</span><br><span class="line">[zclzcl@localhost Playground]$ vim helloScript.sh</span><br><span class="line"><span class="comment">#! /bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;Goodbye, world&quot;</span></span><br><span class="line">[zclzcl@localhost Playground]$ ./helloScript.sh</span><br><span class="line">Goodbye, world</span><br><span class="line">[zclzcl@localhost Playground]$ bash ./helloScript.sh</span><br><span class="line">Goodbye, world</span><br></pre></td></tr></table></figure>

<blockquote>
<p>æœ€ç®€å•çš„Bashè„šæœ¬å°±æ˜¯ä¸€ç³»åˆ—Linuxå‘½ä»¤è¡ŒæŒ‡ä»¤çš„ç»„åˆã€‚æ­¤æ—¶ï¼Œè¿è¡Œè¯¥Bashè„šæœ¬ç›¸å½“äºŽåœ¨å½“å‰ç›®å½•ä¸‹ä¸²è¡Œåœ°è¿è¡Œè¿™äº›æŒ‡ä»¤ã€‚è¦æƒ³è¿è¡ŒBashè„šæœ¬ï¼Œé¦–å…ˆè¦ç¡®ä¿è„šæœ¬æ–‡ä»¶æ˜¯å¯æ‰§è¡Œçš„ï¼Œè‹¥ç”¨æˆ·æ— æ‰§è¡Œè„šæœ¬æ–‡ä»¶çš„æƒé™ï¼Œåˆ™éœ€è¦ç”¨<code>chmod +x [Script File]</code>å¢žåŠ æƒé™ã€‚</p>
</blockquote>
<p>ä¸Žå…¶ä»–ç¼–ç¨‹è¯­è¨€ä¸€æ ·ï¼Œæˆ‘ä»¬ä¹Ÿèƒ½åœ¨è„šæœ¬æ–‡ä»¶ä¸­å¢žåŠ æ³¨é‡Šæ¥å¢žå¼ºè„šæœ¬æ–‡ä»¶çš„å¯è¯»æ€§ã€‚åœ¨Bashè„šæœ¬ä¸­ï¼Œå•è¡Œæ³¨é‡Šç”¨<code>#</code>ï¼Œå¤šè¡Œæ³¨é‡Šç”¨<code>: &#39;&#39;</code>ï¼ˆå¼•å·å†…æ˜¯æ³¨é‡Šå†…å®¹ï¼‰ã€‚</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zclzcl0223.github.io/2023/09/02/BashScript/" data-id="clzik1qt10009m07k21yy1cih" data-title="Bash Script" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Linux/" rel="tag">Linux</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Tool/" rel="tag">Tool</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-RegularExpressions" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/08/04/RegularExpressions/" class="article-date">
  <time class="dt-published" datetime="2023-08-04T06:38:55.000Z" itemprop="datePublished">2023-08-04</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Tool/">Tool</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/08/04/RegularExpressions/">Regular Expressions</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <span id="more"></span>

<h1 id="åŸºæœ¬æ ¼å¼ä¸ŽåŒ¹é…èŒƒå›´"><a href="#åŸºæœ¬æ ¼å¼ä¸ŽåŒ¹é…èŒƒå›´" class="headerlink" title="åŸºæœ¬æ ¼å¼ä¸ŽåŒ¹é…èŒƒå›´"></a>åŸºæœ¬æ ¼å¼ä¸ŽåŒ¹é…èŒƒå›´</h1><p>åŸºæœ¬æ ¼å¼ä¸º<code>/[regx]/[g\i\m\s\u\y]</code>ã€‚å…¶ä¸­<code>[g\i\m\s\u\y]</code>æ¯ä¸ªå­—æ¯è¡¨ç¤ºä¸€ç§åŒ¹é…èŒƒå›´ï¼Œå¯åŒæ—¶å‡ºçŽ°ï¼š</p>
<p><code>/g</code>ï¼šè¾“å‡ºæ‰€æœ‰åŒ¹é…ç»“æžœï¼›<br><code>/i</code>ï¼šä¸åŒºåˆ†å¤§å°å†™ï¼›<br><code>/m</code>ï¼š<code>^</code>æˆ–<code>$</code>å®šä½ç¬¦åœ¨åœºæ—¶æ‰å‘æŒ¥ä½œç”¨ï¼›<br><code>/</code>ï¼šåªè¾“å‡ºç¬¬ä¸€ä¸ªåŒ¹é…ç»“æžœã€‚</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://regexr.com/">Regexr Website</a>æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„ç»ƒä¹ æ­£åˆ™è¡¨è¾¾å¼çš„ç½‘ç«™ï¼Œå®ƒçš„ç•Œé¢ç®€æ´ï¼šä¸Šæ–¹æ˜¯æ­£åˆ™è¡¨è¾¾å¼ï¼Œä¸­é—´æ˜¯å¾…åŒ¹é…æ–‡æœ¬ï¼Œä¸‹æ–¹åˆ™ä¼šè¯¦ç»†ç»™å‡ºæ¯ä¸ªè¡¨è¾¾å¼çš„åŒ¹é…ç»†èŠ‚åŒæ—¶è¿˜æä¾›äº†ä¸€äº›å®žç”¨å·¥å…·ã€‚</p>
</blockquote>
<p><img src="/2023/08/04/RegularExpressions/1.png" alt="1"></p>
<center style="font-size:12px; font-weight:bold">Fig. 1. Regexr Website</center>

<hr>
<h1 id="è¿ç®—è§„åˆ™ä¸Žä¼˜å…ˆçº§"><a href="#è¿ç®—è§„åˆ™ä¸Žä¼˜å…ˆçº§" class="headerlink" title="è¿ç®—è§„åˆ™ä¸Žä¼˜å…ˆçº§"></a>è¿ç®—è§„åˆ™ä¸Žä¼˜å…ˆçº§</h1><p>æ‰€æœ‰çš„æ­£åˆ™è¡¨è¾¾å¼ï¼Œæ— è®ºæ˜¯åŸºæœ¬è¿ç®—ç¬¦è¿˜æ˜¯å¤æ‚è¿ç®—ç¬¦ï¼Œéƒ½éµå¾ªä¼˜å…ˆçº§ä»Žå·¦åˆ°å³åŒ¹é…ã€‚åŸºæœ¬çš„ä¼˜å…ˆçº§ä»Žé«˜åˆ°ä½Žé¡ºåºä¸ºï¼š</p>
<table>
<thead>
<tr>
<th align="center">è¿ç®—ç¬¦</th>
<th align="center">æè¿°</th>
</tr>
</thead>
<tbody><tr>
<td align="center">\\</td>
<td align="center">è½¬ä¹‰å­—ç¬¦</td>
</tr>
<tr>
<td align="center">(), []</td>
<td align="center">åœ†æ‹¬å·ï¼Œæ–¹æ‹¬å·</td>
</tr>
<tr>
<td align="center">*, +, ?, {}</td>
<td align="center">é™å®šç¬¦</td>
</tr>
<tr>
<td align="center">^, $, \wç­‰å…ƒå­—ç¬¦, ä»»ä½•å­—ç¬¦</td>
<td align="center">å®šä½ç‚¹å’Œå­—ç¬¦</td>
</tr>
<tr>
<td align="center">|</td>
<td align="center">é€»è¾‘è¿ç®—ç¬¦ï¼Œæˆ–</td>
</tr>
</tbody></table>
<p>å¯¹äºŽå¾…åŒ¹é…å­—ç¬¦ä¸²ï¼Œä¹Ÿéµå¾ªä»Žå·¦åˆ°å³ã€ä»Žä¸Šåˆ°ä¸‹çš„åŒ¹é…é¡ºåºã€‚å› æ­¤ï¼Œä¸€æ—¦æŸå­ä¸²ç¬¦åˆæ­£åˆ™è¡¨ç¤ºå¼çš„åŒ¹é…è¦æ±‚ï¼Œè¯¥å­ä¸²ä¾¿ä¼šç«‹å³è¢«â€œæ‘˜å‡ºâ€å­—ç¬¦ä¸²ï¼Œä¸å†å‚ä¸ŽåŽç»­çš„åŒ¹é…ã€‚</p>
<hr>
<h1 id="ç®€å•çš„è¿ç®—ç¬¦"><a href="#ç®€å•çš„è¿ç®—ç¬¦" class="headerlink" title="ç®€å•çš„è¿ç®—ç¬¦"></a>ç®€å•çš„è¿ç®—ç¬¦</h1><ul>
<li><code>+</code>ï¼šå¯¹<code>+</code>çš„å‰ä¸€ä¸ªå­—ç¬¦åŒ¹é…1ä¸ªä»¥ä¸Šï¼›</li>
<li><code>?</code>ï¼šå¯¹<code>?</code>çš„å‰ä¸€ä¸ªå­—ç¬¦åŒ¹é…0æˆ–1ä¸ªï¼›</li>
<li><code>*</code>ï¼šå¯¹<code>*</code>çš„å‰ä¸€ä¸ªå­—ç¬¦åŒ¹é…0æˆ–å¤šä¸ªï¼›</li>
<li><code>.</code>ï¼šåŒ¹é…ä»»ä½•1ä¸ªå­—ç¬¦ï¼ˆé™¤æ¢è¡Œç¬¦ï¼‰ï¼›</li>
<li><code>\</code>ï¼šè½¬ä¹‰å­—ç¬¦ï¼Œä½¿è¿ç®—ç¬¦å¤±åŽ»æ­£åˆ™æ„ä¹‰ï¼›</li>
<li><code>\d</code>ï¼šåŒ¹é…æ•°å­—ï¼›</li>
<li><code>\w</code>ï¼šåŒ¹é…å•è¯ï¼›</li>
<li><code>\s</code>ï¼šåŒ¹é…ç©ºç™½ç¬¦ï¼ŒåŒ…æ‹¬spaceã€tabã€line breakã€‚</li>
</ul>
<blockquote>
<p><code>\w</code>ç­‰çš„å¤§å†™è¡¨ç¤ºåŒ¹é…ä¸ŽåŽŸå°å†™ç›¸åçš„é¡¹ç›®ï¼Œå¦‚<code>\W</code>åŒ¹é…æ‰€æœ‰çš„éžå•è¯ï¼Œå®žé™…ç›¸å½“äºŽå¯¹å°å†™çš„åŒ¹é…ç»“æžœå–éžã€‚</p>
</blockquote>
<hr>
<h1 id="è¾ƒå¤æ‚çš„è¿ç®—ç¬¦"><a href="#è¾ƒå¤æ‚çš„è¿ç®—ç¬¦" class="headerlink" title="è¾ƒå¤æ‚çš„è¿ç®—ç¬¦"></a>è¾ƒå¤æ‚çš„è¿ç®—ç¬¦</h1><h2 id="æ‹¬å·ç±»"><a href="#æ‹¬å·ç±»" class="headerlink" title="æ‹¬å·ç±»"></a>æ‹¬å·ç±»</h2><ul>
<li><code>&#123;[minLength],[maxLength]&#125;</code>ï¼šå¯¹<code>&#123;[minLength],[maxLength]&#125;</code>å‰çš„æ‰€æœ‰åŒ¹é…é¡¹ï¼Œé™åˆ¶æ¯ä¸ªåŒ¹é…é¡¹çš„é•¿åº¦ä¸º<code>minLength</code>-<code>maxLength</code>ï¼Œå…¶ä¸­åŒ¹é…é¡¹é•¿åº¦å°äºŽ<code>[minLength]</code>çš„å°†ä¼šè¢«å‰”é™¤ï¼Œè€Œé•¿åº¦å¤§äºŽ<code>[maxLength]</code>çš„åˆ™ä¼šåªå–<code>[maxLength]</code>éƒ¨åˆ†ã€‚å¦‚<code>&#123;4,5&#125;</code>åªä¼šåŒ¹é…å…¶ä¹‹å‰çš„åŒ¹é…é¡¹ä¸­æ‰€æœ‰é•¿åº¦å¤§äºŽ4çš„ï¼Œå¹¶å¯¹å¤§äºŽ5çš„é™å®šé•¿åº¦ä¸º5ï¼Œé™å®šé•¿åº¦åŽçš„å‰©ä¸‹éƒ¨åˆ†è‹¥ä»ç¬¦åˆå‰é¢çš„åŒ¹é…è¦æ±‚ï¼Œåˆ™å½“ä½œæ–°çš„åŒ¹é…é¡¹ç»§ç»­å‚ä¸Ž<code>&#123;[minLength],[maxLength]&#125;</code>ã€‚è‹¥åªæœ‰<code>&#123;[minLength]&#125;</code>åˆ™é»˜è®¤<code>[maxLength]=[minLength]</code>ï¼›è‹¥æ˜¯<code>&#123;[minLength],&#125;</code>åˆ™å°†åŒ¹é…æ‰€æœ‰é•¿åº¦å¤§äºŽ<code>[minLength]</code>çš„ï¼›</li>
<li><code>[[Groups]]</code>ï¼šåŒ¹é…ä»»ä½•ä¸€ä¸ª<code>[[Groups]]</code>ä¸­çš„å­—ç¬¦ã€‚è¿™å¾ˆåƒé€šé…ç¬¦ä¸­<code>[]</code>çš„ç”¨æ³•ï¼Œä½†æ˜¯ä¸åŒçš„æ˜¯ï¼Œæ­£åˆ™è¡¨è¾¾å¼ä¸­æ²¡æœ‰ç»„çš„æ¦‚å¿µï¼Œå› æ­¤<code>[Groups]</code>åªèƒ½æ˜¯ç”¨æˆ·è‡ªå®šçš„ä¸€ç»„å¤‡é€‰çš„å­—ç¬¦æˆ–åºåˆ—ï¼Œå¦‚<code>[abg]</code>å°†åŒ¹é…<code>a</code>æˆ–<code>b</code>æˆ–<code>g</code>ï¼›<code>a-f</code>å°†åŒ¹é…<code>a</code>åˆ°<code>f</code>çš„ä»»æ„ä¸€ä¸ªå­—ç¬¦ï¼›<code>1-9</code>å°†åŒ¹é…<code>1</code>åˆ°<code>9</code>çš„ä»»æ„ä¸€ä¸ªå­—ç¬¦ã€‚<code>[]</code>æ”¯æŒç»„åˆä½¿ç”¨ï¼Œå¦‚<code>a-f1-9</code>å°†åŒ¹é…<code>a</code>åˆ°<code>f</code>æˆ–<code>1</code>åˆ°<code>9</code>çš„ä»»æ„ä¸€ä¸ªå­—ç¬¦ï¼›</li>
<li><code>()</code>ï¼š<code>()</code>è¡¨å°†é‡Œé¢çš„å†…å®¹è§†ä½œä¸€ä¸ªæ•´ä½“ï¼Œå³é‡Œé¢çš„å†…å®¹åœ¨åŒ¹é…æ—¶å®žé™…ä¸Šæ˜¯ä¸€ä¸ªå­—ç¬¦ï¼Œå¦‚<code>(re)&#123;2,3&#125;</code>å°†åŒ¹é…æ‰€æœ‰çš„<code>re</code>é‡å¤è¶…è¿‡2æ¬¡çš„è¿žç»­ä¸²ä¸­çš„<code>rere</code>æˆ–<code>rerere</code>éƒ¨åˆ†ï¼Œæ­¤æ—¶<code>re</code>æ•´ä½“è¢«è§†ä¸ºä¸€ä¸ªå­—ç¬¦ï¼›</li>
</ul>
<h2 id="é€»è¾‘è¿ç®—ç¬¦ä¸Žå®šä½ç¬¦"><a href="#é€»è¾‘è¿ç®—ç¬¦ä¸Žå®šä½ç¬¦" class="headerlink" title="é€»è¾‘è¿ç®—ç¬¦ä¸Žå®šä½ç¬¦"></a>é€»è¾‘è¿ç®—ç¬¦ä¸Žå®šä½ç¬¦</h2><ul>
<li><code>|</code>ï¼šé€»è¾‘è¿ç®—ç¬¦<code>or</code>ï¼ŒåŒ¹é…ç»“æžœæ˜¯ç¬¦åˆ<code>|</code>å·¦è¾¹æˆ–å³è¾¹å†…å®¹çš„å­—ç¬¦ä¸²ã€‚è‹¥æ— æ‹¬å·å°†æ•´ä¸ª<code>|</code>è¡¨è¾¾å¼æ‹¬èµ·ï¼Œ<code>|</code>ä¼šé»˜è®¤å°†å·¦è¾¹å’Œå³è¾¹çš„è§†ä¸ºä¸€ä¸ªæ•´ä½“ï¼Œå› ä¸º<code>|</code>çš„ä¼˜å…ˆçº§æœ€ä½Žï¼Œå¦‚<code>tre|it</code>åªä¼šåŒ¹é…<code>tre</code>æˆ–<code>it</code>ï¼Œè€Œ<code>t(re|it)</code>åˆ™ä¼šåŒ¹é…<code>&#39;tre</code>æˆ–<code>tit</code>ï¼›</li>
<li><code>^</code>ï¼šéœ€æ”¾åœ¨æ­£åˆ™è¡¨è¾¾å¼çš„å¼€å¤´æ‰èµ·ä½œç”¨ï¼Œè¡¨ç¤ºåŒ¹é…å­—ç¬¦ä¸²çš„å¼€å¤´ï¼Œä¹Ÿå°±æ˜¯è¯´<code>^</code>åŽé¢çš„è¡¨è¾¾å¼åªæœ‰å‡ºçŽ°åœ¨å­—ç¬¦ä¸²çš„å¼€å¤´æ‰ä¼šè¢«åŒ¹é…ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œæ•´ç¯‡æ–‡ç« ä¼šè¢«è§†ä½œä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œä½†æ˜¯è‹¥ä½¿ç”¨<code>/m</code>é€‰é¡¹ï¼Œåˆ™æ¯ä¸€è¡Œä¼šè¢«è§†ä½œä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œæ­¤æ—¶<code>^</code>çš„æ„ä¹‰å°±æ˜¯åŒ¹é…æ¯ä¸€è¡Œçš„å¼€å¤´ï¼›</li>
<li><code>$</code>ï¼šéœ€æ”¾åœ¨æ­£åˆ™è¡¨è¾¾å¼çš„æœ€åŽæ‰èµ·ä½œç”¨ï¼Œç±»ä¼¼äºŽ<code>^</code>ï¼Œè¡¨åŒ¹é…æ–‡ç« æˆ–è¡Œçš„æœ«å°¾ï¼›</li>
<li><code>?&lt;=</code>ï¼šå‘å‰çœ‹ï¼Œä¸€èˆ¬è€Œè¨€ï¼Œè¿™ä¸ªè¡¨è¾¾å¼å¿…é¡»å’Œæ‹¬å·<code>()</code>ä¸€èµ·ä½¿ç”¨ã€‚å®ƒæ˜¯<code>^</code>çš„æ‹“å±•ç‰ˆï¼Œè¡¨ç¤ºä»Žä»¥æ‹¬å·å†…çš„è¡¨è¾¾å¼å¼€å¤´çš„éƒ¨åˆ†å¼€å§‹åŒ¹é…ï¼Œä½†æ˜¯ä¸åŒ¹é…æ‹¬å·å†…çš„å†…å®¹ï¼Œå¦‚<code>(?&lt;=[tT]he).</code>å°†åŒ¹é…<code>the</code>æˆ–<code>The</code>åŽé¢çš„ä»»æ„ä¸€ä¸ªå­—ç¬¦ï¼›</li>
<li><code>?&lt;!</code>ï¼š<code>?&lt;=</code>çš„å–åï¼Œå¦‚<code>(?&lt;![tT]he).</code>å°†åŒ¹é…é™¤<code>(?&lt;=[tT]he).</code>åŒ¹é…ç»“æžœçš„æ‰€æœ‰å­—ç¬¦ï¼›</li>
<li><code>?=</code>ï¼šå‘åŽçœ‹ï¼Œ<code>?&lt;=</code>çš„å€’è£…ç‰ˆï¼Œæ˜¯<code>$</code>çš„æ‹“å±•ï¼Œè¡¨ç¤ºä»Žä»¥æ‹¬å·å†…çš„è¡¨è¾¾å¼ç»“å°¾çš„éƒ¨åˆ†å¼€å§‹åŒ¹é…ï¼Œå¦‚<code>.(?=[tT]he)</code>å°†åŒ¹é…<code>the</code>æˆ–<code>The</code>å‰é¢çš„ä»»æ„ä¸€ä¸ªå­—ç¬¦ï¼›</li>
<li><code>?!</code>ï¼š<code>?=</code>çš„å–åã€‚</li>
</ul>
<blockquote>
<p>éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå¹¶ä¸æ˜¯æ‰€æœ‰çš„æ­£åˆ™è¡¨è¾¾å¼æ ‡å‡†éƒ½åŒ…å«äº†<code>?&lt;=</code>å’Œ<code>?&lt;!</code>ã€‚</p>
</blockquote>
<hr>
<h1 id="å®žæˆ˜-ç”µè¯å·ç åŒ¹é…"><a href="#å®žæˆ˜-ç”µè¯å·ç åŒ¹é…" class="headerlink" title="å®žæˆ˜-ç”µè¯å·ç åŒ¹é…"></a>å®žæˆ˜-ç”µè¯å·ç åŒ¹é…</h1><p>å¯¹äºŽä¸€èˆ¬çš„11ä½ç”µè¯å·ç ï¼Œå…¶æœ€å¸¸è§„çš„å†™æ³•æœ‰å¦‚ä¸‹è¿™ä¸‰ç§ï¼š</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1234567890</span><br><span class="line">123-456-7890</span><br><span class="line">123 456 7890</span><br></pre></td></tr></table></figure>

<p>ä¸éš¾çœ‹å‡ºï¼Œä¸Šé¢çš„å†™æ³•éƒ½å¯ä»¥ç»Ÿä¸€æˆ334å†™æ³•ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥å…ˆæŒ‰334å°†ç”µè¯å·ç åˆ†ç»„ï¼š</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/(\d&#123;3&#125;)(\d&#123;3&#125;)(\d&#123;4&#125;)/g</span><br></pre></td></tr></table></figure>

<p>ä¸‰è€…çš„åŒºåˆ«å°±åœ¨äºŽ<code> </code>ã€<code>-</code>ä»¥åŠæ— å­—ç¬¦ï¼Œå¯¹äºŽ<code> </code>å’Œ<code>-</code>ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸­æ‹¬å·è¡¨ä»»é€‰å…¶ä¸€<code>[ -]</code>ï¼Œè€Œå¯¹äºŽæ— å­—ç¬¦åˆ™å¯ä»¥é‡‡ç”¨<code>?</code>æ¥åŒ¹é…å‰æ–¹çš„0æˆ–1ä¸ªå­—ç¬¦ï¼š</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/(\d&#123;3&#125;)[ -]?(\d&#123;3&#125;)[ -]?(\d&#123;4&#125;)/g</span><br></pre></td></tr></table></figure>

<p>ä¸Šè¿°å¼å­å·²ç»å¯ä»¥åŒ¹é…æ‰€æœ‰çš„è¿™3ç§å†™æ³•ã€‚æœ‰æ—¶ï¼Œè¿˜ä¼šå‡ºçŽ°åŠ äº†å›½é™…åŒºå·å’Œä½¿ç”¨æ‹¬å·çš„å†™æ³•ï¼Œå¦‚ï¼š</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(123) 456-7890</span><br><span class="line">+1 123 456 7890</span><br></pre></td></tr></table></figure>

<p>å¯¹äºŽåŠ æ‹¬å·çš„ï¼Œç”¨<code>\(?</code>å’Œ<code>\)?</code>å¤„ç†å³å¯ã€‚å¯¹äºŽå¸¦åŒºå·çš„ï¼Œåˆ™å¯ä»¥<code>(\+\d )?</code>å¤„ç†ã€‚æ‰€æœ‰çš„åˆèµ·æ¥å°±æ˜¯ï¼š</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/(\+\d )?(\(?\d&#123;3&#125;\)?)[ -]?(\d&#123;3&#125;)[ -]?(\d&#123;4&#125;)/g</span><br></pre></td></tr></table></figure>

<h1 id="å‚è€ƒ"><a href="#å‚è€ƒ" class="headerlink" title="å‚è€ƒ"></a>å‚è€ƒ</h1><ul>
<li><a target="_blank" rel="noopener" href="https://youtu.be/rhzKDrUiJVk">Learn Regular Expressions In 20 Minutes</a></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zclzcl0223.github.io/2023/08/04/RegularExpressions/" data-id="clzik1qu20088m07kaedh3j09" data-title="Regular Expressions" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Linux/" rel="tag">Linux</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Tool/" rel="tag">Tool</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-WildcardsPatternMatching" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/08/03/WildcardsPatternMatching/" class="article-date">
  <time class="dt-published" datetime="2023-08-03T10:08:31.000Z" itemprop="datePublished">2023-08-03</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Tool/">Tool</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/08/03/WildcardsPatternMatching/">Wildcards and Pattern Matching</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <span id="more"></span>

<h1 id="ï¼šåŒ¹é…ä»»æ„ä¸ªæ•°çš„ä»»æ„å­—ç¬¦"><a href="#ï¼šåŒ¹é…ä»»æ„ä¸ªæ•°çš„ä»»æ„å­—ç¬¦" class="headerlink" title="*ï¼šåŒ¹é…ä»»æ„ä¸ªæ•°çš„ä»»æ„å­—ç¬¦"></a><code>*</code>ï¼šåŒ¹é…ä»»æ„ä¸ªæ•°çš„ä»»æ„å­—ç¬¦</h1><p><code>*</code>å¯ä»¥ä»£è¡¨ä»»æ„ä¸ªæ•°çš„ä»»æ„çš„è¿žç»­å­—ç¬¦ï¼š</p>
<ul>
<li><code>*.txt</code>ï¼šåŒ¹é…æ‰€æœ‰ä»¥<code>.txt</code>ç»“å°¾çš„å­—ç¬¦ä¸²ï¼›</li>
<li><code>g*</code>ï¼šåŒ¹é…æ‰€æœ‰ä»¥<code>g</code>å¼€å¤´çš„å­—ç¬¦ä¸²ï¼›</li>
<li><code>g*.txt</code>ï¼šåŒ¹é…æ‰€æœ‰ä»¥<code>g</code>å¼€å¤´ã€<code>.txt</code>ç»“å°¾çš„å­—ç¬¦ä¸²ã€‚</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[zclzcl@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">file1.txt  file2.txt  gd  gg  ground  ground.txt  g.txt</span><br><span class="line">[zclzcl@localhost Playground]$ <span class="built_in">ls</span> *.txt</span><br><span class="line">file1.txt  file2.txt  ground.txt  g.txt</span><br><span class="line">[zclzcl@localhost Playground]$ <span class="built_in">ls</span> g*</span><br><span class="line">gd  gg  ground  ground.txt  g.txt</span><br><span class="line">[zclzcl@localhost Playground]$ <span class="built_in">ls</span> g*.txt</span><br><span class="line">ground.txt  g.txt</span><br></pre></td></tr></table></figure>

<h1 id="ï¼šåŒ¹é…ä¸€ä¸ªå­—ç¬¦"><a href="#ï¼šåŒ¹é…ä¸€ä¸ªå­—ç¬¦" class="headerlink" title="?ï¼šåŒ¹é…ä¸€ä¸ªå­—ç¬¦"></a><code>?</code>ï¼šåŒ¹é…ä¸€ä¸ªå­—ç¬¦</h1><p><code>?</code>å¯ä»¥ä»£è¡¨ä»»æ„çš„1ä¸ªå­—ç¬¦ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œ<code>?</code>å¿…é¡»ä»£è¡¨1ä¸ªå­—ç¬¦ï¼Œä¹Ÿå°±æ˜¯è¯´æ—¢ä¸èƒ½æ˜¯0ä¸ªï¼Œä¹Ÿä¸èƒ½å¤šäºŽ1ä¸ªï¼š</p>
<ul>
<li><code>?.txt</code>ï¼šåŒ¹é…æ‰€æœ‰ä»¥<code>.txt</code>ç»“å°¾ä¸”<code>.txt</code>ä¹‹å‰åªæœ‰ä¸€ä¸ªå­—ç¬¦çš„å­—ç¬¦ä¸²ï¼›</li>
<li><code>g?</code>ï¼šåŒ¹é…æ‰€æœ‰ä»¥<code>g</code>å¼€å¤´ä¸”åªæœ‰ä¸¤ä¸ªå­—ç¬¦çš„å­—ç¬¦ä¸²ï¼›</li>
<li><code>g?.txt</code>ï¼šåŒ¹é…æ‰€æœ‰ä»¥<code>g</code>å¼€å¤´ã€<code>.txt</code>ç»“å°¾ä¸”<code>g</code>å’Œ<code>.txt</code>ä¹‹é—´åªæœ‰ä¸€ä¸ªå­—ç¬¦çš„å­—ç¬¦ä¸²ã€‚</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[zclzcl@localhost Playground]$ <span class="built_in">ls</span> ?.txt</span><br><span class="line">g.txt</span><br><span class="line">[zclzcl@localhost Playground]$ <span class="built_in">ls</span> g?</span><br><span class="line">gd  gg</span><br><span class="line">[zclzcl@localhost Playground]$ <span class="built_in">ls</span> g?.txt</span><br><span class="line"><span class="built_in">ls</span>: cannot access g?.txt: No such file or directory</span><br></pre></td></tr></table></figure>

<h1 id="ï¼šåŒ¹é…æ–¹æ¡†ä¸­çš„ä»»æ„ä¸€ä¸ªå­—ç¬¦"><a href="#ï¼šåŒ¹é…æ–¹æ¡†ä¸­çš„ä»»æ„ä¸€ä¸ªå­—ç¬¦" class="headerlink" title="[]ï¼šåŒ¹é…æ–¹æ¡†ä¸­çš„ä»»æ„ä¸€ä¸ªå­—ç¬¦"></a><code>[]</code>ï¼šåŒ¹é…æ–¹æ¡†ä¸­çš„ä»»æ„ä¸€ä¸ªå­—ç¬¦</h1><p><code>[]</code>ä¼šåŒ¹é…å†…éƒ¨çš„ä»»æ„ä¸€ä¸ªä¸”å¿…é¡»æ˜¯ä¸€ä¸ªå­—ç¬¦ï¼Œæ¯”å¦‚ï¼š</p>
<ul>
<li>ä¸Ž<code>g[dg]</code>ç›¸åŒ¹é…çš„åªæœ‰<code>gd</code>ä¸Ž<code>gg</code>ã€‚</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[zclzcl@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">file1.txt  file2.txt  gd  gg  ground  ground.txt  g.txt</span><br><span class="line">[zclzcl@localhost Playground]$ <span class="built_in">ls</span> g[dg]</span><br><span class="line">gd  gg</span><br><span class="line">[zclzcl@localhost Playground]$ <span class="built_in">ls</span> g[rd]*</span><br><span class="line">gd  ground  ground.txt</span><br></pre></td></tr></table></figure>

<h1 id="ï¼šåŒ¹é…ä»»æ„ä¸åœ¨æ–¹æ¡†ä¸­çš„ä¸€ä¸ªå­—ç¬¦"><a href="#ï¼šåŒ¹é…ä»»æ„ä¸åœ¨æ–¹æ¡†ä¸­çš„ä¸€ä¸ªå­—ç¬¦" class="headerlink" title="[^]ï¼šåŒ¹é…ä»»æ„ä¸åœ¨æ–¹æ¡†ä¸­çš„ä¸€ä¸ªå­—ç¬¦"></a><code>[^]</code>ï¼šåŒ¹é…ä»»æ„ä¸åœ¨æ–¹æ¡†ä¸­çš„ä¸€ä¸ªå­—ç¬¦</h1><p><code>[^]</code>è¡¨æ˜Žå…¶æ‰€åœ¨ä½ç½®ï¼ˆåªå ä¸€ä½ï¼‰çš„åŒ¹é…ç»“æžœè¦ä¸åŒ…å«æ–¹æ¡†å†…çš„å­—ç¬¦ï¼Œæ¯”å¦‚ï¼š</p>
<ul>
<li>ä¸Ž<code>g[^dg]</code>ç›¸åŒ¹é…çš„æ˜¯ä»»ä½•ç¬¬äºŒä¸ªå­—ç¬¦ä¸æ˜¯<code>d</code>æˆ–<code>g</code>çš„ä»¥<code>g</code>å¼€å¤´çš„ä¸¤å­—ç¬¦å­—ç¬¦ä¸²ã€‚</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[zclzcl@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">file1.txt  file2.txt  gd  gg  ground  ground.txt  g.txt</span><br><span class="line">[zclzcl@localhost Playground]$ <span class="built_in">ls</span> g[^dg]*</span><br><span class="line">ground  ground.txt  g.txt</span><br></pre></td></tr></table></figure>

<h1 id="start-end-ï¼šåŒ¹é…-start-å’Œ-end-ä¹‹é—´çš„ä»»æ„ä¸€ä¸ªå­—ç¬¦"><a href="#start-end-ï¼šåŒ¹é…-start-å’Œ-end-ä¹‹é—´çš„ä»»æ„ä¸€ä¸ªå­—ç¬¦" class="headerlink" title="[[start]-[end]]ï¼šåŒ¹é…[start]å’Œ[end]ä¹‹é—´çš„ä»»æ„ä¸€ä¸ªå­—ç¬¦"></a><code>[[start]-[end]]</code>ï¼šåŒ¹é…<code>[start]</code>å’Œ<code>[end]</code>ä¹‹é—´çš„ä»»æ„ä¸€ä¸ªå­—ç¬¦</h1><p><code>[[start]-[end]]</code>å…è®¸æˆ‘ä»¬åŒ¹é…ä»»ä½•ä¸€ä¸ªåœ¨<code>[start]</code>å’Œ<code>[end]</code>ä¹‹é—´çš„å­—ç¬¦ï¼Œå…¶ä¸­<code>[start]</code>å’Œ<code>[end]</code>å¿…é¡»æ˜¯ä¸¤ä¸ªå­˜åœ¨é¡ºåºå…³ç³»çš„å­—ç¬¦ï¼Œæ¯”å¦‚æ•°å­—ã€å­—æ¯ç­‰ï¼š</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[zclzcl@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">file1.txt  file2.txt  gd  gg  ground  ground.txt  g.txt</span><br><span class="line">[zclzcl@localhost Playground]$ <span class="built_in">ls</span> g[g-r]*</span><br><span class="line">gg  ground  ground.txt</span><br></pre></td></tr></table></figure>

<p>ä¸Šé¢æŒ‡ä»¤ä¼šåŒ¹é…æ‰€æœ‰ä»¥<code>g</code>å¼€å¤´ï¼Œä¸”ç¬¬äºŒä¸ªå­—ç¬¦åœ¨<code>g</code>å’Œ<code>r</code>ä¹‹é—´çš„å­—ç¬¦ä¸²ã€‚</p>
<blockquote>
<p>ä»¥ä¸Šè¿™äº›æ¶‰åŠ<code>[]</code>çš„åŒ¹é…ï¼Œ<code>[]</code>å†…çš„å†…å®¹é™¤äº†å¯ä»¥æ˜¯ç”¨æˆ·æŒ‡å®šçš„å­—ç¬¦ä¸²å¤–ï¼Œè¿˜å¯ä»¥æ˜¯ä¸€ç±»å­—ç¬¦ä¸²ï¼Œå¦‚ï¼š</p>
<ul>
<li><code>[:alnum:]</code>è¡¨ä»»æ„ä¸€ä¸ªå­—ç¬¦æˆ–å­—æ¯ï¼›</li>
<li><code>[:alpha:]</code>è¡¨ä»»æ„ä¸€ä¸ªå­—æ¯ï¼›</li>
<li><code>[:digit:]</code>è¡¨ä»»æ„ä¸€ä¸ªæ•°å­—ï¼›</li>
<li><code>[:lower:]</code>è¡¨ä»»æ„ä¸€ä¸ªå°å†™å­—æ¯ï¼›</li>
<li><code>[:upper:]</code>è¡¨ä»»æ„ä¸€ä¸ªå¤§å†™å­—æ¯ã€‚</li>
</ul>
<p><code>g[[:lower:]]</code>å°†åŒ¹é…ä»»æ„æ•°é‡çš„ä»¥<code>g</code>å¼€å¤´ã€ç¬¬äºŒä¸ªå­—ç¬¦ä¸ºå°å†™å­—æ¯çš„ä¸¤å­—ç¬¦å­—ç¬¦ä¸²ã€‚</p>
</blockquote>
<h1 id="ï¼šè½¬ä¹‰å­—ç¬¦"><a href="#ï¼šè½¬ä¹‰å­—ç¬¦" class="headerlink" title="\ï¼šè½¬ä¹‰å­—ç¬¦"></a><code>\</code>ï¼šè½¬ä¹‰å­—ç¬¦</h1><p>æœ‰æ—¶ï¼Œæˆ‘ä»¬ä¼šæƒ³è¦åŒ¹é…åå­—ä¸­å¸¦æœ‰é€šé…ç¬¦å­—ç¬¦ï¼Œå¦‚<code>*</code>å’Œ<code>?</code>çš„å­—ç¬¦ä¸²ï¼Œè¿™æ—¶å€™å°±éœ€è¦ä½¿ç”¨è½¬ä¹‰å­—ç¬¦<code>\</code>è®©é€šé…ç¬¦å¤±åŽ»é€šé…ç¬¦æ„ä¹‰è€Œè½¬ä¸ºæ™®é€šçš„å­—ç¬¦ä¸²ï¼š</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[zclzcl@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">chap?  file1.txt  file2.txt  gd  gg  ground  ground.txt  g.txt  what?</span><br><span class="line">[zclzcl@localhost Playground]$ <span class="built_in">ls</span> *\?</span><br><span class="line">chap?  what?</span><br></pre></td></tr></table></figure>

<p>ä¸Šé¢çš„å‘½ä»¤ï¼Œ<code>\?</code>ä»£è¡¨å­—ç¬¦<code>?</code>ï¼Œå› æ­¤å…¶åŒ¹é…çš„æ˜¯æ‰€æœ‰ä»¥<code>?</code>ç»“å°¾çš„å­—ç¬¦ä¸²ã€‚</p>
<h1 id="èŠ±æ‹¬å·å±•å¼€"><a href="#èŠ±æ‹¬å·å±•å¼€" class="headerlink" title="èŠ±æ‹¬å·å±•å¼€"></a>èŠ±æ‹¬å·å±•å¼€</h1><p>èŠ±æ‹¬å·å±•å¼€ååˆ†ç±»ä¼¼äºŽ<code>[[start]-[end]]</code>å½¢å¼çš„é€šé…ç¬¦æ¨¡å¼åŒ¹é…ã€‚é€šè¿‡èŠ±æ‹¬å·å±•å¼€ï¼Œæˆ‘ä»¬å¯ä»¥ä»Žä¸€ä¸ªåŒ…å«èŠ±æ‹¬å·çš„æ¨¡å¼ä¸­åˆ›å»ºå‡ºå¤šä¸ªå­—ç¬¦ä¸²ï¼Œå…¶åŸºæœ¬ä½¿ç”¨æ¨¡å¼ä¸ºï¼š</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;[stringA],[stringB],...,[string]&#125;</span><br><span class="line">æˆ–</span><br><span class="line">&#123;[start]..[end]&#125;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>éœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒèŠ±æ‹¬å·çš„å†…å®¹åªèƒ½ç”¨<code>,</code>åˆ†å‰²ï¼Œè€Œä¸èƒ½æœ‰ç©ºç™½ç¬¦ï¼Œå¦åˆ™èŠ±æ‹¬å·ä¼šè¢«è§†ä¸ºä¸€ä¸ªæ™®é€šçš„å­—ç¬¦ä¸²ã€‚</p>
</blockquote>
<p>æ¯”å¦‚ï¼Œ<code>&#123;AB,CB,CC&#125;</code>å°†äº§ç”Ÿ3ä¸ªå­—ç¬¦ä¸²<code>AB</code>ã€<code>CB</code>å’Œ<code>CC</code>ï¼š</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[zclzcl@localhost Playground]$ <span class="built_in">rm</span> *</span><br><span class="line">[zclzcl@localhost Playground]$ <span class="built_in">touch</span> &#123;AB,CB,CC&#125;</span><br><span class="line">[zclzcl@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">AB  CB  CC</span><br></pre></td></tr></table></figure>

<p>è€Œ<code>&#123;2..6&#125;</code>å°†äº§ç”Ÿ5ä¸ªå­—ç¬¦ä¸²<code>2</code>ã€<code>3</code>ã€<code>4</code>ã€<code>5</code>å’Œ<code>6</code>ï¼š</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[zclzcl@localhost Playground]$ rm *</span><br><span class="line">[zclzcl@localhost Playground]$ touch &#123;2..6&#125;</span><br><span class="line">[zclzcl@localhost Playground]$ ls</span><br><span class="line">2  3  4  5  6</span><br></pre></td></tr></table></figure>

<blockquote>
<p>æ³¨æ„æ˜¯ä¸¤ä¸ª<code>.</code>ã€‚</p>
</blockquote>
<p>èŠ±æ‹¬å·å±•å¼€è¿˜å¯ä»¥åµŒå¥—ã€‚åµŒå¥—æ—¶ï¼Œæ¯ä¸ªèŠ±æ‹¬å·è¢«è§†ä¸ºä¸€ä¸ªæ•´ä½“ï¼Œä¾æ¬¡å±•å¼€ï¼Œå¦‚ï¼š</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[zclzcl@localhost Playground]$ rm *</span><br><span class="line">[zclzcl@localhost Playground]$ touch &#123;00&#123;1..9&#125;,0&#123;10..99&#125;&#125;</span><br><span class="line">[zclzcl@localhost Playground]$ ls</span><br><span class="line">001  007  013  019  025  031  037  043  049  055  061  067  073  079  085  091  097</span><br><span class="line">002  008  014  020  026  032  038  044  050  056  062  068  074  080  086  092  098</span><br><span class="line">003  009  015  021  027  033  039  045  051  057  063  069  075  081  087  093  099</span><br><span class="line">004  010  016  022  028  034  040  046  052  058  064  070  076  082  088  094</span><br><span class="line">005  011  017  023  029  035  041  047  053  059  065  071  077  083  089  095</span><br><span class="line">006  012  018  024  030  036  042  048  054  060  066  072  078  084  090  096</span><br></pre></td></tr></table></figure>

<p>ä¸Šé¢çš„å¼å­å°†ä¸€æ¬¡æ€§ç”Ÿæˆ99ä¸ªæ–‡ä»¶ã€‚å…¶åŸºæœ¬åŽŸç†æ˜¯ï¼šæœ€å¤–å±‚èŠ±æ‹¬å·å†…çš„ä¸¤é¡¹<code>00&#123;1..9&#125;</code>å’Œ<code>0&#123;10..99&#125;</code>åˆ†åˆ«å•ç‹¬å±•å¼€ã€‚</p>
<h1 id="å‚è€ƒ"><a href="#å‚è€ƒ" class="headerlink" title="å‚è€ƒ"></a>å‚è€ƒ</h1><ul>
<li><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=LH4eF75Z_7U&list=WL&index=3&t=33s">Linux Tutorials | Wildcards and Pattern Matching | GeeksforGeeks</a></li>
<li><a target="_blank" rel="noopener" href="https://www.kancloud.cn/thinkphp/linux-command-line/39435">ç¬¬äº”ç« ï¼šæ“ä½œæ–‡ä»¶å’Œç›®å½•</a></li>
<li><a target="_blank" rel="noopener" href="https://www.kancloud.cn/thinkphp/linux-command-line/39438">ç¬¬å…«ç« ï¼šä»Žshellçœ¼ä¸­çœ‹ä¸–ç•Œ</a></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zclzcl0223.github.io/2023/08/03/WildcardsPatternMatching/" data-id="clzik1qug00dzm07kgl15bduq" data-title="Wildcards and Pattern Matching" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Linux/" rel="tag">Linux</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Tool/" rel="tag">Tool</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-ProgramCompilation" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/25/ProgramCompilation/" class="article-date">
  <time class="dt-published" datetime="2023-07-25T10:24:08.000Z" itemprop="datePublished">2023-07-25</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Tool/">Tool</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/07/25/ProgramCompilation/">Program Compilation</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <span id="more"></span>

<h1 id="Compile-Assembly-and-Link"><a href="#Compile-Assembly-and-Link" class="headerlink" title="Compile, Assembly and Link"></a>Compile, Assembly and Link</h1><p>ä¸€ä¸ªC&#x2F;C++ç¨‹åºä»Žæºæ–‡ä»¶<code>.c/.cpp</code>åˆ°å¯æ‰§è¡Œæ–‡ä»¶<code>.exe</code>ä¸€èˆ¬è¦ç»è¿‡ä»¥ä¸‹å››ä¸ªæ­¥éª¤ï¼š</p>
<p><img src="/2023/07/25/ProgramCompilation/1.png" alt="1"></p>
<center style="font-size:12px; font-weight:bold">Fig. 1. ä»Žæºæ–‡ä»¶åˆ°å¯æ‰§è¡Œæ–‡ä»¶</center>

<ol>
<li>é¢„å¤„ç†é˜¶æ®µï¼šä¸»è¦å®Œæˆæºæ–‡ä»¶çš„å®æ›¿æ¢ï¼›</li>
<li>ç¼–è¯‘ï¼ˆCompileï¼‰é˜¶æ®µï¼šå°†é«˜çº§è¯­è¨€ç¿»è¯‘ä¸ºæ±‡ç¼–è¯­è¨€ã€æºç¨‹åºç¿»è¯‘ä¸ºæ±‡ç¼–ç¨‹åºï¼›</li>
<li>æ±‡ç¼–ï¼ˆAssemblyï¼‰é˜¶æ®µï¼šå°†æ±‡ç¼–è¯­è¨€ç¿»è¯‘ä¸ºæœºå™¨èƒ½è¯†åˆ«çš„äºŒè¿›åˆ¶æœºå™¨è¯­è¨€ï¼Œç”Ÿæˆçš„<code>.o</code>æ–‡ä»¶ç§°å¯é‡å®šä½ç›®æ ‡æ–‡ä»¶ï¼Œç”¨äºŽåŽç»­çš„é“¾æŽ¥æ“ä½œï¼›</li>
<li>é“¾æŽ¥ï¼ˆLinkï¼‰é˜¶æ®µï¼šå°†ç¨‹åºç”¨åˆ°çš„åº“ç¨‹åºã€è‡ªå®šä¹‰çš„ä¾èµ–ç¨‹åºç­‰ä¸Žç¨‹åºé“¾æŽ¥åˆ°ä¸€èµ·ï¼Œå½¢æˆæœ€ç»ˆçš„å¯æ‰§è¡Œæ–‡ä»¶ä»¥åŠé€»è¾‘åœ°å€ã€‚</li>
</ol>
<blockquote>
<p>äº‹å®žä¸Šï¼ŒçŽ°åœ¨å¤§å¤šæ•°ç¼–è¯‘å™¨ï¼ˆCompilerï¼‰ä¼šåŒæ—¶å®Œæˆç¼–è¯‘å’Œæ±‡ç¼–çš„ä»»åŠ¡ã€‚</p>
</blockquote>
<h1 id="GCC"><a href="#GCC" class="headerlink" title="GCC"></a>GCC</h1><p>GCCï¼Œå…¨ç§°GNU C Compileræˆ–CNU Compiler Collectionï¼Œå‰è€…æ˜¯å…¶æœ€åˆçš„ç§°å‘¼ï¼Œæ˜¯GNU Projectçš„å‘èµ·è€…ä¸ºå®Œå–„ç±»Unixæ“ä½œç³»ç»Ÿï¼ˆå³Linuxï¼‰è€Œå¼€å‘çš„C&#x2F;C++ç¼–è¯‘å™¨ï¼ŒåŽæ¥éšç€GCCçš„å‘å±•ï¼Œå…¶æ”¯æŒçš„è¯­è¨€ä¹Ÿé€æ¸å¢žå¤šï¼Œå¦‚Javaã€Goç­‰ï¼Œç”±æ­¤æ‰æœ‰äº†åŽé¢çš„ç§°å‘¼ã€‚é€šå¸¸ï¼ŒLinuxå‘è¡Œç‰ˆçš„æ“ä½œç³»ç»Ÿéƒ½ä¼šè‡ªå¸¦GCCï¼Œå¦‚æžœæ²¡æœ‰ï¼Œåˆ™éœ€è¦æ‰‹åŠ¨å®‰è£…ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨<code>gcc --version</code>æˆ–<code>g++ --version</code>æ¥æŸ¥çœ‹æœ¬æœºçš„GCCç‰ˆæœ¬ã€‚</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[meme@localhost Playground]$ gcc --version</span><br><span class="line">gcc (GCC) 11.2.1 20220127 (Red Hat 11.2.1-9)</span><br><span class="line">Copyright (C) 2021 Free Software Foundation, Inc.</span><br><span class="line">This is free software; see the <span class="built_in">source</span> <span class="keyword">for</span> copying conditions.  There is NO</span><br><span class="line">warranty; not even <span class="keyword">for</span> MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.</span><br><span class="line"></span><br><span class="line">[meme@localhost Playground]$ g++ --version</span><br><span class="line">g++ (GCC) 11.2.1 20220127 (Red Hat 11.2.1-9)</span><br><span class="line">Copyright (C) 2021 Free Software Foundation, Inc.</span><br><span class="line">This is free software; see the <span class="built_in">source</span> <span class="keyword">for</span> copying conditions.  There is NO</span><br><span class="line">warranty; not even <span class="keyword">for</span> MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<blockquote>
<p><code>gcc</code>æ˜¯Cç¼–è¯‘ç¨‹åºï¼Œè€Œ<code>g++</code>æ˜¯C++ç¼–è¯‘ç¨‹åºã€‚æœ¬èŠ‚å°†ä»¥<code>gcc</code>ä¸ºä¾‹è®°å½•ä¸€äº›GCCç¼–è¯‘å™¨çš„ç”¨æ³•ã€‚</p>
</blockquote>
<p>åœ¨ä½¿ç”¨<code>gcc</code>å‰ï¼Œæˆ‘ä»¬å…ˆåˆ›å»ºä¸€ä¸ªç®€å•çš„Cç¨‹åºï¼š</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[meme@localhost Playground]$ <span class="built_in">cat</span> &gt; main.c</span><br><span class="line"><span class="comment">#include &lt;stdio.h&gt;</span></span><br><span class="line"></span><br><span class="line">int <span class="function"><span class="title">main</span></span>() &#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Goodbye, world!\n&quot;</span>);</span><br><span class="line">    <span class="built_in">return</span> 0;</span><br><span class="line">&#125;</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">main.c</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">cat</span> main.c</span><br><span class="line"><span class="comment">#include &lt;stdio.h&gt;</span></span><br><span class="line"></span><br><span class="line">int <span class="function"><span class="title">main</span></span>() &#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Goodbye, world!\n&quot;</span>);</span><br><span class="line">    <span class="built_in">return</span> 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="a-out"><a href="#a-out" class="headerlink" title="a.out"></a><code>a.out</code></h2><p>è‹¥æˆ‘ä»¬ä¸ä¸º<code>gcc</code>æä¾›ä»»ä½•é€‰é¡¹è€Œç›´æŽ¥ä½¿ç”¨<code>gcc</code>ç¼–è¯‘æ–‡ä»¶ï¼Œ<code>gcc</code>ä¼šç”Ÿæˆ<code>a.out</code>ä½œä¸ºè¯¥ç¨‹åºçš„å¯æ‰§è¡Œæ–‡ä»¶ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œåœ¨Linuxæ“ä½œç³»ç»Ÿä¸­ï¼Œé»˜è®¤è·¯å¾„å¹¶ä¸åŒ…å«å½“å‰å·¥ä½œç›®å½•ï¼Œå› æ­¤éœ€è¦ä½¿ç”¨<code>./a.out</code>æ¥è¿è¡Œ<code>a.out</code>ã€‚</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[meme@localhost Playground]$ gcc main.c</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">a.out  main.c</span><br><span class="line">[meme@localhost Playground]$ ./a.out</span><br><span class="line">Goodbye, world!</span><br></pre></td></tr></table></figure>

<blockquote>
<p>è‹¥<code>a.out</code>æ— æ³•è¿è¡Œï¼Œåˆ™éœ€è¦æ£€æŸ¥ä¸€ä¸‹å½“å‰ç”¨æˆ·æ˜¯å¦æœ‰è¿è¡Œ<code>a.out</code>çš„æƒé™ã€‚è‹¥æ— ï¼Œåˆ™éœ€ç”¨<code>chmod a+x a.out</code>æ¥èµ‹äºˆå½“å‰ç”¨æˆ·æƒé™ã€‚</p>
</blockquote>
<h2 id="c-o-g"><a href="#c-o-g" class="headerlink" title="-c, -o, -g"></a><code>-c</code>, <code>-o</code>, <code>-g</code></h2><p>è‹¥æˆ‘ä»¬æƒ³è¦æŒ‡å®šå¯æ‰§è¡Œæ–‡ä»¶çš„åå­—ï¼Œæˆ‘ä»¬å°±éœ€è¦æŒ‡å®šé€‰é¡¹æ¥é€æ­¥ç¼–è¯‘ã€‚</p>
<ul>
<li><code>-c</code>é€‰é¡¹ç¤ºæ„<code>gcc</code>å®Œæˆé™¤Linkä»¥å¤–çš„å…¨éƒ¨æ­¥éª¤ï¼Œç”Ÿæˆå¯é‡å®šä½çš„<code>.o</code>æ–‡ä»¶ï¼š<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[meme@localhost Playground]$ gcc -c main.c</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">a.out  main.c  main.o</span><br></pre></td></tr></table></figure></li>
<li><code>-o</code>é€‰é¡¹ç¤ºæ„<code>gcc</code>å®Œæˆå¯é‡å®šä½æ–‡ä»¶åŠå…¶åº“æ–‡ä»¶çš„Linkã€‚å…¶å¯¹è±¡å¯ä»¥æ˜¯<code>.o</code>æ–‡ä»¶ï¼Œä¹Ÿå¯ä»¥æ˜¯<code>.c</code>æ–‡ä»¶ã€‚è‹¥ä¸º<code>.o</code>åˆ™<code>gcc</code>åªå®ŒæˆLinkï¼›è‹¥ä¸º<code>.c</code>åˆ™<code>gcc</code>å°†å®Œæˆä»Žæºæ–‡ä»¶åˆ°å¯æ‰§è¡Œæ–‡ä»¶çš„æ‰€æœ‰æ­¥éª¤ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå¯æ‰§è¡Œæ–‡ä»¶çš„åå­—åº”<strong>ä¸¥æ ¼</strong>ç½®äºŽ<code>-o</code>ä¹‹åŽï¼š<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[meme@localhost Playground]$ gcc -o main main.o  <span class="comment"># ç­‰ä»·äºŽgcc main.o -o main</span></span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">a.out  main  main.c  main.o</span><br><span class="line">[meme@localhost Playground]$ ./main</span><br><span class="line">Goodbye, world!</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">rm</span> main main.o</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">a.out  main.c</span><br><span class="line">[meme@localhost Playground]$ gcc -o main main.c</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">a.out  main  main.c</span><br><span class="line">[meme@localhost Playground]$ ./main</span><br><span class="line">Goodbye, world!</span><br></pre></td></tr></table></figure></li>
<li><code>-g</code>é€‰é¡¹ä½¿å¾—ç¨‹åºä»¥Debugæ¨¡å¼ç¼–è¯‘ï¼Œä»¥è¯¥æ–¹å¼ç¼–è¯‘çš„ç¨‹åºå¯ä»¥ä½¿ç”¨GDBæ¥è¿›è¡ŒDebugï¼š<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[meme@localhost Playground]$ <span class="built_in">rm</span> a.out main</span><br><span class="line">[meme@localhost Playground]$ gcc -g main.c</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">a.out  main.c</span><br><span class="line">[meme@localhost Playground]$ gdb a.out</span><br><span class="line">GNU gdb (GDB) Red Hat Enterprise Linux 10.2-6.el7</span><br><span class="line">...</span><br><span class="line">(gdb) q</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">a.out  main.c</span><br><span class="line">[meme@localhost Playground]$ gcc -g -o main main.c</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">a.out  main  main.c</span><br><span class="line">[meme@localhost Playground]$ gdb main</span><br><span class="line">GNU gdb (GDB) Red Hat Enterprise Linux 10.2-6.el7</span><br><span class="line">...</span><br><span class="line">(gdb) q</span><br></pre></td></tr></table></figure></li>
</ul>
<p>ä»¥ä¸Šè¿™3ä¸ªå°±æ˜¯GCCçš„3ä¸ªåŸºæœ¬é€‰é¡¹ï¼Œè¿˜æœ‰å…¶ä»–çš„é€‰é¡¹å¦‚<code>-l</code>ç”¨äºŽåŠ å…¥ä¸åœ¨æ ‡å‡†åº“ä¸­çš„ç¬¬ä¸‰æ–¹åº“ç­‰ã€‚</p>
<h1 id="Make"><a href="#Make" class="headerlink" title="Make"></a>Make</h1><p>ä¸€ä¸ªé¡¹ç›®å¾€å¾€ä¼šæœ‰å¤šä¸ªç›¸äº’åŒ…å«çš„æ–‡ä»¶ï¼Œå¦‚ï¼Œæˆ‘ä»¬ç§»é™¤ä¹‹å‰ç”Ÿæˆçš„<code>a.out</code>ä»¥åŠ<code>main</code>æ–‡ä»¶ï¼Œå¹¶é‡æ–°åˆ›å»ºä¸¤ä¸ªæ–°æ–‡ä»¶<code>add.c</code>å’Œ<code>add.h</code>ï¼ŒåŒæ—¶ä¿®æ”¹<code>main.c</code>çš„å†…å®¹è®©<code>main.c</code>å¼•ç”¨<code>add.c</code>ä¸­çš„å‡½æ•°<code>add</code>ï¼š</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[meme@localhost Playground]$ ls</span><br><span class="line">main.c</span><br><span class="line">[meme@localhost Playground]$ cat &gt; add.c</span><br><span class="line">int add(int a, int b) &#123;</span><br><span class="line">    return a + b;</span><br><span class="line">&#125;</span><br><span class="line">[meme@localhost Playground]$ cat &gt; add.h</span><br><span class="line">int add(int a, int b);</span><br><span class="line">[meme@localhost Playground]$ ls</span><br><span class="line">add.c  add.h  main.c</span><br><span class="line">[meme@localhost Playground]$ vim main.c</span><br><span class="line">[meme@localhost Playground]$ cat main.c</span><br><span class="line">#include &lt;stdio.h&gt;</span><br><span class="line">#include &quot;add.h&quot;</span><br><span class="line"></span><br><span class="line">int main() &#123;</span><br><span class="line">    printf(&quot;Goodbye, world!\n&quot;);</span><br><span class="line">    printf(&quot;%d\n&quot;, add(5, 3));</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>ç”±äºŽä¸¤ä¸ªæ–‡ä»¶çš„å…³ç³»å¾ˆç®€å•ï¼Œæ‰€ä»¥æˆ‘ä»¬ä»å¯ä»¥ç®€å•åœ°ç”Ÿæˆ<code>a.out</code>ï¼š</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[meme@localhost Playground]$ gcc main.c add.c</span><br><span class="line">[meme@localhost Playground]$ ./a.out</span><br><span class="line">Goodbye, world!</span><br><span class="line">8</span><br></pre></td></tr></table></figure>

<p>æˆ–è€…ç”Ÿæˆè‡ªå‘½åçš„æ–‡ä»¶ï¼š</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[meme@localhost Playground]$ gcc -o main_add main.c add.c</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">add.c  add.h  a.out  main_add  main.c</span><br><span class="line">[meme@localhost Playground]$ ./main_add</span><br><span class="line">Goodbye, world!</span><br><span class="line">8</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">rm</span> main_add</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">add.c  add.h  a.out  main.c</span><br><span class="line">[meme@localhost Playground]$ gcc -c main.c add.c</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">add.c  add.h  add.o  a.out  main.c  main.o</span><br><span class="line">[meme@localhost Playground]$ gcc -o main_add main.o add.o</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">add.c  add.h  add.o  a.out  main_add  main.c  main.o</span><br><span class="line">[meme@localhost Playground]$ ./main_add</span><br><span class="line">Goodbye, world!</span><br><span class="line">8</span><br></pre></td></tr></table></figure>

<p>ä¸Šè¿°ä¸¤ä¸ªç¨‹åºå¾ˆç®€å•ï¼Œå› æ­¤æ‰‹åŠ¨åœ°ç”Ÿæˆå¯æ‰§è¡Œæ–‡ä»¶ä»æ˜¯å¯è¡Œçš„ã€‚ä½†æ˜¯å¯¹äºŽå¤æ‚çš„é¡¹ç›®ï¼Œå…¶åŒ…å«çš„ç¨‹åºå¯èƒ½æœ‰åå‡ äºŒåï¼Œç”šè‡³ä¸Šç™¾ä¸ªï¼Œæ­¤æ—¶å†æ‰‹åŠ¨åœ°ç¼–è¯‘ã€é“¾æŽ¥å°±ä¸å¤ªçŽ°å®žäº†ã€‚</p>
<p><code>make</code>å·¥å…·å¯ä»¥å¸®åŠ©æˆ‘ä»¬çœåŽ»æ¯æ¬¡é‡æ–°ç¼–è¯‘æ—¶æ•²æ‰“æ–‡ä»¶åçš„éº»çƒ¦ã€‚<code>make</code>åŸºäºŽç”¨æˆ·é¢„å…ˆç¼–å†™çš„<code>Makefile</code>æ–‡ä»¶ï¼Œå®žçŽ°è‡ªåŠ¨ç¼–è¯‘ã€é“¾æŽ¥ã€‚ä½¿ç”¨<code>make --version</code>å¯ä»¥æŸ¥çœ‹æœ¬æœºçš„<code>make</code>ç‰ˆæœ¬ï¼Œåœ¨æ­¤æˆ‘ä»¬å…ˆåˆ›å»ºæˆ‘ä»¬çš„<code>Makefile</code>æ–‡ä»¶ï¼š</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[meme@localhost Playground]$ make --version</span><br><span class="line">GNU Make 4.3</span><br><span class="line">Built <span class="keyword">for</span> x86_64-redhat-linux-gnu</span><br><span class="line">Copyright (C) 1988-2020 Free Software Foundation, Inc.</span><br><span class="line">License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;</span><br><span class="line">This is free software: you are free to change and redistribute it.</span><br><span class="line">There is NO WARRANTY, to the extent permitted by law.</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">rm</span> a.out main_add main.o add.o</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">add.c  add.h  main.c</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">touch</span> Makefile</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">add.c  add.h  main.c  Makefile</span><br></pre></td></tr></table></figure>

<p><code>Makefile</code>æ˜¯<code>make</code>æŒ‡å®šä½¿ç”¨çš„æ–‡ä»¶åï¼Œå®ƒåªæ˜¯ä¸€ä¸ªæ™®é€šçš„æ–‡æœ¬æ–‡ä»¶ï¼Œå…¶å†…éƒ¨çš„å†…å®¹ç”¨äºŽæŒ‡å¯¼<code>make</code>å®Œæˆç¼–è¯‘æ“ä½œï¼Œä¸€ä¸ª<code>Makefile</code>æ–‡ä»¶çš„åŸºæœ¬å†…å®¹æœ‰ï¼š</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">all: main</span><br><span class="line"></span><br><span class="line">main: main.o add.o</span><br><span class="line">    gcc -o main main.o add.o</span><br><span class="line"></span><br><span class="line">main.o: main.c</span><br><span class="line">    gcc -c main.c</span><br><span class="line"></span><br><span class="line">add.o: add.c</span><br><span class="line">    gcc -c add.c</span><br><span class="line"></span><br><span class="line">clean:</span><br><span class="line">    rm main main.o add.o</span><br></pre></td></tr></table></figure>

<p>å…¶ä¸­ï¼Œ<code>all</code>åŽé¢çš„æ˜¯æœ€ç»ˆè¦ç”Ÿæˆçš„å¯æ‰§è¡Œæ–‡ä»¶çš„åç§°ï¼Œå…¶åŽç»­çš„<code>main</code>&amp;<code>main.o</code>&amp;<code>add.o</code>ã€å†’å·åŽçš„éƒ¨åˆ†åŠä¸‹æ–¹çš„æŒ‡ä»¤åˆ†åˆ«ä»£è¡¨<strong>è¦ç”Ÿæˆçš„æ–‡ä»¶</strong>ã€<strong>ç”Ÿæˆè¿™äº›æ–‡ä»¶è¦ä¾èµ–çš„å…¶ä»–æ–‡ä»¶</strong>å’Œ<strong>ç›¸åº”çš„GCCæŒ‡ä»¤</strong>ã€‚æœ€åŽçš„<code>clean</code>ä½¿å¾—æˆ‘ä»¬èƒ½æ‰§è¡Œ<code>make clean</code>æ¥æ¸…é™¤éƒ¨åˆ†æˆ–æ‰€æœ‰ç”Ÿæˆçš„æ–‡ä»¶ã€‚</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">[meme@localhost Playground]$ vim Makefile</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">cat</span> Makefile</span><br><span class="line">all: main</span><br><span class="line"></span><br><span class="line">main: main.o add.o</span><br><span class="line">	gcc -o main main.o add.o</span><br><span class="line"></span><br><span class="line">main.o: main.c</span><br><span class="line">	gcc -c main.c</span><br><span class="line"></span><br><span class="line">add.o: add.c</span><br><span class="line">	gcc -c add.c</span><br><span class="line"></span><br><span class="line">clean:</span><br><span class="line">	<span class="built_in">rm</span> main main.o add.o</span><br><span class="line">[meme@localhost Playground]$ make</span><br><span class="line">gcc -c main.c</span><br><span class="line">gcc -c add.c</span><br><span class="line">gcc -o main main.o add.o</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">add.c  add.h  add.o  main  main.c  main.o  Makefile</span><br><span class="line">[meme@localhost Playground]$ ./main</span><br><span class="line">Goodbye, world!</span><br><span class="line">8</span><br><span class="line">[meme@localhost Playground]$ make clean</span><br><span class="line"><span class="built_in">rm</span> main main.o add.o</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">add.c  add.h  main.c  Makefile</span><br></pre></td></tr></table></figure>

<p><code>make</code>çš„å¦ä¸€ä¸ªä¼˜ç‚¹åœ¨äºŽï¼šåœ¨ä¸€æ¬¡ç¼–è¯‘è¿‡åŽå†æ¬¡ç¼–è¯‘æ—¶ï¼Œå®ƒåªä¼šç¼–è¯‘è¢«ä¿®æ”¹è¿‡çš„æ–‡ä»¶ã€‚æ¯”å¦‚ï¼Œè‹¥æˆ‘ä»¬å°†<code>main.c</code>ä¸­çš„<code>add(5, 3)</code>ä¿®æ”¹ä¸º<code>add(5, 4)</code>å†é‡æ–°ç¼–è¯‘ï¼Œæˆ‘ä»¬å°†å¾—åˆ°å¦‚ä¸‹ç»“æžœï¼ˆç¬¬ä¸€ä¸ª<code>make</code>ç¼–è¯‘çš„æ˜¯æœªä¿®æ”¹å‰çš„ç¨‹åºï¼‰ï¼š</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[meme@localhost Playground]$ make</span><br><span class="line">gcc -c main.c</span><br><span class="line">gcc -c add.c</span><br><span class="line">gcc -o main main.o add.o</span><br><span class="line">[meme@localhost Playground]$ vim main.c</span><br><span class="line">[meme@localhost Playground]$ make</span><br><span class="line">gcc -c main.c</span><br><span class="line">gcc -o main main.o add.o</span><br><span class="line">[meme@localhost Playground]$ ./main</span><br><span class="line">Goodbye, world!</span><br><span class="line">9</span><br></pre></td></tr></table></figure>

<p>å¯è§ï¼Œ<code>add.o</code>å¹¶æ²¡æœ‰è¢«é‡æ–°ç”Ÿæˆã€‚ä»¥ä¸Šæ˜¯<code>make</code>åŠ<code>Makefile</code>çš„ä¸€äº›åŸºæœ¬æ“ä½œã€‚æƒ³è¦äº†è§£æ›´å¤šæœ‰å…³GCCå’ŒMakeçš„çŸ¥è¯†å¯ä»¥çœ‹å—æ´‹ç†å·¥å¤§å­¦çš„ä¸€ä»½æŒ‡å—ï¼š<a target="_blank" rel="noopener" href="https://www3.ntu.edu.sg/home/ehchua/programming/cpp/gcc_make.html#zz-2.">Compiling, Linking and Building C&#x2F;C++ Applications</a>ã€‚</p>
<h1 id="CMake"><a href="#CMake" class="headerlink" title="CMake"></a>CMake</h1><p>å³ä¾¿æœ‰äº†<code>make</code>ï¼Œæˆ‘ä»¬ä»ä¼šé‡åˆ°ä¸€äº›ä»…ä»…æ˜¯ç¼–å†™<code>Makefile</code>å°±å¾ˆéº»çƒ¦çš„é¡¹ç›®ã€‚<code>cmake</code>å°±æ˜¯ä¸ºäº†è§£å†³è¿™é¡¹é—®é¢˜è€Œå‡ºçŽ°çš„ã€‚ç±»ä¼¼äºŽ<code>make</code>ï¼Œ<code>cmake</code>ä¹Ÿæœ‰å…¶ç‰¹æœ‰çš„æ–‡ä»¶<code>CMakeLists.txt</code>ã€‚ä½†æ˜¯ä¸åŒäºŽ<code>make</code>çš„æ˜¯ï¼Œ<code>cmake</code>çš„ç‰¹æœ‰æ–‡ä»¶æ˜¯ç”¨äºŽ<strong>ç”Ÿæˆ</strong><code>Makefile</code>çš„ã€‚<code>cmake</code>ã€<code>make</code>å’Œ<code>gcc</code>çš„å…³ç³»å¦‚ä¸‹æ‰€ç¤ºï¼š</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">              cmake           make       gcc</span><br><span class="line">CMakeLists.txt -----&gt; Makefile ----&gt; Cmds ---&gt; Binary</span><br></pre></td></tr></table></figure>

<p>åŒæ ·åœ°ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨<code>cmake --version</code>æŸ¥çœ‹æœ¬ç³»ç»Ÿçš„<code>cmake</code>ç‰ˆæœ¬ï¼ˆè‹¥æ²¡æœ‰åˆ™éœ€è¦å®‰è£…ï¼‰ã€‚</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[meme@localhost Playground]$ cmake --version</span><br><span class="line">cmake version 3.15.3</span><br><span class="line"></span><br><span class="line">CMake suite maintained and supported by Kitware (kitware.com/cmake).</span><br></pre></td></tr></table></figure>

<p>å®‰è£…å¥½<code>cmake</code>åŽï¼Œæˆ‘ä»¬å°±å¯ä»¥åœ¨å½“å‰ç›®å½•ä¸‹åˆ›å»º<code>CMakeLists.txt</code>æ–‡ä»¶ï¼š</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[meme@localhost Playground]$ <span class="built_in">touch</span> CMakeLists.txt</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">add.c  add.h  add.o  CMakeLists.txt  main  main.c  main.o  Makefile</span><br></pre></td></tr></table></figure>

<p><code>CMakeLists.txt</code>çš„ç¼–å†™æ¯”<code>Makefile</code>è¦æ›´åŠ å¤æ‚ï¼Œäº‹å®žä¸Šï¼Œå…¶ç¼–å†™çš„æ–¹å¼æœ¬èº«å°±å¯ä»¥è¢«è§†ä¸ºä¸€ç§æ–°çš„è¯­è¨€ã€‚æ­¤å¤„åªè®°å½•ä¸€äº›åŸºæœ¬çš„è¯­æ³•ï¼Œæ›´å¤šçš„è¦åŽ»çœ‹å®˜æ–¹æ–‡æ¡£<a target="_blank" rel="noopener" href="https://cmake.org/cmake/help/latest/guide/tutorial/index.html">CMake Tutorial</a>ã€‚</p>
<p>ä¸€ä¸ªæœ€åŸºæœ¬çš„<code>CMakeLists.txt</code>ä¼šåŒ…å«3ä¸ªåŸºæœ¬å‘½ä»¤ï¼š</p>
<ul>
<li><code>cmake_minimum_required()</code>ï¼šå‚æ•°ä¸ºè¯¥<code>CMakeLists.txt</code>æ–‡ä»¶æ‰€è¦æ±‚çš„æœ€ä½Ž<code>cmake</code>ç‰ˆæœ¬ï¼Œæ˜¯ä¸ºäº†ç¨‹åºçš„å¯ç§»æ¤æ€§è€ƒè™‘ï¼›</li>
<li><code>project()</code>ï¼šå‚æ•°ä¸ºæœ€åŽç”Ÿæˆçš„å¯æ‰§è¡Œæ–‡ä»¶åï¼›</li>
<li><code>add_executable()</code>ï¼šå‚æ•°ä¸ºå¯æ‰§è¡Œæ–‡ä»¶ååŠå…¶éœ€è¦çš„æºæ–‡ä»¶ã€‚</li>
</ul>
<p>ä»¥<code>make</code>ä¸­ä½¿ç”¨çš„<code>main.c</code>å’Œ<code>add.c</code>ä¸ºä¾‹ï¼Œå…¶<code>CMakeLists.txt</code>åº”ä¸ºï¼š</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cmake_minimum_required(VERSION 3.10)</span><br><span class="line"></span><br><span class="line"># set the project name</span><br><span class="line">project(main)</span><br><span class="line"></span><br><span class="line"># add the executable</span><br><span class="line">add_executable(main main.c add.c)</span><br></pre></td></tr></table></figure>

<p>ç”±äºŽ<code>cmake</code>åˆ©ç”¨<code>CMakeLists.txt</code>æœ€ç»ˆç”Ÿæˆçš„æ˜¯<code>Makefile</code>æ–‡ä»¶ä»¥åŠä¸€äº›é™„å±žæ–‡ä»¶ï¼Œæˆ‘ä»¬é€šå¸¸ä¼šæ–°å»ºä¸€ä¸ªæ–‡ä»¶å¤¹æ¥æ‰§è¡Œ<code>cmake</code>ï¼Œä¸€èˆ¬æˆ‘ä»¬ä¼šå°†è¯¥æ–‡ä»¶å¤¹å‘½åä¸º<code>build</code>ï¼ˆä¹Ÿå¯è‡ªç”±å‘½åï¼‰ï¼š</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[meme@localhost Playground]$ <span class="built_in">mkdir</span> build</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">ls</span></span><br><span class="line">add.c  add.h  add.o  build  CMakeLists.txt  main  main.c  main.o  Makefile</span><br><span class="line">[meme@localhost Playground]$ <span class="built_in">cd</span> build</span><br></pre></td></tr></table></figure>

<p>ç„¶åŽï¼Œåœ¨<code>build</code>æ–‡ä»¶å¤¹ä¸­æ‰§è¡Œæˆ‘ä»¬çš„<code>cmake</code>æŒ‡ä»¤ã€‚ç”±äºŽ<code>CMakeLists.txt</code>å­˜åœ¨äºŽçˆ¶ç›®å½•ä¸­ï¼Œæˆ‘ä»¬åº”ä½¿ç”¨<code>cmake ..</code>è€Œä¸æ˜¯å•å•çš„<code>cmake</code></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[meme@localhost build]$ cmake ..</span><br><span class="line">-- The C compiler identification is GNU 11.2.1</span><br><span class="line">...</span><br><span class="line">-- Build files have been written to: /home/meme/Playground/build</span><br><span class="line">[meme@localhost build]$ <span class="built_in">ls</span></span><br><span class="line">CMakeCache.txt  CMakeFiles  cmake_install.cmake  Makefile</span><br></pre></td></tr></table></figure>

<p>å¾—åˆ°<code>Makefile</code>åŽå†æ‰§è¡Œ<code>make</code>å³å¯ç”Ÿæˆç›¸åº”çš„å¯æ‰§è¡Œæ–‡ä»¶ï¼š</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[meme@localhost build]$ make</span><br><span class="line">make[1]: Entering directory <span class="string">&#x27;/home/meme/Playground/build&#x27;</span></span><br><span class="line">...</span><br><span class="line">make[1]: Leaving directory <span class="string">&#x27;/home/meme/Playground/build&#x27;</span></span><br><span class="line">[meme@localhost build]$ <span class="built_in">ls</span></span><br><span class="line">CMakeCache.txt  CMakeFiles  cmake_install.cmake  main  Makefile</span><br><span class="line">[meme@localhost build]$ ./main</span><br><span class="line">Goodbye, world!</span><br><span class="line">9</span><br></pre></td></tr></table></figure>

<blockquote>
<p><code>cmake</code>èƒ½è·¨ç›®å½•æ‰§è¡Œï¼Œä½†æ˜¯<code>make</code>åªèƒ½åœ¨æœ‰<code>Makefile</code>çš„ç›®å½•æ‰§è¡Œã€‚</p>
</blockquote>
<h1 id="å‚è€ƒ"><a href="#å‚è€ƒ" class="headerlink" title="å‚è€ƒ"></a>å‚è€ƒ</h1><ul>
<li><a target="_blank" rel="noopener" href="https://www.kancloud.cn/thinkphp/linux-command-line/39455">The Linux Command Line ä¸­æ–‡ç‰ˆ ç¬¬äºŒåå››ç« ï¼šç¼–è¯‘ç¨‹åº</a></li>
<li><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=qON2D3vDIt8&list=WL&index=1&t=193s">[Environment Setup 8] Compile programs from source code using GCC, GNU Make, and CMake</a></li>
<li><a target="_blank" rel="noopener" href="https://www3.ntu.edu.sg/home/ehchua/programming/cpp/gcc_make.html#zz-2.">Compiling, Linking and Building C&#x2F;C++ Applications</a></li>
<li><a target="_blank" rel="noopener" href="https://cmake.org/cmake/help/latest/guide/tutorial/index.html">CMake Tutorial</a></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zclzcl0223.github.io/2023/07/25/ProgramCompilation/" data-id="clzik1qty0076m07kc7sn2i8y" data-title="Program Compilation" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Linux/" rel="tag">Linux</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Tool/" rel="tag">Tool</a></li></ul>

    </footer>
  </div>
  
</article>



  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/page/6/">&laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><a class="page-number" href="/page/6/">6</a><span class="page-number current">7</span><a class="page-number" href="/page/8/">8</a><a class="page-number" href="/page/9/">9</a><a class="page-number" href="/page/10/">10</a><a class="extend next" rel="next" href="/page/8/">Next &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Advanced-Model/">Advanced Model</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/CS7313-Computational-Complexity/">CS7313: Computational Complexity</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Configuration/">Configuration</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Dive-Into-Deep-Learning/">Dive Into Deep Learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/GNN/">GNN</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Kernel-Method/">Kernel Method</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/MATH6005-Matrix-Theory/">MATH6005: Matrix Theory</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning-by-AndrewNg/">Machine Learning by AndrewNg</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Math/">Math</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Paper/">Paper</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Programming-Language/">Programming Language</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Tool/">Tool</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Attention-Mechanism/" rel="tag">Attention Mechanism</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CNN/" rel="tag">CNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Configuration/" rel="tag">Configuration</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Data-Analysis/" rel="tag">Data Analysis</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Dataset-Distillation/" rel="tag">Dataset Distillation</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Decision-Trees/" rel="tag">Decision Trees</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Deep-Learning/" rel="tag">Deep Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GCN/" rel="tag">GCN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GNN/" rel="tag">GNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Generative-AI/" rel="tag">Generative AI</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hexo/" rel="tag">Hexo</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Lab/" rel="tag">Lab</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux/" rel="tag">Linux</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-Learning/" rel="tag">Machine Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Markup-Language/" rel="tag">Markup Language</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Math/" rel="tag">Math</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Paper/" rel="tag">Paper</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/" rel="tag">Python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RNN/" rel="tag">RNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Recommender-System/" rel="tag">Recommender System</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Reinforcement-Learning/" rel="tag">Reinforcement Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SVM/" rel="tag">SVM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Supervised-Learning/" rel="tag">Supervised Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Theoretical-Computer-Science/" rel="tag">Theoretical Computer Science</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Tool/" rel="tag">Tool</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Unsupervised-Learning/" rel="tag">Unsupervised Learning</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Attention-Mechanism/" style="font-size: 11.82px;">Attention Mechanism</a> <a href="/tags/CNN/" style="font-size: 12.73px;">CNN</a> <a href="/tags/Configuration/" style="font-size: 10px;">Configuration</a> <a href="/tags/Data-Analysis/" style="font-size: 10px;">Data Analysis</a> <a href="/tags/Dataset-Distillation/" style="font-size: 13.64px;">Dataset Distillation</a> <a href="/tags/Decision-Trees/" style="font-size: 10px;">Decision Trees</a> <a href="/tags/Deep-Learning/" style="font-size: 20px;">Deep Learning</a> <a href="/tags/GCN/" style="font-size: 11.82px;">GCN</a> <a href="/tags/GNN/" style="font-size: 15.45px;">GNN</a> <a href="/tags/Generative-AI/" style="font-size: 10.91px;">Generative AI</a> <a href="/tags/Hexo/" style="font-size: 10.91px;">Hexo</a> <a href="/tags/Lab/" style="font-size: 10.91px;">Lab</a> <a href="/tags/Linux/" style="font-size: 16.36px;">Linux</a> <a href="/tags/Machine-Learning/" style="font-size: 17.27px;">Machine Learning</a> <a href="/tags/Markup-Language/" style="font-size: 10px;">Markup Language</a> <a href="/tags/Math/" style="font-size: 19.09px;">Math</a> <a href="/tags/Paper/" style="font-size: 14.55px;">Paper</a> <a href="/tags/Python/" style="font-size: 18.18px;">Python</a> <a href="/tags/RNN/" style="font-size: 10.91px;">RNN</a> <a href="/tags/Recommender-System/" style="font-size: 10.91px;">Recommender System</a> <a href="/tags/Reinforcement-Learning/" style="font-size: 10.91px;">Reinforcement Learning</a> <a href="/tags/SVM/" style="font-size: 10px;">SVM</a> <a href="/tags/Supervised-Learning/" style="font-size: 11.82px;">Supervised Learning</a> <a href="/tags/Theoretical-Computer-Science/" style="font-size: 15.45px;">Theoretical Computer Science</a> <a href="/tags/Tool/" style="font-size: 18.18px;">Tool</a> <a href="/tags/Unsupervised-Learning/" style="font-size: 11.82px;">Unsupervised Learning</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/06/">June 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/01/">January 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/12/">December 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/06/">June 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/05/">May 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/04/">April 2023</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2024/06/17/Diffusion/">Diffusion</a>
          </li>
        
          <li>
            <a href="/2024/01/20/VAE/">VAE</a>
          </li>
        
          <li>
            <a href="/2023/12/02/CountingComplexity/">Computational Complexity: Complexity of Counting</a>
          </li>
        
          <li>
            <a href="/2023/11/24/MatrixTheory5/">MatrixTheory: ç‰¹æ®ŠçŸ©é˜µä¸ŽçŸ©é˜µåˆ†è§£</a>
          </li>
        
          <li>
            <a href="/2023/11/13/RandomizedComputation/">Computational Complexity: Randomized Computation</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 ChaosTsang<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/" class="mobile-nav-link">home</a>
  
    <a href="/about/" class="mobile-nav-link">about</a>
  
    <a href="/tags/" class="mobile-nav-link">tags</a>
  
    <a href="/categories/" class="mobile-nav-link">categories</a>
  
    <a href="/archives/" class="mobile-nav-link">archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>





<script src="/js/script.js"></script>





  </div>
</body>
</html>