<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=[object Object]"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', '[object Object]');
</script>
<!-- End Google Analytics -->

  
  <title>JourneyToCoding</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="Start of Something New">
<meta property="og:type" content="website">
<meta property="og:title" content="JourneyToCoding">
<meta property="og:url" content="https://zclzcl0223.github.io/page/2/index.html">
<meta property="og:site_name" content="JourneyToCoding">
<meta property="og:description" content="Start of Something New">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="ChaosTsang">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="JourneyToCoding" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/%5Bobject%20Object%5D">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">JourneyToCoding</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">Code for fun</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/">home</a>
        
          <a class="main-nav-link" href="/about/">about</a>
        
          <a class="main-nav-link" href="/tags/">tags</a>
        
          <a class="main-nav-link" href="/categories/">categories</a>
        
          <a class="main-nav-link" href="/archives/">archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://zclzcl0223.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-ComputationalComplexityNPComp" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/10/10/ComputationalComplexityNPComp/" class="article-date">
  <time class="dt-published" datetime="2023-10-10T05:53:02.000Z" itemprop="datePublished">2023-10-10</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/CS7313-Computational-Complexity/">CS7313: Computational Complexity</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/10/10/ComputationalComplexityNPComp/">Computational Complexity NP-Completeness</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <span id="more"></span>

<h1 id="textbf-NP-Completeness"><a href="#textbf-NP-Completeness" class="headerlink" title="$\textbf{NP}-$Completeness"></a>$\textbf{NP}-$Completeness</h1><p>判定一个问题是否是NP问题，需要证据与验证，即：</p>
<blockquote>
<p>当存在一个多项式$p: \textbf{N}\to \textbf{N}$和一个$\text{P}-\text{time}$图灵机$\mathbb{M}$，使得对于语言$L$以及$x \in \{0,1\} ^*$：<br>$$<br>x \in L \space\text{iff}\space \exists u\in \{0,1\} ^{p(|x|)},\mathbb{M}(x,u)&#x3D;1<br>$$<br>则称$L$是一个$\textbf{NP}$问题，并称这个图灵机$\mathbb{M}$为一个验证器。</p>
</blockquote>
<p>上面的定理中，$u$就称为能够证明$L$是一个$\textbf{NP}$问题的“证据”。该定理说明，若某个问题是$\textbf{NP}$问题，则一定存在一个确定图灵机$\mathbb{M}$，它能通过不超过多项式长度的证据判定该问题在$\textbf{NP}$中。</p>
<blockquote>
<p>事实上，$u$就是非确定图灵机$\mathbb{N}$正确判断$x$的一条路径。因为要验证的是$\textbf{NP}$，所以要求路径长度（即$u$的长度）为多项式。</p>
</blockquote>
<h2 id="Karp-Reduction"><a href="#Karp-Reduction" class="headerlink" title="Karp Reduction"></a>Karp Reduction</h2><p>Karp Reduction即为多项式时间归约，其定义为：</p>
<blockquote>
<p>若存在一个多项式时间的可计算函数$r:\{0,1\} ^* \to \{0,1\} ^*$，使得对所有的$x \in \{0,1\} ^*$，<br>$$<br>x \in L \space \text{iff} \space r(x) \in L &#39;<br>$$<br>则称$L$可多项式归约到$L&#39;$，记为$L \le _K L&#39;$。</p>
</blockquote>
<p>多项式归约也具有传递性。</p>
<ul>
<li>如果$L \in \text{NP}$且$L \le _K L&#39;$，则称$L&#39;$是$\textbf{NP}$难的；</li>
<li>若$L&#39; \in \textbf{NP}$，则称$L &#39;$是$\textbf{NP}$完全的。</li>
</ul>
<p>若P &#x3D; NP，则EXP &#x3D; NEXP。底层相等，上层也相等。</p>
<h1 id="Propositional-Logic：命题逻辑"><a href="#Propositional-Logic：命题逻辑" class="headerlink" title="Propositional Logic：命题逻辑"></a>Propositional Logic：命题逻辑</h1><p>命题公式，即一个逻辑公式，是由真假值（0&#x2F;1）、命题变量和逻辑操作子通过归纳构造的。一个命题变量$x$是一个布尔量，也称作“字（Literal）”，其否定形式为$\overline{x}$。逻辑运算符包括$\lor$（或、析取），$\land$（与、合取）。若干个“字”的析取称<em>语句</em>，若干个“字”的合取称<em>单项式</em>。</p>
<ul>
<li>合取范式（Conjunctive Normal Form，CNF）：若干语句的合取，记作$\lor _{i}(\land _j v _{ij})$。</li>
<li>析取范式（Disjunctive Normal Form，DNF）：若干单向式的析取，记作$\land _{i}(\lor _j v _{ij})$。</li>
</ul>
<p>对于某一公式（合取或析取，记作$\phi$），若存在$v _{ij}$的某种真值指派，使得$\phi$为真，则称该公式是可满足的（Satisfiable）。对于所有可满足的合取范式组成的集合$\textbf{SAT}$，存在如下定理：</p>
<p>$$<br>\textbf{SAT} \in \textbf{NP}<br>$$</p>
<p>$k\textbf{SAT}$是指每个语句只含有$k$个字的合取范式。对于$k\textbf{SAT}$，有如下结论：</p>
<ol>
<li>$2\textbf{SAT}\in \textbf{P}$</li>
<li>$\textbf{SAT}\le _K 3\textbf{SAT}$</li>
</ol>
<p>结论2表明，$k$（$k \ge 3$）$\textbf{SAT}$可由$3\textbf{SAT}$表示。</p>
<p>布尔函数$f:\{0,1\} ^\ell \to \{0,1\}$将$\ell$长的$01$串映射为一个布尔值，它可以被表示为一个合取范式，且该合取范式的大小不会超过$\ell 2 ^\ell$。</p>
<h2 id="Cook-Levin-Theorem"><a href="#Cook-Levin-Theorem" class="headerlink" title="Cook-Levin Theorem"></a>Cook-Levin Theorem</h2><p>定理：$\textbf{SAT}$是$\textbf{NP}$完全的。该定理证明如下：</p>
<blockquote>
<p>由于$\textbf{SAT}\in \textbf{NP}$，故要证上述定理成立，只需要证明$\forall L \in \textbf{NP}$，$L \le _K \textbf{SAT}$，即证明$x \in L$当且仅当$\phi(x) \in \textbf{SAT}$，又$x \in L$等价于$\exists u \in \{0,1\} ^{ ^{p (|x|)}},\mathbb{M}(x,u)&#x3D;1$，所以即证明：<br>$$<br>\phi(x) \in \textbf{SAT}\space \text{iff}\space \exists u \in \{0,1\} ^{ ^{p (|x|)}},\mathbb{M}(x,u)&#x3D;1<br>$$<br>要证明上式，即证明<strong>图灵机的计算过程可以一个合取范式表示</strong>。</p>
<p>由于任何多带的图灵机都可以单带图灵机在多项式的代价下模拟，所以此处只考虑一条带子。则对于图灵机的整个计算过程，其产生的所有格局中的每一个格子、带头是否在该格子、当前的状态可用$1+\log \Gamma +\log Q$个比特编码，记其长度为$\ell$。对于格局$C _{i+1}$和其上一个格局$C _{i}$，$C _{i + 1}$的每一个格子的值只会受到其正上方、左上方、右上方三个$C _{i}$格子的影响。所以，对于$C _i \to C _{i+1}$每一个格子的转移函数可以表示为$f:\{0,1\} ^{3\ell}\to \{0,1\} ^\ell$，其等价于$f:\{0,1\} ^{4\ell} \to \{0,1\}$，转换为合取范式，其大小为$4\ell 2 ^{4 \ell}$，由于$\ell$为常量，所以前式为常数。前后格局所有格子的转换都可用上式表示，记该转移函数为$C$。终止格局数为多项式，格局的大小也是多项式，所以$x \to \phi(x)$的转换是多项式的。证毕。</p>
</blockquote>
<p>此外，计算$C _{ij}$的空间是对数的，只需要记录$i$和$j$即可。</p>
<h3 id="性质"><a href="#性质" class="headerlink" title="性质"></a>性质</h3><ol>
<li>库克-莱文归约是个<em>莱文归约</em>（Levin Reduction），即使得$\phi(x)$成立的一组真值指派$u$就是使得$\mathbb{M}(x,u)&#x3D;1$的证据$u$。</li>
<li>归约是对数空间的。</li>
</ol>
<p>判定问题有多项式时间算法，则相应的计算问题也有。</p>
<blockquote>
<p>判定（Decision）和计算（Search）问题在是否有高效算法上是等价的，即若我们能在多项式时间内判定某个问题，我们就能在多项式时间内计算其对应的计算问题。</p>
</blockquote>
<h1 id="Karp-Theorem"><a href="#Karp-Theorem" class="headerlink" title="Karp Theorem"></a>Karp Theorem</h1><p>Karp证明了21个NP完全问题，如$\textbf{THEOREM}$——能否在多项式的公式长度的行内证明某公式是NP完全的。</p>
<h2 id="Berman-Hartmanis-Conjecture"><a href="#Berman-Hartmanis-Conjecture" class="headerlink" title="Berman-Hartmanis Conjecture"></a>Berman-Hartmanis Conjecture</h2><p>猜测（Conjecture）：任何的NP完全问题都是多项式同构的，即只有一个NP完全问题。若该猜测为真，则$\textbf{P} \neq \textbf{NP}$。</p>
<blockquote>
<p>多项式同构：$A \le _K ^1 B$表存在从$A$到$B$的单射Karp归约，若$A \le _K ^1 B \le _K ^1 A$，则称$A$和$B$是多项式同构的，即$A \simeq _P B$</p>
</blockquote>
<p>对于集合$S \subseteq \{0,1\} ^* $，若$S$内部长度不超过$n$的串有指数个，即$|S ^{\le n}|&#x3D; 2 ^{n ^{O(1)}}$，则称该集合是稠密的；反之若只有多项式个，即$|S ^{\le n}|&#x3D; {n ^{O(1)}}$，则称该集合是稀疏的。$\textbf{SAT}$是稠密的，而$\textbf{P}$类问题是稀疏的，稠密集合不可能和稀疏集合同构，因为大的映射小的或小的映射大的不可能是单射的，所以$\textbf{SAT} \ne \textbf{P}$。</p>
<h1 id="Ladner-Theorem"><a href="#Ladner-Theorem" class="headerlink" title="Ladner Theorem"></a>Ladner Theorem</h1><p>定理：若$\textbf{NP} \neq \textbf{P}$，则存在介于$\textbf{P}$和$\textbf{NP}$之间的语言。</p>
<p>证明的基本思路：从$\textbf{SAT}$中排除一些元素，但不至于退化为$2\textbf{SAT}$，则新的语言就介于$\textbf{NP}$和$\textbf{P}$之间。记该新语言为$\textbf{SAT} _H$。</p>
<p>$$<br>\textbf{SAT} _H(x)&#x3D;<br>\begin{cases}<br>    1, &amp;x \in \textbf{SAT} _H\\<br>    0, &amp;x \notin \textbf{SAT} _H<br>\end{cases}<br>$$</p>
<p>$$<br>H(n)&#x3D;<br>\begin{cases}<br>    i, &amp;\text{对}|x|\le \log n\text{可在}i |x| ^i\text{时间内判定}\textbf{SAT} _H(x)\text{的最小图灵机下标}\\<br>    \log \log n, &amp;\text{otherwise}<br>\end{cases}<br>$$</p>
<p>$H(n)$是单调不减的，其时间复杂度为：</p>
<p>$$<br>T(n)&#x3D;(\log\log n)2 ^{\log n}(c\cdot C\log C+ 2 ^{\log n}+ T(\log n)+...)<br>$$</p>
<ul>
<li>$\log\log n$是因为我们要枚举所有的$i &lt; \log \log n$以找到下标最小的图灵机；</li>
<li>$2 ^{\log n}$是因为对于每一次枚举，我们都要验证所有长度小于$\log n$的序列，共有指数个；</li>
<li>$c\cdot C\log C$是用通用图灵机模拟$\mathbb{M} _i(x)$计算$i |x| ^i$步的时间；</li>
<li>$2 ^{\log n}$是暴力计算$\textbf{SAT} _H(x)$的时间；</li>
<li>后面这两项需要是因为我们要将模拟结果与$textbf{SAT} _H(x)$的真实值进行比较；</li>
<li>$T(\log n)$是因为在计算$\textbf{SAT} _H(x)$时还要计算$H(|\psi|)$，而$\psi$的长度为$\log n$。</li>
<li>可以算得$T(n) &#x3D; o(n ^3)$。</li>
</ul>
<h2 id="H-n-的性质"><a href="#H-n-的性质" class="headerlink" title="$H(n)$的性质"></a>$H(n)$的性质</h2><ol>
<li>如果$H(n)$是有限的，由于$H(n)$是非递减的，$H(n)$最终会收敛到$i$，即$\exists n \ge N$，使$H(n)&#x3D;i$恒成立。于是$\exists$图灵机$\mathbb{M}_i$，它在多项式时间$i|x| ^i$内判定$\textbf{SAT} _H$。</li>
<li>$\textbf{SAT} _H \in \textbf{P}\space \space \text{iff} \space \space H(n)$是有限的。右到左即性质1，左到右：</li>
</ol>
<blockquote>
<p>设图灵机$\mathbb{M} _i$在$c n ^c$内判定$\textbf{SAT}_H$。由于图灵机编码的任意性，我们可以取$i \ge c$，而$H(n)$输出的是图灵机的最小编码，所以一定有$H(n) \le i$。</p>
</blockquote>
<h2 id="两个方向的证明"><a href="#两个方向的证明" class="headerlink" title="两个方向的证明"></a>两个方向的证明</h2><p>推论1：如果$\textbf{P} \ne \textbf{NP}$，则$\textbf{SAT} _H \notin \textbf{P}$。</p>
<blockquote>
<p>因为若$\textbf{SAT} _H \in \textbf{P}$，则由$\textbf{SAT} _H$的定义，$\textbf{SAT}\le _K \textbf{SAT} _H$。</p>
</blockquote>
<p>推论2：如果$\textbf{P} \ne \textbf{NP}$，则$\textbf{SAT} _H$不是NP完全的。</p>
<blockquote>
<p>不懂</p>
</blockquote>
<h1 id="Baker-Gill-Solovay-Theorem"><a href="#Baker-Gill-Solovay-Theorem" class="headerlink" title="Baker-Gill-Solovay Theorem"></a>Baker-Gill-Solovay Theorem</h1><p>定理：存在$A$使得$\textbf{P} ^A &#x3D; \textbf{NP} ^A$，也存在$B$$\textbf{P} ^B \ne \textbf{NP} ^B$</p>
<blockquote>
<p>对前项的证明：<br>$$<br>\textbf{PSPACE}&#x3D;\textbf{P} ^{\textbf{PSPACE}} \subset \textbf{NP} ^{\textbf{PSPACE}} &#x3D; \textbf{PSPACE}<br>$$<br>于是$\textbf{P} ^{\textbf{PSPACE}} &#x3D; \textbf{NP} ^{\textbf{PSPACE}}$。该项说明，若$A$的表达能力足够强，则$\textbf{P} ^A$会与$\textbf{NP} ^A$的表达能力相当。</p>
<p>对后项的证明：<br>令$B _0 &#x3D; \{\}$，$n _0 &#x3D; 0$，并使得$B _{i + 1}$和$B _i$存在如下递推关系：</p>
<ul>
<li>让图灵机$\mathbb{M} _i ^{B _i}$在问题$1 ^{n _i + 1}$上运行$2 ^{n _{i+1}-1}$步；</li>
<li>若不停机，则令$B _{i+1} &#x3D; B _i$；</li>
<li>若接受，则令$B _{i+1} &#x3D; B _i$；</li>
<li>若拒绝，则令$B _{i+1} &#x3D; B _i \cup \{s\}$，其中$s$是长度为$n _{i+1}$的字符串，且在图灵机运行的过程中没有被用于问子程序$B _i$（因为图灵机只运行$2 ^{n _{i+1}-1}$步，不能枚举完所有的长度为$n _{i+1}$的串）。</li>
</ul>
<p>此外，还规定$n _{i+1}$是严格递增的，且必须大于所有的会被作为图灵机$\mathbb{M} _0 ^{B _0}(x _1)...\mathbb{M} _{i-1} ^{b _{i-1}}(x _i)$的输入串$x$。<br>最终，$B$就定义为$B &#x3D; \bigcup _{i \in \textbf{N}} B _i$。</p>
<p>再定义$U _B &#x3D; \{1 ^n | B \text{包含长度为}n\text{的串}\}$，即，若$B$包含长度为$n$的串，则$U _B$包含长度为$n$的全1串。对于任意的全1串，我们可证$U _B \in \textbf{NP} ^B,U _B \notin \textbf{P} ^B$。左边易证，我们只需枚举所有长度为$n$的字符串并询问$B$即可，对于右边：</p>
<p>假设存在$\mathbb{M} _i ^B$在多项式时间判定$U _B$，则，如果：</p>
<ul>
<li>$\mathbb{M} _i ^B(1 ^{n+1})&#x3D;0$，那么$B$中不包含长度为$n + 1$的字符串，而长度为$n + 1$的字符串只能在$B _{i+1} &#x3D; B _i \cup \{s\}$被加入到$B _{i+1}$中，所以$B _{i+1}$中不包含长度为$n+1$的串。</li>
</ul>
<p>由此我们可以推出矛盾：</p>
<ul>
<li>$\mathbb{M} _i ^{B _i}(1 ^{n+1})&#x3D;1\to B _{i+1} &#x3D; B _i \to\mathbb{M} _i ^{B _{i+1}}(1 ^{n+1})&#x3D;1 \to \mathbb{M} _i ^B(1 ^{n+1})&#x3D;1$</li>
</ul>
<p>证毕。</p>
</blockquote>
<h2 id="Oracle-Turing-Machine：神谕图灵机"><a href="#Oracle-Turing-Machine：神谕图灵机" class="headerlink" title="Oracle Turing Machine：神谕图灵机"></a>Oracle Turing Machine：神谕图灵机</h2><p>神谕图灵机$\mathbb{M} ^?$是一种特殊的图灵机，它有一条额外的读写带和新的三种状态$q _{text{query}}, q _{\text{yes}}, q _{\text{no}}$。</p>
<blockquote>
<p>这条额外的带子实际上就是“子程序”。“$?$”可被替换为一个实际的问题&#x2F;语言，如$B$。</p>
</blockquote>
<p>神谕图灵机通过进入$q _{\text{query}}$状态来询问某个字符串$b$是否属于$B$，这相当于一次“子程序”调用。如果$b \in B$，则进入$q _\text{yes}$状态；否则，进入$q _{\text{no}}$状态</p>
<blockquote>
<p>子程序调用只算作一次计算！</p>
</blockquote>
<p>对神谕图灵机的编码是对机器的编码，与神谕无关。</p>
<h2 id="Cook-Reduction"><a href="#Cook-Reduction" class="headerlink" title="Cook Reduction"></a>Cook Reduction</h2><p>如果存在一个多项式时间复杂度的神谕图灵机$\mathbb{M} ^?$，使得语言$L$可以被图灵机$\mathbb{M} ^{L&#39;}$判定，则称语言$L$可以Cook归约到$L&#39;$，记作$L \le _C L&#39;$。</p>
<h2 id="Lowness"><a href="#Lowness" class="headerlink" title="Lowness"></a>Lowness</h2><p>如果$\textbf{A} ^{\textbf{B}}&#x3D;\textbf{A}$，则称复杂性类$\textbf{B}$相对于复杂性类$\textbf{A}$是低的。</p>
<blockquote>
<p>换句话说，$\textbf{A}$调用$\textbf{B}$并不能增强自身判定问题的能力，这说明相对于$\textbf{A}$，$\textbf{B}$解决的问题是更简单的问题。</p>
</blockquote>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zclzcl0223.github.io/2023/10/10/ComputationalComplexityNPComp/" data-id="clzik0lbz000nv47k93w53brg" data-title="Computational Complexity NP-Completeness" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Math/" rel="tag">Math</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Theoretical-Computer-Science/" rel="tag">Theoretical Computer Science</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-ComputationalComplexitySpaceComp1" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/09/28/ComputationalComplexitySpaceComp1/" class="article-date">
  <time class="dt-published" datetime="2023-09-28T02:04:17.000Z" itemprop="datePublished">2023-09-28</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/CS7313-Computational-Complexity/">CS7313: Computational Complexity</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/09/28/ComputationalComplexitySpaceComp1/">Computational Complexity Space Complexity 1</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <span id="more"></span>

<h1 id="Space-Bounded-Computation：空间边界的计算"><a href="#Space-Bounded-Computation：空间边界的计算" class="headerlink" title="Space Bounded Computation：空间边界的计算"></a>Space Bounded Computation：空间边界的计算</h1><p>空间是计算资源，是可重用的。研究空间复杂度，只考虑只有一条带子的图灵机即可。</p>
<p>空间复杂性（函数）的定义：对函数$S: \mathbf{N}\to \mathbf{N}$和语言$L \in \{0,1\} ^ *$，如果存在常数$c$和图灵机$\mathbb{M}$能使用不超过$cS(n)$的非空工作带格子判定输入长度为$n$的$L$，就称$L \in \textbf{SPACE}(S(n))$。</p>
<blockquote>
<p>不计算输入所占用的格子。空间可构造和完全空间可构造完全类比时间可构造和完全时间可构造。</p>
</blockquote>
<p>对非确定图灵机，$L \in \textbf{NSPACE}((S(n)))$当且仅当每条路径上使用的非空工作带格子都不超过$cS(n)$。</p>
<h2 id="Configuration-Graph：格局图"><a href="#Configuration-Graph：格局图" class="headerlink" title="Configuration Graph：格局图"></a>Configuration Graph：格局图</h2><ul>
<li>$C _{\text{start}}$：初始格局；</li>
<li>$C _{\text{accept}}$：接受格局，图灵机中止且带子上的结果为1。</li>
</ul>
<p>格局图：一个图灵机$\mathbb{M}$的格局图$G _{\mathbb{M},x}$是一个有向图，它的结点是某个格局$C$，边是一次计算。若使用格局图，则$\mathbb{M}$接受$x$当且仅当在格局图$G _{\mathbb{M},x}$上存在从$C _{\text{start}}$到$C _{\text{accept}}$的路径。</p>
<h3 id="Size-of-Configuration-Graph：格局图的大小"><a href="#Size-of-Configuration-Graph：格局图的大小" class="headerlink" title="Size of Configuration Graph：格局图的大小"></a>Size of Configuration Graph：格局图的大小</h3><p>对于一个时间复杂度为$S(n)$的图灵机$\mathbb{M}$，由于只考虑一条工作带，因此，某个长度为$n$输入$x$，其某一步的格局可用$O(S(n))个$比特编码。因此，整个格局图的结点个数就是$2 ^{O(S(n))}$。</p>
<p>当我们想要判断某两个结点间是否存在边时，我们只需要判断某两个格局$C _1$和$C _2$是否存在一步到达的关系，即看能否通过仅修改某个格局$C _1$编码的$1$个bit和表示读写头的几个bit，就让$C _1&#x3D;C _2$。不难看出，该算法的时间复杂度是$O(S(n))$，空间复杂度是$O(\log S(n))$，因为我们只需要按位比较，并记录位置的编码即可，而$O(S(n))$个位置可用$O(\log S(n))$的二进制串表示。</p>
<blockquote>
<p>但要求$C _1$和$C _2$交叉输入，否则带头的来回移动会让时间复杂度为平方。</p>
</blockquote>
<h2 id="Space-vs-Time"><a href="#Space-vs-Time" class="headerlink" title="Space vs Time"></a>Space vs Time</h2><p>设空间函数$S(n)$是空间可构造的，则：</p>
<p>$$<br>\textbf{TIME}(S(n))\subseteq \textbf{SPACE}(S(n))\subseteq \textbf{NSPACE}(S(n))\subseteq \textbf{TIME}(2 ^{O(S(n))})<br>$$</p>
<blockquote>
<p>第一个包含关系：一步最多多用一个格子；<br>第二个包含关系：确定是非确定的特例；<br>第三个包含关系：判定一个非确定图灵机能否在$O(S(n))$的空间内判定每个问题，等价于验证格局图中是否存在由$C _\text{start}$到$C _{\text{accept}}$的路径，而这可通过BFS在多项式时间复杂度内完成。格局图中总结点数为$2 ^{O(S(n))}$，因此第三个包含关系成立。</p>
</blockquote>
<p>注意：以上相互包含的都是<strong>问题集合</strong>，如第一个包含关系意为：时间复杂度为$S(n)$的问题是空间复杂度为$S(n)$问题的子集。</p>
<blockquote>
<p>对所有的空间可构造的$S(n)$，$\textbf{TIME}(S(n))\subseteq \textbf{SPACE}(S(n)&#x2F; \log S(n))$</p>
</blockquote>
<h2 id="Space-Complexity-Class"><a href="#Space-Complexity-Class" class="headerlink" title="Space Complexity Class"></a>Space Complexity Class</h2><ul>
<li>$\textbf{L}$类：$\textbf{SPACE}(\log (n))$</li>
<li>$\textbf{NL}$类：$\textbf{NSPACE}(\log (n))$</li>
<li>$\textbf{PSPACE}$类：$\bigcup _{c&gt;0} \textbf{SPACE}(n ^c)$</li>
<li>$\textbf{NPSPACE}$类：$\bigcup _{c&gt;0} \textbf{NSPACE}(n ^c)$</li>
</ul>
<p>$$<br>\textbf{NP}\subseteq \textbf{PSPACE}<br>$$</p>
<p>上式成立是因为空间是可以重复利用的，故我们可以用<strong>相同</strong>的空间在<em>多项式时间内</em>（<em>空间</em>）验证<strong>不同</strong>的路径是否成立。</p>
<h3 id="PATH-is-in-NL"><a href="#PATH-is-in-NL" class="headerlink" title="PATH is in NL"></a>PATH is in NL</h3><p>$\text{PATH}&#x3D;\{&lt;G,s,t&gt;\}$定义为判断在图$G$中是否存在从$s$到$t$的路径。</p>
<p>$$\text{PATH}\in \textbf{NL}$$</p>
<p>这是因为：在有$n$个结点的图中，不考虑重复边，$s$到$t$的最长路径长度为$n-1$。因此，我们总可以不断地从$s$做$n-1$次猜测，每次猜测选择一个邻居，走到邻居，并继续猜测。总会有那么一个猜测路径，使得我们可以到达$t$（若$s$和$t$之间确实存在路径）。这一过程中，我们需要记录当前结点，而当前结点可用$\log n$的空间编码，还要记录步数，而步数可用$\log (n-1)$的空间编码，又因为空间是可以重用的，所以总的空间复杂度为$O(\log n)$。</p>
<blockquote>
<p>对空间复杂度为$S(n)$的图灵机，通用图灵机模拟它的空间复杂度为$O(S(n))$。</p>
</blockquote>
<h2 id="Space-Hierarchy-Theorem：空间谱系定理"><a href="#Space-Hierarchy-Theorem：空间谱系定理" class="headerlink" title="Space Hierarchy Theorem：空间谱系定理"></a>Space Hierarchy Theorem：空间谱系定理</h2><p>若$f$和$g$都是空间可构造的，且$f(n)&#x3D;o(g(n))$，则：</p>
<p>$$<br>\textbf{SPACE}(f(n)) \subsetneq \textbf{SPACE}(g(n))<br>$$</p>
<blockquote>
<p>证明类似时间谱系定理。</p>
</blockquote>
<h1 id="Logspace-Reduction：对数空间归约"><a href="#Logspace-Reduction：对数空间归约" class="headerlink" title="Logspace Reduction：对数空间归约"></a>Logspace Reduction：对数空间归约</h1><p>隐式对数空间可计算（Implicitly Logspacec Computable）函数$f:\{0,1\} ^ *\to \{0,1\} ^ *$满足如下三个性质：</p>
<ol>
<li>输出长度有限：$\exists c,\forall x,|f(x)|\le c |x| ^c$</li>
<li>输出长度可在对数空间内计算：$\{&lt;x,i&gt;|i \le |f(x)|\}\in L$</li>
<li>输出结果可确定：$\{&lt;x,i&gt;|f(x) _i &#x3D; 1\}\in L$</li>
</ol>
<p>以上三个性质的推断都依赖于前一个性质。如果存在这么一个函数$f$，使得对于问题$B$和问题$C$，$x\in B$当且仅当$f(x) \in C$，那么我们称$B$是可在对数空间归约于$C$的，即$B \le _L C$。</p>
<blockquote>
<p>$f(x)$作为$C$的输入，是不能被直接写到$C$的输入带上的，因为$f(x)$的长度是多项式的。不过，$f(x)$的三个性质使得图灵机可以在对数空间内计算出$f(x)$任意位置的值，由于空间的可重用性，输入$f(x)$实际只用了对数空间（每次一位一位地输入）。</p>
<p>$B$对数空间归约到$C$，说明在<strong>不消耗更多空间资源</strong>的情况下，$C$是更复杂的问题。</p>
<p>对数空间归约具有传递性，即，若$B \le _L C$，$C \le _L D$，则$B \le _L D$。证明：</p>
<ul>
<li>$B \le _L C$，所以对$x \in B$，可在对数空间$O(\log |x|)$内计算$f(x)$。</li>
<li>$C \le _L D$，所以对$f(x) \in C$，可在对数空间$O(\log |f(x)|)$内计算$g(f(x))$。</li>
<li>又由于空间的可重复利用性，用于计算$f(x)$的对数空间可被重复用于计算$g(f(x))$，因此$O(\log |f(x)|)&#x3D;O(\log |x|)$。</li>
</ul>
</blockquote>
<h1 id="text-PSPACE-Completeness"><a href="#text-PSPACE-Completeness" class="headerlink" title="$\text{PSPACE}$ Completeness"></a>$\text{PSPACE}$ Completeness</h1><p>空间完备性（Space Completeness）：如果$\forall L \in \textbf{PSPACE}$，$L \le _L L&#39;$，则称语言$L&#39;$是$\textbf{PSPACE}-\text{hard}$的。如果进一步地，$L &#39; \in \textbf{PSPACE}$，则称语言$L&#39;$是$\textbf{PSPACE}-\text{complete}$的。某个完备的（Complete）语言是这个空间复杂性类中最难的语言&#x2F;问题。</p>
<h2 id="Quantified-Boolen-Formula"><a href="#Quantified-Boolen-Formula" class="headerlink" title="Quantified Boolen Formula"></a>Quantified Boolen Formula</h2><p>量化布尔公式（Quantified Boolen Formula，QBF）被定义为这样的式子：</p>
<p>$$<br>Q _1 x _1 Q _2 x _2...Q _n x _n.\space \phi(x _1,...,x _n)<br>$$</p>
<p>其中，$Q _i$是量词（Quantifier），要么是$\forall$，要么是$\exists$；$x _i$是布尔值，要么是$0$，要么是$1$；$\phi$是不包含$x _1,...,x _n$以外的任何自由变量的布尔公式，该公式要么为$\text{True}$，要么为$\text{False}$，如：</p>
<p>$$<br>\forall x,\exists y,x&#x3D;y<br>$$</p>
<p>显然，上述公式值为$True$。</p>
<p>空间复杂度两个原则：格局图、深度优先</p>
<h2 id="Stockmeyer-Meyer-Theorem"><a href="#Stockmeyer-Meyer-Theorem" class="headerlink" title="Stockmeyer-Meyer Theorem"></a>Stockmeyer-Meyer Theorem</h2><p>定理：TQBF是$\textbf{PSPACE}-\text{complete}$的。定理中，$\text{TQBF}$是所有值为$\text{True}$的QBF集合。定理的证明如下：</p>
<blockquote>
<p>要证明上述定理，即要证明$\forall L \in \textbf{PSPACE}$，$L \le _L $TQBF，且TQBF$\in \textbf{PSPACE}$。</p>
<p>1.TQBF$\in \textbf{PSPACE}$</p>
<p>假定某个QBF为$\psi &#x3D; Q _1 x _1 Q _2 x _2...Q _n x _n.\space \phi(x _1,...,x _n)$，由于$x _i$只有两种取值可能，因此我们可以将$\psi$所有可能的形式表示为一棵二叉树：<br><img src="/2023/09/28/ComputationalComplexitySpaceComp1/1.png" alt="1"></p>
<center style="font-size:12px;font-weight:bold">Fig. 1. QBF</center><br>

<p>对该二叉树使用一次深度优先遍历（实际上应该使用<strong>后序遍历</strong>）即可判定该问题。DFS过程中，我们可以自下而上求出所有非叶子结点的值，最终根结点的值就是QBF的判定结果，整个过程中，我们最多需要记录$n$个祖先，因而该算法的空间复杂度实际上为$O(n)$。因此TQBF$\in \textbf{PSPACE}$。证毕。</p>
<p>2.$\forall L \in \textbf{PSPACE}$，$L \le _L $TQBF</p>
<p>$L \in \textbf{PSPACE}$说明存在图灵机$\mathbb{M}$，对输入$x \in L$，它能在多项式空间内输出$\mathbb{M}(x)&#x3D;1$以及格局图中存在$C _{\text{start}}$到$C _{\text{accept}}$的路径。</p>
<p>要证明条件2，我们只需要证明对$x \in L$，存在$\phi(x)\in $TQBF。不妨令$\phi(x)&#x3D;\psi _i (C,C&#39;)$，其中，当格局$C$和$C&#39;$之间存在长度不超过$2 ^i$的路径时$\psi _i (C,C&#39;)&#x3D;1$。</p>
<p>对于$\psi _i (C,C&#39;)$，若其值为$1$，则在$C$和$C&#39;$之间一定存在一个格局$C&#39;&#39;$，它到这两个格局的路径长度不超过$2 ^{i - 1}$，因此$\psi _i (C,C&#39;)$就可以被转换为$\psi _{i-1} (C,C&#39;&#39;) \lor  \psi _{i-1} (C&#39;,C&#39;&#39;)$。该问题可被表示为：<br>$$<br>\exists C&#39;&#39; \forall D ^1 \forall D ^2.((D ^1&#x3D;C \land D ^2 &#x3D;C&#39;))\lor ((D ^1&#x3D;C&#39;&#39; \land D ^2 &#x3D;C)) \rightarrow \psi _{i-1}(D ^1, D ^2)<br>$$<br>只要我们能够判断上面前蕴含后的关系成立，则$\psi _i (C,C&#39;)&#x3D;1$成立。判别上式的空间复杂度为$|\psi _i|&#x3D;|\psi _{i-1}(D ^1, D ^2)|+cS(|x|)$，其中，后一项是用于记录箭头前的格局的空间（每个格局空间复杂度$S(|x|)$）。那么对于空间复杂度为$S(|x|)$的$L$，它的格局图中最多有$2 ^{S(|x|)}$个结点，于是判断$L$，就可以转为判断$\psi _{S(|x|)}$，即判断QBF$\phi (x)$。$|\phi (x)|&#x3D;|\psi _{S(|x|)}|&#x3D;O(S(|x|) ^2)$，多项式的平方仍然是多项式，所以$x\in L$，$\phi (x) \in $TQBF，证毕。前面的式子能成立是因为最终$\psi _{S(|x|)}$会被转化为$\psi _{1}$，期间会多出$S(|x|)$个$c S(|x|)$。</p>
</blockquote>
<h1 id="Savitch-Theorem"><a href="#Savitch-Theorem" class="headerlink" title="Savitch Theorem"></a>Savitch Theorem</h1><p>定理：如果$S$是空间可构造的，那么$\textbf{NSPACE}(S(n)) \subseteq \textbf{SPACE}(S(n) ^2)$。证明如下：</p>
<blockquote>
<p>该定理的证明与上面的条件2证明几乎一致。对于空间复杂度为$S(n)$的非确定图灵机$\mathbb{N}$，它可以在$S(n)$的空间内判定$L$，其格局图的大小为$2 ^{S(n)}$。很自然地，判定$L$可以被转换为判定格局图中是否存在$C _{\text{start}}$到$C _{\text{accept}}$的路径。该问题可以被$\phi (x)$在$c S(n) ^2$空间内解决。所以$\textbf{NSPACE}(S(n)) \subseteq \textbf{SPACE}(S(n) ^2)$。证毕。</p>
</blockquote>
<p>由于多项式的平方还是多项式，所以由上述定理，我们可以得到：</p>
<p>$$<br>\textbf{NPSPACE} \subseteq\textbf{PSPACE}<br>$$</p>
<p>又因为确定图灵机是非确定图灵机的特例：</p>
<p>$$<br>\textbf{PSPACE} \subseteq \textbf{NPSPACE}<br>$$</p>
<p>所以：</p>
<p>$$<br>\textbf{PSPACE} &#x3D; \textbf{NPSPACE}<br>$$</p>
<h1 id="NL-Completeness"><a href="#NL-Completeness" class="headerlink" title="NL Completeness"></a>NL Completeness</h1><p>如果$C \in \textbf{NL}$，且对$\forall B \in \textbf{NL}$，$B \le _L C$，则$C$是$\textbf{NL}-$ complete。</p>
<p>定理：$\text{PATH}$是$\textbf{NL}$完全的。证明如下：</p>
<blockquote>
<p>由于前面已经证明$\text{PATH} \in \textbf{NL}$，所以要证明上述结论，即要证明对$\forall L \in \textbf{NL}$，$L \le _L \text{PATH}$，即要证明$\forall x \in L$，$\exists f$，$f(x) \in \text{PATH}$。</p>
<p>与前面的证明类似，对在$\log(n)$空间内判定$L$的非确定图灵机$\mathbb{N}$，设其输入为$x$，则该问题可以被转换为在格局图$G _{\mathbb{N},x}$中判断$C _\text{start}$和$C _\text{accept}$间是否存在路径，即：<br>$$<br>x \to &lt; G _{\mathbb{N},x},C _\text{start},C _\text{accept} &gt;<br>$$<br>由于$\mathbb{N}$的空间复杂度是$\log (n)$，所以格局图中的每一个格局都可以用$\log (n)$的bit编码，且共有多项式个格局（顶点）。用邻接矩阵$A$表示格局图$G _{\mathbb{N},x}$的结构关系，由于各个顶点的编码长度为$\log (n)$，所以任意两个顶点的一步可达性可在对数空间内判定，又由于空间的可重复利用性，整个邻接矩阵可以在对数空间内被构造出来，而起始格局和终止格局都是固定的$01$串，所以$&lt; G _{\mathbb{N},x},C _\text{start},C _\text{accept} &gt;$可以在对数空间内被构造出来，而$&lt; G _{\mathbb{N},x},C _\text{start},C _\text{accept} &gt; \in \text{PATH}$，所以$L \le _L \text{PATH}$。证毕。</p>
</blockquote>
<p>有向无环图的$\text{PATH}$问题也是$\textbf{NL}$完全的。</p>
<h1 id="Immerman-Szelepcsenyi-Theorem"><a href="#Immerman-Szelepcsenyi-Theorem" class="headerlink" title="Immerman-Szelepcsenyi Theorem"></a>Immerman-Szelepcsenyi Theorem</h1><p>补类（补问题）：一个类的补类指在所有类中刨去改类后剩下的类，如，对$\textbf{T}$类，其补类记为$\text{co}\textbf{T}$或$\overline{\textbf{T}}$，定义为：</p>
<p>$$<br>\text{co}\textbf{T}&#x3D;\overline{\textbf{T}}&#x3D;\{\{0,1 ^*\}-L \in \textbf{T}\}<br>$$</p>
<p>一个有趣的结论：</p>
<p>$$<br>\begin{align*}<br>    \text{co}\textbf{NPSPACE}<br>    &amp;&#x3D;\overline{\textbf{NPSPACE}} \\<br>    &amp;&#x3D;\overline{\textbf{PSPACE}}\\<br>    &amp;&#x3D;\textbf{PSPACE}\\<br>    &amp;&#x3D;\textbf{NPSPACE}<br>\end{align*}<br>$$</p>
<p>定理：$\overline{\text{PATH}}\in \textbf{NL}$。$\overline{\text{PATH}}$的含义为：从$A$出发到不了$B$。的证明如下：</p>
<blockquote>
<p>???????</p>
</blockquote>
<p>由上述定理，我们可以得到$\overline{\textbf{NL}}&#x3D; \textbf{NL}$：</p>
<blockquote>
<p>$A \le _L B$，则$\overline{A} \le _L \overline{B}$</p>
<p>因为$L \le _L \text{PATH}$，所以$\overline{L} \le _L \overline{\text{PATH}}$，所以$\overline{\text{PATH}}$是$\overline{\textbf{NL}}$完全的，而$\overline{\text{PATH}}\in \textbf{NL}$，所以$\overline{\textbf{NL}}&#x3D; \textbf{NL}$。</p>
</blockquote>
<p>事实上，对$\forall S(n) \ge \log (n)$，若$S(n)$时间可构造，则：</p>
<p>$$<br>\overline{\textbf{NSPACE}(S(n))}&#x3D;\textbf{NSPACE}(S(n))<br>$$</p>
<p>$$<br>\textbf{L}\subseteq \textbf{NL}\subseteq \textbf{P}\subseteq \textbf{NP}\subseteq \textbf{PSPACE}\subseteq \textbf{EXP}<br>$$</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zclzcl0223.github.io/2023/09/28/ComputationalComplexitySpaceComp1/" data-id="clzik0lc4001av47k3rpz5g8a" data-title="Computational Complexity Space Complexity 1" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Math/" rel="tag">Math</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Theoretical-Computer-Science/" rel="tag">Theoretical Computer Science</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-ContrastiveLearning" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/09/27/ContrastiveLearning/" class="article-date">
  <time class="dt-published" datetime="2023-09-27T11:30:28.000Z" itemprop="datePublished">2023-09-27</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Paper/">Paper</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/09/27/ContrastiveLearning/">Contrastive Learning</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <span id="more"></span>

<h1 id="CV领域的对比学习发展路径"><a href="#CV领域的对比学习发展路径" class="headerlink" title="CV领域的对比学习发展路径"></a>CV领域的对比学习发展路径</h1><p>什么是对比学习？对比学习是属于无监督&#x2F;自监督学习范式的。在监督学习的分类问题中，我们希望模型能够精确地预测输入属于的类别，而在对比学习中，模型不需要知道数据的真实标签，只要最终的输出能够把不同的类别区分开就好。因而，对比学习的模型就是一个特征提取器，其模型将输入的特征提取出来作为输出，使得在输出的特征空间中，相似的数据尽可能地相邻，而不相似的数据尽可能地远离，如下图所示</p>
<p><img src="/2023/09/27/ContrastiveLearning/1.png" alt="1"></p>
<center style="font-size:12px; font-weight:bold">Fig. 1. Contrastive Learning</center><br>

<p>对比学习的典型范式是<strong>代理任务</strong>+<strong>目标函数</strong>。</p>
<ul>
<li>代理任务：代理任务是一些不像分类、目标检测一样具有实际意义的应用场景，但我们假定该模型是为了解决这个代理问题而训练的，而实际上它只是用于生成自监督信号以更新特征提取器，从而能够让我们获得更好的预训练模型。在NLP中，BERT预训练中用到的填词等任务就可以被视为代理任务。在CV中，如下面会提到的九宫格图像相对位置预测、图片着色等都属于代理任务。不过，在CV的对比学习中，更常用的代理任务是个体判别（Instance discrimination），即将同类的个体与其他个体区分开来。&gt; 更通俗地来说，代理任务是为了生成类似监督学习的“标签”，使得无监督学习也有比较的对象（像监督学习中的Ground Truth和prediction一样），有了比较对象，我们才能用合适的metric构建目标函数。</li>
<li>目标函数：产生梯度。<ol>
<li>生成式网络：用生成的图片与原图片做对比，可以是$L1$或$L2$ losses。</li>
<li>判别式网络：对图片本身做划分，如作九宫格划分，用一个格子预测另一个格子在其哪个方位，实际上转化为了一个交叉熵损失。</li>
<li>对比式：衡量被提取的数据特征间在特征空间的相似性，不同于前两种的是（特别是生成式），由于编码器是在不断更新的，被提取的数据特征也是在不断被更新的，因而对比的对象不像前两者是固定的。</li>
<li>对抗性：衡量概率分布的差异。（不太懂）</li>
</ol>
</li>
</ul>
<h2 id="百花齐放"><a href="#百花齐放" class="headerlink" title="百花齐放"></a>百花齐放</h2><ol>
<li>InstDisc: Memory bank。字典内容一致性不好。</li>
<li>InvaSpread: 端到端，两个编码器都梯度下降。字典大小受限。</li>
<li>CPC: InfoNCE。</li>
<li>CMC: 多视角。</li>
</ol>
<h2 id="CV双雄"><a href="#CV双雄" class="headerlink" title="CV双雄"></a>CV双雄</h2><ol>
<li>MoCoV1: 动态编码器、Memory bank变队列。</li>
<li>SimCLRV1: 端到端。</li>
<li>MoCoV2</li>
<li>SimCLRV2</li>
<li>SWaV</li>
</ol>
<h2 id="不用负样本"><a href="#不用负样本" class="headerlink" title="不用负样本"></a>不用负样本</h2><ol>
<li>BYOL: MSE LOSS，一个编码器预测另一个编码器。</li>
<li>SimSiam</li>
</ol>
<h2 id="基于Transformer"><a href="#基于Transformer" class="headerlink" title="基于Transformer"></a>基于Transformer</h2><ol>
<li>MoCoV3</li>
<li>DINO</li>
</ol>
<blockquote>
<p>骨干网络由ResNet换为ViT。</p>
</blockquote>
<p><img src="/2023/09/27/ContrastiveLearning/1.png" alt="1"></p>
<center style="font-size:12px; font-weight:bold">Fig. 1. </center>

<h1 id="MoCo"><a href="#MoCo" class="headerlink" title="MoCo"></a>MoCo</h1><p>对比学习是一次字典查询的过程。</p>
<ol>
<li><p>字典要大；</p>
</li>
<li><p>字典的内容连续性要好。</p>
</li>
<li><p>队列作为字典的数据结构：每个mini batch，老key出去，更新后的key作为new key进来。</p>
</li>
<li><p>动量编码器：$\theta _k &#x3D; m*\theta _k+(1-m)*\theta _q$，保证字典中key的一致性。</p>
</li>
</ol>
<blockquote>
<p>$m$很大，文中取$0.99$或$0.999$。</p>
</blockquote>
<h2 id="NCE-Noise-Contrastive-Estimation"><a href="#NCE-Noise-Contrastive-Estimation" class="headerlink" title="NCE(Noise Contrastive Estimation)"></a>NCE(Noise Contrastive Estimation)</h2><p>当分类任务的类别很多时，交叉熵的计算时间是难以承受的，因为交叉熵的分母必须对样本在<strong>所有类别</strong>上出现的可能进行求和。对于Instance discrimination，每个样本就是一个类，在这种情况下，用交叉熵是不现实的。<br>NCE(Noise Contrastive Estimation)将多分类问题转化为了多个<strong>二分类</strong>问题，所有的样本都只有两类：来自data samples的<strong>正类</strong>和来自noise samples的负类。</p>
<h2 id="InfoNCE"><a href="#InfoNCE" class="headerlink" title="InfoNCE"></a>InfoNCE</h2><p>InfoNCE是对NCE的改进，它比NCE更加接近交叉熵。InfoNCE将正例视作一类，将单个的负例也视作一类。因此，对于$1$正例$K$负例的采样，总类别数是$K+1$。InfoNCE实际上就是类别数为$K+1$且带温度参数$\tau$的交叉熵。还有一点特殊的是，由于我们想匹配的只是$q$和正例$k _+$，所以$\mathcal{L} _q$的分子永远都只会是$\exp(q\cdot k _+ &#x2F; \tau)$。</p>
<p>$$<br>\mathcal{L} _q&#x3D;-\log \frac{\exp(q\cdot k _+ &#x2F; \tau)}{\sum _ {i&#x3D;0} ^K\exp(q\cdot k _i &#x2F; \tau)}<br>$$</p>
<p>上式中，$k _0$即为$k _+$。$\mathcal{L} _q$很好地体现了我们的优化目标：**$q$和正例的相似性出现在分子，所以越大越好，相应地，分母上$q$与负例的相似性越小越好**。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># f_q, f_k: encoder networks for query and key</span></span><br><span class="line"><span class="comment"># queue: dictionary as a queue of K keys (CxK)</span></span><br><span class="line"><span class="comment"># m: momentum</span></span><br><span class="line"><span class="comment"># t: temperature</span></span><br><span class="line"></span><br><span class="line">f_k.params = f_q.params <span class="comment"># initialize</span></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> loader: <span class="comment"># load a minibatch x with N samples</span></span><br><span class="line">   x_q = aug(x) <span class="comment"># a randomly augmented version</span></span><br><span class="line">   x_k = aug(x) <span class="comment"># another randomly augmented version</span></span><br><span class="line"></span><br><span class="line">   q = f_q.forward(x_q) <span class="comment"># queries: NxC  256x128</span></span><br><span class="line">   k = f_k.forward(x_k) <span class="comment"># keys: NxC  256x128</span></span><br><span class="line">   k = k.detach() <span class="comment"># no gradient to keys</span></span><br><span class="line"></span><br><span class="line">   <span class="comment"># positive logits: Nx1  256x1</span></span><br><span class="line">   l_pos = bmm(q.view(N,<span class="number">1</span>,C), k.view(N,C,<span class="number">1</span>))</span><br><span class="line">   <span class="comment"># negative logits: NxK  256x65536</span></span><br><span class="line">   l_neg = mm(q.view(N,C), queue.view(C,K))</span><br><span class="line"></span><br><span class="line">   <span class="comment"># logits: Nx(1+K)  256x65537</span></span><br><span class="line">   logits = cat([l_pos, l_neg], dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">   <span class="comment"># contrastive loss, Eqn.(1)</span></span><br><span class="line">   labels = zeros(N) <span class="comment"># positives are the 0-th</span></span><br><span class="line">   loss = CrossEntropyLoss(logits/t, labels)</span><br><span class="line"></span><br><span class="line">   <span class="comment"># SGD update: query network</span></span><br><span class="line">   loss.backward()</span><br><span class="line">   update(f_q.params)</span><br><span class="line"></span><br><span class="line">   <span class="comment"># momentum update: key network</span></span><br><span class="line">   f_k.params = m*f_k.params+(<span class="number">1</span>-m)*f_q.params</span><br><span class="line"></span><br><span class="line">   <span class="comment"># update dictionary</span></span><br><span class="line">   enqueue(queue, k) <span class="comment"># enqueue the current minibatch</span></span><br><span class="line">   dequeue(queue) <span class="comment"># dequeue the earliest minibatch</span></span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zclzcl0223.github.io/2023/09/27/ContrastiveLearning/" data-id="clzik0lc5001dv47kfvbde96c" data-title="Contrastive Learning" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Paper/" rel="tag">Paper</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Unsupervised-Learning/" rel="tag">Unsupervised Learning</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-MatrixTheory2" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/09/27/MatrixTheory2/" class="article-date">
  <time class="dt-published" datetime="2023-09-27T04:56:54.000Z" itemprop="datePublished">2023-09-27</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/MATH6005-Matrix-Theory/">MATH6005: Matrix Theory</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/09/27/MatrixTheory2/">MatrixTheory: Finite Dimensional Linear Space</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <span id="more"></span>


<h1 id="有限维线性空间"><a href="#有限维线性空间" class="headerlink" title="有限维线性空间"></a>有限维线性空间</h1><ul>
<li>线性组合：对数域$\mathbb{F}$上的线性空间$V$，对$\alpha _1,...,\alpha _k \in V$，$x _1,...,x _k \in \mathbb{F}$，称：<br>$$<br>x _1 \cdot\alpha _1+x _2\cdot \alpha + ...+x _k\cdot\alpha _k<br>$$为其线性组合。</li>
</ul>
<blockquote>
<p>注意，“$\cdot$”为该线性空间所定义的数乘运算。</p>
</blockquote>
<ul>
<li>定义$\text{span}[\alpha _1,...,\alpha _k]&#x3D;\{x _1 \cdot\alpha _1+x _2\cdot \alpha + ...+x _k\cdot\alpha _k\}$为向量组$\alpha _1,...,\alpha _k$的张成子空间。该子空间为包含了$\alpha _1,...,\alpha _k$的最小子空间。</li>
</ul>
<p>对某个线性空间，若其可以由有限个向量张成，则称该线性空间为<em>有限维线性空间</em>，反之为<em>无限维线性空间</em>。</p>
<h2 id="线性无关-amp-线性相关"><a href="#线性无关-amp-线性相关" class="headerlink" title="线性无关 &amp; 线性相关"></a>线性无关 &amp; 线性相关</h2><ul>
<li>对某个张成$\text{span}[\alpha _1,...,\alpha _k]$，$\forall \alpha \in \text{span}[\alpha _1,...,\alpha _k]$，若$\alpha &#x3D; x _1 \cdot\alpha _1+x _2\cdot \alpha + ...+x _k\cdot\alpha _k$的系数$x _1,x _2,...,x _k$唯一，则称$\alpha _1,...,\alpha _k$线性无关。</li>
</ul>
<p><strong>判断的充要条件：$x _1 \cdot\alpha _1+x _2\cdot \alpha + ...+x _k\cdot\alpha _k&#x3D;0$当且仅当$x _1&#x3D;x _2&#x3D;...&#x3D;x _k&#x3D;0$。</strong> 反之若$x _i$不全为0，则称线性相关。</p>
<h3 id="线性相关定理"><a href="#线性相关定理" class="headerlink" title="线性相关定理"></a>线性相关定理</h3><p>若$\alpha _1,...,\alpha _m$线性相关：</p>
<ol>
<li>$\exists j \in \{1,...,m\}$，使得$\alpha _j \in \text{span}[\alpha _1,...,\alpha _{j-1}]$；<blockquote>
<p>证明：<br>$x _1 \cdot\alpha _1+x _2\cdot \alpha _2 + ...+x _m\cdot\alpha _m&#x3D;0$<br>假设$x _i$不为零的最大下标为$j$，则：<br>$x _1 \cdot\alpha _1+x _2\cdot \alpha _2 + ...+x _j\cdot\alpha _j&#x3D;0$<br>则：<br>$x _j\cdot\alpha _j&#x3D;-x _1 \cdot\alpha _1-x _2\cdot \alpha _2 - ...-x _{j-1}\cdot\alpha _{j-1}$<br>则：<br>$\alpha _j&#x3D;-x _1&#x2F;x _j \cdot\alpha _1-x _2 &#x2F;x _j \cdot \alpha _2 - ...-x _{j-1} &#x2F;x _j \cdot\alpha _{j-1}$</p>
</blockquote>
</li>
<li>如果将$\alpha _j$从向量组中删除，则剩余的向量组成的张成与原张成一样。</li>
</ol>
<blockquote>
<p>$j&#x3D;1$时说明$\alpha _1$是零向量。</p>
</blockquote>
<h2 id="基"><a href="#基" class="headerlink" title="基"></a>基</h2><ul>
<li>基：一组线性无关的向量$\alpha _1,...,\alpha _k$张成的空间$V$。这组向量称为该张成的<em>基</em>。</li>
</ul>
<p>有限维线性空间一定存在一组基。</p>
<ul>
<li>易由线性相关定理推出，即删除法；</li>
<li>也可由基的扩充定理推出，即添加法：$\alpha _1,...,\alpha _k$，但$\text{span}[\alpha _1,...,\alpha _k]&lt; V$，则可在其后添加向量，使之成为$V$的一组基。</li>
</ul>
<p>基向量的数目$k$则称为该空间的维数$\text{dim}$。$\{0\}$的维数为0。</p>
<ul>
<li>补空间：$U\oplus W&#x3D;V$，则称$U$为$W$补空间。补空间可以有无穷多个，除非$U&#x3D;W$，则其补空间只能为$\{0\}$。</li>
</ul>
<h2 id="维数公式"><a href="#维数公式" class="headerlink" title="维数公式"></a>维数公式</h2><p>空间$V$，子空间$U$、$W$，则：</p>
<p>$$<br>\text{dim}(U+W)&#x3D;\text{dim}(U)+\text{dim}(W)-\text{dim}(U\cap W)<br>$$</p>
<p>$$<br>\text{dim}(U _1\oplus U _2 \oplus...\oplus U _n)&#x3D;\text{dim}(U _1)+\text{dim}(U _2)+...+\text{dim}(U _n)<br>$$</p>
<h2 id="与矩阵相关的空间"><a href="#与矩阵相关的空间" class="headerlink" title="与矩阵相关的空间"></a>与矩阵相关的空间</h2><p>$A _{m\times n}$：</p>
<ul>
<li>$N(A)$：零空间，$Ax&#x3D;0$，$R ^n$的子空间，可用行变换求解。</li>
<li>$C(A)$：列空间，列向量张成的空间，$R ^m$的子空间，也可用行变换求解。因为行变换不会改变列向量的相关性。</li>
<li>$R(A)$：行空间，行向量张成的空间，$R ^n$的子空间，可用行变换求解。</li>
<li>$N(A ^T)$：左零空间，$A ^Tx&#x3D;0$，$R ^m$的子空间，也可用行变换。</li>
</ul>
<p>Hermite标准型：阶梯阵的基础上令行第一个非零为$1$，且其所在列的其他位置都是$0$。Hermite标准化后，行空间即为Hermite的线性无关行，但列空间由原列向量组成。求左零空间时，可求$(A|E)$的行变换，得到$(\frac{H}{0}|P)$，则$PA&#x3D;\frac{H}{0}$。</p>
<h2 id="满秩分解"><a href="#满秩分解" class="headerlink" title="满秩分解"></a>满秩分解</h2><p>对$A _{m\times n},r(A)&#x3D;r$，$A _{m\times n}&#x3D;P _{m\times r} \times Q _{r\times n}$，其中$P$列满秩，$Q$行满秩，这样的分解称<em>满秩分解</em>。</p>
<h3 id="存在性"><a href="#存在性" class="headerlink" title="存在性"></a>存在性</h3><p>$$<br>P _1 A Q _1 &#x3D;<br>\begin{pmatrix}<br>    E _r&amp; 0 \\<br>    0&amp; 0<br>\end{pmatrix}<br>$$</p>
<p>则：</p>
<p>$$<br>\begin{align*}<br>    A<br>    &amp;&#x3D;<br>P _1 ^{-1}<br>\begin{pmatrix}<br>    E _r&amp; 0 \\<br>    0&amp; 0<br>\end{pmatrix}<br>Q _1 ^{-1}\\<br>    &amp;&#x3D;<br>P _1 ^{-1}<br>\begin{pmatrix}<br>    E _r\\<br>    0<br>\end{pmatrix}<br>\begin{pmatrix}<br>    E _r\space 0<br>\end{pmatrix}<br>Q _1 ^{-1}<br>\end{align*}<br>$$</p>
<p>则：</p>
<p>$$<br>\begin{align*}<br>    P &#x3D; P _1 ^{-1}<br>\begin{pmatrix}<br>    E _r\\<br>    0<br>\end{pmatrix}\\<br>    Q &#x3D;\begin{pmatrix}<br>    E _r\space 0<br>\end{pmatrix}<br>Q _1 ^{-1}<br>\end{align*}<br>$$</p>
<h3 id="唯一性"><a href="#唯一性" class="headerlink" title="唯一性"></a>唯一性</h3><p>$$<br>A &#x3D; P \times Q&#x3D;P \times E _r \times Q&#x3D;P\times U ^{-1}\times U \times Q<br>$$</p>
<p>所以一定不唯一。</p>
<h3 id="求法"><a href="#求法" class="headerlink" title="求法"></a>求法</h3><p>求Hermite标准型$H _A$，则非零行所组成矩阵即为$Q$，线性无关列所对应的$A$的列向量组成的矩阵即为$P$</p>
<h1 id="线性变换"><a href="#线性变换" class="headerlink" title="线性变换"></a>线性变换</h1><p>对$\mathbb{F}$上的线性空间$U$和$V$，若：</p>
<ol>
<li>$\forall \alpha,\beta \in U$，$T(\alpha + \beta)&#x3D;T(\alpha)+T(\beta)$</li>
<li>$\forall k \in \mathbb{F}$，$\alpha \in U$，$T(k \cdot \alpha)&#x3D;k\cdot T(\alpha)$</li>
</ol>
<p>则称$T$是线性空间$U$到$V$的线性变换。</p>
<h2 id="线性变换的性质"><a href="#线性变换的性质" class="headerlink" title="线性变换的性质"></a>线性变换的性质</h2><ol>
<li>$T(0)\to 0$。证明：$T(0)&#x3D;T(0+0)&#x3D;T(0)+T(0)$</li>
<li>$T(-\alpha) &#x3D; 1T(\alpha)$</li>
<li>设$\alpha _1,\alpha _2,...,\alpha _k \in U$线性相关，则$T(\alpha _1),T(\alpha _2),...,T(\alpha _k)$线性相关</li>
<li>线性变换由一组基的像<strong>唯一</strong>确定</li>
</ol>
<h2 id="线性变换的运算"><a href="#线性变换的运算" class="headerlink" title="线性变换的运算"></a>线性变换的运算</h2><p>对线性空间$U$和$V$的线性变换集合$L(U,V)$，$T _1,T _2 \in L(U,V)$，定义：</p>
<ul>
<li>$(T _1 + T _2)(\alpha)&#x3D;T _1(\alpha)+T _2 (\alpha)$</li>
<li>$k \in \mathbb{F}$，$(k\cdot T _1)(\alpha)&#x3D;k\cdot T _1(\alpha)$</li>
</ul>
<blockquote>
<p>$(T _1 + T _2)$和$(k\cdot T _1)$也是$U\to V$的线性变换。换句话说，集合$L(U,V)$的加法和数乘运算是封闭的。<br>零变换是零向量。<br>负变换是负函数。</p>
</blockquote>
<p>因此，集合$L(U,V)$是一个线性空间。</p>
<p>$T \in L(U,V),S\in L (V,W)$，定义：</p>
<p>$$<br>(S\cdot T)(\alpha)&#x3D;S(T(\alpha))<br>$$</p>
<p>类似于复合函数的定义。易证$S\cdot T$是$U\to W$的线性变换。</p>
<blockquote>
<p>上述复合运算的结合律成立，即$(T _1\cdot T _2)\cdot T _3&#x3D; T _1\cdot (T _2\cdot T _3)$。<br>分配律亦成立。但交换律不一定成立。</p>
</blockquote>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zclzcl0223.github.io/2023/09/27/MatrixTheory2/" data-id="clzik0lcm005kv47k9tk29e24" data-title="MatrixTheory: Finite Dimensional Linear Space" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Math/" rel="tag">Math</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-MatrixTheory1" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/09/24/MatrixTheory1/" class="article-date">
  <time class="dt-published" datetime="2023-09-24T11:49:32.000Z" itemprop="datePublished">2023-09-24</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/MATH6005-Matrix-Theory/">MATH6005: Matrix Theory</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/09/24/MatrixTheory1/">MatrixTheory: Linear Space and Subspace</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <span id="more"></span>

<h1 id="Linear-Space：线性空间"><a href="#Linear-Space：线性空间" class="headerlink" title="Linear Space：线性空间"></a>Linear Space：线性空间</h1><ul>
<li>定义：设$V$为一个非空集合，$\mathbb{F}$为一个数域，$ V$中定义了一个封闭的加法运算“$+$”，对$\forall \alpha,\beta,\gamma\in V$,满足：</li>
</ul>
<ol>
<li>结合律：$(\alpha+\beta)+\gamma&#x3D;\alpha+(\beta+\gamma)$</li>
<li>交换律：$\alpha+\beta&#x3D;\beta+\alpha$</li>
<li>存在<em>零向量</em>：$\alpha+0&#x3D;\alpha$</li>
<li>存在<em>负向量</em>：$\alpha+(-\alpha)&#x3D;0$</li>
</ol>
<blockquote>
<p>满足2、3、4的集合称<em>群</em>，满足1的<em>群</em>称<em>交换群</em>或<em>阿贝尔群</em>。</p>
</blockquote>
<p>定义了另一个封闭的数乘运算“$\cdot$”，对$\forall \alpha,\beta\in V,k _1,k _2\in\mathbb{F}$,满足：</p>
<ol>
<li>数乘的结合律：$k _1\cdot(k _2\cdot\alpha)&#x3D;(k _1k _2)\cdot\alpha$</li>
<li>数乘关于向量加法的分配律：$k _1\cdot(\alpha+\beta)&#x3D;k _1\cdot\alpha+k _1\cdot\beta$</li>
<li>数乘关于数的加法的分配律：$(k _1+k _2)\cdot\alpha&#x3D;k _1\cdot\alpha+k _2\cdot\alpha$</li>
<li>数乘的初始条件：$1\cdot\alpha&#x3D;\alpha$</li>
</ol>
<p>则称$ V$为数域$\mathbb{F}$上的线性空间。</p>
<blockquote>
<p>若某个运算的结果仍处于运算子所在的集合，则该运算是封闭的。对不同的线性空间，其内部的基本元素可以是任意的，如标量、向量、矩阵、函数等。但是对于一个确定的线性空间，其内部的元素类型必须是一致的。</p>
<p>加法和数乘只是一个符号或者说法，它们不一定是真的“$+$”和“$\cdot$”。</p>
</blockquote>
<h2 id="Proof-of-Linear-Space：线性空间的证明（重要）"><a href="#Proof-of-Linear-Space：线性空间的证明（重要）" class="headerlink" title="Proof of Linear Space：线性空间的证明（重要）"></a>Proof of Linear Space：线性空间的证明（重要）</h2><p>要想证明某个集合$V$是数域$\mathbb{F}$上的线性空间，只需要干两件事：</p>
<ol>
<li>证明加法和数乘运算的封闭性；</li>
<li>证明上面的<strong>8</strong>项规则成立。</li>
</ol>
<h2 id="Properties-of-Linear-Space：线性空间的性质"><a href="#Properties-of-Linear-Space：线性空间的性质" class="headerlink" title="Properties of Linear Space：线性空间的性质"></a>Properties of Linear Space：线性空间的性质</h2><ol>
<li><em>零向量</em>是唯一的。</li>
<li><em>负向量</em>是唯一的。</li>
<li>$\forall k\in\mathbb{F},\alpha\in V$，$k\cdot\alpha&#x3D;0$当且仅当$k&#x3D;0$或$\alpha&#x3D;0$。</li>
</ol>
<blockquote>
<p>以上三个性质都可用反证法证明。</p>
</blockquote>
<h1 id="Subspace：子空间"><a href="#Subspace：子空间" class="headerlink" title="Subspace：子空间"></a>Subspace：子空间</h1><ul>
<li>定义：$ V$是数域$\mathbb{F}$上的线性空间，$U\subset V$，若$U$对$V$上定义的两个运算仍构成$\mathbb{F}$上的线性空间，则称$U$为$V$的子空间。</li>
</ul>
<p>要判别一个集合$V$的子集$U$是否是原集合所构成线性空间的子空间，我们不再需要逐一地证明<strong>8</strong>项规则，只需要证明：</p>
<ol>
<li>加法和数乘运算在集合$U$中封闭；</li>
<li>集合$U$不是空集。</li>
</ol>
<p>只要证明了上面这两项成立，则<strong>8</strong>项规则自动成立。这是因为：$U\subset V$，所以只要$U$中有元素，那么它们就自动地满足结合、交换律，<em>负向量</em>也相应地存在；若$U$的加法、数乘运算还是封闭的，那么由向量与其<em>负向量</em>的加法又可得出<em>零向量</em>存在，由此，<strong>8</strong>项规则成立。</p>
<h1 id="Intersection-Space"><a href="#Intersection-Space" class="headerlink" title="Intersection Space"></a>Intersection Space</h1><ul>
<li>对线性空间$V$的两个子空间$U$和$W$，若它们的交集非空，则其交空间$U\cap W$也是$V$的子空间。</li>
</ul>
<blockquote>
<p>上述结论可以推广至$n$个子空间的交集。</p>
</blockquote>
<h1 id="Sum-Space"><a href="#Sum-Space" class="headerlink" title="Sum Space"></a>Sum Space</h1><ul>
<li>对线性空间$V$的两个子空间$U$和$W$，它们的和空间定义为：</li>
</ul>
<p>$$<br>U+W&#x3D;\{\alpha+\beta;\alpha\in U,\beta\in W\}<br>$$</p>
<p>和空间是$V$的子空间。</p>
<blockquote>
<p>上述结论可以推广至$n$个子空间的和。</p>
<p>要注意把和空间和并空间$U\cup W$区分开来。并空间并不一定是$V$的子空间，只有两个空间之间存在包含关系时其并空间才是$V$的子空间。</p>
</blockquote>
<h1 id="Direct-Sum"><a href="#Direct-Sum" class="headerlink" title="Direct Sum"></a>Direct Sum</h1><ul>
<li>定义：对线性空间$V$的两个子空间$U$和$W$，$\forall\alpha\in U+W$，即$\alpha&#x3D;\alpha _1+\alpha _2$，其中$\alpha _1\in U$，$\alpha _2 \in W$，若$\alpha _1$和$\alpha _2$唯一，则称这种和为<em>直和</em>，记作$U\oplus W$。</li>
</ul>
<p>直和也可以推广至$n$个子空间的和。我们有通用的方法来判定某个和是否是直和，但对于$n&#x3D;2$的情况，还有专门的特殊方法。</p>
<h2 id="Judgment-of-Direct-Sum：-n-ge-2"><a href="#Judgment-of-Direct-Sum：-n-ge-2" class="headerlink" title="Judgment of Direct Sum：$n\ge 2$"></a>Judgment of Direct Sum：$n\ge 2$</h2><p>对线性空间$V$的$n$个子空间$U _1,...,U _n$，它们的和$(U _1+...+U _n)$是直和当且仅当其和空间的<em>零向量</em>只能表示为$0&#x3D;0+...+0$。</p>
<blockquote>
<ul>
<li>必要性：<br>若$U _1+...+U _n$是直和$U _1\oplus ...\oplus U _n$，则其零<em>向量</em>的表示唯一。又$0&#x3D;0+...+0$，所以必要性显然成立。</li>
<li>充分性：<br>假设$\alpha\in U _1+...+U _n$，且$\alpha$存在两个表示$\alpha&#x3D;\alpha _1 + ... + \alpha _n$与$\alpha&#x3D;\beta _1 + .. + \beta _n$。令第二个表示取<em>逆向量</em>并与第一个表示相加，则有：<br>$$0&#x3D;(\alpha _1-\beta _1)+...+(\alpha _n - \beta _n)$$<br>因为$0$的表示唯一，所以：$\alpha _1 &#x3D;\beta _1,...,\alpha _n &#x3D;\beta _n$。证毕。</li>
</ul>
</blockquote>
<h2 id="Judgment-of-Direct-Sum：-n-x3D-2"><a href="#Judgment-of-Direct-Sum：-n-x3D-2" class="headerlink" title="Judgment of Direct Sum：$n&#x3D; 2$"></a>Judgment of Direct Sum：$n&#x3D; 2$</h2><ul>
<li>若$n&#x3D;2$，则有特殊的判别法：</li>
</ul>
<p>对线性空间$V$的$2$个子空间$U _1,U _2$，它们的和$(U _1 + U _2)$是直和当且仅当其它们的交空间只有<em>零向量</em>，即$U \cap W &#x3D; \{0\}$。</p>
<blockquote>
<ul>
<li>必要性：<br>若$(U _1 + U _2)$是直和$U _1 \oplus U _2$，假设存在$\alpha \in U\cap W$，因为$U \cap W$是子空间，所以存在<em>负向量</em>使得：<br>$$0 &#x3D; \alpha + (-\alpha)$$<br>而$0&#x3D;0+0$，矛盾，所以必要性成立。</li>
<li>充分性：<br>若$U \cap W &#x3D; \{0\}$，假设$\alpha \in U\cap W$存在两个不同的表示$\alpha &#x3D; \alpha _1 + \beta _1$和$\alpha &#x3D; \alpha _2 + \beta _2$，则：<br>$$\alpha _1 - \alpha _2 &#x3D; \beta _2 - \beta _1$$<br>易知$\alpha _1 - \alpha _2\in U$，$\beta _2 - \beta _1\in W$，所以它们是$U$和$W$的公共部分，而$U \cap W&#x3D;0$，所以只能$\alpha _1&#x3D;\alpha _2$，$\beta _1 &#x3D; \beta _2$。</li>
</ul>
</blockquote>
<p>对$n&gt;2$，仅两两相交为<em>零向量</em>不行。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zclzcl0223.github.io/2023/09/24/MatrixTheory1/" data-id="clzik0lck004xv47k03g10dyu" data-title="MatrixTheory: Linear Space and Subspace" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Math/" rel="tag">Math</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-RidgeRegression" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/09/22/RidgeRegression/" class="article-date">
  <time class="dt-published" datetime="2023-09-22T12:55:02.000Z" itemprop="datePublished">2023-09-22</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/09/22/RidgeRegression/">Ridge Regression</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <span id="more"></span>

<h1 id="向量求导"><a href="#向量求导" class="headerlink" title="向量求导"></a>向量求导</h1><p>更确切地说是标量对向量求导，即，对：</p>
<p>$$<br>x\in\mathbb{R}^n,f:\mathbb{R}^{n}\to \mathbb{R}<br>$$</p>
<p>求标量$f(x)$对向量$x$的导数。与其一个个地展开，下列两个性质可以帮助我们快速地求解标量对向量的导数以及二阶导的海森矩阵：</p>
<ol>
<li>$\frac{\partial f(x)}{\partial v}&#x3D;v^T\nabla f(x)$</li>
</ol>
<blockquote>
<p>其中$v$是方向向量，即$f(x)$对$v$的方向导数是$v^T\nabla f(x)$，这是很显然的，因为在二维的方向导数我们就有$\frac{\partial f}{\partial l}&#x3D;f _x\cos \alpha+f _y \cos \beta$。推广到$n$维，$\cos\alpha...\cos\gamma$就是$v$的各个元素。</p>
<ul>
<li>例1：求$f(x)&#x3D;x^T\mathbf{A}x$的梯度（$\mathbf{A}$是对称矩阵）：<br>$$<br>\begin{align*}<br>  \frac{\partial f(x)}{\partial v}&#x3D;v^T\nabla f(x)<br>  &amp;&#x3D;\lim_{t\to 0}\frac{f(x+tv)-f(x)}{t} \\<br>  &amp;&#x3D;\lim_{t\to 0}\frac{(x+tv)^T\mathbf{A}(x+tv)-x^T\mathbf{A}x}{t}\\<br>  &amp;&#x3D;\lim_{t\to 0}\frac{tv^t\mathbf{A}x+x^T\mathbf{A}tv}{t}\\<br>  &amp;&#x3D;v^t\mathbf{A}x+x^T\mathbf{A}v\\<br>  &amp;&#x3D;2v^t\mathbf{A}x<br>\end{align*}$$<br>由于$v$的任意性，$\nabla f(x)&#x3D;2\mathbf{A}x$</li>
<li>例2：$f(\mathbf{\beta})&#x3D;||\mathbf{X}\beta-y||^2$，求其对$\mathbf{\beta}$的梯度。<br>$$<br>\begin{align*}<br>  \frac{\partial f(\beta)}{\partial v}&#x3D;v^T\nabla f(\beta)<br>  &amp;&#x3D;\lim_{t\to 0}\frac{f(\beta+tv)-f(\beta)}{t} \\<br>  &amp;&#x3D;\lim_{t\to 0}\frac{||\mathbf{X}(\beta+tv)-y||^2-||\mathbf{X}\beta-y||^2}{t}\\<br>  &amp;&#x3D;\lim_{t\to 0}\frac{||\mathbf{X}\beta-y+\mathbf{X}tv||^2-||\mathbf{X}\beta-y||^2}{t}\\<br>  &amp;&#x3D;\lim_{t\to 0}\frac{||\mathbf{X}tv||^2+2\sum((\mathbf{X}\beta-y)\odot (\mathbf{X}tv))}{t}\\<br>  &amp;&#x3D;2\sum((\mathbf{X}\beta-y)\odot (\mathbf{X}v))\\<br>  &amp;&#x3D;2(\mathbf{X}v)^T(\mathbf{X}\beta-y)\\<br>  &amp;&#x3D;2v^T\mathbf{X}^T(\mathbf{X}\beta-y)<br>\end{align*}$$<br>故$\nabla f(\beta)&#x3D;2\mathbf{X}^T(\mathbf{X}\beta-y)$</li>
</ul>
</blockquote>
<ol start="2">
<li>$\nabla ^2f(x)\cdot v&#x3D;\frac{\partial \nabla f(x)}{\partial v}$</li>
</ol>
<blockquote>
<p>其中$\nabla ^2f(x)$是海森矩阵$\mathbf{H}$，$\mathbb{H} _{[i,j]}&#x3D;\frac{\partial ^2 f}{\partial x _i \partial x_j}$。不难看出，$\nabla ^2f(x)\cdot v$是一个向量，其第$i$个元素为：$(\frac{\partial ^2 f}{\partial x _i \partial x_1},\frac{\partial ^2 f}{\partial x _i \partial x_2},...,\frac{\partial ^2 f}{\partial x _i \partial x_n})\cdot v$。</p>
<p>若令$g(x _i)&#x3D;\frac{\partial f}{\partial x _i}$，则第$i$个元素实际上就是$\frac{\partial g(x _i)}{\partial v}$。</p>
<p>继续推广，令$g(x)&#x3D;(\frac{\partial f}{\partial x _1},\frac{\partial f}{\partial x _2},...,\frac{\partial f}{\partial x _n})&#x3D;\nabla f(x)$，则有$\nabla ^2f(x)\cdot v&#x3D;\frac{\partial g(x)}{\partial v}&#x3D;\frac{\partial \nabla f(x)}{\partial v}$。</p>
<ul>
<li>例3：求$f(x)&#x3D;x^T\mathbf{A}x$的海森矩阵（$\mathbf{A}$是对称矩阵）：<br>$$<br>\begin{align*}<br>  \nabla ^2f(x)\cdot v&#x3D;\frac{\partial \nabla f(x)}{\partial v}<br>  &amp;&#x3D;\lim_{t\to 0}\frac{\nabla f(x+tv)-\nabla f(x)}{t} \\<br>  &amp;&#x3D;\lim_{t\to 0}\frac{2\mathbf{A}(x+tv)-2\mathbf{A}x}{t}\\<br>  &amp;&#x3D;\lim_{t\to 0}\frac{2\mathbf{A}tv}{t}\\<br>  &amp;&#x3D;2\mathbf{A}v<br>\end{align*}<br>$$<br>由于$v$的任意性，$\nabla  ^2 f(x)&#x3D;2\mathbf{A}$</li>
</ul>
</blockquote>
<h1 id="矩阵求导"><a href="#矩阵求导" class="headerlink" title="矩阵求导"></a>矩阵求导</h1><p>前面提到的向量求导的性质可以很容易地推广到矩阵求导（标量对矩阵求导），对：</p>
<p>$$<br>A\in\mathbb{R}^{n\times m},f:\mathbb{R}^{n\times m}\to \mathbb{R}<br>$$</p>
<p>有：</p>
<ol>
<li>$\frac{\partial f(\mathbf{A})}{\partial D}&#x3D;&lt; D,\nabla f(\mathbf{A}) &gt;$</li>
<li>$\nabla ^2f(\mathbf{A})\cdot D&#x3D;\frac{\partial \nabla f(\mathbf{A})}{\partial D}$</li>
</ol>
<p>其中，$D$是一个矩阵，$D\in \mathbb{R} ^{n\times m}$。由于内积是各元素相乘求和，故实际上$v^T\nabla f(x)$也可以表示为$&lt; v,  \nabla f(x)&gt;$。性质$1$是显然的，只要把$D$拉伸为一个长$n\times m$的方向向量即可。</p>
<blockquote>
<ul>
<li>例4：$f(\mathbf{A})&#x3D;\text{tr}(\mathbf{AB})$，求其对$\mathbf{A}$的梯度。<br>$$<br>\begin{align*}<br>  \frac{\partial f(\mathbf{A})}{\partial D}&#x3D;&lt; D,\nabla f(\mathbf{A}) &gt;<br>  &amp;&#x3D;\lim_{t\to 0}\frac{f(\mathbf{A}+tD)-f(\mathbf{A})}{t} \\<br>  &amp;&#x3D;\lim_{t\to 0}\frac{\text{tr}[(\mathbf{A}+tD)\mathbf{B}]-\text{tr}(\mathbf{AB})}{t}\\<br>  &amp;&#x3D;\lim_{t\to 0}\frac{\text{tr}(tD\mathbf{B})}{t}\\<br>  &amp;&#x3D;\text{tr}(D\mathbf{B})\\<br>  &amp;&#x3D;&lt; D,\mathbf{B}^T &gt;<br>\end{align*}<br>$$<br>故$\nabla f(\mathbf{A})&#x3D;\mathbf{B}^T$。最后的等式成立因为$\text{tr}(D\mathbf{B})&#x3D;&lt;D_1,{\mathbf{B} ^T} _1&gt;+...+&lt;D_m,{\mathbf{B} ^T} _m&gt; &#x3D;&lt; D,\mathbf{B}^T &gt;$，其中$D_i$表$D$的第$i$个行向量。</li>
</ul>
</blockquote>
<h1 id="Ridge-Regression：岭回归"><a href="#Ridge-Regression：岭回归" class="headerlink" title="Ridge Regression：岭回归"></a>Ridge Regression：岭回归</h1><p>岭回归（英文名：ridge regression, Tikhonov regularization）是一种专用于共线性数据分析的有偏估计回归方法，实质上是一种改良的最小二乘估计法，通过放弃最小二乘法的无偏性，以损失部分信息、降低精度为代价获得回归系数更为符合实际、更可靠的回归方法，对病态数据的拟合要强于最小二乘法。</p>
<blockquote>
<p>共线性数据是指一些相互间存在比例关系的数据，这些数据的存在会导致输入参数$\mathbf{X}^T\mathbf{X}$的行列式为0，进而导致其不可逆。</p>
</blockquote>
<p>例2中$f(\mathbf{\beta})&#x3D;||\mathbf{X}\beta-y||^2$实际上就是对分类问题的最小二乘估计函数。令其对$\beta$的梯度为0，我们就可以得到参数$\beta$的精确值：</p>
<p>$$<br>\begin{align*}<br>\nabla f(\beta)&#x3D;2&amp;\mathbf{X}^T(\mathbf{X}\beta-y)&#x3D;0\\\<br>&amp;\downarrow\\<br>\hat{\beta}&#x3D;&amp;(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^Ty<br>\end{align*}<br>$$</p>
<p>但是，由于上面提到的问题，$(\mathbf{X}^T\mathbf{X})^{-1}$不一定存在，因而上述式子也就不一定成立，且仅仅一阶导为0我们不能判定此时的$\beta$就是极小值点。岭回归就是为了解决这一问题而提出的。对岭回归，有两种合理的解释，其中第一种从特征值的角度出发，第二种从惩罚项（正则）的角度出发。两种解释的结果是一样的，但是第二种解释与神经网络中的正则项联系更加紧密。</p>
<h2 id="特征值角度"><a href="#特征值角度" class="headerlink" title="特征值角度"></a>特征值角度</h2><p>由：</p>
<p>$$<br>y^T\mathbf{X}^T\mathbf{X}y&#x3D;(\mathbf{X}y)^T(\mathbf{X}y)\ge 0<br>$$</p>
<p>可知，$\mathbf{X}^T\mathbf{X}$是半正定矩阵，也就是说，其特征值$\lambda _i \ge 0$。要让$\mathbf{X}^T\mathbf{X}$可逆，我们只要使得其特征值$\lambda _i &gt; 0$即可。故，我们将$\hat{\beta}$更改为：</p>
<p>$$<br>\hat{\beta}(k)&#x3D;(\mathbf{X}^T\mathbf{X}+k\mathbf{I})^{-1}\mathbf{X}^Ty,\quad k&gt;0\tag{1}<br>$$</p>
<p>由此，我们可以确保$\mathbf{X}^T\mathbf{X}$是正定矩阵，也就可以保证其可逆。</p>
<h2 id="惩罚项角度"><a href="#惩罚项角度" class="headerlink" title="惩罚项角度"></a>惩罚项角度</h2><p>回到最小二乘函数$f(\mathbf{\beta})&#x3D;||\mathbf{X}\beta-y||^2$，我们想从中确定一个$\hat{\beta}$，使得$f(\beta)$取最小值，即：</p>
<p>$$<br>\hat{\beta}&#x3D;\text{arg}\min _{\beta}||\mathbf{X}\beta-y||^2<br>$$</p>
<p>使用原始式子我们可以得到：</p>
<p>$$<br>\hat{\beta}&#x3D;(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^Ty<br>$$</p>
<p>而有些极端情况下$\mathbf{X}^T\mathbf{X}$的行列式即便不为0也是一个接近0的极小值。这将导致$(\mathbf{X}^T\mathbf{X})^{-1}$的行列式极大、内部元素的差异大，而这将间接地导致$\beta$数值的不稳定（这里面的关系不太懂）。为了限制$\beta$的波动，使其更稳定，我们需要对$\beta$的取值进行限制，即对其过大的取值进行一定程度的惩罚：</p>
<p>$$<br>\hat{\beta}&#x3D;\text{arg}\min _{\beta}||\mathbf{X}\beta-y||^2+\lambda ||\beta||^2\tag{2}<br>$$</p>
<p>式$(2)$中新加入的项就是惩罚项，实际上就是深度学习中的L2正则项，对上式求导并令梯度为0可得：</p>
<p>$$<br>\begin{align*}<br>2\mathbf{X}^T(\mathbf{X}\beta-y)&amp;+2\lambda \beta&#x3D;0\\<br>\downarrow&amp;\\<br>\hat{\beta}&#x3D;(\mathbf{X}^T\mathbf{X}+&amp;\lambda\mathbf{I}) ^{-1}\mathbf{X}^Ty\tag{3}<br>\end{align*}<br>$$</p>
<p>式$(3)$与式$(1)$完全一致。</p>
<h2 id="极小值验证"><a href="#极小值验证" class="headerlink" title="极小值验证"></a>极小值验证</h2><p>进一步地，我们要验证$\hat{\beta}$就是极小值点，即我们要证明：</p>
<p>$$<br>\nabla ^2f(\beta)&#x3D;\nabla ^2 (||\mathbf{X}\beta-y||^2+\lambda ||\beta||^2)<br>$$</p>
<p>是正定矩阵。这可以由向量求导的性质2得出：</p>
<p>$$<br>\begin{align*}<br>    \nabla ^2f(\beta)\cdot v&#x3D;\frac{\partial \nabla f(x)}{\partial v}<br>    &amp;&#x3D;\frac{\partial 2\mathbf{X}^T(\mathbf{X}\beta-y)+2\lambda \beta}{\partial v} \\<br>    &amp;&#x3D;\lim_{t\to 0}\frac{2\mathbf{X}^T(\mathbf{X}(\beta+tv)-y)+2\lambda (\beta+tv)-2\mathbf{X}^T(\mathbf{X}\beta-y)-2\lambda \beta}{t}\\<br>    &amp;&#x3D;\lim_{t\to 0}\frac{2\mathbf{X}^T\mathbf{X}tv+2\lambda tv}{t}\\<br>    &amp;&#x3D;2\mathbf{X}^T\mathbf{X}v+2\lambda v\\<br>    &amp;&#x3D;(2\mathbf{X}^T\mathbf{X}+2\lambda\mathbf{I})v<br>\end{align*}<br>$$</p>
<p>即$\nabla ^2f(\beta)&#x3D;2\mathbf{X}^T\mathbf{X}+2\lambda\mathbf{I}$，易知这是正定矩阵。所以$\hat{\beta}$就是我们要求的极小值点。</p>
<h2 id="岭回归的收缩性"><a href="#岭回归的收缩性" class="headerlink" title="岭回归的收缩性"></a>岭回归的收缩性</h2><p>有定理能够证明：</p>
<p>$$<br>||\hat{\beta}(k)||\le ||\hat{\beta}||<br>$$</p>
<p>是严格成立的。事实上，岭回归的作用在于：保留蕴含信息量多的维度（参数），而剔除包含信息量较少的维度（参数）。这表现为某些不重要的参数$\beta _i$的数值接近0。</p>
<h1 id="LASSO"><a href="#LASSO" class="headerlink" title="LASSO"></a>LASSO</h1><p>LASSO采用的惩罚项是L1正则，即$\lambda \sum _{i&#x3D;1} ^{p}|\beta _i|$。惩罚项的不同使得LASSO表现出与岭回归不同的性质，如：</p>
<ul>
<li>LASSO会使得一些参数项变为0，让原参数向量变<strong>稀疏</strong>，而岭回归只会让参数向量收缩，并让一些接近0。</li>
</ul>
<p>它们之间的区别可以用下面的图表示：</p>
<p><img src="/2023/09/22/RidgeRegression/1.png" alt="1"></p>
<center style="font-size:12px; font-weight:bold">Fig. 1. Ridge Regression</center>

<p><img src="/2023/09/22/RidgeRegression/2.png" alt="2"></p>
<center style="font-size:12px; font-weight:bold">Fig. 2. LASSO</center><br>

<p>其中，上图为岭回归中的参数随$\lambda$的变化，下图为LASSO。横坐标均为$\lambda$，负轴方向为$\lambda$增大的方向。之所以会有这些的区别，是因为：若把岭回归和LASSO的最小二乘函数都视作拉格朗日函数，并将其惩罚项视作对$\beta$的约束条件，则其求解可转化为在约束条件下求$||\mathbf{X}\beta-y||^2$的最小值。最终的取值点表现为约束边界与$||\mathbf{X}\beta-y||^2$所确定等高线的切点：</p>
<p><img src="/2023/09/22/RidgeRegression/3.png" alt="3"></p>
<center style="font-size:12px; font-weight:bold">Fig. 3. Lagrange</center><br>

<p>而由于LASSO约束边界的不光滑性，切点更容易落在让某个参数分量为0的尖点上。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Bg4y1i76R?p=3&vd_source=2d980a0365f3ebea674b32924d8a4ce8">手把手教会机器学习与数据挖掘理论：岭回归（Ridge Regression）与LASSO</a></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zclzcl0223.github.io/2023/09/22/RidgeRegression/" data-id="clzik0lcw0083v47kce821hie" data-title="Ridge Regression" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Math/" rel="tag">Math</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-ComputationalComplexityTimecom3" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/09/22/ComputationalComplexityTimecom3/" class="article-date">
  <time class="dt-published" datetime="2023-09-22T11:02:15.000Z" itemprop="datePublished">2023-09-22</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/CS7313-Computational-Complexity/">CS7313: Computational Complexity</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/09/22/ComputationalComplexityTimecom3/">Computational Complexity: Time Complexity 3</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <span id="more"></span>

<h1 id="Time-Hierarchy-Theorem：时间谱系理论"><a href="#Time-Hierarchy-Theorem：时间谱系理论" class="headerlink" title="Time Hierarchy Theorem：时间谱系理论"></a>Time Hierarchy Theorem：时间谱系理论</h1><p>理论：如果$f$和$g$都是时间可构造的，且$f(n)\log f(n)&#x3D;o(g(n))$，则$\text{TIME}(f(n))\subsetneq\text{TIME}(g(n))$。</p>
<blockquote>
<p>小o表示算法运行时间的<strong>严格上限</strong>，$f&#x3D;o(g)$表示$f$运行的上限是$g$，换句话说，$f$的运行时间比$g$增长得慢。</p>
</blockquote>
<p>该理论表明若$g$的运行时间增长快于$f\log f$运行时间的增长，则$f(n)$时间内能接受的语言是$g(n)$时间内能接受语言的子集。该理论的证明如下：</p>
<blockquote>
<p>要证明$\text{TIME}(f(n))\subsetneq\text{TIME}(g(n))$，只要反证$\text{TIME}(g(n))\subsetneq\text{TIME}(f(n))$不成立即可。也就是说只要找到一个语言L，$L\in\text{TIME}(g(n))$，但是$L\notin\text{TIME}(f(n))$即可。</p>
<p>假设存在TM $\mathbb{D}$，它接受语言L，并且：</p>
<ul>
<li>对于输入$x$，$\mathbb{D}$模拟$\mathbb{M} _x(x)$运行$g(|x|)$步（这是可行的，因为$g$是时间可构造的）；</li>
<li>如果模拟可以在$g(|x|)$步内完成（即停机），则$\mathbb{D}(x)$输出$\overline{\mathbb{M} _x(x)}$，否则输出$0$（$1$也行，不影响）。<br>按照这样定义，$\mathbb{D}$会在$g(|x|)$停机，因此$L\in\text{TIME}(g(n))$。</li>
</ul>
<p>此时再假设$L\in\text{TIME}(f(n))$，且图灵机$\mathbb{M} _z$在至多$2f(n)$内接受L。由于图灵机编码的任意性，取足够长的$z$，使得$f(|z|)\log f(|z|)&lt; g(|z|)$严格成立，则会出现：</p>
<ul>
<li>$\mathbb{D}(z)&#x3D;\mathbb{M} _z(z)$，因为$\mathbb{D}$和$\mathbb{M} _z$都接受L；</li>
<li>$\mathbb{D}(z)&#x3D;\overline{\mathbb{M} _z(z)}$，因为$\mathbb{M} _z(z)$可被通用图灵机在$O(f(|z|)\log f(|z|))$的时间内模拟，而$\mathbb{D}(x)$会模拟$\mathbb{M} _z (z)$进行$g(|x|)$步，又$f(|z|)\log f(|z|)&lt; g(|z|)$严格成立，所以在$\mathbb{D}(x)$模拟$\mathbb{M} _z (z)$进行的$g(|x|)$步内，$\mathbb{D}(x)$已经停机，即$\mathbb{D}(z)&#x3D;\overline{\mathbb{M} _z(z)}$。</li>
</ul>
<p>上述结论相互矛盾，原假设不成立，$L\notin\text{TIME}(f(n))$。证毕。</p>
</blockquote>
<p>上面的证明用到了对角线方法。</p>
<p>由时间谱系定理：</p>
<p>$$<br>\begin{align*}<br>    \text{EXP}&#x3D;&amp;\bigcup _{c\ge 1}\text{TIME}(2^{n^c}) \\<br>    \text{2EXP}&#x3D;&amp;\bigcup _{c\ge 1}\text{TIME}(2 ^{2^{n^c}}) \\<br>    \vdots<br>\end{align*}<br>$$</p>
<p>存在严格的包含关系，即$\text{EXP}\subset \text{2EXP}...$</p>
<h1 id="Nondeterministic-Time-Hierarchy-Theorem：非确定时间谱系理论"><a href="#Nondeterministic-Time-Hierarchy-Theorem：非确定时间谱系理论" class="headerlink" title="Nondeterministic Time Hierarchy Theorem：非确定时间谱系理论"></a>Nondeterministic Time Hierarchy Theorem：非确定时间谱系理论</h1><p>理论：如果$f$和$g$都是时间可构造的，且$f(n+1)&#x3D;o(g(n))$，则$\text{NTIME}(f(n))\subsetneq\text{NTIME}(g(n))$。</p>
<p>设有非确定图灵机$\mathbb{Z}$，只要证明$\mathbb{Z}$判定的语言L在$\text{NTIME}(g(n))$但不在$\text{NTIME}(f(n))$中即可。$\mathbb{Z}$的定义如下：</p>
<ol>
<li><strong>只</strong>处理长度大于1的全1串$1 ^n$；</li>
<li>输入带和<strong>第一条工作带</strong>同步地移动，第一条工作带在第一个工作区写入1，然后在后续的工作区中持续写入0，期间偶尔写入1；</li>
<li>用$h _0, h _1...$记录下所有写有1的工作区的下标；</li>
<li>在<strong>第二条工作带</strong>上，枚举所有的和时钟$2f(n)$硬连接的非确定图灵机NDTMs（的编码），记这些图灵机为$\mathbb{L} _1,\mathbb{L} _2...$；</li>
<li>在生成非确定图灵机$\mathbb{L} _i$后，$\mathbb{Z}$模拟计算$\mathbb{L} _{i-1}(1 ^{h _{i -1}+1})$，并将结果（0或1）写入第二条工作带上，<strong>同时</strong>，在<strong>第一条工作带</strong>的$h _i$位置写上1。<blockquote>
<p>注意，因为$h _0&#x3D;1$，所以$\mathbb{L} _0(1 ^{h _0+1})$是一开始就可以确定的，不需要等第一条工作带运行。<br>$\mathbb{L} _{i-1}(1 ^{h _{i -1}+1})$中的$\mathbb{L} _{i-1}$和$1 ^{h _{i-1}+1}$都通过回溯之前的历史得到，回溯的时间复杂度为$O(n)$，因为$h _{i -1}$是单调递增的，故只需要局部回溯。而$\mathbb{L} _{i-1}(1 ^{h _{i -1}+1})$则用新的工作带（写入$\mathbb{L} _{i-1}$与$1 ^{h _{i-1}+1}$）、草稿带来计算（在$2f(n)$必内停机）。</p>
</blockquote>
</li>
</ol>
<p>$\mathbb{Z}$的输出规定为：</p>
<ol>
<li>如果在输入串$1 ^n$扫描结束时，最新的$h _i &#x3D; n$且$\mathbb{L} _{i-1}(1 ^{h _{i -1}+1})&#x3D;0$，则$\mathbb{Z}$接受$1 ^n$；</li>
<li>反之如果$h _{i-1} &lt; n &lt; h _i$，则$\mathbb{Z}$模拟$\mathbb{L} _{i-1}$以非确定图灵机的方式计算$1 ^{n+1}$$g(n)$步，并以其结果作为最后的输出。</li>
</ol>
<p>矛盾性的证明：</p>
<blockquote>
<p>假设语言L被$\mathbb{Z}$接受：</p>
<ul>
<li>则L$\in \text{NTIME}(g(n))$，因为$\mathbb{Z}$运行的时间复杂度只由完全扫描完输入后的计算时间决定，而输出规定2可在$g(n)$时间内完成（输出规定1为常量时间）。</li>
</ul>
<p>再假定$\exists \mathbb{L} _i$在$f(n)$时间内接受L。于是，因为$f(n+1)&#x3D;o(g(n))$，所以输出规定2中$\mathbb{Z}$对$\mathbb{L} _i$（此处的$i$指输入扫描结束后计算的最后一个$h$的前一个）的模拟可以被完成。故，由于$\mathbb{L} _i$和$\mathbb{Z}$都接受L：<br>$$\mathbb{L} _i(1 ^{h _i + 1})&#x3D;\mathbb{Z}(1 ^{h _i + 1})$$<br>又由输出规定2：对输入串的长度$m&#x3D; h_i+1$，$\mathbb{Z}$的输出结果与$\mathbb{L} _i(1 ^{m+1})&#x3D;\mathbb{L} _i(1 ^{h _i + 1+1})$的结果相同：<br>$$\mathbb{Z}(1 ^{h _i + 1})&#x3D;\mathbb{L} _i(1 ^{h _i + 2})$$<br>又由两者都接受L：<br>$$\mathbb{L} _i(1 ^{h _i + 2})&#x3D;\mathbb{Z}(1 ^{h _i + 2})$$<br>若输入长度一直这样增加下去，则总会有一个时刻，$h _i +k&#x3D;h _{i+1}$，此时：<br>$$\mathbb{L} _i(1 ^{h _{i+1}})&#x3D;\mathbb{Z}(1 ^{h _{i+1}})$$<br>若$n&#x3D;h _{i+1}$，则由输出规定1：<br>$$\mathbb{Z}(1 ^{n})&#x3D;\mathbb{Z}(1 ^{h _{i+1}})\ne \mathbb{L} _i (1 ^{h _i+1})$$<br>出现矛盾，因此不存在这样的$\mathbb{L} _i$在$f(n)$时间内接受L。证毕。</p>
</blockquote>
<h1 id="Gap-Theorem：间隙定理"><a href="#Gap-Theorem：间隙定理" class="headerlink" title="Gap Theorem：间隙定理"></a>Gap Theorem：间隙定理</h1><p>描述：对任何一个可计算函数$r(x)\ge x$，一定存在一个可计算函数$b(x)$，使得$\text{TIME}(b(x))&#x3D;\text{TIME}(r(b(x)))$。</p>
<p>时间谱系定理告诉我们$\text{TIME}(n ^c)\subsetneq \text{TIME}(2 ^{n ^c})$。而间隙定理说明，并不是对所有的$b(n)$，$\text{TIME}(b(n))\subsetneq \text{TIME}(2 ^{b(n)})$都成立，即有可能$\text{TIME}(b(n))&#x3D;\text{TIME}(2 ^{b(n)})$。证明如下：</p>
<blockquote>
<ul>
<li>定义一个$r(x)\ge x$，以及$[0,r(k _x)]$上的$x+1$个不相交区间$[k _0, r(k _0)],...,[k _x, r(k _x)]$，其中：<br>$$<br>\begin{align*}<br>  k _0&#x3D;&amp;0 \\<br>  k _{i+1}&#x3D;&amp; r(k _i)+1,i &lt; x<br>\end{align*}$$</li>
<li>再定义$P(i,k)$为：对所有的$j \le i$及其编码的图灵机$\mathbb{M} _j$，对长度为$i$的字符串输入$z$，若$\mathbb{M} _j(z)$在$k$步内停机或者在$r(k)$步仍未停机，则$P(i,k)&#x3D;\text{True}$；否则$P(i,k)&#x3D;\text{False}$。</li>
</ul>
<p>对这些图灵机$\mathbb{M} _0,...,\mathbb{M} _i$，它们能够接受总输入个数为：<br>$$n _i&#x3D;\sum ^i _{j&#x3D;0}|\Gamma _j| ^i$$<br>因此它们的运行时间有 $n _i$种可能。而$[0,r(k _{n _i})]$上有$n _i+1$个不相交区间$[k _0, r(k _0)],...,[k _{n _i}, r(k _{n _i})]$。也就是说，总会有那么几个区间，在这些区间，没有任何的$\mathbb{M} _j(z)$运行步数在它们的范围内。换句话说，在这些区间的左端点$k _m$上，$P(i,k _m)$是$\text{True}$的。<br>由此，我们可以定义一个函数$b(i)$，对任何一个$i$，它的值是这些区间中的某一个的左端点，即$b(i)&#x3D;k _m$。于是我们有$P(i,b(i))$对于所有的$i$都成立。</p>
<ul>
<li>进一步地，假设$\mathbb{M} _j$在$r(b(n))$的时间内接受L。</li>
</ul>
<p>则对于任何的输入$x$，若其长度$n \ge j$，由$P(n,b(n))&#x3D;\text{True}$，我们有：<br>$\mathbb{M} _j(x)$要么在$b(n)$步内停机，要么$r(b(n))$步还不停机。<br>然而，$\mathbb{M} _j$在$r(b(n))$的时间内接受L，所以$\mathbb{M} _j(x)$在$r(b(n))$步内一定已经停机，所以$\mathbb{M} _j(x)$在$b(n)$步内停机，所以$\mathbb{M} _j$在$b(n)$的时间内接受L，所以：<br>$$\text{TIME}(b(n))&#x3D;\text{TIME}(r(b(n)))$$</p>
</blockquote>
<p>我们可以确保$b(n)$和$r(b(n))$满足$f(n)$和$g(n)$的大小关系，因为$r(x)$可以是任意的，只要$r(x)\ge x$即可。但$b(n)$却不满足时间谱系定理，这说明这样的$b(n)$是时间不可构造的。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zclzcl0223.github.io/2023/09/22/ComputationalComplexityTimecom3/" data-id="clzik0lc1000wv47k7xyp5dxm" data-title="Computational Complexity: Time Complexity 3" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Math/" rel="tag">Math</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Theoretical-Computer-Science/" rel="tag">Theoretical Computer Science</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-ComputationalComplexityTimecom2" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/09/21/ComputationalComplexityTimecom2/" class="article-date">
  <time class="dt-published" datetime="2023-09-21T11:59:36.000Z" itemprop="datePublished">2023-09-21</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/CS7313-Computational-Complexity/">CS7313: Computational Complexity</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/09/21/ComputationalComplexityTimecom2/">Computational Complexity: Time Complexity 2</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <span id="more"></span>

<h1 id="Diagonal-Method：对角线方法"><a href="#Diagonal-Method：对角线方法" class="headerlink" title="Diagonal Method：对角线方法"></a>Diagonal Method：对角线方法</h1><p>通用图灵机$\mathbb{U}(\alpha, x)$能够模拟$\mathbb{M} _\alpha (x)$的计算，因此，我们可以枚举每一个$\alpha$和$x$的编码，并制定如下所示的表：</p>
<p><img src="/2023/09/21/ComputationalComplexityTimecom2/1.jpg" alt="1"></p>
<center style="font-size:12px; font-weight:bold">Fig. 1. UTM</center><br>

<p>在上表中，每列从上到下枚举TM的编码 ，每行从左到右枚举输入$x$，使用UTM可以计算出每一格内的值。右表是一个假想的结果，其中$\uparrow$表示死循环。而这个表中，对角线上的值是一个特殊值，因为在对角线上，$\alpha&#x3D;x$，即UTM计算的是$\mathbb{M} _\alpha(\alpha)$。利用对角线做反证法，可以证明很多有趣的结论，这就是所谓的对角线方法（Diagonal Method）。</p>
<blockquote>
<p>通俗地来说，对角线方法就是用自己来推翻自己。</p>
</blockquote>
<p>下面给出了两个应用对角线方法的案例。</p>
<h2 id="UC：不可计算函数"><a href="#UC：不可计算函数" class="headerlink" title="UC：不可计算函数"></a>UC：不可计算函数</h2><p>定义函数：</p>
<p>$$<br>\text{UC}(\alpha) &#x3D;<br>\begin{cases}<br>    0,&amp; \text{if}\space \mathbb{M} _{\alpha} (\alpha)&#x3D;1\\<br>    1,&amp; \text{otherwise}<br>\end{cases}<br>$$</p>
<p>利用对角线方法可以证明上述函数是不可解的，也就是说没有图灵机能够求解该函数：</p>
<blockquote>
<p>假设存在图灵机$\mathbb{W}$能够计算UC，那么我们有：<br>$$<br>UC(\alpha) &#x3D; \mathbb{M} _{\llcorner \mathbb{W} \lrcorner}(\alpha)<br>$$<br>若：<br>$$<br>\mathbb{M} _{\llcorner \mathbb{W} \lrcorner}(\llcorner \mathbb{W} \lrcorner)&#x3D;1<br>$$<br>则：<br>$$<br>UC(\llcorner \mathbb{W} \lrcorner)&#x3D;\mathbb{M} _{\llcorner \mathbb{W} \lrcorner}(\llcorner \mathbb{W} \lrcorner)&#x3D;0<br>$$<br>同理，若$\mathbb{M} _{\llcorner \mathbb{W} \lrcorner}(\llcorner \mathbb{W} \lrcorner)&#x3D;0$，我们也会推出$\mathbb{M} _{\llcorner \mathbb{W} \lrcorner}(\llcorner \mathbb{W} \lrcorner)&#x3D;1$。两者互相矛盾。<br>因此，不存在这么一个图灵机，也就是说UC是不可计算的。</p>
</blockquote>
<h2 id="HALT：停机问题"><a href="#HALT：停机问题" class="headerlink" title="HALT：停机问题"></a>HALT：停机问题</h2><p>HALT，停机问题是这样定义的：</p>
<p>$$<br>\text{HALT}(\alpha,x) &#x3D;<br>\begin{cases}<br>    1,&amp; \text{if}\space \mathbb{M} _{\alpha} (x)\space \text{terminates}\\<br>    0,&amp; \text{otherwise}<br>\end{cases}<br>$$</p>
<p>停机问题可以用UC函数来证明：</p>
<blockquote>
<p>假设存在$\mathbb{M} _H$计算了停机问题，则对于任意的$\alpha$都有：<br>$$<br>\mathbb{M} _H(\alpha,\alpha)&#x3D;\text{HALT}(\alpha,\alpha) &#x3D;<br>\begin{cases}<br>    1,&amp; \text{if}\space \mathbb{M} _{\alpha} (\alpha)\space \text{terminates}\\<br>    0,&amp; \text{otherwise}<br>\end{cases}<br>$$<br>于是我们可以定义一个新的函数$\text{MU}(\alpha)$：<br>$$<br>\text{MU}(\alpha) &#x3D;<br>\begin{cases}<br>    0,&amp; \text{if}\space \mathbb{M} _{H} (\alpha,\alpha)&#x3D;1\space\&amp;\space\mathbb{M} _{\alpha} (\alpha)&#x3D;1\\<br>    1,&amp; \text{otherwise}<br>\end{cases}<br>$$<br>在上述函数的定义下，如果$\mathbb{M} _{\alpha} (\alpha)$停机了，那么其值要么是$1$，要么是$0$，其中$1$会使得$\text{MU}(\alpha)$输出$0$；如不停机，则$\text{MU}(\alpha)$会和$\mathbb{M} _{\alpha} (\alpha)$值为$1$一样输出$1$。换句话说，条件$ \mathbb{M} _{H} (\alpha,\alpha)&#x3D;1$在$\text{MU}$中并不发挥作用，只要$\mathbb{M} _{\alpha} (\alpha)&#x3D;1$，$\text{MU}(\alpha)$就只会输出$1$。于是：<br>$$<br>\text{MU}(\alpha) &#x3D;\text{UC}(\alpha)<br>$$<br>而UC是不可计算的，所以上面的所有证明都不成立，所以不存在这么一个图灵机计算了停机问题。</p>
</blockquote>
<p>停机问题不可计算说明，对于任意的图灵机$\mathbb{M} _\alpha$和输入$x$，在实际运行之前我们无法预先知道$\mathbb{M} _\alpha(x)$是否能停机。</p>
<blockquote>
<p>实际上上面的证明还用了归约法（Reduction Method），即通过放宽判定条件，使得某一问题$q$成为另一个问题$Q$的子问题。此时我们若证明$Q$不可计算，那么$q$也就相应地无法计算了。</p>
</blockquote>
<h1 id="Speedup-Theorem：加速理论"><a href="#Speedup-Theorem：加速理论" class="headerlink" title="Speedup Theorem：加速理论"></a>Speedup Theorem：加速理论</h1><p>加速理论解决的是对于某个问题，是否存在最快的算法的问题。直觉告诉我们，这个答案是否定的，因为只要我们有足够的空间，我们就可以空间换时间，通过“打表”把每一种输入的答案都列举出来，那么对于所有的问题我们都可以用<strong>常量的时间</strong>解决。而实际上也正是如此，没有最快的算法，只有更快的算法（理论上）。</p>
<blockquote>
<p><em>Blum’s Speedup Theorem</em>是关于此的很强的一个结论，但它的具体内容和证明都比较复杂，课程中没有涉及，本文也只记录关于线性加速理论的内容。</p>
</blockquote>
<h2 id="Linear-Speedup-Theorem：线性加速理论"><a href="#Linear-Speedup-Theorem：线性加速理论" class="headerlink" title="Linear Speedup Theorem：线性加速理论"></a>Linear Speedup Theorem：线性加速理论</h2><p>Hartmanis和Stearns于1965年证明了线性加速理论：</p>
<p><em>Linear Speedup Theorem：设图灵机$\mathbb{M}$在$T(n)$步内计算了$f$，任取$\epsilon &gt; 0$，总存在图灵机$\mathbb{M}&#39;$，它能在$\epsilon T(n)+n+2$步内计算$f$。</em></p>
<p>这里用到的方法是“压缩带子”，即通过编码，使得$\mathbb{M}&#39;$上的一格能够记录$\mathbb{M}$上$m$格的内容，这样$\mathbb{M}&#39;$的一步就相当于$\mathbb{M}$上的$m$步，其证明如下：</p>
<blockquote>
<p>对于$\mathbb{M}(\Gamma,Q,\delta)$，用一个新的符号集$\Gamma &#39;$为$\Gamma$中的$m$个字符用$1$个字符编码。不难看出，这样的符号集$\Gamma &#39;$是巨大的，其字符种类至少为$|\Gamma &#39;| &gt; |\Gamma|^m$。我们用$\Gamma &#39;$构建一个新的图灵机$\mathbb{M}&#39;(\Gamma &#39;,Q&#39;,\delta &#39;)$，对该图灵机：</p>
<ul>
<li>使用与$\mathbb{M}$相同的带子数；</li>
<li>对于$\mathbb{M}$纸带上长度为$n$的编码，$\mathbb{M}&#39;$可以在$n+2$步内将其转换为$n&#x2F;m$长的编码；（多出的2步是起始符和终止）</li>
<li>$\mathbb{M}&#39;$在$n&#x2F;m$步可以将所有的带头回归到原点；</li>
<li>$\mathbb{M}&#39;$纸带上的任何一个格子对应着$\mathbb{M}$上的$m$个格子。某一时刻$\mathbb{M}$在$m$个格子内<strong>任意一个格子</strong>上的$m$步在$\mathbb{M}&#39;$上至多会涉及到<strong>3</strong>个格子，因此$\mathbb{M}&#39;$只需要记录下这三个格子的值、做一步写入操作、移动一次带头，至多<strong>5</strong>步就可以模拟$\mathbb{M}$上的$m$步。<br>因此，总时间：<br>$$<br>n+2+\frac{n}{m}+5\frac{T(n)}{m}\le n+2 + \frac{6}{m}T(n)<br>$$<br>我们可以取$\epsilon&#x3D;6&#x2F;m$，$m&#x3D;6&#x2F;\epsilon$，证毕。</li>
</ul>
</blockquote>
<p><img src="/2023/09/21/ComputationalComplexityTimecom2/2.jpg" alt="2"></p>
<center style="font-size:12px; font-weight:bold">Fig. 2. 1 -> m</center>

<h1 id="Time-Complexity-Class：时间复杂性类"><a href="#Time-Complexity-Class：时间复杂性类" class="headerlink" title="Time Complexity Class：时间复杂性类"></a>Time Complexity Class：时间复杂性类</h1><p>对于语言L（即一个判定问题集合），如果存在图灵机$\mathbb{M}$和常数$c&gt;0$，使得$\mathbb{M}$能在$cT(n)$的时间内判定L，那么我们说$L\subseteq \text{TIME}(T(n))$。换句话说，$\text{TIME}(T(n))$中的语言都具有$O(T(n))$的算法。</p>
<p>复杂性类（Complexity Class）是一些具有类似复杂度的问题的集合，或者说是一些<strong>具有类似复杂度解</strong>的问题的集合。因为对于大多数的问题，我们都能用简单或复杂的方法将其解出来，换句话说，我们不能定义问题的复杂性，我们只能定义<strong>解的复杂性</strong>。显然，上面提到的$\text{TIME}(T(n))$不是一个复杂性类，因为它是和模型相关的。比如，对于回文问题，我们可以在$O(n)$的时间内解决，此时回文问题在$\text{TIME}(n)$中，但我们也可以用$O(n^2)$的方法解决，此时回文问题在$\text{TIME}(n^2)$中。</p>
<p>下面给出一些常见的时间复杂性类，这里面就有之前提到的P、NP问题。</p>
<h2 id="Complexity-Class-P"><a href="#Complexity-Class-P" class="headerlink" title="Complexity Class P"></a>Complexity Class P</h2><p>$\text{P}$类，即多项式类，所有具有多项式时间算法的问题都属于这一类：</p>
<p>$$<br>\text{P}&#x3D;\bigcup _{c\ge 1}\text{TIME}(n^c)<br>$$</p>
<p>在承认强CT论题的情况下，$\text{P}$类是与模型无关的，也就是说它是一个复杂性类。</p>
<blockquote>
<p>强CT论题（Strong Church-Turing Thesis）：所有物理上可实现的机器都能在多项式的时间复杂度内被图灵机模拟。</p>
</blockquote>
<h2 id="Complexity-Class-EXP"><a href="#Complexity-Class-EXP" class="headerlink" title="Complexity Class EXP"></a>Complexity Class EXP</h2><p>$\text{EXP}$类，即指数时间复杂性类：</p>
<p>$$<br>\text{EXP}&#x3D;\bigcup _{c\ge 1}\text{TIME}(2^{n^c})<br>$$</p>
<p>底数是无关紧要的，既可以是2，也可以是任何数字。出于规定，$\text{P}\subseteq\text{EXP}$，所以$n^c$不可以写成$cn$，否则集合内部的封闭性复合运算将不被满足。如：$n^3\in\text{P}$，所以$n^3\in\text{EXP}$，$2^n\in\text{EXP}$，那么将$n^3$和$2^n$复合得到的$2 ^{n^3}$理应也在$\text{EXP}$中。若将$n^c$写成$cn$则$2 ^{n^3}$将不被包含在$\text{EXP}$中，这显然是错误的。</p>
<h1 id="Nondeterministic-Turing-Machine：非确定图灵机"><a href="#Nondeterministic-Turing-Machine：非确定图灵机" class="headerlink" title="Nondeterministic Turing Machine：非确定图灵机"></a>Nondeterministic Turing Machine：非确定图灵机</h1><p>非确定性图灵机（Nondeterministic Turing Machine）具有两个迁移函数$\delta _1,\delta _2$，它在运行过程中会<strong>随机</strong>地选择一个迁移函数来执行，并产生相应的结果。不难看出，NDTM的计算路径可以被视为一棵二叉树，叶子结点表示某一条会停机并产生结果的执行路径，有些路径可能无限长：</p>
<p><img src="/2023/09/21/ComputationalComplexityTimecom2/3.jpg" alt="3"></p>
<center style="font-size:12px; font-weight:bold">Fig. 3. DTM and NDTM</center><br>

<p>上述的非确定图灵机在物理上是无法实现的，而之前所提到的物理上可实现的图灵机都称为确定图灵机（Deterministic Turing Machine，DTM）。</p>
<h2 id="Guessing：猜测"><a href="#Guessing：猜测" class="headerlink" title="Guessing：猜测"></a>Guessing：猜测</h2><p>NDTM通常被用来解决存在性问题。事实上，对于输入$x$，NDTM的每一条计算路径都在试图构造或者<strong>猜测</strong>一个对$x$存在性的证明，若构造&#x2F;猜测成功，则NDTM接受$x$，否则拒绝$x$。对于某一个决策问题或者语言，在NDTM中有如下规定：</p>
<ul>
<li>对于一台NDTM$\mathbb{N}$，当输入为$x$时，若$\mathbb{N}(x)$存在<strong>一条</strong>计算路径中止于接受格局（即$\mathbb{N}(x)&#x3D;1$），则称$\mathbb{N}$接受$x$；否则若所有的计算路径都失败，则称$\mathbb{N}$拒绝$x$。</li>
<li>对于一语言L，如果$x\in L$当且仅当$\mathbb{N}$接受$x$，则称$\mathbb{N}$接受L。</li>
</ul>
<blockquote>
<p>这实际上就是<strong>枚举</strong>，其中，有些解路径可能很短，能达到<strong>多项式的时间复杂度</strong>，而有些解路径可能很长，需要指数的时间复杂度。</p>
</blockquote>
<h2 id="Complexity-Class-NP-amp-NEXP"><a href="#Complexity-Class-NP-amp-NEXP" class="headerlink" title="Complexity Class NP &amp; NEXP"></a>Complexity Class NP &amp; NEXP</h2><p>类似于DTM，对于NDTM，我们也可以定义时间复杂性类：对于语言L（即一个判定问题集合），如果存在非确定图灵机$\mathbb{N}$和常数$c&gt;0$，使得$\mathbb{N}$能在$cT(n)$的时间内判定L，那么我们说$L\subseteq \text{NTIME}(T(n))$。换句话说，$\text{NTIME}(T(n))$中的语言都具有$O(T(n))$的算法。</p>
<p>同样地，定义非确定多项式（Non-deterministic Polynomial）复杂性类$\text{NP}$：</p>
<p>$$<br>\text{NP}&#x3D;\bigcup _{c\ge 1}\text{NTIME}(n^c)<br>$$</p>
<p>非确定指数复杂性类$\text{NEXP}$：</p>
<p>$$<br>\text{NEXP}&#x3D;\bigcup _{c\ge 1}\text{NTIME}(2^{n^c})<br>$$</p>
<p>它们与DTM的关系为：</p>
<p>$$<br>\text{P}\subseteq\text{NP}\subseteq\text{EXP}\subseteq\text{NEXP}<br>$$</p>
<ul>
<li>第一、三层包含是因为DTM是NDTM的特例，即$\delta_1&#x3D;\delta_2$；</li>
<li>第二层包含是因为DTM可以用指数的时间将NDTM的所有路径走一遍。</li>
</ul>
<blockquote>
<p>$\text{NP}$和$\text{P}$的区别在于，$\text{P}$的解的复杂度都是多项式的，而$\text{NP}$只是存在多项式复杂度的特解。能否证明$\text{P}&#x3D;\text{NP}$，是理论计算机科学界中的一大难题。</p>
</blockquote>
<h2 id="39-Universal-39-Nondeterministic-Turing-Machine：“通用”非确定图灵机"><a href="#39-Universal-39-Nondeterministic-Turing-Machine：“通用”非确定图灵机" class="headerlink" title="&#39;Universal&#39; Nondeterministic Turing Machine：“通用”非确定图灵机"></a>&#39;Universal&#39; Nondeterministic Turing Machine：“通用”非确定图灵机</h2><p>一个“通用”的非确定图灵机$\mathbb{V}$在模拟某个时间复杂度为$T(n)$的图灵机$\mathbb{M}$时需要四条带子：</p>
<ol>
<li>输入带；</li>
<li>存放snapshot的工作带，该工作带存放$\mathbb{V}$对$\mathbb{M}$执行时产生的所有snapshot的<strong>猜测</strong>，快照数为$O(T(n))$；</li>
<li>存放转移函数序列（即$01$串）的工作带，该工作带$01$串的长度也是$O(T(n))$，实际上就是$\delta_1$和$\delta_2$序列；</li>
<li>存放对被模拟工作带验证结果的工作带，它会记录一些snapshot，以验证对某个被模拟带子的模拟是否正确，若正确则清空，验证下一个带子。</li>
</ol>
<p>快照个数$O(T(n))$，$01$串的个数也是$O(T(n))$，故模拟的时间复杂度也是$T(n)$。这是在正确猜测的情况下，但实际上猜测序列、快照不一定是正确的，$T(n)$也不一定时间可构造，所以这台“通用”的非确定图灵机甚至有可能不停机，故“通用”是带引号的。若$T(n)$时间可构造则可掐表，以确保$\mathbb{V}$会停机。</p>
<blockquote>
<p>注意，以上的$T(n)$只对非确定图灵机的一条路径，即对非确定图灵机所“猜测”的任意一条路径，模拟它的时间复杂度可以到$T(n)$。</p>
</blockquote>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/348250098">计算复杂性（6）——世界难题：非确定型图灵机，NP类</a></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zclzcl0223.github.io/2023/09/21/ComputationalComplexityTimecom2/" data-id="clzik0lc6001iv47k42qtgrge" data-title="Computational Complexity: Time Complexity 2" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Math/" rel="tag">Math</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Theoretical-Computer-Science/" rel="tag">Theoretical Computer Science</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-DDSummary" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/09/20/DDSummary/" class="article-date">
  <time class="dt-published" datetime="2023-09-20T10:11:46.000Z" itemprop="datePublished">2023-09-20</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Paper/">Paper</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/09/20/DDSummary/">Dataset Distillation: A Summary</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <span id="more"></span>

<p>文中大多数内容都基于Yu et al.的论文<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2301.07014.pdf">Dataset Distillation: A Comprehensive Review</a>。</p>
<h1 id="Performance-Matching"><a href="#Performance-Matching" class="headerlink" title="Performance Matching"></a>Performance Matching</h1><p>Performance Matching，表现&#x2F;效果匹配，是最先被提出的一种基于机器学习的数据蒸馏方法。这种方法核心思想在于将<em>Synthetic Data</em>视作神经网络中的一个“超参数”，如学习率一般，但不同的是，它是可学习的超参数。其优化过程分两步：</p>
<ol>
<li>用<em>Synthetic Data</em>训练神经网络；</li>
<li>将原训练集$\mathcal{T}$输入用<em>Synthetic Data</em>训练的神经网络，计算得到预测的<em>Loss</em>，以该<em>Loss</em>作为损失函数更新<em>Synthetic Data</em>。</li>
</ol>
<p>最初的基于Performance Matching的方法的精度和训练速度都不怎么样，但后来有学者将第一步中的神经网络替换为了核函数，反而使得Performance Matching超过了多数后来的方法。</p>
<h2 id="Meta-Learning-Based-Methods"><a href="#Meta-Learning-Based-Methods" class="headerlink" title="Meta Learning Based Methods"></a>Meta Learning Based Methods</h2><p>该方法最先在Wang et al.的<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1811.10959.pdf">DATASET DISTILLATION</a>中被提出，也是数据蒸馏开山立宗的方法，一般简称<em>DD</em>。<em>Meta Learning</em>也就是把<em>Synthetic Data</em>视作一个可学习的超参数。它获得<em>Synthetic Data</em>的思路就是上面所提到的，基本公式与流程如下：</p>
<p>$$<br>\begin{align*}<br>    \theta ^{(t)}&amp;&#x3D;\theta ^{(t-1)}-\eta\nabla l(\mathcal{S};\theta ^{(t-1)})\tag{1}\\<br>    \mathcal{S}^{(\tau)}&amp;&#x3D;\mathcal{S}^{(\tau-1)}-\eta&#39;\nabla L(\theta^{(T)},\mathcal{T})\tag{2}<br>\end{align*}<br>$$</p>
<p>其中，式$(1)$是内层循环，用于更新神经网络，式$(2)$是外层循环，用于更新<em>Synthetic Data</em>。</p>
<p><img src="/2023/09/20/DDSummary/1.png" alt="1"></p>
<center style="font-size:12px; font-weight:bold">Fig. 1. Meta Learning Based Methods</center><br>

<p>显而易见，这是一种<em>Bi-level Optimization Algorithm</em>：内层循环更新神经网络，外层循环更新<em>Synthetic Data</em>。而内层循环又需要用到<em>Synthetic Data</em>来产生梯度以训练网络，因而<em>Synthetic Data</em>的更新内在地会包含一个二阶导，这是极耗内存和时间的。而实际上蒸馏出来的数据效果也不是特别好。</p>
<h2 id="Kernel-Ridge-Regression-Based-Methods"><a href="#Kernel-Ridge-Regression-Based-Methods" class="headerlink" title="Kernel Ridge Regression Based Methods"></a>Kernel Ridge Regression Based Methods</h2><p>该方法最初在Nguyen et al.的<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2011.00050.pdf">DATASET META-LEARNING FROM KERNEL RIDGEREGRESSION</a>中被提出，一般简称<em>KIP</em>。这种方法是建立在<strong>无限宽的全连接层可以用特殊的核函数来拟合</strong>这一研究成果的。<em>KIP</em>用到的核函数被称为<em>Neural Tangent Kernel</em>（NTK）。它本质上也是<em>Meta Learning</em>，只不过使用核函数来替代了内层循环，并用<em>Kernel Ridge Regression</em>计算损失函数，使得原本的双层优化变为了一层：</p>
<p>$$<br>\begin{align*}<br>    L(\mathcal{S},\mathcal{T})&amp;&#x3D;||Y _\mathcal{T}-K _{X _\mathcal{T}X _\mathcal{S}}(K _{X _\mathcal{S}X _\mathcal{S}}+\lambda I)^{-1}Y _\mathcal{S}||^2\\<br>    \mathcal{S}^{(\tau)}&amp;&#x3D;\mathcal{S}^{(\tau-1)}-\eta&#39;\nabla L(\mathcal{S},\mathcal{T})<br>\end{align*}<br>$$</p>
<p>当然，还有使用其他核函数的版本，但一般用的都是<em>NTK</em>，此处就不再赘述了。</p>
<blockquote>
<p>文中还提到了另一个脱胎于<em>Kernel Ridge Regression</em>的方法<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2206.00719.pdf"><em>FRePo</em></a>，该方法在目前来说取得了最好的蒸馏效果。</p>
</blockquote>
<h1 id="Parameter-Matching"><a href="#Parameter-Matching" class="headerlink" title="Parameter Matching"></a>Parameter Matching</h1><p>Parameter Matching，参数匹配，这种方法的想法很直观：要想<em>Synthetic Data</em>取得和训练集相近的泛化精度，只要<em>Synthetic Data</em>训练出来的网路的参数（Parameter）和训练集训练出来的网络的参数相近即可。不过实际上并没有人会直接匹配参数，而是转而去匹配训练过程中的梯度，即让<em>Synthetic Data</em>产生和训练集相近的梯度，因而这类方法也可以叫<em>Gradient Matching</em>。</p>
<h2 id="Single-Step-Parameter-Matching"><a href="#Single-Step-Parameter-Matching" class="headerlink" title="Single-Step Parameter Matching"></a>Single-Step Parameter Matching</h2><p>Single-Step Parameter Matching在<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2006.05929.pdf">DATASET CONDENSATION WITH GRADIENT MATCHING</a>中被Zhao et al.提出，是最先被提出的基于参数匹配的数据蒸馏方法，一般称<em>DC</em>。其核心思想很简单：</p>
<ol>
<li>训练集$\mathcal{T}$和<em>Synthetic Data</em> $\mathcal{S}$分别过一次神经网络；</li>
<li>分别计算梯度；</li>
<li>计算梯度差异，作为$Loss$；</li>
<li>更新<em>Synthetic Data</em>，用更新后的<em>Synthetic Data</em>更新神经网络。</li>
</ol>
<p>文中用到的作为$Loss$的梯度差异函数为$\cos$函数，即更关心梯度的方向：</p>
<p>$$<br>Loss(\mathcal{S},\mathcal{T})&#x3D;1-\frac{grad(\mathcal{S})\cdot grad(\mathcal{T})}{||grad(\mathcal{S})||\space||grad(\mathcal{T})||}<br>$$</p>
<p>上面的$grad$是指输入数据在神经网络上所产生的梯度。基本流程如下：</p>
<p><img src="/2023/09/20/DDSummary/2.png" alt="2"></p>
<center style="font-size:12px; font-weight:bold">Fig. 2. Meta Learning Based Methods</center><br>

<blockquote>
<p>后续有不少学者在损失函数$Loss$上做文章，如不仅关心方向，还关心大小（欧式距离）等，此处就不再赘述了。</p>
</blockquote>
<h2 id="Multi-Step-Parameter-Matching"><a href="#Multi-Step-Parameter-Matching" class="headerlink" title="Multi-Step Parameter Matching"></a>Multi-Step Parameter Matching</h2><p>有一步梯度匹配，就会有多步梯度匹配。Multi-Step Parameter Matching最先在<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2203.11932.pdf">Dataset Distillation by Matching Training Trajectories</a>中被Cazenavette et al.提出，一般简称<em>MTT</em>。在Kernel Ridge Regression Based Methods出现之前，这是表现最好的方法。其本质上是对Single-Step的一种优化，因为单步的匹配本质上是一种<em>贪心</em>的算法，它不一定能够得到全局的最优，而用训练集的多步梯度下降来匹配<em>Synthetic Data</em>的一步或两步梯度下降能够为更新<em>Synthetic Data</em>带来更加全局的、稳定的优化路径。方法示意图如下：</p>
<p><img src="/2023/09/20/DDSummary/3.png" alt="3"></p>
<center style="font-size:12px; font-weight:bold">Fig. 3. Multi-Step Parameter Matching</center><br>

<p>此外，文中用到的损失函数也不同，作者使用的损失函数是规范化了的欧式距离（的平方）：</p>
<p>$$<br>Loss(\mathcal{S},\mathcal{T})&#x3D;\frac{||\theta ^{\mathcal{S}} _{t+N}-\theta ^{\mathcal{T}} _{t+M}||^2 _2}{||\theta ^{\mathcal{T}} _{t}-\theta ^{\mathcal{T}} _{t+M}||^2 _2}<br>$$</p>
<p><em>MTT</em>的另一大优点是蒸馏速度很快，因为其训练集训练的网络都是预先训练好的，这些预训练好的网络被称为<em>buffer</em>或者<em>teacher net</em>。后续也有不少的学者对<em>MTT</em>进行了优化，如：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2211.11004.pdf"><em>FTD</em></a>：优化了<em>buffer</em>，使得生成的<em>buffer</em>能够提供更接近于<em>Synthetic Data</em>训练网络的优化路径；</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2209.14609.pdf"><em>DDPP</em></a>：优化了更新<em>Synthetic Data</em>的时机，即预先评估本次更新<em>Synthetic Data</em>能够带来的收益，根据收益大小来决定是否要执行本次更新。评估的方式是为$\theta ^{\mathcal{T}} _{t+M}&#x2F;\theta ^{\mathcal{S}} _{t+N}$设置一个阈值$\epsilon$，当两者的比例小于该阈值时就不执行本次<em>Synthetic Data</em>的更新。</li>
<li>...</li>
</ul>
<h1 id="Distribution-Matching"><a href="#Distribution-Matching" class="headerlink" title="Distribution Matching"></a>Distribution Matching</h1><p>Distribution Matching，分布匹配，核心思想是让<em>Synthetic Data</em>的统计分布尽可能地接近训练集。接近程度的衡量指标一般用的是<em>Maximum Mean Discrepancy</em>（MMD），即用<em>高阶矩</em>来衡量两个分布的接近程度，而高级矩的计算则常以通过神经网络将输入特征映射到高维空间后，计算高维空间向量距离的方式进行。这种方法最大的优点就是蒸馏速度很快。</p>
<h2 id="Single-Layer-Distribution-Matching"><a href="#Single-Layer-Distribution-Matching" class="headerlink" title="Single Layer Distribution Matching"></a>Single Layer Distribution Matching</h2><p>该方法是分布匹配最初的方法，由Zhao et al.在<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2110.04181.pdf">Dataset Condensation with Distribution Matching</a>提出，一般简称<em>DM</em>。思路与前文提到的一致。其计算高阶矩所采用的神经网络是原来用于分类任务的神经网络，不过去掉了softmax操作。基本流程为：</p>
<p><img src="/2023/09/20/DDSummary/4.png" alt="4"></p>
<center style="font-size:12px; font-weight:bold">Fig. 4. Single Layer Distribution Matching</center>

<h2 id="Multi-Layer-Distribution-Matching"><a href="#Multi-Layer-Distribution-Matching" class="headerlink" title="Multi Layer Distribution Matching"></a>Multi Layer Distribution Matching</h2><p>该方法由Wang et al.在<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2203.01531.pdf">Cafe: Learning to condense dataset by aligning features</a>上提出，一般简称<em>CAFE</em>。实际上是对<em>DM</em>的优化，主要的不同点在于：<em>DM</em>只对整个神经网络的输出向量进行匹配，而<em>CAFE</em>则是对每一个隐藏层的输出向量进行匹配并相加。本质上差不多，没有跳出Distribution Matching的框架。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zclzcl0223.github.io/2023/09/20/DDSummary/" data-id="clzik0lc7001qv47k6f0ac5h5" data-title="Dataset Distillation: A Summary" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Dataset-Distillation/" rel="tag">Dataset Distillation</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Paper/" rel="tag">Paper</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-ComputationalComplexityTimeComp" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/09/12/ComputationalComplexityTimeComp/" class="article-date">
  <time class="dt-published" datetime="2023-09-12T09:51:52.000Z" itemprop="datePublished">2023-09-12</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/CS7313-Computational-Complexity/">CS7313: Computational Complexity</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/09/12/ComputationalComplexityTimeComp/">Computational Complexity: Time Complexity 1</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <span id="more"></span>

<h1 id="Notation：记号约定"><a href="#Notation：记号约定" class="headerlink" title="Notation：记号约定"></a>Notation：记号约定</h1><ul>
<li>$\log x$默认为$\log _2x$。</li>
<li>若$S$是有限的符号集，如$\left\{0,1\right\}$，则$S ^n$表示所有长度为$n$的由$S$中字符组成的字符串集合，$S ^*$表示任意长度的字符串集合。对于单个符号$x$，$x ^n$则表示$n$个$x$组成的字符串。</li>
<li>字符串$x$的长度记为$|x|$或$n$，这一节及后续的相关章节都会用$n$。</li>
<li>记$\llcorner x\lrcorner$为$x$的二进制编码。</li>
<li>记$1^n$为长度为$n$的任意$01$串。</li>
</ul>
<h1 id="Turing-Machine：图灵机模型"><a href="#Turing-Machine：图灵机模型" class="headerlink" title="Turing Machine：图灵机模型"></a>Turing Machine：图灵机模型</h1><p>图灵机（Turing Machine），又称图灵计算机，指一个抽象的机器，是英国数学家艾伦・麦席森・图灵于1936年提出的一种抽象的计算模型，即将人们使用纸笔进行数学运算的过程进行抽象，由一个虚拟的机器替代人类进行数学运算。它有多条无限长的纸带，纸带分成了一个一个的小方格，每个方格有不同的内容。有一个机器头在纸带上移来移去。机器头有一组内部状态，还有一些固定的程序。在每个时刻，机器头都要从当前纸带上读入一个方格信息，然后结合自己的内部状态查找程序表，根据程序输出信息到纸带方格上，并转换自己的内部状态，然后进行移动。</p>
<h2 id="K-Tap-Turing-Machine"><a href="#K-Tap-Turing-Machine" class="headerlink" title="$K$-Tap Turing Machine"></a>$K$-Tap Turing Machine</h2><p>$k$带图灵机$\mathbb{M}$有$k$条纸带，其中：</p>
<ul>
<li>第一条纸带是<strong>只读纸带</strong>（Input Tape），它包含了输入图灵机的问题或数据规模；</li>
<li>其他纸带是<strong>可读写</strong>的工作纸带（Work Tape），其中最后一条纸带又同时作为记录最后输出的纸带（Output Tape）；</li>
<li>各纸带的<strong>带头</strong>（Tape Head）指明了正在读&#x2F;写的纸带信息。</li>
</ul>
<p><img src="/2023/09/12/ComputationalComplexityTimeComp/1.png" alt="1"></p>
<center style="font-size:12px;font-weight:bold">Fig. 1. K-tape turing machine</center><br>

<blockquote>
<p>不难看出，图灵机模拟了人类解题时<strong>读题（Input）</strong>、<strong>打草稿（Work）</strong>和<strong>写答案（Output）</strong>的过程。</p>
</blockquote>
<p>一个$k$带的图灵机$\mathbb{M}$可由一个三元组$(\Gamma,Q,\delta)$唯一描述：</p>
<ul>
<li>$\Gamma$是一个有限的符号集，称<em>Alphabet</em>。我们规定，符号集必须只由$\left\{0,1,\square,\triangleright\right\}$组成，其中$\left\{0,1\right\}$是有效字符，而$\left\{\square,\triangleright\right\}$则是分别表示<em>空白字符</em>和<em>纸带开始</em>的功能字符；</li>
<li>$Q$是一个有限状态集，包含了该图灵机的所有状态，包括起始状态$q _{start}$和停机状态$q _{halt}$；</li>
<li>$\delta$是一个转移函数（Transition Function），实际上相当于烧至在图灵机中的一个<em>程序</em>。</li>
</ul>
<p>图灵机当前的状态（State）会被记录在一个状态寄存器$q$中。在任何时刻$t$，图灵机都能够获取图灵机在这个时刻的快照（Snapshot）：每个纸带的符号$\Gamma ^k$以及图灵机当前的状态$q _t$，记该快照为$Q\times\Gamma ^k$，那么转移函数$\delta$便可指挥图灵机进入新的快照$Q\times\Gamma ^{k-1}\times\left\{L,S,R\right\} ^k$，即：</p>
<p>$$<br>Q\times\Gamma ^k\to Q\times\Gamma ^{k-1}\times\left\{L,S,R\right\} ^k<br>$$</p>
<p>其中，$Q\times\Gamma ^{k-1}\times\left\{L,S,R\right\} ^k$分别表示“图灵机进入什么状态，$k-1$个读写纸带写入什么内容，$k$个带头分别怎么移动”。我们规定，带头一次只能向左或向右移动一个位置或者不移动。图灵机初始化时，状态为$q _{start}$，每个带头都在左边，带子上都写着$\triangleright$，除输入带以外，其他带子的其他位置都写着$\square$。</p>
<p>图灵机在每个时刻的信息除了快照，还有格局（Configuration），格局拥有比快照更加丰富的内容：</p>
<ul>
<li>图灵机当前的状态；</li>
<li>所有纸带的内容；</li>
<li>当前带头的位置。</li>
</ul>
<p>不难看出，格局包含了一次转移函数计算所需要的全部内容（带头位置结合纸带内容唯一确定了当前纸带的符号），因此，一次转移函数计算就是由<strong>一个格局到另一个格局</strong>的过程，这也称一次计算（Computation）。</p>
<blockquote>
<p>格局不仅与$\mathbb{M}$有关，还与输入串的长度有关，而快照则只和$\mathbb{M}$有关。</p>
</blockquote>
<h2 id="Function、Problem、Language：函数、问题、语言"><a href="#Function、Problem、Language：函数、问题、语言" class="headerlink" title="Function、Problem、Language：函数、问题、语言"></a>Function、Problem、Language：函数、问题、语言</h2><p><strong>一个问题就是一个函数</strong>。对于任何一个问题，它都有解决的前提条件以及根据这些条件得到的问题答案，而函数接受输入，得到输出，两者在本质上是具有同一性的。在图灵机中，任何问题的前提条件（规模、数据结构、数据等）都可以用有限个$01$串表示，记为$\left\{0,1\right\}^*$。</p>
<p>故而，<strong>问题</strong>（Problem）就是一个<strong>函数</strong>（Function）：</p>
<p>$$<br>f:\left\{0,1\right\}^* \to<br> \left\{0,1\right\}^*<br>$$</p>
<p>这个函数实现了对问题的回答，即由问题的$01$串表示得到对这个问题答案的$01$串表示。</p>
<p>我们称图灵机$\mathbb{M}$计算&#x2F;解决（Compute&#x2F;Solve）了问题$f$，当且仅当对于任意的$x\in\left\{0,1\right\}^*$均有$\mathbb{M}(x)&#x3D;f(x)$。其中$\mathbb{M}(x)$的含义是：如果$x$在$f$的定义域内，当给图灵机$\mathbb{M}$的输入带上写上$x$并进行计算时，它最终会停机，且输出带上写着$\mathbb{M}(x)$；如果$x$不在$f$的定义域内，则图灵机永不停机（死循环）。图灵机会停机记为$\mathbb{M}(x)\downarrow$，永不停机记为$\mathbb{M}(x)\uparrow$。</p>
<h3 id="Decision-Problem：判定问题"><a href="#Decision-Problem：判定问题" class="headerlink" title="Decision Problem：判定问题"></a>Decision Problem：判定问题</h3><p>判定问题是一个特殊的问题，它的输出是个布尔值，即只有$0$或$1$：</p>
<p>$$<br>d:\left\{0,1\right\}^* \to<br> \left\{0,1\right\}<br>$$</p>
<p>对于判定问题，我们不再用解决问题，而称图灵机$\mathbb{M}$判定（Decide）了问题$d$。</p>
<h3 id="Language：语言"><a href="#Language：语言" class="headerlink" title="Language：语言"></a>Language：语言</h3><p>语言（Language）$L$是一个字符串集合$L\subseteq \left\{0,1\right\}^*$。若：</p>
<p>$$<br>\begin{align*}<br>    &amp;\forall x\in L,\mathbb{M}(x)&#x3D;1\\<br>    &amp;\forall x\notin L,\mathbb{M}(x)&#x3D;0<br>\end{align*}<br>$$<br>我们则称图灵机$\mathbb{M}$接受（Accept）了语言$L$。换句话说，图灵机$\mathbb{M}$能够识别出$L$及其子集。</p>
<h3 id="语言与判定问题"><a href="#语言与判定问题" class="headerlink" title="语言与判定问题"></a>语言与判定问题</h3><p>不难看出，一个语言就是一类判定问题：</p>
<p>$$<br>d(x)&#x3D;<br>\begin{cases}<br>    1,&amp;x\in L\\<br>    0,&amp;x\notin L<br>\end{cases}<br>$$</p>
<h2 id="Time-Function：时间函数"><a href="#Time-Function：时间函数" class="headerlink" title="Time Function：时间函数"></a>Time Function：时间函数</h2><p>时间函数与时间复杂度有着近乎相同的含义，它们都不是现实世界中连续的时间，而是图灵&#x2F;计算机世界中离散的时间。对于时间函数，其单位是一步计算，即一次转移函数；对时间复杂度，其单位是计算机的一次基本操作，如加、乘等。由于我们并不能知道图灵机$\mathbb{M}$在解决某个问题时转移函数的执行次数，我们就需要一个计时器，这个计时器也是一个图灵机$\mathbb{T}$，它与$\mathbb{M}$同启同停，并在结果输出$\mathbb{M}$转移函数执行的次数。这样的计时器定义的函数就称为时间函数：</p>
<p>$$<br>T: N \to N<br>$$</p>
<p>它实现问题的规模（输入串的<strong>长度</strong>$n$）到转移函数执行步数$T(n)$的映射。时间函数定义了图灵机解决某一问题时间的上限，即：若图灵机$\mathbb{M}$解决输入长度为$n$的$f$需要<strong>至多</strong>$T(n)$步，那么我们称$\mathbb{M}$<strong>在</strong>$T(n)$<strong>的时间内解决了</strong>$f$。</p>
<h3 id="Time-Constructible-Function：时间可构造函数"><a href="#Time-Constructible-Function：时间可构造函数" class="headerlink" title="Time Constructible Function：时间可构造函数"></a>Time Constructible Function：时间可构造函数</h3><p>用图灵机$\mathbb{T}$实现时间函数要求：</p>
<ol>
<li>图灵机$\mathbb{T}$的执行步数为$T(n)$；</li>
<li>图灵机$\mathbb{T}$的输出结果$\mathbb{T}(n)$为$T(n)$。</li>
</ol>
<p>如，若定义$T(n)&#x3D;n^2$，则该图灵机必须在$n^2$步由初始状态$\llcorner n\lrcorner$转变为停止状态$\llcorner n^2 \lrcorner$，然而这样的图灵机是不一定存在的。</p>
<p>时间可构造函数是为了尽量避免上述的问题而采用的退而求其次的方法。它不同于时间函数的地方在于：</p>
<ol>
<li>时间可构造函数$T$不要求输入串为$\llcorner n \lrcorner$，任意的$01$串$1^n$均可；</li>
<li>时间可构造函数$T$不要求步数严格为$T(n)$，只要是同一个数量级即可，即$O(T(n))$即可。</li>
</ol>
<p>只要存在这么一个$1 ^n$，使得图灵机$\mathbb{T}$能在$O(T(n))$步下转移到$\llcorner T(n) \lrcorner$状态并且停机，那么我们就称$T$是可构造时间函数。若将条件2升级为时间函数的条件，则称$T$为完全时间可构造的（Fully Time Constructible Function），即$T$转移到$\llcorner T(n) \lrcorner$状态的步数严格为$T(n)$。</p>
<h2 id="计算复杂性与模型无关"><a href="#计算复杂性与模型无关" class="headerlink" title="计算复杂性与模型无关"></a>计算复杂性与模型无关</h2><p>在不同的模型、不同的图灵机上，计算同一个函数$f$所花的时间可能不同。但是直观上来看，问题&#x2F;函数的复杂性是其客观性质，应该与模型无关。</p>
<ul>
<li><em>强邱奇-图灵论题（Church-Turing Thesis）</em>：任何物理上可以实现的计算装置A，都可以被图灵机以多项式的代价模拟。即，A的$t$个步骤可以用图灵机的$t^c$个步骤模拟，其中$c$是常数，与A有关（即取决于将A编码的最简单方式）。直观地理解，只要正确地编码，简单的图灵机可以以时间为代价模拟复杂的图灵机。<em>另一种说法：物理上能实现的机器，其最大表达能力不会超过图灵机</em>。</li>
</ul>
<blockquote>
<p>Oblivious Turing Machine（遗忘图灵机）：这种图灵机的读写头的位置只与输入字符串的长度$n$、当前步数$i$有关。换句话说，无论输入的字符串是什么，只要长度一致，那么它们在第$i$步时，读写头的位置就是固定的。</p>
</blockquote>
<h1 id="Turing-Machine-as-String：图灵机的编码"><a href="#Turing-Machine-as-String：图灵机的编码" class="headerlink" title="Turing Machine as String：图灵机的编码"></a>Turing Machine as String：图灵机的编码</h1><p>计算理论的核心：二进制位串可以编码任何有限的语法对象，如程序、图灵机、算法。一台图灵机是一个有限对象，因此也可以用$01$串对它进行编码。由于字母表$\Gamma$和状态集$Q$可以直接从转移函数$\delta$中获取，要编码一台图灵机，只要对它的所有转移函数进行编码即可。</p>
<blockquote>
<p>转移函数实际上就是计算机指令。</p>
</blockquote>
<p>如，对于一台TM：$|\Gamma|\le 32$，$|Q|\le 4$，设其中的一条指令$\delta$为：</p>
<p>$$<br>&lt;q_1, a, b, c&gt;\to &lt;q_2, e, d, L, S, R&gt;<br>$$</p>
<p>则，我们可以用5位$01$串对字母表进行编码，用4位$01$串对状态码进行编码（不用2位是为了更好区分？），用2位$01$串对带头的移动指令进行编码，再引入分隔符号$-$，可得到该指令的编码：</p>
<p>$$<br>0001- 00001- 00010- 00011-- 0010- 00100 - 00101 - 10 - 00 - 01<br>$$</p>
<p>记上面的指令为$\delta _i$。对于一台图灵机的全部指令，再用一个新的分隔符$&#x3D;$隔开，在将指令串起，就得到整台图灵机的编码：</p>
<p>$$<br>\delta _1&#x3D;\delta _2&#x3D; ...&#x3D;\delta _n<br>$$</p>
<p>上述编码形式中，我们一共使用了4种符号$\{0,1,-,&#x3D;\}$。此时，我们在将这些符号映射为$01$串：</p>
<p>$$<br>\{0,1,-,&#x3D; \}\to\{01,10,00,11\}<br>$$</p>
<p>我们便得到了一台图灵机$\mathbb{M}$的$01$串形式的编码。</p>
<blockquote>
<p>由于此处打不出十字<code>\dag</code>和双十字<code>\ddag</code>，故用$-$和$&#x3D;$代替。</p>
<p>任何一台图灵机，只要我们知道它的转移函数，都可以用这种方式进行编码。</p>
</blockquote>
<h2 id="图灵机编码的性质"><a href="#图灵机编码的性质" class="headerlink" title="图灵机编码的性质"></a>图灵机编码的性质</h2><ol>
<li>每一台图灵机都有无数个$01$串编码。</li>
</ol>
<p>这点很好理解。一台图灵机代表一种解决某个问题的方式。对于一台解决特定问题的图灵机，我们完全可以为其加上许多<strong>多余的、无用的</strong>状态或符号，此时图灵机的编码变长了，但是它还是在用相同的方法解决完全一样的问题，因而我们将前后两台图灵机视作<strong>同一台图灵机</strong>。</p>
<ol start="2">
<li>每一个$01$串编码都对应这一台图灵机。</li>
</ol>
<p>由于$01$串编码的任意性，某个$01$串对应的图灵机可能完全没有实际用处，如对于任意输入都直接停机的空图灵机。</p>
<p>上述的两条规定有一定的方便性。它们使得我们可以将所有的图灵机及其对应的函数列举出来：</p>
<ul>
<li>记$\llcorner \mathbb{M}\lrcorner$为某台图灵机的编码；</li>
<li>记$\mathbb{M} _\alpha$为编码$\alpha$所对应的图灵机；</li>
<li>记一个特殊的双射函数将任意的$01$串映射为一个自然数：$\{ 0,1 \}^* \to N$。</li>
</ul>
<p>由此，我们可以得到所有图灵机的排列：</p>
<p>$$<br>\mathbb{M}_0,\mathbb{M}_1,...,\mathbb{M}_i,...<br>$$</p>
<p>以及其对应的所有可计算函数的排列：</p>
<p>$$<br>\phi_0,\phi_1,...,\phi _i,...<br>$$</p>
<blockquote>
<p>由于一台图灵机有无数的$01$串编码，在这些排列中，每台图灵机都会出现无数次。</p>
<p>一个函数是可计算的当且仅当有一台图灵机能实现这个函数。</p>
</blockquote>
<h1 id="Universal-Turing-Machine：通用图灵机（套娃开始）"><a href="#Universal-Turing-Machine：通用图灵机（套娃开始）" class="headerlink" title="Universal Turing Machine：通用图灵机（套娃开始）"></a>Universal Turing Machine：通用图灵机（套娃开始）</h1><p>如上所述，任何一台图灵机都可以用$01$串编码，而输入图灵机的数据的形式也是$01$串，那么，我们就可以把图灵机的$01$串形式<strong>输入到另一台图灵机中，这台图灵机是一台特殊的图灵机，它可以模拟输入图灵机的运行</strong>。这样的图灵机就称<strong>通用图灵机</strong>（UTM）。事实上，计算机就是这样一台通用图灵机。因为我们在计算机上所编写的每一个程序都可以被视为一台图灵机，而任何一个正确的程序（只要有相应的编码器）都可以在现代的计算机上运行。换句话说，现代的计算机能够模拟任何一台图灵机的运行。</p>
<ul>
<li><p>高效通用图灵机定理：存在通用图灵机$\mathbb{U}$，使得下面两个式子定理成立：</p>
<ol>
<li>对于任意的$x,\alpha \in \{0,1\} ^*$，均有$\mathbb{U}(\alpha,x)\simeq\mathbb{M}_\alpha(x)$；</li>
<li>若$\mathbb{M}_\alpha(x)$的时间函数是$T(n)$，则$\mathbb{U}(\alpha,x)$的时间函数是$cT(n)\log T(n)$，其中$c$是$\alpha$的多项式$|\alpha| ^c$，但是与$\alpha,x$均无关。</li>
</ol>
<p>通俗地来讲，对于任意一台图灵机$\mathbb{M}_\alpha$，通用图灵机$\mathbin{U}$都可以模拟它的计算过程，且至多多耗费对数倍的时间。</p>
</li>
</ul>
<p>下面两小节是对于定理$2$的简单证明。</p>
<h2 id="Proof-of-Hartmanis-and-Stearns"><a href="#Proof-of-Hartmanis-and-Stearns" class="headerlink" title="Proof of Hartmanis and Stearns"></a>Proof of Hartmanis and Stearns</h2><p>Hartmanis和Stearns在1965年给出了通用图灵机$O(cT^2(n))$的证明。</p>
<p>用通用图灵机$\mathbb{U}$模拟一台$k$带的图灵机$\mathbb{M} _{\alpha}$，通用图灵机的输入带上给出图灵机的编码$\alpha$以及图灵机的输入$x$，我们想要得到的效果是通用图灵机的输出带上输出结果$\mathbb{M} _{\alpha}(x)$或者如$\mathbb{M} _\alpha$一样永不停机。构造这样的通用图灵机需要<strong>5</strong>条带子：</p>
<ol>
<li>输入带：输入为被模拟图灵机的编码$\alpha$和被模拟图灵机的输入$x$；</li>
<li>输出带：输出为$\mathbb{M}_{\alpha}(x)$；</li>
<li>模型工作带：记录$\mathbb{M} _{\alpha}$的转移函数；</li>
<li>状态工作带：记录$\mathbb{M} _{\alpha}$当前的工作状态；</li>
<li>主工作带：通用图灵机的带子数目必须是确定的，我们不能用不能确定数目的$k-2$条带子去模拟被模拟图灵机的工作带，因此，主工作带上记录了被模拟图灵机工作带及其带头的内容。</li>
</ol>
<p><img src="/2023/09/12/ComputationalComplexityTimeComp/2.jpg" alt="2"></p>
<center style="font-size:12px;font-weight:bold">Fig. 2. 5-tape universal turing machine</center><br>

<p>这样的通用图灵机工作过程是很明确的：</p>
<ol>
<li>从输入带、输出带、主工作带、状态工作带获取$\mathbb{M}_{\alpha}$当前的快照：$k$条带子的值以及当前状态$q$；</li>
<li>扫描模型工作带，获取转移函数$\delta$；</li>
<li>模拟$\mathbb{M} _{\alpha}$的转移函数，向主工作带、输出带写入$k-1$个值，更改状态工作带上的状态；</li>
<li>移动输入带、主工作带、输出带，以模拟$\mathbb{M} _{\alpha}$中$k$个带头的移动。</li>
</ol>
<h3 id="主工作带的构造"><a href="#主工作带的构造" class="headerlink" title="主工作带的构造"></a>主工作带的构造</h3><p>要使主工作带上能存放被模拟图灵机的$k-2$条工作带及其带头的信息，较好的方法是将主工作带横向划分为$k-2$层，每层存放一条带子的信息，并让带子双向无限延伸（线性代价）、读写图对齐与0号位，如下图所示：</p>
<p><img src="/2023/09/12/ComputationalComplexityTimeComp/3.jpg" alt="3"></p>
<center style="font-size:12px;font-weight:bold">Fig. 3. Main working tape</center><br>

<p>此时，主工作带的读写头为虚拟读写头，始终指向0号位，读写头的移动“虚拟地”转变为了<strong>各层带子的移动</strong>。但是，<strong>实际上主工作带仍然只有一个带头，每次要获取被模拟图灵机的快照时，带头便一层一层地扫描，每扫描完一层，便回归到0号位，再扫描下一层，并记录本层的数据。</strong>最终，$k-2$层纸带的数据组成的$k-2$元组被特殊编码记录在主工作带的特定区域上。</p>
<blockquote>
<p>实际上的主工作带上只有对齐后的$k-2$元组的编码。</p>
</blockquote>
<h3 id="时间分析"><a href="#时间分析" class="headerlink" title="时间分析"></a>时间分析</h3><p>模拟$\mathbb{M}_\alpha$的一步计算时，主要的开销在主工作带的移动上，因此只用考虑主工作带移动的开销即可：$\mathbb{M}_\alpha$的时间函数是$T(n)$，因此$\mathbb{M}_\alpha$的工作带上可能出现的最长字符串量级为$O(T(n))$（即每一步都在记录新的数据），字符串、$k-2$元组在主工作带上的编码不会超过$c$（$c$为$|\alpha|$的多项式），$\mathbb{M} _\alpha$每移动一次带头，可能引起主工作带的带头在某一层上移动一整层，时间复杂度为$O(cT(n))$。</p>
<p>因此，$\mathbb{U}(\alpha,x)$模拟$\mathbb{M} _\alpha$的时间复杂度为$O(cT(n))*O(T(n))&#x3D;O(c T^2 (n))$</p>
<h2 id="Proof-of-Hennie-and-Stearns"><a href="#Proof-of-Hennie-and-Stearns" class="headerlink" title="Proof of Hennie and Stearns"></a>Proof of Hennie and Stearns</h2><p>Hennie和Stearns在1966年给出了$O(cT(n)\log T(n))$的证明。在他们的证明中，通用图灵机的构造基本不变，只不过增加了一条草稿工作带，并在主工作带中的有效字符间插入了新的缓冲符号$\boxtimes$，这些缓冲符号可以被其他有效符号覆盖，如下图所示：</p>
<p><img src="/2023/09/12/ComputationalComplexityTimeComp/4.png" alt="4"></p>
<center style="font-size:12px;font-weight:bold">Fig. 4. Main working tape</center><br>

<p>此外，还进一步地将无限长的双向带作了左右分区，每个分区$R _i$的范围为$[2 ^{i+1}-1,2 ^{i+2}-2]$，$L _i$的范围为$[-2 ^{i+2}+2, -2^{i+1}+1]$，长度均为$2 ^{i+1}$，$R_0$和$L_0$以0号为分界。并且规定：</p>
<ol>
<li>每个分区要么全是满（全是有效符号）的，要么全是空（全是缓冲符号）的，要么是半满的（有一般是有效符号）；</li>
<li>$R_i$和$L_i$的有效符号之和始终为$2^{i+1}$；</li>
<li>0号位上的符号只能是有效符号。</li>
</ol>
<p>假设初始时某层右半区的有效符号全部在$R _i$分区，$R_i$是全满的，那么自然地$L_0\sim L _{i-1}$也是全满的。若此时$\mathbb{M} _{\alpha}$进行了一步右移操作，则该层的纸带应“相对地左移”。此时，主工作带上的操作是：带头移动到$R_i$范围，在草稿带上记录下$2^i$个字符内容，分别覆盖掉$R _0,R _1 \sim R _{i-1}$上$1,1\sim 2 ^{i-1}$个缓冲字符，使得右半区全部变成半满。对于左半区，相似的操作，不过是将$L _0 \sim L _{i-1}$中的一半有效字符都放到$L _i$上，使得左半区变成全半满。这样便完成了整层左移（对应$\mathbb{M} _\alpha$右移）的操作。</p>
<h3 id="时间分析-1"><a href="#时间分析-1" class="headerlink" title="时间分析"></a>时间分析</h3><p>同样地，只用考虑主工作带上的移动情况。在主工作带上，一次移动，最远要跑到$R_i$或者$L_i$。而当跑过一次$R_i$或者$L_i$后，其他的分区都变成了半满，此时考虑最坏的情况，即$\mathbb{M}_\alpha$一直往一个方向移动，则在某一层上要把左或右分区$0\sim i-1$的有效符号搬空，至少需要$1+2+...+2 ^{i-1}&#x3D;2^i-1$步，再下一次移动才会涉及到$R_i$或者$L_i$。换句话说，两次最远距离的移动间隔至少为$2^i$步。</p>
<p>$\mathbb{M}_\alpha$至多有$T(n)$步，$\mathbb{U}(\alpha,x)$对最长区间的操作不超过$T&#x2F;2^i$次，操作最长区间时耗时$O(c2^i)$，而区间的总个数$i$不会超过$\log T(n)$，故时间复杂度为：</p>
<p>$$<br>\sum\limits _{i&#x3D;1} ^{\log T(n)}\frac{T(n)}{2^i}O(c2^i)&#x3D;cT(n)\sum\limits _{i&#x3D;1} ^{\log T(n)}O(1)&#x3D;cT(n)\log T(n)<br>$$</p>
<blockquote>
<p>缓冲字符的作用在于，它使得每层纸带上的内容可以通过移动而聚集在带头，即0号位附近，使得单步下的局部扫描成为可能，不像之前那样每次可能都要扫描一整层。</p>
</blockquote>
<h2 id="Oblivious-Turing-Machine-Theorem：遗忘图灵机理论"><a href="#Oblivious-Turing-Machine-Theorem：遗忘图灵机理论" class="headerlink" title="Oblivious Turing Machine Theorem：遗忘图灵机理论"></a>Oblivious Turing Machine Theorem：遗忘图灵机理论</h2><p>该理论的内容为：<em>假设语言L由一个在$O(T(n))$时间内运行的$T(n)$时间可构造的TM $\mathbb{M}$计算，那么存在一个遗忘图灵机$\mathbb{U}$，在$O(T(n) \log T(n))$时间内判定L</em>。</p>
<p>上述的遗忘图灵机可以采用通用图灵机来构造。通用图灵机在划分区间$L_i$和$R_i$时，是按需划分的，也就是说它扫描到哪划分到哪，因为它并不知道要计算的函数的时间复杂度是多少，也就无法预先分配好足够的区间来完成函数的计算。而对于上述理论，由于$T(n)$是时间可构造的，所以：</p>
<ul>
<li>该通用图灵机可以先在$O(\log T(n))$的时间内计算出$logT(n)$（$T(n)$时间可构造则$\log T(n)$时间可构造），完成区间的划分。</li>
</ul>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/260217775">计算复杂性（2）——运行时间与效率</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/260512272">计算复杂性（3）——计算机的套娃：通用图灵机</a></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zclzcl0223.github.io/2023/09/12/ComputationalComplexityTimeComp/" data-id="clzik0lc1000zv47k6g30117r" data-title="Computational Complexity: Time Complexity 1" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Math/" rel="tag">Math</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Theoretical-Computer-Science/" rel="tag">Theoretical Computer Science</a></li></ul>

    </footer>
  </div>
  
</article>



  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/">&laquo; Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/10/">10</a><a class="extend next" rel="next" href="/page/3/">Next &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Advanced-Model/">Advanced Model</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/CS7313-Computational-Complexity/">CS7313: Computational Complexity</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Configuration/">Configuration</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Dive-Into-Deep-Learning/">Dive Into Deep Learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/GNN/">GNN</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Kernel-Method/">Kernel Method</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/MATH6005-Matrix-Theory/">MATH6005: Matrix Theory</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning-by-AndrewNg/">Machine Learning by AndrewNg</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Math/">Math</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Paper/">Paper</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Programming-Language/">Programming Language</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Tool/">Tool</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Attention-Mechanism/" rel="tag">Attention Mechanism</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CNN/" rel="tag">CNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Configuration/" rel="tag">Configuration</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Data-Analysis/" rel="tag">Data Analysis</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Dataset-Distillation/" rel="tag">Dataset Distillation</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Decision-Trees/" rel="tag">Decision Trees</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Deep-Learning/" rel="tag">Deep Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GCN/" rel="tag">GCN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GNN/" rel="tag">GNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Generative-AI/" rel="tag">Generative AI</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hexo/" rel="tag">Hexo</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Lab/" rel="tag">Lab</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux/" rel="tag">Linux</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-Learning/" rel="tag">Machine Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Markup-Language/" rel="tag">Markup Language</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Math/" rel="tag">Math</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Paper/" rel="tag">Paper</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/" rel="tag">Python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RNN/" rel="tag">RNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Recommender-System/" rel="tag">Recommender System</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Reinforcement-Learning/" rel="tag">Reinforcement Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SVM/" rel="tag">SVM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Supervised-Learning/" rel="tag">Supervised Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Theoretical-Computer-Science/" rel="tag">Theoretical Computer Science</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Tool/" rel="tag">Tool</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Unsupervised-Learning/" rel="tag">Unsupervised Learning</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Attention-Mechanism/" style="font-size: 11.82px;">Attention Mechanism</a> <a href="/tags/CNN/" style="font-size: 12.73px;">CNN</a> <a href="/tags/Configuration/" style="font-size: 10px;">Configuration</a> <a href="/tags/Data-Analysis/" style="font-size: 10px;">Data Analysis</a> <a href="/tags/Dataset-Distillation/" style="font-size: 13.64px;">Dataset Distillation</a> <a href="/tags/Decision-Trees/" style="font-size: 10px;">Decision Trees</a> <a href="/tags/Deep-Learning/" style="font-size: 20px;">Deep Learning</a> <a href="/tags/GCN/" style="font-size: 11.82px;">GCN</a> <a href="/tags/GNN/" style="font-size: 15.45px;">GNN</a> <a href="/tags/Generative-AI/" style="font-size: 10.91px;">Generative AI</a> <a href="/tags/Hexo/" style="font-size: 10.91px;">Hexo</a> <a href="/tags/Lab/" style="font-size: 10.91px;">Lab</a> <a href="/tags/Linux/" style="font-size: 16.36px;">Linux</a> <a href="/tags/Machine-Learning/" style="font-size: 17.27px;">Machine Learning</a> <a href="/tags/Markup-Language/" style="font-size: 10px;">Markup Language</a> <a href="/tags/Math/" style="font-size: 19.09px;">Math</a> <a href="/tags/Paper/" style="font-size: 14.55px;">Paper</a> <a href="/tags/Python/" style="font-size: 18.18px;">Python</a> <a href="/tags/RNN/" style="font-size: 10.91px;">RNN</a> <a href="/tags/Recommender-System/" style="font-size: 10.91px;">Recommender System</a> <a href="/tags/Reinforcement-Learning/" style="font-size: 10.91px;">Reinforcement Learning</a> <a href="/tags/SVM/" style="font-size: 10px;">SVM</a> <a href="/tags/Supervised-Learning/" style="font-size: 11.82px;">Supervised Learning</a> <a href="/tags/Theoretical-Computer-Science/" style="font-size: 15.45px;">Theoretical Computer Science</a> <a href="/tags/Tool/" style="font-size: 18.18px;">Tool</a> <a href="/tags/Unsupervised-Learning/" style="font-size: 11.82px;">Unsupervised Learning</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/06/">June 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/01/">January 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/12/">December 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/06/">June 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/05/">May 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/04/">April 2023</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2024/06/17/Diffusion/">Diffusion</a>
          </li>
        
          <li>
            <a href="/2024/01/20/VAE/">VAE</a>
          </li>
        
          <li>
            <a href="/2023/12/02/CountingComplexity/">Computational Complexity: Complexity of Counting</a>
          </li>
        
          <li>
            <a href="/2023/11/24/MatrixTheory5/">MatrixTheory: 特殊矩阵与矩阵分解</a>
          </li>
        
          <li>
            <a href="/2023/11/13/RandomizedComputation/">Computational Complexity: Randomized Computation</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 ChaosTsang<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/" class="mobile-nav-link">home</a>
  
    <a href="/about/" class="mobile-nav-link">about</a>
  
    <a href="/tags/" class="mobile-nav-link">tags</a>
  
    <a href="/categories/" class="mobile-nav-link">categories</a>
  
    <a href="/archives/" class="mobile-nav-link">archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>





<script src="/js/script.js"></script>





  </div>
</body>
</html>